# Task 5：AI Books—stage 2

## 任务描述  

* 选择Task5(AIbooks)，第二阶段选择的书为Artificial Intelligence Machine Learning and Deep Learning  
* 第二阶段翻译的部分为本书4、5章
  * 深度学习概论  
  * 深度学习：RNN和LSTM模型 

---

## 一、深度学习概论（报告人：李逸博）

&emsp;&emsp;本章第一部分介绍了深度学习的概念、深度学习可以解决的问题。第二部分讲了神经网络的基本概念和构建，同时还介绍了几个基本的神经网络，包括ANN、MLP。第三部分详细介绍了卷积神经网络(CNN)，之后还给出了一个使用MNIST数据及训练基于keras的CNN示例。  


## 1.深度学习相关概念  

### 1.1 深度学习的定义

* 用于研究神经网络、训练神经网络。 
* 需要神经网络中至少有两个隐藏层
* 深度学习是学习样本数据的内在规律和表示层次，这些学习过程中获得的信息对诸如文字，图像和声音等数据的解释有很大的帮助。  

### 1.2 深度学习可以解决的问题  

&emsp;&emsp;因为反向传播涉及到更新边之间的权重，在权重更新的过程中，会产生梯度消失和梯度爆炸的问题。深度学习可以解决这些问题（可以通过LSTM实现）。深度学习模型用ReLU函数替代sigmoid激活函数。  

### 1.3 深度学习中的挑战  

- 算法中的偏差（包括数据中心的偏差和运算过程中的偏差）  
- 缺乏可解释性
- 概括能力有限
- 结果为相关关系，无法表达因果关系  

## 2.深度学习模型

### 2.1 感知器  

&emsp;&emsp;感知器使用特征向量来表示的前馈神经网络，是一种二元分类器，把矩阵上的输入映射到输出值$f(x)$（一个二元的值）。  
$$
f(x)=
\begin{cases}
    1& w*x+b>0\\
    0&else
\end{cases}
$$
&emsp;&emsp;$x$是输入向量,$w$是实数的表式权重的向量，$w*x$是点积。$b$是偏置，一个不依赖于任何输入值的常数。偏置可以认为是激励函数的偏移量，或者给神经元一个基础活跃等级。$f(x)$(0 或 1)用于对进行分类，看它是肯定的还是否定的，这属于二元分类问题。

### 2.2 深度学习中的超参数   

&emsp;&emsp;超参数包括隐藏层数量、隐藏层神经元数量、边的权重、激活函数、损失函数、优化器、学习率、丢弃率。 

- 初始化模型的超参数  
  &emsp;&emsp;隐藏层数量、隐藏层神经元数量和边的权重是初始化模型中必要的部分。隐藏层是中间的计算层，神经元的数量可以自己决定，神经元之间的连接也可以决定。连接神经元之间的边具有权重，权重的初始值通常介于0，,之间，这个权值会在训练过程中进行调整。  
- 激活函数  
  &emsp;&emsp;激活函数应用于每对连接层之间的权重，通过计算改变神经元的输出。具有许多层的神经网络通常包含不同的激活函数。  
- 损失函数  
  &emsp;&emsp;损失函数、优化器、学习率都是在反向误差传播的过程中使用的。常见的损失函数有MSE函数、交叉熵函数等。  
- 优化器  
  &emsp;&emsp;优化器是结合损失函数选择的算法，目的是在训练阶段收敛到损失函数的最小值。常见的优化器包括：SGD、Adagrad、Adam等。  
- 学习率  
  &emsp;&emsp;学习率是一个很小的数字，通常在0.001和0.05之间，在使损失函数取最小值得过程中，可以使得越靠近最低点，降低的速度越慢。  
- 丢弃率  
  &emsp;&emsp;是一个介于0和1之间的数，确定的是前向传播的过程中丢弃的神经元的比例。例如：如果丢失率为0.2，则在前向传播的每个步骤中随机选择20%的神经元忽略不计。  

### 2.3 反向误差传播  

&emsp;&emsp;反向传播的神经网络是由一个输入层、一个输出层和一个或多个隐层构成的，它的激活函数采用$sigmoid$函数。在这其中，输入信号经输入层输入，通过隐层计算由输出层输出，输出值与标记值比较，若有误差，将误差反向由输出层向输入层传播，在这个过程中，利用梯度下降算法对神经元权值进行调整。  

## 3. 多层感知器（MLP）  

- MLP实际上就是人工神经网络。除了输入输出层，它中间可以有多个隐层，最简单的MLP只含一个隐层，即三层的结构。多层感知机层与层之间是全连接的（全连接的意思就是：上一层的任何一个神经元与下一层的所有神经元都有连接）。  

- 输入层：比如输入是一个$n$维向量，就有$ n $个神经元。  

- 隐藏层的神经元，与输入层是全连接的，假设输入层用向量$X$表示，则隐藏层的输出就是
  $$
  f(W_1X+b_1)
  $$
  ，$W_1$是权重（也叫连接系数），$b_1$是偏置，函数$f$可以是常用的$sigmoid$函数或者$tanh$函数。  

- 输出层，隐藏层到输出层可以看成是一个多类别的逻辑回归，也即$softmax$回归，所以输出层的输出就是$softmax(W_2X_1+b_2)$，$X_1$表示隐藏层的输出$f(W_1X+b_1)$。  

## 4. 卷积神经网络  

&emsp;&emsp;卷积神经网络是一种多层神经网络，擅长处理图像特别是大图像的相关机器学习问题。  
&emsp;&emsp;卷积网络通过一系列方法，成功将数据量庞大的图像识别问题不断降维，最终使其能够被训练。  
&emsp;&emsp;卷积神经网络的层级结构包括：  

- 卷积层  
- 池化层  
- 全连接层    
  卷积层与池化层配合，组成多个卷积组，逐层提取特征，最终通过若干个全连接层完成分类。  
  CNN通过卷积来模拟特征区分，并且通过卷积的权值共享及池化，来降低网络参数的数量级，最后通过传统神经网络完成分类等任务。  

### 4.1 卷积层(Conv2D)  

- 有两个关键操作：  
  - 局部关联。每个神经元看做一个滤波器(filter)
  - 窗口(receptive field)滑动， filter对局部数据计算  
- 卷积核大小（kernel size）  
  &emsp;&emsp;决定一个点的输出值与多大的范围的输入有关->感受野（respective filed）通常是奇数，因为奇数大小的核，才能保证中心点四周的长度是相等的。通常卷积核越大效果越好，但增大卷积核伴随着参数个数平方增长。
- 填充（padding）  
  目的是不减小feature map的大小  
  常见策略：周边补0；向外对称。
- 步长（stride）  
  用于降低feature maps的尺寸  

### 4.2 ReLU激活函数  

&emsp;&emsp;在创建每个特征图之后，特征图上的某些值可能是负值。ReLU激活函数的功能是将这些负值替换为0。ReLU的定义如下：
$$
ReLU(x)=
\begin{cases}
    x& x>=0\\
    0& x<0
\end{cases}
$$

### 4.3 池化层(pooling)

&emsp;&emsp;即使做完了卷积，图像仍然很大（因为卷积核比较小），所以为了降低数据维度，就进行采样。  
&emsp;&emsp;即使减少了许多数据，特征的统计属性仍能够描述图像，而且由于降低了数据维度，有效地避免了过拟合。  
&emsp;&emsp;在实际应用中，分为最大值采样（Max-Pooling）与平均值采样（Mean-Pooling）。  
&emsp;&emsp;eg:原始图片是20x20的，对其进行采样，采样窗口为10x10，最终将其采样成为一个2x2大小的特征图。  

### 4.4 Dropout

- 训练时，每次随机去掉全连接层中的一些神经元
- 防止或减轻过拟合

### 4.5 Batch normalization

- 对输入数据进行减均值，除方差的操作
- 加快训练速度  

## 5.总结 

&emsp;&emsp;第四章深度学习概论主要讲了相关概念、神经网络的基本结构并介绍了具体的神经网络。  
&emsp;&emsp;在翻译的过程中，我对深度学习有了更清晰的认识，对神经网络的基本结构和构建方法有了了解。通过书中利用keras在MNIST数据集上训练神经网络模型的具体示例，我也学到了如何在代码层面实现一个具体的神经网络。

## 6.规划  

&emsp;&emsp;我们已经了解了机器学习、深度学习等基本知识，第三阶段我们选择翻译的书是*Unsupervised Learning in Space and Time*,阅读并翻译前三章的部分。



## 二、 深度学习：循环神经网络RNN和长短期记忆网络LSTM（汇报人：武婧颖）

&ensp;&ensp;&ensp;&ensp;本章是对第四章内容的扩充，介绍了循环神经网络RNN、长短期记忆网络LSTM、门控循环单元GRU、自编码器AE、生成式对抗网络GAN，以及它们的简单实现。阅读本章，能够对深度学习的神经网络有更加深刻的理解，并通过代码示例学会构建简单的神经网络模型。

### 1. 什么是RNN？

#### RNN简介

循环神经网络，适用于处理包含顺序数据的数据集

#### 主要特点

有状态性、反馈机制、激活函数、多门、BPTT等

#### RNN在时间周期t内的基本关系

$$
h(t)=f(W*x(t)+U*h(t-1))
$$

<!--x(t)表示输入序列，h(t)表示隐藏的状态序列，W和U是权重矩阵，f通常是tanh激活函数-->

#### RNN的反馈机制

将前一时间段的输出和当前时间段的新的输入相结合，来计算新的内部状态

#### BPTT

延时间反向传播算法，与CNN的反向传播对等。在训练神经网络时，在BPTT中更新RNN的权重矩阵

#### RNN的潜在问题

梯度爆炸

处理方法：使用截断BPTT或使用LSTM；指定梯度的最大值

### 2. 使用不同工具构建RNN

#### 用Keras构建RNN

使用tensorflow的keras模块，构建简单的RNN模型

重要参数：时间步长数，每个输入向量的维度，RNN神经元中的隐藏单元数，数据集中的类数等

使用了SimpleRNN()类

#### 在MNIST数据集上使用Keras构建的RNN

首先使用keras构建简单的RNN模型，然后将MNIST数据集分割为训练集和测试集，使用fit()函数训练模型，测试损失和精确度

在代码示例中，精确度达到95.8%，可见RNN的有效性

#### 用TensorFlow构建RNN

可以观察到RNN神经元中每个隐藏层的输入和状态的底层细节

要点：初始化时间步长数、输入数、神经元数；用一维整型向量来表示出现在纯零值向量左侧的时间步长数

### 3. 什么是LSTM？

#### LSTM简介

RNN的一种特殊类型，，尤其适合处理长期依赖性问题

在自然语言处理、语音识别和手写识别等任务上表现出色

#### LSTM组成

三门：遗忘门forge gate，输入门input gate，输出门output gate

两函数：sigmoid激活函数，tanh激活函数

记忆：时间步长t内为短期记忆，内部单元保持长期记忆

输入：基于向量h(t-1)和x(t)的组合

公式：

1. t时刻的forget gate公式：

$$
f(t)=sigma(W(f)*x(t)+U(f)*h(t)+b(f))
$$

<!--W(f)：与x(t)相关的权重矩阵，U(f)：与h(t)相关的权重矩阵，b(f)：forget gate的偏置向量-->

t时刻的input gate公式i(t)、t时刻的output gate公式o(t)与f(t)的计算模式相同，但它们都有对应的权重矩阵。

2. c(t)、i(t)和h(t)的计算公式：
   $$
   c(t)=f(t)*c(t-1)+i(t)*tanh(c'(t))
   $$

   $$
   c'(t)=sigma(W(c)*x(t)+U(c)*h(t-1))
   $$

   $$
   h(t)=o(t)*tanh(c(t))
   $$

LSTM的最终状态：一个一维向量，包含所有其它层的输入。若模型含有多个LSTM，则当前LSTM的最终状态将成为下一个LSTM的输入

#### 双向LSTM

组成：两个常规的LSTM，一个向前，一个向后

实例：ELMo（使用双向LSTM的NLP任务的深层单词表示）、transformer（NLP中的新架构，用于BERT，来解决复杂的NLP问题）

#### LSTM超参数调优

原因：LSTM容易出现过拟合

调参注意事项：更大的网络更易过拟合，更多的数据、多轮训练、使用堆叠层、使用softsign、RMSprop、AdaGrad有助于减少过拟合，泽维尔权重初始化

### 4. 使用不同工具构建LSTM

#### 用TensofFlow构建LSTM

### 5. 什么是GRU？

#### GRU简介

门控循环单元

是一种RNN，是LSTM的简化类型

能够有效跟踪长期依赖关系

解决了梯度消失和梯度爆炸问题

#### GRU组成

两门：复位门reset gate、更新门update gate

reset gate执行LSTM中input gate和forget gate的功能

### 6. 什么是自动编码器AE？

#### AE简介

一种类似于MLP（多层神经网络）的神经网络

输出层与输入层相同

#### AE组成

最简单的AE包含一个隐藏层，其神经元数量小于输入层和输出层

存在包含多个隐藏层的AE类型，其隐藏层包含的神经元数量或多或少

#### AE机制与功能

使用无监督学习和反向传播来学习有效的数据编码，目的是降维

将输入值设置为等于输入，然后尝试找到恒等式

本质：将输入压缩成比输入数据维数更少的中间向量，然后将其转换成与输入形状相同的张量

#### AE应用

文档检索、分类、异常检测、对立AE、图像去噪

#### AE与PCA

PCA：主成分分析法

AE可以用于特征提取，获得比PCA（出成分分析法）更好的结果

如果AE涉及线性激活或仅涉及单个sigmoid隐藏层，则其最佳解决方案与PCA密切相关

#### VAE

可变自动编码器，一种增强的AE

左侧为编码器，右侧为解码器

输入与输出：

1. 编码器的输入为数值向量x，输出具有权重和偏移量的隐藏表示z
2. 解码器的输入a为编码器的输出，输出为数据概率分布的参数，也具有权重和偏移量

### 7. 什么是生成式对抗网络GAN？

#### GAN简介

生成式对抗网络

最初目的：产生合成数据，通常用于扩充小数据集或不平衡数据集

#### GAN的邪恶用途

通过改变像素值，从有效图像生成伪造图像，以欺骗神经网络

数据集包含两种类型的相关性：与数据集数据相关的模式，以及数据集数据中不可一般化的模式。GAN利用后者来欺骗图像识别系统

#### 能否抵御GAN攻击

没有长远的解决办法

目前只有具有短暂有效性的防御技术

### 8.制作GAN

#### 两个主要部分

生成器：类似CNN的结构，用来产生图像

鉴别器：类似CNN的结构，用来检测发生器提供的图像的真假

#### 训练机制

已经初始化的生成器将假图像发送给已经被训练但不再可更新的鉴别器进行分析

如果鉴别器在检测图像真实性方面准确度很高，则生成器需要被修改，以提高其产生的假图像的质量

对生成器的修改通过反向误差传播实现

如果鉴别器的性能很差，则说明生成器正在生成高质量的假图像，因此生成器不需要大的修改

#### GAN的创建步骤

1. 选择数据集
2. 定义和训练鉴别器模型
3. 定义和训练生成器模型
4. 训练生成器模型
5. 评估GAN模型的性能
6. 使用最终的生成器模型

#### VAE-GAN模型

VAE和GAN的混合体

### 9. 小结

&ensp;&ensp;&ensp;&ensp;本章介绍了深度学习中的两种神经网络模型——循环神经网络RNN和长短期记忆网络LSTM，并添加了一些相关的其他网络的介绍。学习本章后，我对RNN和LSTM有了一定的认识，学会了用Python构建简单的神经网络模型。

&ensp;&ensp;&ensp;&ensp;由于本章的内容较为精简，读完之后我对RNN的认识仍然只停留在浅层阶段，因此下一阶段的计划是：从其他的基本书籍中，选择深度学习方面的章节进行翻译和学习，以期实现对深度学习的较为全面的认识和一定水平的应用能力。





