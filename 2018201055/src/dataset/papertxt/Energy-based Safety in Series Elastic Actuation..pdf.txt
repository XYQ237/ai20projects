2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
DOB-Net: Actively Rejecting Unknown Excessive Time-Varying
Disturbances
Tianming Wang1, Wenjie Lu1,2, Zheng Yan3 and Dikai Liu1
Abstract‚ÄîThis paper presents an observer-integrated Rein- DOB-Net
forcement Learning (RL) approach, called Disturbance OB-
Disturbance 
server Network (DOB-Net), for robots operating in environ-
Dynamics  Controller
ments where disturbances are unknown and time-varying, Observer disturbance 
behaviour 
and may frequently exceed robot control capabilities. The embedding
DOB-Net integrates a disturbance dynamics observer network action
and a controller network. Originated from conventional DOB AUV Dynamics Environment
mechanisms, the observer is built and enhanced via Recurrent
Neural Networks (RNNs), encoding estimation of past values
state Disturbances
and prediction of future values of unknown disturbances in ùëë
ùë¢
RNNhiddenstate.Suchencodingallowsthecontrollergenerate ùë°
optimal control signals to actively reject disturbances, under ùë¢
the constraints of robot control capabilities. The observer and
Fig.1. WorkingÔ¨ÇowoftheDOB-Net(uanduarecontrollimits).
the controller are jointly learned within policy optimization
by advantage actor critic. Numerical simulations on position time-varying characteristics of the wave and current distur-
regulationtaskshavedemonstratedthattheproposedDOB-Net
bances, if future disturbances can be predicted, RL may be
signiÔ¨Åcantlyoutperformsconventionalfeedbackcontrollersand
able to generate optimal controls.
classical RL policy.
Conventional DOB [6] and Disturbance-Observer-Based
I. INTRODUCTION Control (DOBC) [7], [8] have been investigated in the last
four decades. The main objective is to estimate the distur-
Autonomous Underwater Vehicles (AUVs) have become
bances, then produce controls to compensate their inÔ¨Çuence.
vital tools in search and rescue, exploration, surveillance,
However, conventional DOBC does have some limitations
monitoring, and other applications [1], [2]. For large AUVs
whenmeetingsuchexcessivetime-varyingdisturbances.The
in deep water applications, the strength and changes of
Ô¨Årst limitation is that DOB normally needs an accurate
underwater wave and current disturbances may be neg-
system model, which may not be available for underwater
ligible to the AUVs, due to their considerable size and
vehicles due to complex hydrodynamics. In this case, model
thrust capabilities. While small AUVs are required for some
uncertaintiesarelumpedwithexternaldisturbances,andthen
shallow water applications, like bridge pile inspection [3],
estimated by DOB together. Thus, the original properties
wherethedisturbancescomingfromtheturbulentÔ¨Çowsmay
for some disturbances, such as harmonic ones, are changed.
frequentlyexceedAUVs‚Äôthrustcapabilities.Theseunknown
Secondly, even with an sufÔ¨Åciently accurate estimate of
disturbances inevitably bring adverse effects and may even
disturbances at the current time step, the optimal control
destabilize robots [4]. Thus, this paper studies an optimal
solution is still unreachable, since disturbances exceeding
control problem of robots subject to excessive time-varying
controlconstraintscannotbewellrejectedonlythroughfeed-
disturbances, which are in the form of forces, and presents
back regulation. Thus, AUV behaviors need to be optimized
an observer-integrated RL solution.
over a future time horizon considering time correlation of
RL [5] is a trial-and-error method that does not require
disturbances.
an explicit system model, and can naturally adapt to noises
ThispaperproposesanovelRLapproachcalledDOB-Net,
and uncertainties in the real system. However, the excessive
which enables integrated learning of disturbance dynamics
disturbances are not appropriate to be regarded as noises
and an optimal controller, for water wave and current dis-
any more, since AUV‚Äôs state transition is heavily affected
turbance rejection control of AUVs in shallow and turbulent
by the external disturbances, thus violating the assumption
water, as shown in Fig 1. The DOB-Net consists of a distur-
of Markov Decision Process (MDP).While considering the
bance dynamics observer network and a controller network.
1Tianming Wang, Wenjie Lu and Dikai Liu are with Centre The observer network is built and enhanced via RNNs,
for Autonomous Systems, University of Technology Sydney, Ultimo, through imitating conventional DOB mechanisms. But this
NSW2007,Australiatianming.wang@student.uts.edu.au network is more Ô¨Çexible, since it encodes the prediction of
{wenjie.lu,dikai.liu}@uts.edu.au
disturbancesinRNNhiddenstate,insteadofonlyestimating
2WenjieLuiswithSchoolofMechanicalEngineeringandAutomation,
HarbinInstituteofTechnology(Shenzhen),Shenzhen,518055Guangdong, the current value of disturbances. Also, the observer is more
P.R.Chinawenjie.lu@outlook.com robusttomodeluncertaintiesandtime-varyingcharacteristics
3Zheng Yan is with Centre for ArtiÔ¨Åcial Intelligence,
of disturbances. Based on the encoded disturbance predic-
University of Technology Sydney, Ultimo, NSW 2007, Australia
yan.zheng@uts.edu.au tion, the controller network is able to actively reject the
978-1-7281-7395-5/20/$31.00 ¬©2020 IEEE 1881
Authorized licensed use limited to: Carleton University. Downloaded on September 22,2020 at 13:35:28 UTC from IEEE Xplore.  Restrictions apply. unknown disturbances. The observer and the controller are the wave and current disturbances should be regarded as
jointlylearnedwithinpolicyoptimizationbyadvantageactor functions of time instead of random noises, due to its large
critic. This integrated learning may achieve an optimized amplitudes and time-varying characteristics, as discussed in
representationofobserveroutputs,comparedwithtraditional Section V.
hand-designedfeatures.Thepolicyistrainedusingsimulated
C. History Window Approach
disturbances consisting of multiple sinusoidal waves, and
evaluated using both simulated disturbances and collected When using RL to deal with unknown disturbances, the
disturbances, the latter is collected in a water tank with problem cannot be deÔ¨Åned as a MDP since the transition
artiÔ¨Åcial waves. function does not only depend on the current state and
In this paper, the related work is presented in Section action, but also heavily on the disturbances. The history
II. Section III introduces problem formulation. Section IV window approaches [20] attempt to resolve the hidden state
provides the detailed description of the DOB-Net. Then, bymakingtheselectedactiondependnotonlyonthecurrent
SectionVpresentsvalidationproceduresandresultanalysis. state, but also on a Ô¨Åxed number of the most recent states
Somepotentialfutureimprovementsarediscussedinthelast andactions.Wangetal.[21]appliedthisapproachtohandle
section. the external disturbances of an AUV through characterizing
thedisturbedAUVdynamicsmodelasamulti-orderMarkov
II. RELATEDWORK
chain xt+1 = fh(Ht,xt,ut), and assuming the unobserved
A. Feedback and Predictive Control time-varying disturbances and their prediction over next
In the early development of disturbance rejection con- planning horizon are encoded in state-action history of Ô¨Åxed
{ ¬∑¬∑¬∑ }
trol, feedback control strategies are used to suppress the length Ht = xt‚àíN,ut‚àíN, ,xt‚àí1,ut‚àí1 . Then, the policy is
unknown disturbances [9]‚Äì[11]. Then, disturbance estima- trainedtogeneratecontrolsignalsbasedonaÔ¨Åxedlengthof
tion and attenuation methods through adding a feedforward state-action history along with current state. However, it is
compensation term have been proposed and practiced, such difÔ¨Åculttodetermineanoptimallengthofthehistory.Shorter
as DOBC [6], [7]. However, these methods are built on the history may not provide sufÔ¨Åcient information about distur-
assumption of bounded disturbances which should be small bances, longer history will make the training difÔ¨Åcult. Wang
enough, thus fail to guarantee stability under disturbances et al. [21] considered the history length as a hyperparameter
exceeding control constraints [12]. that was statistically optimized during training.
To this end, Model Predictive Control (MPC) [13] is
D. Recurrent Policy
often applied due to its constraint handling capacity through
DuetothedifÔ¨Åcultyindeterminingoptimalhistorylength
optimizing plant behaviour over a certain time horizon [12].
through history window approach, RNN is then utilized to
The MPC requires a prediction model of the system to
automatically learn how much past experience should be
optimize future behaviour, this model includes not only the
explored to achieve optimal performance. Using RNN to
robot dynamics, but also the predicted disturbances over
represent policies is a popular approach to handle partial
nextoptimizationhorizon.Thus,acompoundcontrolscheme
observability and solve tasks that require memory [22]‚Äì
consisting of DOB and MPC (DOB-MPC) [14] has been
[24]. The idea being that the RNN will be able to retain
developed. However, its performance heavily relies on the
information from observations and actions further back in
accuracy of given system model, and the requirement for
time, and incorporate this information into predicting better
online optimization at each time step leads to a low com-
actions and value functions, thus performing better on tasks
putational efÔ¨Åciency. Besides, such separated modeling and
thatrequirelongtermplanning.Comparedwiththerecurrent
control optimization process might not be able to produce
RL approach, the contributions of DOB-Net lie in the ex-
models and controls that jointly optimize robot behaviours,
ploration and application of the architectural similarities be-
as evidenced in [15]. In contrast, the DOB-Net uses neural
tween Gated Recurrent Unit (GRU) and conventional DOB,
networks to construct both the observer and the controller,
and then the ability to encode the prediction of disturbance
achieving model-free control, high computational efÔ¨Åciency
dynamics function.
as well as a joint optimization of the observer and the
controller.
III. PROBLEMFORMULATION
B. Classical RL A. System Description
RL has drawn a lot of attention in Ô¨Ånding optimal con- In this research, we assume that a 6 Degree Of Freedom
trollers for systems that are difÔ¨Åcult to model accurately. (DOF) AUV is designed to be sufÔ¨Åciently stable in orien-
Recently, deep RL algorithms based on Q-learning [16], tation even under strong disturbances, thanks to its large
policygradients[17],andactor-criticmethods[18]havebeen restoring forces. Thus, we only consider the disturbance
shown to learn complex skills in high-dimensional state and rejectioncontrolofthevehicle‚Äôs3-DOFposition,andassume
action spaces. RL generally considers stochastic systems of its orientation is well controlled all the time. However, the
tchoentfroorlmsxigt+n1al=uf‚àà(xRt,Kut)a+ndŒµi[.1i.9d].,swyistthemsta(cid:0)tneovisaeriambalergs(cid:1)ixn‚ààaliRzeDd, flarargmeerwnoertkwocarkn banedealsoinlygeerxtteranidneindgtotim6-eDOmFaycabsee,rweqhueirreeda.
‚àº
overtimeŒµ N (0,E),whereE=diag œÉ2,...,œÉ2 .While And it is also applicable for other kinds of mobile robots,
1 D
1882
Authorized licensed use limited to: Carleton University. Downloaded on September 22,2020 at 13:35:28 UTC from IEEE Xplore.  Restrictions apply. such as quadrotors [25], gliders, and surface vessels, but the networkisthenbuiltuponthisencodinginordertogenerate
effectiveness needs further investigation. optimal controls.
TheAUVmodelcanbeconsideredasaÔ¨Çoatingrigidbody
A. Conventional DOB
with external disturbances, which can be represented by
The basic idea of conventional DOB is to estimate dis-
M(q)q¬®+G(q,qÀô)=u+d(t),
(1) turbances based on robot state and executed controls, its
G(q,qÀô)=C(q,qÀô)qÀô+D(q,qÀô)qÀô+g(q),
formulation is proposed as
wtthhheeemmreaattMrriixx(qoo)ff‚ààCdroRarg3io√óflo3isrciasen,tdhge(cqei)nn‚ààterriRptiea3taimsltatehtrremixvs,e,CcDt(o(qrq,o,qÀôqfÀô))t‚ààh‚ààeRRg3r3√óa√óv33itiiyss ydÀôÀÜ==‚àíy+L(pq(,qqÀô,)qyÀô)+, L(q,qÀô)(G(q,qÀô)‚àíp(q,qÀô)‚àíu), (2)
and buoyancy forces, q,qÀô,q¬® ‚àà R3 represent replacements, where dÀÜ‚àà R3 is the estimated disturbances, y ‚àà R3 is
velocities and accelerations of the AUV, u‚ààR3 represents the internal state of the nonlinear observer, p(q,qÀô) is the
the control forces. d(t) ‚àà R3 represents the time-varying nonlinear function to be designed, and L(q,qÀô) is the DOB
disturbance forces, and the variation of d(t) with time from gain. It has been shown in [6] that DOB is globally asymp-
{ }
the past to the future is the disturbance dynamics, which is toticallystablebychoosingL(q,qÀô)=diag c,c ,wherec>0.
whattheobservernetworktriestoproduce.Wealsoassumes The convergence and the performance of the DOB have
thatthemagnitudesofthedisturbanceswillexceedtheAUV been established for slowly time-varying disturbances and
control constraints u‚ààR3 and u‚ààR3, but are constrained disturbances with bounded rate in [27]. A discrete version
within reasonable ranges, ensuring the controller is able to of DOB is also provided (illustrated in Fig. 2(a))
stabilize the AUV in a sufÔ¨Åciently small region around the ‚àí ‚àí
target position. The whole AUV dynamic model is assumed yÀút =G(‚àíxt) p(xt) ut‚àí1,
tohaveÔ¨Åxedparameters,whichareunknownforthelearning yt =(1 L(xt)dt)yt‚àí1+L(xt)dtyÀút, (3)
algorithm or the controller. dÀÜt‚àí1=yt‚àí1+p(xt).
B. Gated Recurrent Unit (GRU)
B. Problem DeÔ¨Ånition
The architecture of GRU [28] is shown in Fig. 2(b). The
InRL,thegoalistolearnapolicythatchoosesactionsu
t formulations are given below:
ateachtimestept inresponsetothecurrentstatex,suchthat
t
the total expected sum of discounted rewards is maximized zt =œÉ(Wz[ht‚àí1,st]+bz),
owTvheeelrlaaacsltlitohtinemcieno.crlrTuehdsepeossnttadhtieengcoofvnetthlrooeclirtfoioebrsoctexsc=oun[‚ààqsTisUtqÀôsT‚àào]TfR‚ààp3o.XsAitit‚ààoenRaca6hs. rhhÀúttt===œÉ(ta1(n‚àíWhr(zW[th)ht‚ó¶‚àí[rh1tt,‚àí‚ó¶st1h]+t+‚àí1zbt,rs‚ó¶)t,]hÀú+t,bh), (4)
time step, the system transitions from x to x in response
t t+1 where s is the input vector, h is the output vector, z is
to the chosen action u and the transition dynamics function t t t
f :X √óU ‚ÜíX,colltectingarewardr accordingtoreward the update gate vector, rt is the reset gate vector, W and b
fRun‚ààctRio3n√ó3r(rxetp,ruets)en=t xwtTeQigxhtt+muatTtRriucte,s.wtThhereedQisc‚ààouRnt6e√ód6 saunmd athree tahcetivwateiiognhtfumnacttriiocne‚ó¶ss (asnidgmboiaids vfuenctcotiros,nœÉanadndhytpaenrhboalriec
‚àí (cid:48)‚àí tangent). The operator denotes the Hadamard product.
of future‚ààrewards is then deÔ¨Åned as ‚àëtT(cid:48)=t1Œ≥t tr(xt(cid:48),ut(cid:48)),
whereŒ≥ [0,1]isadiscountfactorthatprioritizesnear-term C. DOB-Net
rewards over distant rewards [26].
TheDOB-Netisconstructedbasedonclassicalactor-critic
IV. METHODOLOGY architecture.AsdescribedinFig.2(a)andFig.2(b),theDOB
and GRU have a similar architecture, especially the part in
The underwater disturbances present great challenges for
the red box. y of DOB acts as the hidden state, similar
stabilization control due to its excessive amplitudes as well t
to the role of h in GRU, which preserves past processed
astime-varyingcharacteristics.Inthissection,aconventional t
information.InordertoimitatethefunctionofDOB,aGRU
DOB is Ô¨Årst compared with a GRU, the results show some
similaritiesinthestructureofprocessinghiddeninformation. is Ô¨Årst employed to process the state-action pair [xt,ut‚àí1].
Then, fully connected layers are required to further extract
Thus, an enhanced observer network for excessive time-
varying disturbances is designed using GRUs, encoding the Controller
64 64 3
disturbance dynamics into GRU hidden state. A controller
Observer
64 64 n 64
yùë°‚àí1 ùë¶ùë° ‚Ñéùë°‚àí1 ‚Ñéùë° ùë¢ùë°
ùë¢ùë•ùë°‚àíùë°1 ùê∫ùëùùêø(((ùë•ùë•ùë•ùë°ùë°ùë°))) ùëë+ùë°-- ùë¶1‡∑§ùë°- ùëë·àòùë°‚àí1 ùë†ùë° ùúéùëüùë° ùúé1-ùëßùë° ùë°ùëéùëõ‚Ñé‡∑®‚Ñéùë° ùë¢ùë•ùë°‚àíùë°1 GRU FC tanh FC ùëë·àòùë°‚àí1GRU‚Ñéùë° 1 ùëâùë°
tanh tanh
(a) DOB (b) GRU FC FC Output
Fig.2. ArchitectureofDOBandGRU. Fig.3. NetworkarchitectureofDOB-Net.
1883
Authorized licensed use limited to: Carleton University. Downloaded on September 22,2020 at 13:35:28 UTC from IEEE Xplore.  Restrictions apply. Algorithm1:DOB-Net-pseudocodeforeachthread X Y Z Control Limit
AIrnesipstieuaamltizeegglolobbaallpcaoruamntserŒ∏‚ÜêT, Œ∏=v0ananddththrerae‚ÜêdadpcaoraumntserŒ∏t(cid:48),=Œ∏1v(cid:48) (a)Disturbance (N)-2200000
Reset gradients: dŒ∏ 0 and dŒ∏ 0
(cid:48) v (cid:48) 0 5 10 15 20 25 30
Synchronize parameters Œ∏ =Œ∏ and Œ∏v=Œ∏v Time (s)
tstart =t N)200
GreepteSsaattamteplxet,ultasatcaccotridoinngutt‚àío1,œÄl(austt|hxti,dudte‚àín1,shtat‚àíte1;hŒ∏t‚àí(cid:48))1, (b)Disturbance (-2000
receive h 0 5 10 15 20 25 30
t Time (s)
P(cid:26)‚Üêerform ut, recei‚Üêve rt and xt+1 N)200
uRn=titl‚ààte{Vr0tm+(‚àíixn1ta,ulatnx‚àídt1,oThrt‚àít1‚àí};TŒ∏tsv(cid:48)+t)ar1tff=oorr=ntetomrnma-xti;enraml ixntal xt (c)Disturbance (-20000 5 10 15 20 25 30
for i ‚Üêt 1,...,tstart do Fig.4. ExampledisturbancesinX,YTaimned (s)Zdirections,(a)smallsimulated
R ri+Œ≥R (cid:48) ‚Üê disturbances;(b)largesimulateddisturbances;(c)collecteddisturbances.
Accumulate gradients wrt Œ∏ :dŒ∏
| (cid:48) ‚àí √ó √ó
dŒ∏+‚àáŒ∏(cid:48)logœÄ(ui x(cid:48)i,ui‚àí1,hi‚àí1;Œ∏ )(R the size of 0.8 0.8 0.25 m3. Only positional motion and
V(xi,ui‚àí1,hi‚àí1;Œ∏v)) (cid:48) ‚Üê control are considered, thus the AUV has a 6-dimensional
AdcŒ∏cvu+mu‚àÇla(tRe‚àígrVad(ixei,nutsi‚àíw1,rhti‚àíŒ∏v1;:Œ∏dv(cid:48)Œ∏))v2/‚àÇŒ∏v(cid:48) sctoantestrsapinactse|aun|d=a|u3|-=dim[1e2n0sNion1a2l0aNcti1o2n0Nsp]aTc.eE. aTchhetrcaoinntirnogl
end
episode contains 200 steps with 0.05s per step. In each
Perform update of Œ∏, Œ∏ using dŒ∏, dŒ∏
v v episode, the robot starts at a random position with a random
until T >Tmax; velocity, and it is controlled to reach a target position and
staywithinaregion(refertoasconvergedregion)thereafter.
embeddingofestimateddisturbancesdÀÜ‚àí .Afterdisturbance
t 1 In these simulations, the RL algorithms are trained using
estimation, the observer network can be further enhanced
simulated disturbances, and tested using both simulated
through feeding this embedding into another GRU, in order
disturbances and collected disturbances. The simulated dis-
to encode a sequence of past estimated disturbances. The
turbances are constructed as a superposition of multiple
hidden state of the second GRU h is supposed to represent
t sinusoidal waves (three in these simulations) with different
the disturbance dynamics. It can then be combined with the
amplitudes, frequencies and phases. Two different scenarios
current state x, becoming the actual inputs of the controller
t are considered, one has close or slightly excessive ampli-
network. One design parameter of the DOB-Net is the
tudes (around 100-120% of control constraints, Fig. 4 (a)),
embeddingdimensionofdÀÜ‚àí .Inthispaper,3-dimension(the
t 1 the other one has larger amplitudes (130-150% of control
dimensionofdisturbances)and64-dimension(thedimension
constraints, Fig. 4 (b)). Our purpose is to enable the trained
ofRNNhiddenstate)arechosenandcomparedinsimulation.
policy todeal withunknown time-varyingdisturbances, thus
Such comparison shows the Ô¨Çexibility of neural networks
their amplitudes, frequencies and phases are randomly sam-
after building the observer from GRU.
pledfromthegivendistributionsineachtrainingepisode.In
Training: Advantage Actor Critic (A2C) [18] is a con-
ordertofurthervalidatetheefÔ¨ÅcacyoftheDOB-Net,wealso
ceptually simple and lightweight framework for RL that
collected wave and current disturbance data in a water tank
usessynchronousgradientdescentforoptimizationofneural
using wave generators (refer to as collected disturbances),
network controllers. The algorithm synchronously executes
as shown in Fig. 4 (c). The data is collected through an
multiple agents in parallel, on multiple instances of envi-
onboard Inertial Measurement Unit (IMU) of an unactuated
ronment. This parallelism also decorrelates the agents‚Äô data
AUV,themeasuredlinearaccelerationsaremappedtoforces,
into a more stationary process, since at any given time step
whichcanbeassumedasthedisturbances.Wenoticethatthe
theparallelagentswillbeexperiencingavarietyofdifferent
amplitudes of the collected disturbances are not constrained
states.ThealgorithmisdevelopedinA2Cstyle.Pseudocode
within the ranges seen during training, leading to a more
of the DOB-Net is shown in Algorithm 1. Each thread
challenging scenario.
interactswithitsowncopyofenvironment.Thedisturbances
Ten different algorithms for disturbance rejection control
are also different in each thread, and each of them are
are tested and compared:
randomly sampled. We found this setting helps accelerate
(1) Robust Integral of Sign Error (RISE) Control [29]
the convergence of learning and improve performance.
(2) DOBC
V. SIMULATIONEXPERIMENTS
(3) A2C
A. Simulation Setup (4) History Window A2C with state history (HWA2C-x)
ApositionregulationtaskissimulatedtotesttheDOB-Net (5) HWA2C with state-action history (HWA2C-xu)
algorithm.ThesimulatedAUVhasthemassm=60kgwith (6) Recurrent A2C with state history (RA2C-x)
1884
Authorized licensed use limited to: Carleton University. Downloaded on September 22,2020 at 13:35:28 UTC from IEEE Xplore.  Restrictions apply. 0 0 2 3.5
-200 -200
3
Episode Reward----1111---642086400000000000000 ---111--98210000002 2.1 2.2 2.3 2.4105 Episode Reward----1111---642086400000000000000 AHHRR2WWAAC22AACC22--CCxxu--xxu Distance from Target (m)01..155 Distance from Target (m)12..1255
DOB-Net (n=3) 0.5
-1800 -1800 DOB-Net (n=64)
-2000 -2000 0 0
0 0.5 1 1.5 2 0 0.5 1 1.5 2 (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (1) (2) (3) (4) (5) (6) (7) (8) (9) (10)
Training Episode 105 Training Episode 105 (a) (b)
(a) (b) Fig. 6. Distribution of distance from target during last 100 steps, (a)
Fig. 5. Training rewards, (a) small simulated disturbances (amplitude small simulated disturbances (amplitude between 100-120% of control
between100-120%ofcontrolconstraints);(b)largesimulateddisturbances constraints);(b)largesimulateddisturbances(amplitudebetween130-150%
(amplitudebetween130-150%ofcontrolconstraints). ofcontrolconstraints).
(7) RA2C with state-action history (RA2C-xu) proving that RNN is able to utilize the history information
(8) DOB-Net (n=3) more efÔ¨Åciently than naively combining past states and
(9) DOB-Net (n=64) actions into policy input space. Furthermore, including past
(10) Trajectory Optimization actions as additional input besides states yields performance
Noticethat,amongthesealgorithms,thetrajectoryoptimiza- improvement for both HWA2C and RA2C. The DOB-Net
tionassumesthatthefullknowledgeofthedisturbancesover achieves the best performance among all these algorithms,
the whole episode is given in advance, while all other algo- using larger embedding dimension (n=64) of disturbance
rithms deal with unknown disturbances. The comparison is estimate produces higher cumulative reward and smaller
obviouslynotfair,thetrajectoryoptimizationisusedonlyto movement range. We believe enlarging the embedding di-
provideoptimalperformanceunderidealconditions,itcanbe mensionofdisturbanceestimatecanprovidebetterrepresen-
regarded as an informal upper bound for all the algorithms. tationofdisturbancedynamics.Transformingthisembedding
Our goal is to narrow the gap between the performance of from a 64-dimensional variable to a 3-dimensional variable
theproposedalgorithmandthatofthetrajectoryoptimization maycauselossofinformation.However,evenusingthebest
solution.RISEcontrolisaconventionalfeedbackcontroller; RLalgorithmswementionedsofar,thecontrolperformance
HWA2C applies the history window approach into the A2C stillhasalargegapfromthetrajectoryoptimizationsolution.
framework, the used window length is 10 time steps, which There is still room for further improvements.
is 0.5s in our simulation setup; and RA2C employs RNNs
In addition, it is obvious that stronger disturbances lead
to deal with the past states and actions. The applied A2C
to worse performance. But we also found larger amplitude
framework employs a parallel training mode, 16 agents are
range of disturbances gives a more similar results between
usedatthesametime,theequivalentreal-worldtrainingtime
the DOB-Net and the trajectory optimization (the ratio of
for each agent is 43.4 hours. In the remaining part of this
medians between the DOB-Net and the trajectory optimiza-
section, we Ô¨Årst evaluate the training process of different
tionis502.31%and186.65%respectivelyforsmallandlarge
algorithms, then test and compare the control performance
simulateddisturbances).Thisphenomenonmightresultfrom
among them using either the simulated disturbances or the
that, for disturbances with larger amplitudes, the optimal
collected disturbances.
controls for different wave proÔ¨Åles tend to be more similar.
B. Training and Test Results on Simulated Disturbances Thus,itiseasierforRLtolearnacontrolpolicyunderlarger
disturbance amplitudes.
Fig. 5 shows the change of cumulative reward over time
The3DtrajectoriesoftheAUVsubjecttolargesimulated
during training. But the training reward is not sufÔ¨Åcient to
disturbancesarecomparedamongthesealgorithmsinFig.7.
comparetheperformanceamongdifferentalgorithms,weare
TheredballregionrepresentstheAUV‚Äôsmaximumdistance
also interested in state distribution and bounded response
fromthetargetduringlast50steps,calledconvergedregion.
(i.e. converged region) of the AUV disturbed by underwater
According to this region, we can see that the AUV is
waves and currents. As shown in Fig. 6, the box plot is
difÔ¨Åculttoachievesatisfactoryboundedresponseusingeither
used to represent and compare the distribution of the AUV‚Äôs
conventional controllers (RISE and DOBC) or classical RL
distancefromthetargetpositionduringlast100stepsofeach
policy(A2C).WhiletheproposedDOB-NetcansigniÔ¨Åcantly
episode among different algorithms.
reduce the converged region. Using the DOB-Net, the AUV
It is clear that both the history window policies and
canquicklynavigatetothetargetandstabilizeitselfwithina
the recurrent policies perform better than the classical RL
distanceof0.493mfromthetargetthereafter.However,there
policy (A2C) and the conventional controllers (RISE and
is still an obvious difference between the DOB-Net results
DOBC), which means considering history information does
and the optimal trajectories under ideal conditions.
improve the disturbance rejection capability. When only
small disturbances occur, different approaches to use the
C. Test Results on Collected Disturbances
history information have nearly the same results. When the
disturbances become larger, the recurrent policies achieve Besides the simulated disturbances, we also use the col-
better results compared with the history window policies, lectedwaveandcurrentdisturbances(asshowninFig.4(c))
1885
Authorized licensed use limited to: Carleton University. Downloaded on September 22,2020 at 13:35:28 UTC from IEEE Xplore.  Restrictions apply. (a) RISE (b) DOBC (c) A2C
(g) Trajectory Optimization
(d) HWA2C-xu (e) RA2C-xu (f) DOB-Net
Fig.7. 3DtrajectoriesoftheAUVwithlargesimulateddisturbances(Ristheradiusoftheconvergedregion).Notethetrajectoryoptimizationassumes
thatthedisturbancedynamicsisknowninadvance,thusprovidestheidealperformance.
for testing. Note the collected data is only used for testing, and prediction. A controller network is designed using the
no retraining is required. As compared in Fig. 8 and Fig. 9, observeroutputsaswellascurrentstateasinputs,togenerate
theDOB-Netstillhassatisfactoryperformanceonreal-world optimal controls. Multiple control and RL algorithms have
waveproÔ¨Ålesandoutperformsalltheotheralgorithms,which been tested and compared on position regulation tasks using
proves the practical effectivenessof the DOB-Net. However, both simulated disturbances and collected disturbances, the
the converged region of all the algorithms become larger results demonstrate that the proposed DOB-Net does have a
compared with using the simulated disturbances. The reason signiÔ¨Åcant improvement for the disturbance rejection capac-
behind is that the collected disturbances are more diverse ity compared to existing algorithms.
and complicated, thus have a wider range of amplitudes
compared with using the simulated disturbances. The DOB-
Currently, the test disturbances are collected in a water
Net may not be capable of handling such unseen scenarios
tank using wave generator, we plan to seek for the distur-
optimally.Thisgivesrisetoanotherresearchquestion,which
bance data from open water environments with natural wave
is to deal with disturbances with a wide range of parameters
and current for further testing. Also, we have noticed that
based on training on small range of parameters. This may
theperformanceoftheDOB-Netisworseusingthecollected
require the technique of transfer learning [30].
disturbances,duetoitsmorecomplexanddiversedynamics.
An interesting future work is to investigate the usage of
VI. CONCLUSIONS&FUTUREWORK
transferlearningindealingwithrealworldwaveandcurrent
This paper proposes an observer-integrated RL approach disturbances. In addition, the deployment of this algorithm
called DOB-Net, for mobile robot control problems under on real-world robotic systems requires future investigation,
unknownexcessivetime-varyingdisturbances.Adisturbance where the low sample efÔ¨Åciency of generic model-free RL
dynamics observer network employing RNNs has been used might be a problem.
to imitate and enhance the function of conventional DOB,
inordertoproduce the embeddingofdisturbanceestimation
3.5
3
m)
et (2.5
g
Tar 2
m 
o
e fr1.5
nc
Dista 1
0.5
0
(1) (2) (3) (4) (5) (6) (7) (8) (9) (10) Fig. 9. 3D trajectories of the AUV with collected disturbances (R is the
Fig.8. Distributionofdistancefromtargetwithcollecteddisturbances. radiusoftheconvergedregion).
1886
Authorized licensed use limited to: Carleton University. Downloaded on September 22,2020 at 13:35:28 UTC from IEEE Xplore.  Restrictions apply. REFERENCES [16] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.
Bellemare,A.Graves,M.Riedmiller,A.K.Fidjeland,G.Ostrovski,
[1] G.GrifÔ¨Åths,Technologyandapplicationsofautonomousunderwater et al., ‚ÄúHuman-level control through deep reinforcement learning,‚Äù
vehicles. CRCPress,2002,vol.2. Nature,vol.518,no.7540,p.529,2015.
[2] T. Wang, W. Lu, and D. Liu, ‚ÄúA case study: Modeling of a passive [17] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, ‚ÄúTrust
Ô¨Çexible link on a Ô¨Çoating platform for intervention tasks,‚Äù in 2018 regionpolicyoptimization,‚ÄùinInternationalConferenceonMachine
13thWorldCongressonIntelligentControlandAutomation(WCICA). Learning,2015,pp.1889‚Äì1897.
IEEE,2018,pp.187‚Äì193. [18] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,
[3] J. Woolfrey, D. Liu, and M. Carmichael, ‚ÄúKinematic control of an D.Silver,andK.Kavukcuoglu,‚ÄúAsynchronousmethodsfordeeprein-
autonomous underwater vehicle-manipulator system (auvms) using forcementlearning,‚ÄùinInternationalconferenceonmachinelearning,
autoregressive prediction of vehicle motion and model predictive 2016,pp.1928‚Äì1937.
control,‚ÄùinRoboticsandAutomation(ICRA),2016IEEEInternational [19] S.S√¶mundsson,K.Hofmann,andM.P.Deisenroth,‚ÄúMetareinforce-
Conferenceon. IEEE,2016,pp.4591‚Äì4596. mentlearningwithlatentvariablegaussianprocesses,‚ÄùarXivpreprint
[4] L.-L.XieandL.Guo,‚ÄúHowmuchuncertaintycanbedealtwithby arXiv:1803.07551,2018.
feedback?‚ÄùIEEETransactionsonAutomaticControl,vol.45,no.12, [20] L.-J. Lin and T. M. Mitchell, ‚ÄúReinforcement learning with hidden
pp.2203‚Äì2217,2000. states,‚ÄùFromanimalstoanimats,vol.2,pp.271‚Äì280,1993.
[5] R.S.SuttonandA.G.Barto,Reinforcementlearning:Anintroduction. [21] T.Wang,W.Lu,andD.Liu,‚ÄúExcessivedisturbancerejectioncontrol
MITpress,2018. of autonomous underwater vehicle using reinforcement learning,‚Äù in
[6] W.-H.Chen,D.J.Ballance,P.J.Gawthrop,andJ.O‚ÄôReilly,‚ÄúAnonlin- AustralasianConferenceonRoboticsandAutomation,2018.
eardisturbanceobserverforroboticmanipulators,‚ÄùIEEETransactions [22] M.HausknechtandP.Stone,‚ÄúDeeprecurrentq-learningforpartially
onindustrialElectronics,vol.47,no.4,pp.932‚Äì938,2000. observablemdps,‚Äùin2015AAAIFallSymposiumSeries,2015.
[7] W.-H.Chen,J.Yang,L.Guo,andS.Li,‚ÄúDisturbance-observer-based [23] D.Wierstra,A.Foerster,J.Peters,andJ.Schmidhuber,‚ÄúSolvingdeep
control and related methods‚Äîan overview,‚Äù IEEE Transactions on memory pomdps with recurrent policy gradients,‚Äù in International
IndustrialElectronics,vol.63,no.2,pp.1083‚Äì1095,2016. ConferenceonArtiÔ¨ÅcialNeuralNetworks. Springer,2007,pp.697‚Äì
[8] H. Sun and L. Guo, ‚ÄúNeural network-based dobc for a class of 706.
nonlinear systems with unmatched disturbances,‚Äù IEEE transactions [24] I.Sutskever,O.Vinyals,andQ.V.Le,‚ÄúSequencetosequencelearning
onneuralnetworksandlearningsystems,vol.28,no.2,pp.482‚Äì489, withneuralnetworks,‚ÄùinAdvancesinneuralinformationprocessing
2016. systems,2014,pp.3104‚Äì3112.
[9] C. Edwards and S. Spurgeon, Sliding mode control: theory and [25] S.WaslanderandC.Wang,‚ÄúWinddisturbanceestimationandrejec-
applications. CrcPress,1998. tion for quadrotor position control,‚Äù in AIAA Infotech@ Aerospace
[10] W. Lu and D. Liu, ‚ÄúActive task design in adaptive control of redun- Conference and AIAA Unmanned... Unlimited Conference, 2009, p.
dant robotic systems,‚Äù in Australasian Conference on Robotics and 1983.
Automation. ARAA,2017. [26] A.Nagabandi,G.Kahn,R.S.Fearing,andS.Levine,‚ÄúNeuralnetwork
[11] ‚Äî‚Äî,‚ÄúAfrequency-limitedadaptivecontrollerforunderwatervehicle- dynamics for model-based deep reinforcement learning with model-
manipulator systems under large wave disturbances,‚Äù in 2018 13th free Ô¨Åne-tuning,‚Äù in Robotics and Automation (ICRA), 2018 IEEE
World Congress on Intelligent Control and Automation (WCICA). InternationalConferenceon. IEEE,2018,pp.7579‚Äì7586.
IEEE,2018,pp.246‚Äì251. [27] S.Li,J.Yang,W.-H.Chen,andX.Chen,Disturbanceobserver-based
[12] H. Gao and Y. Cai, ‚ÄúNonlinear disturbance observer-based model control:methodsandapplications. CRCpress,2016.
predictive control for a generic hypersonic vehicle,‚Äù Proceedings of [28] K.Cho,B.VanMerri√´nboer,C.Gulcehre,D.Bahdanau,F.Bougares,
the Institution of Mechanical Engineers, Part I: Journal of Systems H. Schwenk, and Y. Bengio, ‚ÄúLearning phrase representations using
andControlEngineering,vol.230,no.1,pp.3‚Äì12,2016. rnnencoder-decoderforstatisticalmachinetranslation,‚ÄùarXivpreprint
[13] C.E.Garcia,D.M.Prett,andM.Morari,‚ÄúModelpredictivecontrol: arXiv:1406.1078,2014.
theory and practice‚Äîa survey,‚Äù Automatica, vol. 25, no. 3, pp. 335‚Äì [29] N.Fischer,D.Hughes,P.Walters,E.M.Schwartz,andW.E.Dixon,
348,1989. ‚ÄúNonlinearrise-basedcontrolofanautonomousunderwatervehicle,‚Äù
[14] U.MaederandM.Morari,‚ÄúOffset-freereferencetrackingwithmodel IEEETransactionsonRobotics,vol.30,no.4,pp.845‚Äì852,2014.
predictivecontrol,‚ÄùAutomatica,vol.46,no.9,pp.1469‚Äì1476,2010. [30] S. J. Pan and Q. Yang, ‚ÄúA survey on transfer learning,‚Äù IEEE
[15] S. Brahmbhatt and J. Hays, ‚ÄúDeepnav: Learning to navigate large Transactions on knowledge and data engineering, vol. 22, no. 10,
cities,‚Äù in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2017, pp.1345‚Äì1359,2009.
pp.3087‚Äì3096.
1887
Authorized licensed use limited to: Carleton University. Downloaded on September 22,2020 at 13:35:28 UTC from IEEE Xplore.  Restrictions apply. 