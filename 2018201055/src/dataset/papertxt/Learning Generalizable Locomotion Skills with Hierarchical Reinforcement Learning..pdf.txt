2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
Feedback Linearization for Uncertain Systems via Reinforcement Learning
Tyler Westenbroek*, David Fridovich-Keil*, Eric Mazumdar*, Shreyas Arora, Valmik Prabhu,
S. Shankar Sastry, and Claire J. Tomlin
Abstract—We present a novel approach to control design Feedback Linearized System

for nonlinear systems which leverages model-free policy op- ⇠˙=Ar⇠+Brv
timization techniques to learn a linearizing controller for a <latexit sha1_base64="Kyjmn0iv4zFZ3tbrIM5T7WfIMn4=">AAACWnicbZBdSxtBFIZPVttq7EfU3vVmaCgUKmE3SaNeCDbe9NJCo0KyhNnJyTo4M7vMnJWGJf/HX+OtCv4YJ2su/OgLAw/v+RjOm+RKOgrDu1qwsvrm7bu19frG+w8fPzU2t05cVliBA5GpzJ4l3KGSBgckSeFZbpHrROFpcnG0qJ9eonUyM39plmOseWrkVApO3ho3+qNqx9CmSVyGrbDb+dnr7oStzl671+156O129sNoXh9NMmKjf5IdsF9jW9EP1vd0OW40/WAl9hqiJTRhqePxZg38NlFoNCQUd24YhTnFJbckhcL5qHCYc3HBUxwqJEKb8xztjuY2leYgkiYuU8w0kp3NWf1pu0fDNbq4rM6as2/embBpZv0zxCr36UTJtXMznfhOzencvawtzP/VhgVN9+JSmrwgNOLxo2mhGGVskTObSIuC1MwDF1b6y5g455YLf46r+8yilwm9hpN2K+q02n+6zcP+Mr01+AJf4TtEsAuH8BuOYQACruAabuC2dh8EwXqw8dga1JYz2/BMwecHLd+wOQ==</latexit>
Nominal Feedback 
physicalplantwithunknowndynamics.Feedbacklinearization Linearizing Controller

 
isatechniquefromnonlinearcontrolwhichrenderstheinput- Learned Feedback 
oocoufntptarunotlldaerypnpharaomspibrciseaetonefcafoennesodtnbrluaincctkeeadrc,opdnleatsrniortelldleinro.euaOtrpnuucntedtreaarjealcpintpoelraiicreaisztiifonongr y<latexit sha1_base64="zRJUbQCnvJGTZGjvnxxEz15xz+w=">AAACHHicbVBNSxxBFHzjR9RVk9UcvTQuQg5hmVEhXgTRi0cD2VXYHeRN79vd1u6eofuNMAz7H3KNB3+NN/EayL9J77oHoyloKKrq0VRlhVae4/hPtLC4tPxhZXWtsb6x+fFTc2u76/PSSerIXOfuKkNPWlnqsGJNV4UjNJmmy+z2bOpf3pHzKrc/uCooNTiyaqgkcpC6/TGyqK6brbgdzyDek2ROWjDHxfVWBP1BLktDlqVG73tJXHBao2MlNU36pacC5S2OqKeJmVyBBbmvBt1I2eNE2bQeUW6IXTURjdfxQC0a8mk9KzcRe0EZiGHuwrMsZurrixqN95XJQtIgj/1bbyr+z+uVPDxKa2WLksnKl4+GpRaci+lSYqAcSdZVICidCs2EHKNDGer4RtgsebvQe9LdbycH7f3vh62T0/l6q7ADu/AFEvgGJ3AOF9ABCTfwE37BffQQPUZP0fNLdCGa33yGfxD9/gsffKFP</latexit>ˆ LineCaor nFteroeldlebrack  v<latexit sha1_base64="e0MhcACAwGbz5No9fXF5paHwpYU=">AAACF3icbVC7SgNBFL3rM8ZntLQZDIKFhF0VtBFEG0sFEwPJIrOTmzg4M7vM3A2EJV9gq4VfYye2lv6Nk0fh68DA4ZxzGc5JMiUdheFnMDM7N7+wWFoqL6+srq1vVDYbLs2twLpIVWqbCXeopME6SVLYzCxynSi8TR4uRv5tH62TqbmhQYax5j0ju1Jw8tJ1/26jGtbCMdhfEk1JFaa4uqsE0O6kItdoSCjuXCsKM4oLbkkKhcN27jDj4oH3sKWQCG3GM7T7mtueNKeRNHHRw1Qj2cGQlb/HPTVco4uLca8h2/VKh3VT658hNla/XxRcOzfQiU9qTvfutzcS//NaOXVP4kKaLCc0YvJRN1eMUjYaiXWkRUFq4AkXVvpmTNxzy4Wv48p+s+j3Qn9J46AWHdYOro+qZ+fT9UqwDTuwBxEcwxlcwhXUQQDCIzzBc/ASvAZvwfskOhNMb7bgB4KPL23Cn2E=</latexit> Linearizing Controller
 + u<latexit sha1_base64="ULrpBDFkA4Q8ynkZ3wAPpdslszo=">AAACF3icbVA9SwNBFHznZ4xfiZY2i0GwkHCngjZC0MYyglEhOcLe5iUu7u4du++EcOQX2Grhr7ETW0v/jZuYwq+BhWFmHstMkinpKAw/gpnZufmFxdJSeXlldW29Ut24cmluBbZEqlJ7k3CHShpskSSFN5lFrhOF18nd2di/vkfrZGouaZhhrPnAyL4UnLx0kXcrtbAeTsD+kmhKajBFs1sNoNNLRa7RkFDcuXYUZhQX3JIUCked3GHGxR0fYFshEdqMZ2j3NLcDaU4iaeJigKlGssMRK3+Pe2q4RhcXk14jtuOVHuun1j9DbKJ+vyi4dm6oE5/UnG7db28s/ue1c+ofx4U0WU5oxNdH/VwxStl4JNaTFgWpoSdcWOmbMXHLLRe+jiv7zaLfC/0lV/v16KC+f3FYa5xO1yvBFmzDLkRwBA04hya0QADCAzzCU/AcvASvwdtXdCaY3mzCDwTvn2wPn2A=</latexit> x<latexit sha1_base64="dLSTlT+bm8uAO2WXfBknYaABpfk=">AAACMnicbVBNSxxBFHyjJtE1iWu8xUuTRTAkLDNG0IsgesnRgKvCOiw9vW/Gxu6epvuNuAwL+TVe9eCf0Zt49Uek9+PgRwoaiqp6r3mVWSU9xfFdNDM79+79h/mFxuLHT5+XmstfjnxZOYEdUarSnWTco5IGOyRJ4Yl1yHWm8Dg73x/5xxfovCzNIQ0sppoXRuZScApSr/n1tF8Su2Q7LO/Z9cvv7AcrJqTqNVtxOx6DvSXJlLRgioPecgRhm6g0GhKKe99NYktpzR1JoXB4Wnm0XJzzArsKidBZbtH91NwV0uwk0qR1gaVGcoMhazyPB2q4Rp/W45OHbC0ofZaXLjxDbKw+n6i59n6gs5DUnM78a28k/s/rVpRvp7U0tiI0YvJRXilGJRv1x/rSoSA1CIQLJ8NlTJxxx0U4xzdCZ8nrht6So4128qu98Weztbs3bW8eVuEbrEMCW7ALv+EAOiDgL1zBNdxEt9F99BA9TqIz0XRmBV4gevoHQsenyw==</latexit>˙=Ufy<latexit sha1_base64="wUErPqywWu21AQb9UXiPoR+ClaA=">AAACIHicbVBBaxNBGP22ahtTW1M9ehkMhRRK2I2CXgpBLx4rmKQ0WcLs5EsydGZ2mPm2uCz5F1714K/xJh711zhJ91ATHww83nsfw3uZVdJTHP+O9h48fLR/0HjcPHxydPy0dfJs6PPCCRyIXOXuKuMelTQ4IEkKr6xDrjOFo+zm/dof3aLzMjefqLSYar4wci4FpyBdl+yCLae28/ls2mrH3XgDtkuSmrShxuX0JILJLBeFRkNCce/HSWwprbgjKRSuJoVHy8UNX+BYIRE6yy26c83dQpqLRJq0WmCukVy5Ys378UAN1+jTalNwxU6DMmPz3IVniG3U+xcV196XOgtJzWnpt721+D9vXND8bVpJYwtCI+4+mheKUc7Wa7GZdChIlYFw4WRoxsSSOy5CHd8MmyXbC+2SYa+bvOr2Pr5u99/V6zXgBbyEDiTwBvrwAS5hAAIMfIGv8C36Hv2Ifka/7qJ7UX3zHP5B9OcvCNeiOw==</latexit>npk=(nxow)hn+p (Pxglap)n(t
x)u
the nonlinear plant can be tracked using a variety of linear x
<latexit sha1_base64="9wmlBrSAkfoMg38SDp4B/Q5cje0=">AAACF3icbVC7SgNBFL3rM8a3ljaDQbCQsKuCNkLQxlLBaCBZZHZyEwdnZpeZu2JY8gW2Wvg1dmJr6d84eRTxcWDgcM65DOckmZKOwvArmJqemZ2bLy2UF5eWV1bX1jeuXZpbgXWRqtQ2Eu5QSYN1kqSwkVnkOlF4k9yfDfybB7ROpuaKehnGmneN7EjByUuXj7drlbAaDsH+kmhMKjDGxe16AK12KnKNhoTizjWjMKO44JakUNhv5Q4zLu55F5sKidBmPEO7p7ntSnMSSRMXXUw1ku31WXky7qnhGl1cDHv12Y5X2qyTWv8MsaE6eVFw7VxPJz6pOd25395A/M9r5tQ5jgtpspzQiNFHnVwxStlgJNaWFgWpnidcWOmbMXHHLRe+jiv7zaLfC/0l1/vV6KC6f3lYqZ2O1yvBFmzDLkRwBDU4hwuogwCEJ3iGl+A1eAveg49RdCoY32zCDwSf33Eon2M=</latexit>
control techniques. However, the calculation of a linearizing
y
controllerrequiresaprecisedynamicsmodelforthesystem.As <latexit sha1_base64="flOrEpNTfZAoV0rd/khcxo6hMd8=">AAACF3icbVA9SwNBFHznt/Er0dJmMQgWEu5U0EYI2lgqmCjEQ/Y2L3Fxd+/YfSccR36BrRb+GjuxtfTfuIkp4sfAwjAzj2UmyZR0FIafwdT0zOzc/MJiZWl5ZXWtWltvuzS3AlsiVam9TrhDJQ22SJLC68wi14nCq+T+dOhfPaB1MjWXVGQYa943sicFJy9dFLfVetgIR2B/STQmdRjj/LYWwE03FblGQ0Jx5zpRmFFccktSKBzc5A4zLu55HzsKidBmPEO7q7ntS3McSROXfUw1ki0GrDIZ99RwjS4uR70GbNsrXdZLrX+G2EidvCi5dq7QiU9qTnfutzcU//M6OfWO4lKaLCc04vujXq4YpWw4EutKi4JU4QkXVvpmTNxxy4Wv4yp+s+j3Qn9Je68R7Tf2Lg7qzZPxeguwCVuwAxEcQhPO4BxaIADhEZ7gOXgJXoO34P07OhWMbzbgB4KPL3Lbn2Q=</latexit>
aresult,model-basedapproachesforlearningexactlinearizing Fig. 1: Schematic diagram of our framework. By learning an appropriate
controllers generally require a simple, highly structured model feedback linearizing controller, we render an initially unknown nonlinear
of the system with easily identiﬁable parameters. In contrast, system(withstatex,outputy,andinputu)linearinanauxiliaryinputv.
Theframeworkcanmakeuseofanominaldynamicsmodel(ifavailable).
the model-free approach presented in this paper is able to
approximate the linearizing controller for the plant using controllers for a plant with unknown dynamics, observing
general function approximation architectures. Speciﬁcally, we
improvement in the performance of the trained controllers
formulate a continuous-time optimization problem over the
with practical amounts of data collects from the plant.
parameters of a learned linearizing controller whose optima
are the set of parameters which best linearize the plant. Speciﬁcally, this paper focuses on a geometric technique
We derive conditions under which the learning problem is calledfeedbacklinearization,whichrenderstheinput-output
(strongly) convex and provide guarantees which ensure the
dynamics of a nonlinear system linear under the application
true linearizing controller for the plant is recovered. We then
of an appropriately chosen feedback controller. Once a
discuss how model-free policy optimization algorithms can be
used to solve a discrete-time approximation to the problem linearizing controller has been constructed, it can be used in
using data collected from the real-world plant. The utility of conjunction with efﬁcient tools from linear systems theory
the framework is demonstrated in simulation and on a real- to generate and track desired output trajectories for the full
world robotic platform.
nonlinear plant [9–11]. Despite its widespread use through-
out robotics [1, 2, 12–14] the primary drawback of feed-
I. INTRODUCTION
back linearization is that exact cancellation of the system’s
Geometric nonlinear control theory has developed a pow- nonlinearities requires a precise model of the dynamics.
erful set of feedback architectures which exploit the under- Complexphenomenasuchasfriction,aerodynamicdrag,and
lying structure of a control system to simplify downstream internal actuator dynamics yield nonlinearities which may
tasks such as trajectory generation and tracking [1, 2]. be challenging to model or identify. While there have been
However, geometric controllers often require an accurate extensiveeffortstoconstructexactlinearizingcontrollersfor
model for the system or a simple parameterization of the unknown plants using extensions of adaptive linear control
dynamics which can be readily identiﬁed. Meanwhile, the theory [15–22], these methods require a highly structured
model-freereinforcementlearningliterature[3–5]hassought representation of the system’s nonlinearites and employ
to automatically compute optimal feedback controllers for complicatedparameterupdateschemestoavoidsingularities
unknown systems without relying on structural assumptions inthelearnedcontroller.Amoredetaileddiscussionofthese
about the dynamics. However, despite a recent resurgence methods is provided in Section II-C.
of research into these methods [6–8], their poor sample
Insharpcontrasttothestandardmethodsdiscussedabove,
complexity has thus-far limited their applicability to many
we propose a general framework for learning a linearizing
robotics applications. In this paper we unify these disparate
controller for a plant with unknown dynamics using model-
approachesbyusingmodel-freereinforcementlearningalgo-
free policy optimization techniques. Our approach requires
rithms to optimize the performance of a class of geometric
only that the order of the relationship between the inputs
and outputs is known, can utilize arbitrary function ap-
This work was supported by HICON-LEARN (design of HIgh
proximation schemes, and remains singularity free during
CONﬁdence LEARNing-enabled systems), Defense Advanced Research
Projects Agency award number FA8750-18-C-0101, and Provable training. Concretely, our approach begins by constructing a
High Conﬁdence Human Robot Interactions, Ofﬁce of Naval linearizing controller for an approximate dynamics model
Research award number N00014-19-1-2066. EECS, UC Berkeley.
of the plant (if available), and then augments this ﬁxed
westenbroekt@berkeley.edu.
∗ indicatesequalcontribution. model-basedcontrollerwithalearnedafﬁnecorrectionterm.
U.S. Government work not protected by 1364
U.S. copyright
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 12:31:13 UTC from IEEE Xplore.  Restrictions apply. We then deﬁne a continuous-time optimization problem ofthislinearizingcontrollaw,wecannowcontroltheoutput
over the learned parameters which selects parameters which trajectory y(t) through its ﬁrst derivative. However if the
better linearize the unknown plant. We provide conditions input does not affect the ﬁrst time derivative of the output
≡
on the structure of the learned component which ensure (that is, if L h 0) then the control law (2) will be
g
that the learning problem is (strongly) convex. Our analy- undeﬁned. In general, we can differentiate y multiple times
sis draws connections between the familiar persisency-of- until the input appears. Assuming that the input does not
−
excitation conditions from the adaptive control literature appear the ﬁrst γ 1 times we differentiate the output, the
and our formulation. Finally, we discuss how off-the-shelf γ-th time derivative of y will be of the form
reinforcement learning algorithms can be used to solve
−
discrete-time approximations to the optimization problem, y(γ) =Lγh(x)+L Lγ 1h(x)u.
f g f
and demonstrate the utility of our framework by learning −
linearizing controllers for highly uncertain robotic systems Here, Lγfh(x) and LgLγf 1h(x) are higher order L−ie deriv(cid:54)a-
in both simulation and on real-world hardware. tives(see[1,∈Chapter9]formore(cid:0)details).IfLgLγf(cid:1) 1h(x)=
The rest of the paper is structured as follows. Section 0 for each x D then the control law
II introduces feedback linearization and discusses prior
1 −
approaches for learning linearizing controllers. Section III u(x,v)= − Lγh(x)+v
L Lγ 1h(x) f
detailsourapproach,providesourtheoreticalresults,anddis- g f
cussespracticalalgorithmsforsolvingthelearningproblem.
enforces the trivial linear relationship y(γ) =v. We refer to
Oursimulatedandreal-worldroboticexamplesarepresented
γ as the relative degree of the nonlinear system, which is
in Section IV, and Section V provides closing remarks and
simply the order of its input-output relationship.
discussesfuturework.TheproofsforLemma2andTheorem
1 can be found in the longer version of this paper [23]. B. Multiple-input multiple-output systems
II. FEEDBACKLINEARIZATION Next,weconsider(square)multiple-input,multiple-output
This section outlines how to compute an input-output (MIMO) systems where q > 1. As in the SISO case, we
linearizing controller for a known dynamics model and differentiate each of the output channels until at least one
discussespriordata-drivenmethodsforlearningalinearizing input appears. Let γ be the number of times we need to
j
controller for an unknown plant. Due to space constraints, differentiate y (the j-th entry of y) for at least one input to
j
we refer the interested reader to [1, Chapter 9] for a more appear. Combining the resulting expressions for each of the
completeintroductiontofeedbacklinearization.Inthispaper, outputs yields an input-output relationship of the form
we consider square control-afﬁne systems of the form
[y(γ1),...,y(γq)]T =b(x)+A(x)u. (3)
1 q
x˙ =f(x)+g(x)u
(1) ∈ R ×
y =h(x), Here, the matrix A(x) ∈q Rq is known as the decoupling
∈ R ∈ R ∈ R matrix and the vector b(x) q is known as the drift term.
with sate x n, input u q and output y q. The ∈
R → R R → R × R → R If A(x) is bounded away from singularity for each x D
mappings f: n n, g: n n q and h: n q
then we observe that the control law
are each assumed to be smooth. We restrict our attention to
a compact subset D ⊂Rn of the state-space. u(x,v)=A−1(x)(−b(x)+v) (4)
A. Single-input single-output systems ∈R
where v q yields the decoupled linear system
Webeginbyintroducingfeedbacklinearizationforsingle-
input, single-output (SISO(cid:16)) systems (i.e.(cid:17), q = 1). We begin [y(γ1),y(γ2),...,y(γq)]T =[v ,v ,...,v ]T, (5)
1 2 q 1 2 q
by examining the ﬁrst time derivative of the output:
dh · wherevj isthej-thentryofvandyj(γj)istheγj-thderivative
y˙ = (x) f(x)+g(x)u of the j-th output. We refer to γ = (γ ,γ ,...,γ ) as
dx 1 2 q
(cid:124)dh (cid:123)(cid:122)· (cid:125) (cid:124)dh (cid:123)(cid:122)· (cid:125) the vector relative degree of the system. The decoupled
= (x) f(x)+ (x) g(x)u dynamics(5)areLTIandcanbecompactlyrepresentedwith
dx dx
the reference model
Lfh(x) Lgh(x)
Here the terms L h(x) and L h(x) are known as Lie ξ˙ =Aξ +Bv , (6)
f g r r r
derivatives [1], and capture the rate of change of y = h(x) − −
aLlohn(gxt)he(cid:54)=v0ecftoorreﬁaeclhdsxf∈anDd,gw,erecsapnecatpivpellyy.thInetchoenctraoslelathwat wAh∈ereRξ|γr|×=|γ|(ya1n,dy˙1B,.∈..,R.|.γ.|×,yq1γ1are1,d.y.n.a,myqi,c.s.m.,aytqγriqce1s)waintdh
g appropriate entries. The reference model can then be used
u(x,v)= 1 (−Lfh(x)+v) , (2) to construct a desired trajectory ξr(·) for the output of the
Lgh(x) nonlinearsystem(anditsderivatives),andtoconstructlinear
which exactly ‘cancels out’ the nonlinearities of the system feeback controllers which can be used in conjunction with
and enforces the linear relationship y˙ =v. After application (4)totrackthereference(see[1,Theorem9.14]fordetails).
1365
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 12:31:13 UTC from IEEE Xplore.  Restrictions apply. C. Constructing linearizing controllers with data don’tknowtheexactrelationshipbetweenthetwoquantities.
Moreover, with this assumption in place, we know there are
The fundamental challenge model-based methods face
linearizing controllers of the form
when constructing a linearizing controller for an unknown
plant is that it is very difﬁcult to identify an estimate for the
u (x,v)=β (x)+α (x)v
system’s decoupling matrix (or possibly its inverse) which m m m
is guaranteed to be invertible. The predominant approaches up(x,v)=βp(x)+αp(x)v
for learning linearizing controllers are founded on the linear forthemodelandplant,respectively,whichmatchtheinput-
model reference adaptive control (MRAC) literature [15], output dynamics of both systems to a common reference
and seek to recursively improve an estimate for the true model (6). Here, β (x),β (x) ∈ Rq and α (x),α (x) ∈
linearizing controller using data collected from the plant Rq×q. While we dopnot knmow u a priori, wepdo knmow that
p
[15–19, 24–26]. These methods update the parameters for
estimates of the decoupling matrix (or its inverse) online, β (x)=β (x)+∆β(x)
p m
but require that the estimated matrix remain invertible at all α (x)=α (x)+∆α(x)
p m
times. Typically, projection-based update rules are used to
keeptheparametersoftheestimatedmatrixinaregionwhich for some continuous functions ∆β and ∆α. We construct
the following parameterized estimates for these functions:
issingularity-free.However,theconstructionofthesebounds
requiresahighlyaccurateyetsimpleparameterizationofthe ≈ ≈
∆β(x) β (x) ∆α(x) α (x)
system, and assumes that the true parameters of the system θ1 θ2
∈ ⊂R ∈ ⊂R
lie within some nominal set. Here,θ1 Θ1 K11 andθ2 Θ2 K2 areparametersto
One alternative approach [27–29] is to perturb the esti- be trained by running experiments on the plant. We will as-
mated linearizing control law to avoid singularities. These sumethatΘ andΘ areconvexsets,andwewillfrequently
1 2 ∈ ×
methods enable the use of more general function approx- abbreviateθ =(θ ,θ ) Θ Θ :=Θ.Weassumethatβ
1 2 1 2 θ1
imation schemes but sacriﬁce some tracking performance. and α are continuous in x and continuously differentiable
θ2
Recently, non-parametric function approximators have been in θ and θ , respectively.
1 2 ∈
been used to learn a linearizing controller [20, 30], but Altogether, for a given θ =(θ ,θ ) Θ our estimate for
1 2
these methods still require structural assumptions to avoid the controller which exactly linearizes the plant is given by
singularities. In the following section we use model-free
uˆ (x,v)=[β (x)+β (x)]+[α (x)+α (x)]v (9)
policy optimization algorithms to update the paramters of θ m θ1 m θ2
a learned linearizing controller while avoiding singularities. When no prior information about the dynamics of the plant
is available (other than its vector relative degree), we simply
III. DIRECTLYLEARNINGALINEARIZINGCONTROLLER ≡ ≡
set β 0 and α 0 in the above expression. Next we
Our goal is to learn a linearizing controller for the plant deﬁnemanoptimizatmionproblemwhichselectstheparameters
for the learned controller which best linearize the plant.
x˙ =f (x )+g (x )u (7)
p p p p p p
y =h (x ) A. Continuous-time optimization problem
p p p
which is unknown. Our approach can incorporate nominal From Section II we know that the input-output dynamics
model of the plant are of the form
x˙m =fm(xm)+gm(xm)um (8) y(γ) =bp(x)+Ap(x)u (10)
ym =hm(xm) where the terms b and A are unknown to us, and we have
p p
which represents our "best guess" for the true dynamics of writtenthehighestorderderivativesoftheoutputsasy(γ) =
theplant.Wewillassumethattheplantandmodelhavewell- (y(γ1)...,y(γq))T tosimplifynotation.Underapplicationof
1 q
deﬁned relative degrees (γ1p,...,γqp)⊂aRnd (γ1m,...,γqm), uˆθ the dynamics are g(cid:124)iven by: (cid:123)(cid:122) (cid:125)
respectively, on some chosen set D n. We make the y(γ) =b (x)+A (x)uˆ (x,v) (11)
following assumption about our plant and model: p p θ
Wθ(x,v)
Assumption 1: The model system (8) and plant (7)
Sinceourgoalistoﬁndparameterswhichlinearizetheplant,
have the same relative degree on D in the sense that ∗ ∈ ≈
we want to ﬁnd θ Θ such that W ∗(x,v) v for each
(γp,...,γp)=(γm,...,γm). ∈ ∈ R θ
1 q 1 q x D and v q. Thus, we deﬁne the point-wise loss
R ×R ×R →R
This is a rather mild assumption, as the order of the (cid:96): n q K1+K2 by
relationship between the inputs and outputs of the plant (cid:107) − (cid:107)
(cid:96)(x,v,θ)= v W (x,v) 2, (12)
can usually be inferred from ﬁrst principles, even without a θ 2
perfect model for the dynamics of the system. For example, whichprovidesameasureofhowwellthelearnedcontroller
for Lagrangian systems, such as the manipulator arms we uˆ linearizes the plant at the state x when the virtual
θ
consider in Section IV, we know the torques applied by an input v is applied to the linear reference model. Next, we
R
actuator produce an acceleration in the joints, even if we specify a probability distribution X over n with support
1366
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 12:31:13 UTC from IEEE Xplore.  Restrictions apply. Fig. 2: (a) Depiction of the tracking task for the double pendulum. The blue curves represent the desired joint angles over time. The orange and green
curves represent trajectories generated by the ideal feedback linearizing controller (perfect model information) and our learned policy. Both trajectories
begin with aninitial tracking error, but quickly convergeto the desired motion. (b) Total(cid:96)2 error on a set-point tracking task for the Baxterrobot after
104minutesoftrainingforthedesiredlinearsystemis(dotted,green),thenominalfeedbacklinearizingcontroller(red),andourlearnedcontroller(blue)
⊂ R (cid:88) (cid:88)
D n and let V be the uniform distribution over the set controller is of the form
{ ∈R (cid:107) (cid:107)≤ }
v q: v 1 . We then deﬁne the weighted loss K1 K2
β (x)= θ1β (x) α (x)= θ2α (x) (15)
L(θ)=E ∼ ∼ (cid:96)(x,v,θ) (13) θ1 k k θ2 k k
x X,v V k=1 k=1
{ } { }
and select our optimal choice for the parameters of the where β K1 and α K2 arenonlinearcontinuousfunc-
k k=1 k k=1
learned controller via the following optimization: tions. When we adopt this structure our optimization be-
comes convex:
(P): mθ∈iΘnL(θ) (14) ThLenemPmais2c:onAvsesxu.mMeotrheaotvβeθr,1iafn{dβαθ}2Ka1reaonfdth{eαfo}rKm2(1a5r)e.
k k=1 k k=1
Here,thedistributionX modelsourpreferenceforhaving each linearly independent then P is strongly convex.
an accurate linearizing controller at different points in the Thus, when our learned controller takes the form (15)
state-space, and the uniformity of V ensures that optimal we can reliably use iterative techniques to ﬁnd its globally
solutions of P accurately linearize the plant for all possible optimal solution. The proof of Lemma 2, which is given in
choices of the virtual input to the reference model. The the accompanying technical report, shows that L is actually
primary challenge in solving P is that we do not know the quadratic in the parameters when the learned controller is of
terms in W (x,v), since we do not know A (x) and b (x). the form (15). The key property being exploited here is that
θ p p
However,usingtherelationship(11),wecanqueryWθ(x,v) Wθ(x,v)becomesafﬁnesinθwhenthecontrollerislinearin
by measuring y(γ) when different inputs are applied to the theparameters.OurconditionsonX andV areanalogousto
plant. Thus, P can be solved by running experiments on the thepersistencyofexcitationresultsfromtheadaptivecontrol
plant and using any stochastic optimization method which literature[17,Chapter2],andensurethateachcomponentof
only requires access to the point-wise loss (13). While we the learned controller is excited while solving P. This is the
focus on the use of policy-gradient reinforcement learning underlying reason why the optimization becomes strongly
algorithms below, there are many possible model-free ap- convex when the components of our controller are linearly
proaches for solving P which merit further investigation. independent. Taken together, the preceding lemmas imply
Importantly, since we have abstracted away the need for our main theoretical result:
a model when solving P, we can iteratively improve the ∗ ∈
Theorem 1: Suppose that for some θ Θ we have
performance of our learned controller without requiring ∈ ∈ R
uˆ ∗(x,v) = u (x,v) for each x D and v q. Further
that it remain invertible at each stage of the optimization. θ p
assume that the learned controller is of the form (15) and
Next, we discuss when we can recover the true linearizing { } { }
thatthesets β K1 and α K2 arelinearlyindependent.
controller for the plant by solving P: ∗ k k=1 k k=1
∗ ∈ Then θ is the unique global (and local) minimizer of P.
Lemma 1: Suppose that there∈exists θ ∈ Θ such t∗hat Thereareanumberofwell-studiedbases,suchaspolyno-
uˆθ∗(x,v)=up(x,v) for each x D and v V. Then θ is mialsorradialbasisfunctions,whichcanbeusedtoapproxi-
a globally optimal solution of P.
mateanycontinuousfunctiontoadesireddegreeofaccuracy.
∈ Proof: N∈ote that if uθ∗(x,v) = up(x,v) for each Thus, by including enough terms in {β }K1 and {α }K2
xhaveXL(θa)nd≥v0 forVeathchenθL∈(θΘ).=Th0u.s,Mθo∗removuestr,bweeacglleoabrlayl wbyescoalnvitnhgeoPre.tiHcaolwlyevreerc,ofvoerr huipghtodaimpernek-ssikop=ne1cailﬁseydstpermekcsi,skit=ohn1e
minimizer of the optimization (14).
number of terms required in such expansions can become
However, P is generally non-convex meaning that in prohibitively large. Other architectures, such as multi-layer
practice we can only hope to ﬁnd locally optimal solutions feed-forward neural networks, yield more compactly repre-
to the problem. Thus, we seek conditions which simplify sented function approximation schemes but complicate the
the structure of the optimization. The standard convergence analysis of P. However, since our approach does not suffer
proofs in the adaptive control literature assume the learned fromsingularitiesduringtheoptimizationprocess,inpractice
1367
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 12:31:13 UTC from IEEE Xplore.  Restrictions apply. we can still use these powerful function approximators to at the state x . We then deﬁn(cid:34)e the following r(cid:35)einforcement
k (cid:88)
ﬁnd an improved linearizing controller for high-dimensional learning problem over the parameters of uˆ :
θ
systems with unknown dynamics. We next demonstrate this
N
point by discussing how policy optimization algorithms can E
min ∼ ∼ ∼N (cid:96)¯(x ,u ,v )
be used to solve a discrete-time approximation to P. θ∈Θ x0 X,vk V,wk (0,σw2) k k k
k=1
B. Discrete-time approximations and reinforcment learning subject to: xk+1 =xk+F(xk,uk), x0 =x0
While the theory and optimization problem we developed uk =uˆθ(xk,vk)+wk
in the previous section are in continuous time, many real
Here, we sample initial conditions for the problem using
world plants have actuators which can only be updated at a
our desired state distribution X and sample inputs to the
ﬁxed sampling frequency. Thus in this section we formulate
linear system according to V at each time step. The zero
a discrete time approximation to P which we cast as a
mean added noise w is used to encourage exploration, and
canonicalreinforcementlearningproblem.Unfortunately,the k
to make the effects of the policy random. Finally, N is the
analysis of linearizable systems in the sampled-data setting
length of the training episodes. This problem can be solved
becomes signiﬁcantly more complex [31], which is why the
to local optimality using standard reinforcement learning
majority of the literature on adaptive control for linearizable
algorithms [32–35] and by running experiments on the real-
systems is formulated in continuous time.
world hardware to evaluate ¯l(x ,u ,v ). While it is not
Letting∆∈tN>0denotethesam·plingrate,wesettk =k∆t immediately clear how to extendkthektheokretical results from
for each k . Next, letting x() denote the state trajectory Section III-A to the present case, we intend to address this
of the plant, we then denote xk = x(tk) and let uk denote issue in a forthcoming article.
the control applied on(cid:90)the interval [t ,t ). This yields the
k k+1
following difference equation for the dynamics of the plant: IV. EXAMPLES
We now use our approach to learn feedback linearizing
(cid:124) tk+1 (cid:123)(cid:122) (cid:125)
x =x + f (x(t))+g (x(t))u dτ policies for three different systems and use the learned
k+1 k p p k
tk policies to construct tracking controllers as in [1, Theorem
F(xk,uk) 9.14]. In all cases, the input to the parameterized policy
R × R → R replaces all angles with their sine and cosine.
The map F : n q n will generally no
p ·
long·er be afﬁn−e in· the contr·ol. Simila−rly,·we let ξ() = A. Simulations
(y (),...,y(γ1 1)(),...,y (),...y(γq 1)()) denote the
1 1 q q 1) Double pendulum with polynomial policies: We ﬁrst
solution to the linearized portion of the state, and set ξ =
k test our approach on a fully actuated double pendulum with
ξ(t ) for each iterate. Now, if we integrate (10) over the
k state x = [θ ,θ ,ω ,ω ]T, output y = [θ ,θ ]T, where θ
interval [t ,t ) we obtain a difference equation for the 1 2 1 2 1 2 1
k k+1 and θ represent the angles of the two joints, with angular
outputs of the form 2
rates ω and ω respectively. The system has two inputs u
1 2 1
ξk+1 =eA∆tξk+H(xk,uk) (16) and u2 that control the torque at both joints. Although the
R ×R →R system is relatively low dimensional and fully actuated, it is
where again the mapping H: n q γ will generally highly nonlinear [36].
no longer be afﬁne in uk. This is the primary hurdle to For the learning problem, we give the nominal model
applying the theory developed in the previous section to the
inaccurate estimates for both the length and mass of each
current setting. Nevertheless, we can still attempt to enforce
(cid:82) of the arms. Speciﬁcally, we scale each of the parameters
anapproximatelinearrelationshipbetweenavirtualinputvk to 1/2 their true values. The learned controller is comprised
andsuccessiveiteratesoftheoutputs.LettingA¯=eA∆t and of 150 radial basis functions, which are centered randomly
B¯ = ∆teAtBdt, the sampled-data version of the reference throughout the state-space. The learned controller is linear
0
model (6) becomes
in its parameters, so as to agree with Theorem 1. We use
ξ =A¯ξ +B¯v , the REINFORCE algorithm [32], and baseline state-value
k+1 k k estimates with the average reward over all states. At each
which is the ideal linear behavior we would like to enforce iteration (or epoch) we collect 50 rollouts of 0.25 seconds
by applying the control u = uˆ (x ,v ). To encourage each, and we train for 500 epochs. Figure 2 (a) presents
k θ k k
actionswhichbettertrackthediscrete-timereferencemodel, the result of using our learned controller to track a desired
R ×R ×R →R
we apply the point-wise loss (cid:96)¯: n q q where sinusoidal trajectory, and compares it to the performance of
(cid:107) − (cid:107) theexactlinearizingcontrollerforthesystem.Thedifference
(cid:96)¯(x ,u ,v )= B¯v H(x ,u ) 2.
k k k k k k 2 between the performance our learned controller and the
We can calculate this quantity using (16) and by observing theoreticalidealobservedtobeverysmall.Wedonotplotthe
ξ and ξ , which can each be calculated by numerically trajectory generated by the nominal model-based controller,
k k+1
differentiating the outputs from the plant. This loss provides sinceitimmediatelydivergesfromthedesiredbehavior.Sim-
a measure of how well the control u enforces the desired ilar results were observed using the same learned controller
k
change in the state of the linear system (as speciﬁed by v ) to track other desired trajectories for the system. The linear
k
1368
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 12:31:13 UTC from IEEE Xplore.  Restrictions apply. Fig. 3: (a) The performance of our learned linearizing controller on two high-performance quadrotor tracking tasks when the parameters of the inital
dynamicsmodelarescaledbyafactorof0.6.Theﬁrsttaskisaﬁgure-eight,andthesecondisacorkscrewmaneuver.Inbothmaneuversthequadrotor
alsotracksanoscillatingreferenceintheyawangle.Inbothﬁgures,thegreentrajectoryistheonetakenbythelearnedcontrollerandthebluetrajectory
isthatofthetruelinearizingcontrollerforthesystem.Inbothcases,thelearnedcontrollercloselymatchesthedesiredbehavior.(b)Learningcurvesfor
learnedlinearizingcontrollerswhichareinstantiatedwithnominaldynamicsmodelswiththreelevelsofscalingofthetruedynamicsparameters.
feedbackmatrixusedinthetrackingcontrollerwasobtained This nominal controller suffers from several inaccuracies.
by solving an inﬁnite horizon LQR problem where the state First, Baxter’s actuators are series-elastic, meaning that each
deviation was penalized 10 times more than the magnitude joint contains a torsion spring [40] which is unmodeled, and
of the input. the URDF itself may not be perfectly accurate. Second, the
2) 14D quadrotor with neural network policies: Our OROCOS solver is numerical, which can lead to errors in
second simulation environment uses the quadrotor model computing the decoupling matrix and drift term. Finally, our
andfeedbacklinearizationcontrollerproposedin[37],which control architecture is implemented in the Robot Operating
makes use of dynamic extension [1]. In particular, the states System [41], which can lead to minor timing inconsistency.
for the model are (x,y,z,ψ,θ,ϕ,x˙,y˙,z˙,p,q,r,ξ,ζ) where We use the PPO algorithm to tune the parameters of a
×
x,yandz aretheCartesiancoordinatesofthequadrotor,and 128 2neuralnetworkwithtanhactivationsforthelearned
ψ,θandϕrepresenttheroll,pitchandyawofthequadrotor, component of the controller. For each training epoch, 1250
respectively.Thenextsixstatesrepresentthetimederivatives rollouts of one timestep (0.05 s) each were collected. We
of these state: d(x,y,z,ψ,θ,ϕ)=(x˙,y˙,z˙,p,q,r). Finally, trained for 100 epochs, which took 104 minutes. Figure 2
dt
ξ and ζ are the extra states obtained from the dynamic (b) summarizes typical results on tracking a square wave
extension procedure. The outputs for the model are the x, reference trajectory for each joint angle with period 5s. The
y, z and ψ coordinates. nominal feedback linearized model from OROCOS has sig-
The inaccurate nominal dynamics model was constructed niﬁcantsteady-stateerror.Ourlearnedapproachsigniﬁcantly
bymultiplyingthemassandinertialconstantsofthetruesys- reduces,butdoesnoteliminate,thiserror.Weconjecturethe
tembyfactorsof0.33,0.6and0.8fordifferentexperiments. remaining error is a sign that the (relatively small) neural
Thetrainedpolicieswerefeed-forwardneuralnetworkswith network may not be sufﬁciently expressive.
tanh activations with 2 hidden layers of width 64. For each
training epoch, 50 rollouts of length 25 were collected and V. DISCUSSION
the parameters were updated using PPO. We trained both While the methods we have presented avoid some of the
policies for 2500 epochs. Figure 3 (b) illustrates how a challenges model-based methods face when trying to con-
better initial dynamics model leads to faster learning of an struct a linearizing controller for an unknown plant, we feel
accurate linearizing controller. In particular, the reward-per- thetechniquespresentedhereshouldbeusedtocomplement
epoch is plotted for policies trained with the three scaling model-baseddesignapproachesratherthanreplacethem.As
factorsindicatedabove.Weobservethatworseinitialmodels we observed empirically, a reasonable (though ultimately
result in worse policy performance, given the same network inaccurate) model can be used to provide a better starting
architecture and training time. Figure 3 (a) demonstrates point for the learning process. Thus, we feel that in practice
the ability of the learned controller to overcome signiﬁcant our approach should be used primarily to overcome difﬁcult
model mismatch (scaling factor of 0.6) to match the desired to model non-linearities, and should be used in combination
linear tracking behavior. with a simple nominal dynamics models with parameters
which are easily identiﬁed. Investigating the performance
B. Robotic experiment: 7-DOF manipulator arm
of different learning algorithms in real-world settings merits
We also evaluate our approach in hardware, on a 7-DOF signiﬁcant future research effort. On the theoretical side, we
Baxter robot arm. The dynamics of this 14-dimensional feel the general way in which we constructed the optimiza-
system are extremely coupled and nonlinear. Taking the 7 tion problem in III-A provides a foundation for combining
joint angles as output y, however, the system is input-output model-free policy optimization techniques with geometric
linearizable with relative degree two. We use the system control architectures. Future work will investigate how to
measurements (i.e., masses, link lengths, etc.) provided with optimize over different classes of controllers by extending
Baxter’s pre-calibrated URDF [38] and the OROCOS Kine- the approach presented here.
matics and Dynamics Library (KDL) [39] to compute a
nominal feedback linearizing control law.
1369
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 12:31:13 UTC from IEEE Xplore.  Restrictions apply. REFERENCES back linearizable systems”. 1991 American Control
Conference. IEEE. 1991.
[1] S. Sastry. Nonlinear systems: analysis, stability, and
[20] J. Umlauft et al. “Feedback linearization using Gaus-
control. Vol. 10. Springer Science & Business Media,
sian processes”. 2017 IEEE 56th Annual Conference
1999.
on Decision and Control (CDC). IEEE. 2017.
[2] A. Isidori. Nonlinear control systems. Springer Sci-
[21] G. Chowdhary et al. “Bayesian nonparametric adap-
ence & Business Media, 2013.
tivecontrolusinggaussianprocesses”.IEEETransac-
[3] D. P. Bertsekas and J. N. Tsitsiklis. Neuro-dynamic
tions on Neural Networks and Learning Systems 26.3
programming. Athena Scientiﬁc, 1996.
(2014).
[4] R.S.SuttonandA.G.Barto.Reinforcementlearning:
[22] G. Chowdhary et al. “Bayesian nonparametric adap-
An introduction. 2018.
tive control of time-varying systems using Gaussian
[5] R. S. Sutton et al. “Policy gradient methods for
processes”.2013AmericanControlConference.IEEE.
reinforcement learning with function approximation”.
2013.
Advances in neural information processing systems.
[23] T.Westenbroeketal.“FeedbackLinearizationforUn-
2000.
known Systems via Reinforcement Learning”. arXiv
[6] J. Schulman et al. “Trust region policy optimization”.
preprint arXiv:1910.13272 (2019).
International conference on machine learning. 2015.
[24] J. T. Spooner and K. M. Passino. “Stable adaptive
[7] T. P. Lillicrap et al. “Continuous control with
control using fuzzy systems and neural networks”.
deep reinforcement learning”. arXiv preprint
IEEE Transactions on Fuzzy Systems 4.3 (1996).
arXiv:1509.02971 (2015).
[25] F.-C. Chen and H. K. Khalil. “Adaptive control of a
[8] J. Schulman et al. “Proximal policy optimization al-
class of nonlinear discrete-time systems using neural
gorithms”. arXiv preprint arXiv:1707.06347 (2017).
networks”. IEEE Transactions on Automatic Control
[9] P. Martin, R. M. Murray, and P. Rouchon. “Flat sys-
40.5 (1995).
tems, equivalence and trajectory generation” (2003).
[26] A. Yesildirek and F. L. Lewis. “Feedback lineariza-
[10] R. E. Kalman et al. “Contributions to the theory of
tion using neural networks”. Proceedings of 1994
optimal control”. Bol. soc. mat. mexicana 5.2 (1960).
IEEE International Conference on Neural Networks
[11] F. Borrelli, A. Bemporad, and M. Morari. Predictive
(ICNN’94). Vol. 4. IEEE. 1994.
control for linear and hybrid systems. Cambridge
[27] E.B.KosmatopoulosandP.A.Ioannou.“Aswitching
University Press, 2017.
adaptivecontrollerforfeedbacklinearizablesystems”.
[12] J. W. Grizzle, G. Abba, and F. Plestan. “Asymptot-
IEEE Transactions on automatic control 44.4 (1999).
ically stable walking for biped robots: Analysis via
[28] E. B. Kosmatopoulos and P. A. Ioannou. “Robust
systems with impulse effects”. IEEE Transactions on
switching adaptive control of multi-input nonlinear
automatic control 46.1 (2001).
systems”. IEEE transactions on automatic control
[13] A. D. Ames et al. “Rapidly exponentially stabilizing
47.4 (2002).
control lyapunov functions and hybrid zero dynam-
[29] C.P.BechlioulisandG.A.Rovithakis.“Robustadap-
ics”. IEEE Transactions on Automatic Control 59.4
tive control of feedback linearizable MIMO nonlinear
(2014).
systemswithprescribedperformance”.IEEETransac-
[14] D.MellingerandV.Kumar.“Minimumsnaptrajectory
tions on Automatic Control 53.9 (2008).
generation and control for quadrotors”. 2011 IEEE
[30] J. Umlauft and S. Hirche. “Feedback Linearization
InternationalConferenceonRoboticsandAutomation.
based on Gaussian Processes with event-triggered
IEEE. 2011.
Online Learning”. IEEE Transactions on Automatic
[15] S. Sastry and M. Bodson. Adaptive control: stabil-
Control (2019).
ity, convergence and robustness. Courier Corporation,
[31] J. Grizzle and P. Kokotovic. “Feedback linearization
1989.
of sampled-data systems”. IEEE Transactions on Au-
[16] J. J. Craig, P. Hsu, and S. S. Sastry. “Adaptive control
tomatic Control 33.9 (1988).
of mechanical manipulators”. The International Jour-
[32] R. S. Sutton and A. G. Barto. Introduction to Rein-
nal of Robotics Research 6.2 (1987).
forcement Learning. 1st. Cambridge, MA, USA: MIT
[17] S. S. Sastry and A. Isidori. “Adaptive control of lin-
Press, 1998.
earizable systems”. IEEE Transactions on Automatic
[33] J. Schulman et al. “Trust region policy optimization”.
Control 34.11 (1989).
InternationalConferenceonMachineLearning.2015.
[18] K. Nam and A. Araposthathis. “A model reference
[34] J. Schulman et al. “Proximal Policy Optimization
adaptive control scheme for pure-feedback nonlinear
Algorithms”. CoRR ().
systems”. IEEE Transactions on Automatic Control
[35] D. Silver et al. “Deterministic Policy Gradient Algo-
33.9 (1988).
rithms”.Proceedingsofthe31stInternationalConfer-
[19] I.Kanellakopoulos,P.V.Kokotovic,andA.S.Morse.
ence on Machine Learning. Proceedings of Machine
“Systematic design of adaptive controllers for feed-
Learning Research. 2014.
1370
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 12:31:13 UTC from IEEE Xplore.  Restrictions apply. [36] T. Shinbrot et al. “Chaos in a double pendulum”.
American Journal of Physics 60.6 (1992).
[37] S. A. Al-Hiddabi. “Quadrotor control using feedback
linearizationwithdynamicextension”.20096thInter-
nationalSymposiumonMechatronicsanditsApplica-
tions. IEEE. 2009.
[38] R. Robotics. “Baxter”. Retrieved Jan 10 (2013).
[39] H. Bruyninckx. “Open robot control software: the
OROCOS project”. Proceedings 2001 ICRA. IEEE
international conference on robotics and automation
(Cat. No. 01CH37164). Vol. 3. IEEE. 2001.
[40] R. L. Williams II. “Baxter Humanoid Robot
(cid:13)
Kinematicsc 2017 Dr. Bob Productions Robert L.
Williams II, Ph. D., williar4@ ohio. edu Mechanical
Engineering, Ohio University, April 2017” (2017).
[41] M. Quigley et al. “ROS: an Open-Source Robot Op-
erating System”. ICRA Workshop on Open Source
Software. 2009.
1371
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 12:31:13 UTC from IEEE Xplore.  Restrictions apply. 