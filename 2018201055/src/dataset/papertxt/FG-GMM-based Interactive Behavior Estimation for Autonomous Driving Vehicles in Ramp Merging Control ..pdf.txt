2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
360SD-Net: 360° Stereo Depth Estimation with Learnable Cost Volume
Ning-Hsu Wang1, Bolivar Solarte1, Yi-Hsuan Tsai3
Wei-Chen Chiu2, Min Sun1
1National Tsing Hua University, 2National Chiao Tung University, 3NEC Labs America
albert100121@gapp.nthu.edu.tw, enrique.solarte.pardo@gmail.com, wasidennis@gmail.com,
walon@cs.nctu.edu.tw, sunmin@ee.nthu.edu.tw
Abstract—Recently, end-to-end trainable deep neural net-
works have signiﬁcantly improved stereo depth estimation
for perspective images. However, 360° images captured un-
der equirectangular projection cannot beneﬁt from directly
adopting existing methods due to distortion introduced (i.e.,
lines in 3D are not projected onto lines in 2D). To tackle
this issue, we present a novel architecture speciﬁcally designed
for spherical disparity using the setting of top-bottom 360°
camera pairs. Moreover, we propose to mitigate the distortion
issue by (1) an additional input branch capturing the posi-
tion and relation of each pixel in the spherical coordinate,
and (2) a cost volume built upon a learnable shifting ﬁlter.
Due to the lack of 360° stereo data, we collect two 360°
stereodatasetsfromMatterport3DandStanford3Dfortraining
and evaluation. Extensive experiments and ablation study are
provided to validate our method against existing algorithms.
Finally, we show promising results on real-world environ-
mentscapturingimageswithtwoconsumer-levelcameras.Our
project page is at https://albert100121.github.io/
360SD-Net-Project-Page.
I. INTRODUCTION Fig.1:Sphericaldisparityunder(a)top-bottomcamerapairs
(P and P ) with baseline B. Panel (b)(c) show top and
top bottom
Stereodepthestimationisalong-lastingyetimportanttask
bottom equirectangular projections, respectively. P and P
t b
in computer vision due to numerous applications such as
are projection points from a 3D point onto the spherical
autonomous driving, 3D scene understanding, etc. Despite
surface (a) and equirectangular coordinate (b)(c). (cid:126)rt and
the majority of studies are for perspective images, disparity
(cid:126)rb are projection vectors for the top and bottom cameras,
can be deﬁned upon various forms of image pairs. For
respectively.θ andθ aretheanglesbetweenthesouthpole
instance, the human binocular disparity is deﬁned as the t b −
anditsrespectiveprojectionvector.d=θb θt istheangular
angledifferencebetweenthepointofprojectionontheretina,
disparity. In panel (b)(c), the 3D point projects to the same
which is part of a sphere rather than a plane. Similar to
◦ horizontal position but different vertical positions reﬂecting
humanvision,theangledifferenceofapairof360 cameras
the disparity.
with spherical projection can also be deﬁned as disparity
◦
(see Fig. 1(a)). By taking the advantage of 360 cameras
for having a complete observation in an environment, the conﬁguretwocamerasinatop-bottommanner,suchthatthe
stereodepthestimationobtainedfromthesecamerasenables epipolar lines on a pair of images are vertically aligned (see
the 3D reconstruction of the entire surrounding. This is a Fig.1).Second,pixelsnearthetopandbottomoftheimages
powerfuladvantageforadvancedapplications,e.g.,3Dscene are stretched more than those located around the equator
understanding. line. Hence, the corresponding patches at different vertical
In this paper, we aim to estimate stereo depth information locations are likely to have different visual characteristics
from a pair of equirectangular images (see Fig. 1(b)(c)), in due to different levels of distortion. This encourages us to
◦
which they are used in most consumer-level 360 cameras. propose a novel framework for learning correspondence in
For simplicity, we thereafter refer equirectangular images to top-bottom aligned equirectangular images.
◦
as360 images. Thecriticalissueneededtocopewithisthe We demonstrate the beneﬁt of each component through
severedistortionintroducedintheprocessofequirectangular extensive ablation study and compare the performance with
projection. First, horizontal lines in 3D are not always the deep-learning baselines (i.e., PSMNet [1] and GCNet [2])
◦
lines in 2D when we use 360 cameras. This implies that andconventionalstereomatchingapproaches(i.e.,ASW[3],
the typical conﬁguration of the left-right stereo rig may not Binocular [4], Kim’s [5]). The efﬁcacy of our full model
◦
preserve the same property of epipolar lines. Therefore, we is validated in improving depth estimation for 360 stereo
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 582
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 15:46:55 UTC from IEEE Xplore.  Restrictions apply. cameras on two synthetic datasets, as well as generalization concatenation with inner-product for cost aggregation on
to real-world images. The main contributions are as follows: deep features extracted from stereo pairs. Considering full-
• Proposetheﬁrstend-to-endtrainablenetworkforstereo trainable models, GCNet [2] proposes an end-to-end deep
◦
depth estimation using 360 images. network, which has a multi-scale 3D convolution module
• Developaseriesofimprovementsoverexistingmethods for producing a more robust disparity regression. Moreover,
to handle the distortion issue, including the usage of PSMNet [1] steps further to have spatial pyramid pooling
polar angle. for taking global context information into cost volume and
• Propose a novel learnable shifting ﬁlter for building the equipping the 3D convolution with a stack of hourglass
cost volume which is empirically better than standard network to achieve better disparity estimation. Despite the
pixel-shifting in the spherical projection. high performance of the mentioned approaches on stereo
◦
• Introduceour360 stereodatasetcollectedfromMatter- perspective views, they do not output desirable results using
port3D [6] and Stanford3D [7], composed of equirect- 360° images since properties such as distortion are not
angular pairs and depth/disparity ground truths. considered in their model design.
• Generalize to real-world environments using two
◦
consumer-level 360 cameras with a model trained on C. Vision Techniques for 360°Camera
the synthetic dataset.
◦
When the consumer-level 360 cameras were made easily
II. RELATEDWORK available and affordable, it attracts signiﬁcant research inter-
A. Classical Methods est from the computer vision and robotics communities. For
instance, Cohen et al. [16] and Esteves et al. [17] process
Prior to the recent advances of deep learning, numerous
spherical information on spectral-domain for classiﬁcation,
research efforts have been devoted to stereo matching and
whereas KTN [18] and Flat2sphere [19] focus on designing
depthestimation.Theseclassicalstereomatchingalgorithms
spherical convolution kernels such that the network can
can be roughly categorized into local and global methods.
◦
support multiple recognition tasks in 360 images. On the
In general, global methods (e.g., Semi-Global Matching
other hand, several works [20], [21], [22] leverage 360-
(SGM) [8]) are able to estimate a better disparity map, but
views to reconstruct layout scenes from equirectangular
they need to solve a complicated optimization problem. On
images as input. Similarly, [23], [24] address the problem
the other hand, local algorithms (e.g., Adaptive Support-
◦
of saliency detection in 360 videos for exploring the rich
Weightapproach(ASW)[3],[9]andWeightedGuidedImage
content of a scene in a more efﬁcient manner using the
Filtering (WGIF) [10]) are faster and widely used in many
full view of equirectangular representation and dealing with
embedded applications, but they suffer from the aperture
distortionproperly.Fordepthestimationpurposes,[25],[26],
problem or ambiguous matches on homogeneous regions.
◦
Regarding 360-view methods, Kang et al. [11] target at [27] tackle monocular depth estimation from 360 images
stereo 360° images on a cylinder projection, but do not via leveraging re-projection models, rendered scenes, and
consider a full 360-view (4π steradians). In addition, Im structures-from-motiontechniques.Recently,SweepNet[28]
et al. [12] tackle monocular 360° depth estimation using targets multi-view stereo depth estimation applying a deep
structure-from-motion and sphere sweeping algorithm. Al- networkonfourﬁsh-eyeimagesre-projectedintoconcentric
though this model leverages a spherical projection, it is virtual spheres to estimate 360° depths.
limited to handle short sequences with a high computational Despite the previous approaches, there exists a literature
cost. Similar to our setting, Li [4] presents a top-bottom gap in 360° stereo-depth estimation using convolutional net-
camera setting to deﬁne spherical disparity, while Kim et works. Therefore, we provide a novel deep network, which
al. [5] follow the same camera setting but with a PDE- reliesontwoequirectangularimagesasinputanddealswith
based regularization method to reﬁne the disparity results. distortioneffectively.Suchinputistheminimumrequirement
Although these methods tackle 360° stereo depth estimation for a stereo setup that keeps the beneﬁts of a full 360°
directlyonsphericalprojection,theystillencounterproblems view [5], [4]. Moreover, our proposed model is capable of
of ambiguous matches, artifacts, or diffused surfaces, where being applied directly by commercial-level 360° cameras,
we address them via designing a learning-based framework. making this solution highly affordable. To the best of our
knowledge, we are the ﬁrst to target at deep learning-based
B. Deep Learning-based Stereo Method
stereodepthestimationfrom360°images.Wenote[29],[30]
Recently, deep learning techniques achieve great progress as concurrent works with similar idea to our paper.
onstereodepthestimation.Thesetechniquescanbesumma-
rizedasaframeworkwithfourmaincomponents:(1)feature III. METHOD
extraction,(2)costaggregation,(3)costvolumeconstruction,
and (4) disparity optimization. For instance, Koch [13] and Theproposedframework,namely360SD-Net,investigates
Zbontar et al. [14] use a deep metric learning network (e.g., a unique stereo depth estimation pipeline for 360° cameras.
Siamese network) to focus on learning a feature represen- Weﬁrstintroduceourcamerasettinganddeﬁnethespherical
tation in order to obtain better matching cost. Furthermore, disparity. Then, we propose the end-to-end trainable model
Luo et al. [15] speed up the computation by replacing the as depicted in Fig. 2.
583
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 15:46:55 UTC from IEEE Xplore.  Restrictions apply. Fig.2:Ournetworkmainlyconsistsofthreeparts:a)two-branchfeatureextractorthatconcatenatesthestereoequirectangular
images and the polar angle in a late fusion setting, b) the ASPP module to enlarge the receptive ﬁeld, and c) the learnable
cost volume to account for the nonlinear spherical projection. Finally, we use the Stacked-Hourglass module to output the
ﬁnal disparity map.
A. Camera Setting and Spherical Disparity the polar angle (see Fig. 2(a)) as the model input for
additional geometry information since it is closely related
We use a top-bottom camera setting (similar to [4], [5]),
to the distortion. In order to separate geometry information
where the stereo correspondence lies on the same vertical
from the RGB appearance information, we apply residual
line on the camera spheres (see Fig. 1(a)). This setting also
blocks for RGB input and three Conv2D layers for polar
ensuresthatthecorrespondenceliesonthesameverticalline
angle instead of directly concatenating model input (i.e.,
in 360° images captured under equirectangular projection
early fusion design). Then, both outputs are concatenated
(see Fig. 1(b,c)). Our setting can be built with relatively low
after feature extraction, in which we refer to this procedure
costsincemostconsumer-level360°camerascaptureimages
as our late fusion design. The comparison of both designs is
under equirectangular projection.
We now deﬁne spherical disparity using the following shown in the experimental section.
terms (see Fig. 1).P and P are projection points froma 3D
t b C. ASPP Module
pointPontothecamerasphereofthetopandbottomcamera,
After fusing image features with the geometry infor-
respectively.(cid:126)rt and(cid:126)rb are projection vectors, while θt/θb are
mation, we still have to manage the spatial relationship
the angles between the south pole and(cid:126)rt/(cid:126)rb for the top and ◦
among pixels, since 360 images provide a larger ﬁeld-of-
bottom cameras, respectively. The disparity is deﬁned as the
view than regular images. In order to consider different
difference between the two angles with following equation
− scales spatially, we adopt recent advances ASPP [31] as
d=θb θt. The depth w(cid:20)ith respect to the t(cid:21)op camera equals
proposed for semantic segmentation (see Fig. 2(b)). This
to the norm of(cid:126)rt, which is computed as follows,
module is a dilated convolution design considering multi-
|(cid:126)rt|=B· stainn((θdt))+cos(θt) , (1) sIncaolerdreersotolutrieodnusceatthdeiflfaerrgeentmleevmeolsryocfotnhseumrepcteiopntivfeorﬁceoldst.
where B is the baseline between top and bottom cameras. volume-based stereo depth estimation, we perform random
Note that the disparity and depth relation is not ﬁxed as in cropping during training.
perspective stereocameras, but varies accordingto the angle
D. Learnable Cost Volume
θ.Hence,themeaningofdisparityestimationerrorbecomes
t
less intuitive. In practice, we mainly evaluate depth instead The following critical step for stereo matching is to
of disparity estimation. construct a 3D cost volume by computing the matching
costs at a pre-deﬁned disparity levels with a ﬁxed step-size.
B. Incorporation with Polar Angle
This step-size in a typical 3D cost volume is one pixel, i.e.,
As described in Section II, deep stereo depth estimation approaches like GCNet [2] and PSMNet [1] concatenate left
methods [1], [14], [2] disregard distortion introduced in and right features to construct 3D cost volume based on
equirectangular images. To address this problem, we add one-pixel step-size. However, with the distortion introduced
584
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 15:46:55 UTC from IEEE Xplore.  Restrictions apply. by equirectangular projection, per-pixel step-size is not con-
sistent with the geometry information from the polar angle
input. Under this premise, we introduce a novel learnable
cost volume (LCV) in our 360SD-Net using a shifting ﬁlter,
which searches the optimal step-size on “degree unit” in
order to precisely construct the optimal cost volume.
×
We design our LCV with a shifting ﬁlter via a 7 1
Conv2D layer, as shown in Fig. 2(c), and apply channel-
wise shifting with the proposed ﬁlter to prevent the mixture
between channels. This ﬁlter design allows vertical shifting
to satisfy our stereo setting and retains the full view of the Fig. 3: Our 360° stereo system composed of two Insta360®
equirectangularimages.Therefore,thebestshiftingstep-size ONE X cameras. In order to align both equirectangular
of the feature map would be learned by convolution. Note images (top and bottom), the extrinsic parameters between
that, we apply replicated-padding instead of zero-padding the cameras is needed. This transformation is obtained by
before each convolution to retain the boundary information. stereo calibration.
To ensure stable training in practice, we still follow the
normal cost volume shifting (freezing the parameters for the
shiftingConv2D)intheﬁrst50epochsandstartlearningthe setting described in Section III-A, we align the polar axis
cost volume shifting afterward. of both equirectangular images using the extrinsic transform
obtained by the calibration.
E. 3D Encoder-Decoder and Regression Loss
We adopt the stacked hourglass [1] as our 3D Encoder- B. Metrics
Decoder and the regression as in [2] to regress continuous
We have evaluated both depth and disparity results using
disparityvalues.Itisreportedthatthisdisparityregressionis
MAEandRMSE.Thedeptherrorispronetoincreasingsig-
morerobustthanclassiﬁcation-basedstereodepthestimation
niﬁcantlybasedonthenon-linearrelationshipbetweendepth
methods. For the loss function, we use the smooth L1 loss
and disparity as in (1), which does not provide informative
with the ground truth disparity.
evaluation. Therefore, we crop out 5% of largely distorted
depth map from the top and bottom, respectively.
IV. EXPERIMENTALRESULTS
A. Dataset and System Conﬁguration C. Experimental Setting
Due to the lack of 360° stereo dataset, we have col- Our model is trained from scratch with Adam (β1 =
lected two photo-realistic datasets MP3D and SF3D through 0.9,β2=0.999)solverfor400epochswithaninitiallearning
Matterport3D [6] with Minos virtual environment [32] and rateof0.001andﬁne-tunedwithalearningrateof0.0001for
re-projection of Stanford3D point clouds [7]. Considering 100epochsonMP3D.ForSF3D,wefollowthesamesetting
the complexity and the extensive effort required to stitch, as MP3D but with 50 epochs using pre-trained model from
calibrate, and collect real-world RGB images and depth MP3D. The entire implementation is based on the PyTorch
maps, which is not suitable for the training of deep models, framework.
we train our model solely on the presented synthetic data.
D. Overall Performance
The setting of our dataset is a pair of 360° top-bottom
aligned stereo images with equirectangular projection. The In Table I, we show results on MP3D and SF3D with
resolutionoftheseimagesis512inheightand1024inwidth, comparisons to state-of-the-art stereo depth estimation ap-
which is commonly used in 360° works [22], [24], [33]. proaches, including the conventional methods (ASW [3],
The baseline of our stereo system is set to 20 cm, and the Binocular [4] and KIM’s [5]) and deep learning-based mod-
number of data we have collected in MP3D/SF3D datasets els (PSMNet [1] and GCNet [2]). Our method achieves
for training, validation, and testing are 1602/800, 431/200, signiﬁcant improvement for both the disparity and depth
341/203,respectively.Eachdataconsistsoffourcomponents, performances,sinceothermethodsdonotconsiderdistortion
a RGB-image pair, depth, and disparity. For data collection, introduced in 360° images. These results demonstrate the
we have diversiﬁed indoor scenarios in each set to prevent effectiveness of our designs for 360° images, including
similarities and repetitiveness. Furthermore, the two datasets polar angle and LCV modules. In addition, compared to the
and code will be made available to the public. baselinePSMNetmodel,ourmethodonlyintroducesaslight
We also provide qualitative results on real-world scenes overhead in runtime, while our model outperforms PSMNet
to show the generalization of our model between synthetic by a large margin.
training and real-world testing. These real-world scenes are
E. Ablation Study
collectedwithtwowell-knownconsumer-level360°cameras,
Insta360® ONE X (Fig. 3). Both cameras are calibrated We present an ablation study in Table II on MP3D
using a 6x6 Aprilgrid and the toolbox calibration Kalibr, for depth estimation to validate the effectiveness of each
inparticular[34].Ontheotherhand,topreserveourcamera componentintheproposedframework.ComparingID1with
585
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 15:46:55 UTC from IEEE Xplore.  Restrictions apply. (cid:3)
TABLE I: Experimental results of the proposed method on MP3D and SF3D compared with other approaches including deep learning-
based networks and conventional algorithms.( represents the lower the better.)
MP3D SF3D
(cid:3) (cid:3) (cid:3) (cid:3) (cid:3) (cid:3) (cid:3) (cid:3)
Disparity Depth Time Disparity Depth Time
MAE RMSE MAE RMSE MAE RMSE MAE RMSE
Binocular[4] 0.7206 2.507 0.1368 0.5399 0.6333 0.3204 1.5494 0.0897 0.4496 0.6333
KIM’s[5] 0.8175 2.2956 0.2191 0.6955 1.8507 2.5327 4.39 0.1163 0.3972 1.8507
ASW[3] 0.4410 1.648 0.1427 0.5193 7.5min 0.2155 0.7754 0.0779 0.2628 7.5min
GCNet[2] 0.486 1.4283 0.0969 0.2953 1.54s 0.1877 0.4971 0.0592 0.1361 1.57s
PSMNet[1] 0.3139 1.049 0.0946 0.2838 0.50s 0.1292 0.4053 0.0418 0.1068 0.51s
360SD-Net(Ours) 0.1447 0.6930 0.0593 0.2182 0.572s 0.1034 0.3691 0.0335 0.0914 0.55s
TABLE II: Ablation study for depth estimation on MP3D. The
ﬁrst row bs is considered as the baseline in this study, which uses
a ﬁxed step-size vertical pixel shifting.. Different components are
denoted as: (Pc) Polar angle with early fusion; (Pb) Polar angle
with late fusion; (ASPP) ASPP module; (LCV) Learnable Cost
Volume; (repli) LCV with replicate padding. (cid:3)
ID AblationStudy DepthRMSE
1 bs 0.2765
2 bs+Pc(TableIIICoordinateID2) 0.2501
3 bs+Pb 0.2494(+9.8%)
4 bs+Pb+ASPP 0.2462(+10.9%)
5 LCV(TableIIIStepSizeID3) 0.2464
6 LCV(repli) 0.2409(+12.9%)
7 (Ours)bs+Pb+ASPP+LCV(repli) 0.2182(+21.1%) Fig. 5: Qualitative point cloud comparison between 360SD-
Net (Ours) and the PSMNet. Our model shows a better
geometryestimationwithlessdistortionandamoreaccurate
structure.
TABLE III: Ablation study of different coordinate information
added and different initial step-size of LCV for depth estimation
on MP3D. (cid:3) (cid:3) 5, it shows the effectiveness of LCV, while ID 2 shows the
ID Coordinate DepthRMSE StepSize DepthRMSE beneﬁtsfromthepolarangle.Theotherrowsgraduallyshow
1 horizontalangle 0.2583 1° 0.2611 the improvement of adding other designs such as ASPP and
2 polarangle 0.2501 1/2° 0.2559
3 radius 0.2541 1/3° 0.2464 replicated-padding. With the combination of ID 4 and 6, we
4 arc-length 0.2516 1/4° 0.2503 form our ﬁnal network that achieves the best performance.
5 area 0.2513 - -
Detailed ablation studies on polar angle and LCV are
shown in Table III, which compares different geometry
measurements from spherical projection and different initial
step-sizes in degree applied in LCV. Through comparing
various geometry measurements, including area, arc-length,
radius, and horizontal angle, using the polar angle performs
thebestindealingwithdistortion.Regardinginitialstep-size
in LCV, we demonstrate empirically that the performance
increases when the initial step-size value decreases. The
improvementssaturateat 1°,whichischosentobeourinitial
3
step-size value.
F. Qualitative Results
We present qualitative results in depth maps and point
clouds,mainlycomparedwithPSMNet[1]basedonitsgood
Fig. 4: Qualitative depth map and error map comparison performance in Table I. As shown in Fig. 4 and Fig. 5,
between 360SD-Net (Ours) and PSMNet. Our depth map our model results in sharper depth maps and our projected
shows sharper and clearer details in both close and distant point clouds are able to reconstruct scenes more accurately
regions. For the zoom-in views, the armchair and table in comparison to PSMNet. Furthermore, Fig. 6 shows more
present a notable geometry structure compared to the one qualitative results on both datasets of our model.
from PSMNet. Our error map also shows higher accuracy in
G. Qualitative Results for Real-World Images
regions with higher distortion and object boundaries.
To show the generalization of our model (trained on the
synthetic MP3D dataset) on real-world scenes, we take still
586
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 15:46:55 UTC from IEEE Xplore.  Restrictions apply. Fig. 6: More qualitative results for depth map on MP3D and SF3D. For MP3D, our estimated depth maps preserve object
andsurfacedetailswithresultssimilartoGT.ForSF3D,ourmodeloutputsdensedepthmapsofhighaccuracy,withtraining
on sparse GT.
Fig. 7: Qualitative results on real scenes using two Insta360® ONE X cameras in a top-bottom conﬁguration. The furniture
can be clearly seen in the depth maps and also well reconstructed in the point clouds.
and moving images with a pair of well-known consumer- directly on 360° stereo images via designing a series of
level 360° cameras. In order to reduce the domain gap, we improvements over existing methods. In experiments, we
applyourmodelonthesereal-worldimagesusinggray-scale. show state-of-the-art performance on our collected synthetic
In Fig. 7, we show the results in depth maps, frontal view, datasetswithextensiveablationstudythatvalidatesproposed
and perspective view of point clouds with their regarded modules, including the usage of polar angle and learnable
RGB images. The details of objects and room layouts are costvolumedesign.Finally,wetestonreal-worldscenesand
elegantly reconstructed, which shows great compatibility present promising results with the model trained on the pure
of our network between synthetic and real-world scenes. synthetic data to show the generalization and compatibility
Moreover, our model produces promising depth maps for of our presented network.
handheld videos (refer to supplementary video for more
results). Acknowledgement.ThisprojectissupportedbytheNational
Center for High-performance Computing, MOST Joint Re-
V. CONCLUSIONS search Center for AI Technology and All Vista Healthcare
In this paper, we introduce the ﬁrst end-to-end trainable with program MOST 108-2634-F-007-006 and MOST 109-
deep network, namely 360SD-Net, for depth estimation 2634-F-007-016.
587
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 15:46:55 UTC from IEEE Xplore.  Restrictions apply. REFERENCES [23] Z. Zhang, Y. Xu, J. Yu, and S. Gao, “Saliency detection in 360°
videos,” in The European Conference on Computer Vision (ECCV),
[1] J.-R. Chang and Y.-S. Chen, “Pyramid stereo matching network,” in September2018.
ProceedingsoftheIEEEConferenceonComputerVisionandPattern [24] H.-T. Cheng, C.-H. Chao, J.-D. Dong, H.-K. Wen, T.-L. Liu, and
Recognition,2018,pp.5410–5418. M.Sun,“Cubepaddingforweakly-supervisedsaliencypredictionin
[2] A. Kendall, H. Martirosyan, S. Dasgupta, P. Henry, R. Kennedy, 360°videos,”inTheIEEEConferenceonComputerVisionandPattern
A. Bachrach, and A. Bry, “End-to-end learning of geometry and Recognition(CVPR),June2018.
contextfordeepstereoregression,”inProceedingsoftheInternational [25] N. Zioulis, A. Karakottas, D. Zarpalas, and P. Daras, “Omnidepth:
ConferenceonComputerVision(ICCV),2017. Dense depth estimation for indoors spherical panoramas,” in The
[3] K. J. Yoon and I. S. Kweon, “Adaptive support-weight approach for EuropeanConferenceonComputerVision(ECCV),September2018.
correspondence search,” IEEE Transactions on Pattern Analysis and [26] F. Wang, H. Hu, H. Cheng, J. Lin, S. Yang, M. Shih, H. Chu, and
MachineIntelligence,vol.28,2006. M.Sun,“Self-supervisedlearningofdepthandcameramotionfrom
[4] S. Li, “Binocular spherical stereo,” IEEE Transactions on intelligent 360°videos,”CoRR,vol.abs/1811.05304,2018.[Online].Available:
transportationsystems,vol.9,no.4,pp.589–600,2008. http://arxiv.org/abs/1811.05304
[5] H.KimandA.Hilton,“3dscenereconstructionfrommultiplespher- [27] G. Payen de La Garanderie, A. Atapour Abarghouei, and T. P.
icalstereopairs,”InternationalJournalofComputerVision,vol.104, Breckon, “Eliminating the blind spot: Adapting 3d object detection
082013. andmonoculardepthestimationto360°panoramicimagery,”inThe
[6] A.Chang,A.Dai,T.Funkhouser,M.Halber,M.Niessner,M.Savva, EuropeanConferenceonComputerVision(ECCV),September2018.
S.Song,A.Zeng,andY.Zhang,“Matterport3D:LearningfromRGB- [28] C.Won,J.Ryu,andJ.Lim,“Sweepnet:Wide-baselineomnidirectional
D data in indoor environments,” International Conference on 3D depthestimation,”in2019InternationalConferenceonRoboticsand
Vision(3DV),2017. Automation(ICRA),May2019,pp.6073–6079.
[7] I. Armeni, A. Sax, A. R. Zamir, and S. Savarese, “Joint 2D-3D- [29] M. Eder, P. Moulon, and L. Guan, “Pano popups: Indoor 3d
SemanticDataforIndoorSceneUnderstanding,”ArXive-prints,Feb. reconstruction with a plane-aware network,” 2019 International
2017. Conference on 3D Vision (3DV), Sep 2019. [Online]. Available:
[8] C.Banz,S.Hesselbarth,H.Flatt,H.Blume,andP.Pirsch,“Real-time http://dx.doi.org/10.1109/3DV.2019.00018
stereovisionsystemusingsemi-globalmatchingdisparityestimation: [30] N. Zioulis, A. Karakottas, D. Zarpalas, F. Alvarez, and P. Daras,
ArchitectureandFPGA-implementation,”in2010InternationalCon- “Sphericalviewsynthesisforself-supervised360°depthestimation,”
ferenceonEmbeddedComputerSystems:Architectures,Modelingand 2019 International Conference on 3D Vision (3DV), Sep 2019.
Simulation,2010. [Online].Available:http://dx.doi.org/10.1109/3DV.2019.00081
[9] G.S.HongandB.G.Kim,“Alocalstereomatchingalgorithmbased [31] L.-C.Chen,G.Papandreou,I.Kokkinos,K.Murphy,andA.L.Yuille,
on weighted guided image ﬁltering for improving the generation of “Semanticimagesegmentationwithdeepconvolutionalnetsandfully
depthrangeimages,”Displays,2017. connectedcrfs,”arXivpreprintarXiv:1412.7062,2014.
[10] R.A.Hamzah,M.S.Hamid,A.F.Kadmin,S.F.A.Gani,S.Salam, [32] M.Savva,A.X.Chang,A.Dosovitskiy,T.Funkhouser,andV.Koltun,
and T. M. Wook, “Accurate Disparity Map Estimation Based on “MINOS: Multimodal indoor simulator for navigation in complex
Edge-preserving Filter,” in 2018 International Conference on Smart environments,”arXiv:1712.03931,2017.
Computing and Electronic Enterprise, ICSCEE 2018. IEEE, 2018. [33] F.-E.Wang,H.-N.Hu,H.-T.Cheng,J.-T.Lin,S.-T.Yang,M.-L.Shih,
[Online].Available:https://ieeexplore.ieee.org/document/8538360/ H.-K.Chu,andM.Sun,“Self-supervisedlearningofdepthandcamera
[11] S.B.KangandR.Szeliski,“3-dscenedatarecoveryusingomnidirec- motion from 360°videos,” in Asian Conference on Computer Vision.
tionalmultibaselinestereo,”Internationaljournalofcomputervision, Springer,2018,pp.53–68.
vol.25,no.2,pp.167–183,1997. [34] J.KannalaandS.S.Brandt,“Agenericcameramodelandcalibration
method for conventional, wide-angle, and ﬁsh-eye lenses,” IEEE
[12] S.Im,H.Ha,F.Rameau,H.-G.Jeon,G.Choe,andI.S.Kweon,“All-
transactions on pattern analysis and machine intelligence, vol. 28,
arounddepthfromsmallmotionwithasphericalpanoramiccamera,”
no.8,pp.1335–1340,2006.
in European Conference on Computer Vision. Springer, 2016, pp.
156–172.
[13] G.Koch,“Siameseneuralnetworksforone-shotimagerecognition,”
2015.
[14] J.ZbontarandY.LeCun,“Stereomatchingbytrainingaconvolutional
neural network to compare image patches,” Journal of Machine
LearningResearch,vol.17,pp.1–32,2016.
[15] W.Luo,A.G.Schwing,andR.Urtasun,“Efﬁcientdeeplearningfor
stereomatching,”inProceedingsoftheIEEEConferenceonComputer
VisionandPatternRecognition,2016,pp.5695–5703.
[16] T. S. Cohen, M. Geiger, J. Ko¨hler, and M. Welling, “Spherical
cnns,” CoRR, vol. abs/1801.10130, 2018. [Online]. Available:
http://arxiv.org/abs/1801.10130
[17] C. Esteves, C. Allen-Blanchette, A. Makadia, and K. Daniilidis,
“Learning so (3) equivariant representations with spherical cnns,” in
ProceedingsoftheEuropeanConferenceonComputerVision(ECCV),
2018,pp.52–68.
[18] Y. Su and K. Grauman, “Kernel transformer networks for compact
spherical convolution,” CoRR, vol. abs/1812.03115, 2018. [Online].
Available:http://arxiv.org/abs/1812.03115
[19] ——, “Flat2sphere: Learning spherical convolution for fast features
from 360° imagery,” CoRR, vol. abs/1708.00919, 2017. [Online].
Available:http://arxiv.org/abs/1708.00919
[20] C.Zou,A.Colburn,Q.Shan,andD.Hoiem,“Layoutnet:Reconstruct-
ingthe3droomlayoutfromasinglergbimage,”inCVPR,2018.
[21] S.-T. Yang, F.-E. Wang, C.-H. Peng, P. Wonka, M. Sun, and H.-K.
Chu,“Dula-net:Adual-projectionnetworkforestimatingroomlayouts
fromasinglergbpanorama,”inProceedingsoftheIEEEConference
onComputerVisionandPatternRecognition,2019,pp.3363–3372.
[22] C.Sun,C.-W.Hsiao,M.Sun,andH.-T.Chen,“Horizonnet:Learning
room layout with 1d representation and pano stretch data augmen-
tation,” in The IEEE Conference on Computer Vision and Pattern
Recognition(CVPR),June2019.
588
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 15:46:55 UTC from IEEE Xplore.  Restrictions apply. 