2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
Intensity Scan Context: Coding Intensity and Geometry Relations for
Loop Closure Detection
Han Wang, Chen Wang, and Lihua Xie
Abstract—Loop closure detection is an essential and chal-
lenging problem in simultaneous localization and mapping
(SLAM). It is often tackled with light detection and ranging
(LiDAR) sensor due to its view-point and illumination invari-
ant properties. Existing works on 3D loop closure detection
often leverage on matching of local or global geometrical-only
descriptorswhichdiscardintensityreading.Inthispaperweex-
ploretheintensitypropertyfromLiDARscanandshowthatit
canbeeffectiveforplacerecognition.Weproposeanovelglobal
descriptor, intensity scan context (ISC), that explores both
geometryandintensitycharacteristics.Toimprovetheefﬁciency
forloopclosuredetection,anefﬁcienttwo-stagehierarchicalre-
identiﬁcation process is proposed, including binary-operation
based fast geometric relation retrieval and intensity structure Fig. 1: Example of loop closure detected from KITTI se-
re-identiﬁcation. Thorough experiments including both local quence 02. The scenario is challenging due to reverse visit.
experiment and public datasets test have been conducted to Our proposed intensity scan context on the right images
evaluatetheperformanceoftheproposedmethod.Ourmethod
shows high similarity identiﬁes loop closure.
achieves better recall rate and recall precision than existing
geometric-only methods.
I. INTRODUCTION
distance) and instrument effects (e.g., transmitted energy)
Loop closure detection, aka place recognition, typically
[6]. However, the intensity channel reveals the reﬂectance
refers to the capability of identifying a re-visited place. In a
structure of surrounding environment, e.g., retro-reﬂective
SLAM system, the estimation of robot’s state and trajectory
material such as metal plate usually returns high value
oftencomeswithinevitabledriftthatfailslocalizationeasily
and concrete returns low value. This information is often
[1]. By identifying loop pairs, a robot can eliminate drifting
unique for different places. Moreover, recent works have
error.Moreimportantly,itcanalsopreventmultipleregistra-
shown some preliminary results that intensity reading can
tion of identical landmarks so that a globally consistent map
be effective for place recognition [7], [8].
can be created. A revisited place usually varies from light
In this paper, we propose a novel global descriptor, in-
illumination, weather or viewing angle, which is difﬁcult to
tensity scan context (ISC), that integrates both geometry
besolvedinvisionbasedplacerecognition.However,LiDAR
and intensity characteristics for loop closure detection. We
is less affected by such environmental changes and hence is
ﬁrst explain how intensity information can be distinguish-
widely used for place recognition in the recent years.
able to represent a place. Then we propose intensity scan
Existing works on place recognition often leverage on
context as a global signature for place recognition. To
matching of 3D descriptors such as fast point feature
further improve the efﬁciency for loop closure detection,
histogram (FPFH) [2], fast laser interest region transform
a two-stage hierarchical place re-identiﬁcation strategy is
(FLIRT) [3] and signature of histograms of orientations
proposed, including binary-operation based fast geometry
(SHOT) [4]. These descriptors explore either global or local
retrieval and intensity structure matching. To evaluate the
geometry information such as surface normal or neighbour
performance of the proposed intensity scan context, our
points distribution, leaving the intensity information unused.
method is tested under different scenarios including outdoor
A main justiﬁcation is that intensity information is less
autonomous driving and indoor warehouse robot navigation.
straightforwardthangeometryreading[5],sinceitisaffected
Theresultsshowthattheproposedapproachachieveshigher
by not only target surface characteristics (e.g., roughness,
recall rate and recall rate than existing geometric-only loop
surface reﬂectance), but also acquisition geometry (e.g.,
closure detection methods.
The main contributions of this paper are as follows:
The work is supported by Delta-NTU Corporate Laboratory for Cyber-
Physical Systems under the National Research Foundation Corporate Lab
• We propose a novel global descriptor for 3D LiDAR
@UniversityScheme.
HanWangandLihuaXiearewiththeSchoolofElectricalandElectronic scan that integrates both geometry and intensity char-
Engineering,NanyangTechno{logicalUniversity,50Nan}yangAvenue,Sin- acteristics.
gapore639798.e-mail: wang.han,elhxie @ntu.edu.sg
• An efﬁcient loop closure detection strategy based on
Chen Wang is with the Robotics Institute, Carnegie Mellon University,
Pittsburgh,PA15213,USA.e-mail: chenwang@dr.com a two-stage hierarchical Intensity Scan Context re-
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 2095
Authorized licensed use limited to: Heriot-Watt University. Downloaded on September 22,2020 at 01:08:50 UTC from IEEE Xplore.  Restrictions apply. identiﬁcationisproposed.Itonlycosts1.2msperquery
on average.
• A thorough evaluation on the proposed descriptor, in-
cluding both local experiment and public datasets test,
is conducted.
This paper is organized as follows: Section II reviews the
related works on both vision based and LiDAR based ap-
proachesforloopclosuredetection.SectionIIIdescribesthe
idea of using Intensity Scan Context for place recognition,
followedbythetwo-stagehierarchicalplacere-identiﬁcation.
Section IV shows experiment results and comparison with (a) Intensity scan reading of a crossroad.
existing works, followed by conclusion in Section V.
II. RELATEDWORK
Existing works on loop closure detection consist of vision
based methods and LiDAR based methods according to
different perception systems used. Vision based approaches
aredevelopedforplacerecognitionintheearlystage.Those
methods often leverage on bag of words model (BoW) that
(b) Camera view of the same place.
measures the distance of visual words according to a pre-
trained visual vocabulary, e.g., FAB-MAP [9] and DBoW2 Fig. 2: An example of intensity reading from KITTI dataset.
[10]. They are widely used in visual SLAM such as ORB The relationship of intensity information and landmarks are
SLAM [11] and LDSO [12]. However, image stream is not highlighted with red rectangles.
resistant to light illumination or view-point so that vision
based place recognition is not robust enough in practice.
The global descriptors are more efﬁcient but the perfor-
Recently some works target to solve environmental changes
mance is not competitive. However, recent works ﬁnd that
suchas[13],[14],butarelimitedtosomespeciﬁcscenarios.
the performance can be improved by integrating intensity
In comparison, LiDAR detection is more robust due to characteristics. Guo et al. justify that intensity informa-
its illumination and view-point invariant properties and is
tion can be distinctive for places and propose a novel
subsequently introduced for loop closure detection. Existing
local descriptor called intensity signature of histograms of
works on LiDAR based place recognition strive for an efﬁ-
orientations (ISHOT) that consists of both geometry and
cient local descriptor or place signature that can accurately
intensity information. The place re-identiﬁcation is solved
and concisely present a place. One of the most popular
by a probabilistic voting strategy similar to [16]. Despite
local descriptors is fast point feature histogram (FPFH) [2]
computationally expensive, the new descriptor outperforms
which explores the local surface normal of each neighbour
geometrical-only descriptors in place recognition. This in-
point. It is effective in estimating afﬁne transform between
spiresourworkonﬁndingamoreefﬁcientandmoreaccurate
two point clouds and is used for place recognition in the
descriptor for loop closure detection.
later work such as [15]. Bosse et al. propose a probabilistic
voting approach based on Gasalt3D descriptor [16]. Despite III. METHODOLOGY
the good performance achieved, the point cloud retrival
Inthissection,theproposedmethodisdescribedindetail.
is inefﬁcient due to the high dimension of the proposed
We ﬁrstly present the concept of intensity scan and brieﬂy
descriptor.
explainonhowintensitycharacteristicscanbeusedforplace
Re-identiﬁcation on the local descriptor usually requires
recognition. Then we present the idea of using intensity
key-point extraction and massive local geometry calcula-
scan context as a global 3D descriptor. Furthermore, an
tion. In comparison, matching on global descriptor is more
efﬁcient two-stage hierarchical intensity scan context based
efﬁcient in place recognition. Rizzini introduces a novel
retrieval is introduced, consisting of fast binary-operation
descriptor named GLAROT that encodes the relative geo-
based geometry retrieval and intensity structure matching.
metric position of key-point pairs into a histogram [17]. The
experiment result achieves very satisfactory recall precision
A. Intensity Calibration and Pre-processing
and recall rate. However, building key-point relation is still
computationally expensive. Kim et al. propose scan context LiDARperceives theenvironment byemittingand receiv-
which projects laser scan into global descriptor. The match- ing laser beam. Generally, the distance value is measured
ingofscancontextonlyrequireselement-wisemultiplication by traveling time while the surface reﬂectance can be esti-
so that the query speed is fast. However, the matching matedbyreturnedenergylevel(i.e.,intensity).Theintensity
precisionisnothighenoughandfalsepositiveoccursduring reading reveals surrounding surface reﬂectance structure.
public datasets test. Existing works on LiDAR have shown that the returned
2096
Authorized licensed use limited to: Heriot-Watt University. Downloaded on September 22,2020 at 01:08:50 UTC from IEEE Xplore.  Restrictions apply. Fig. 3: A visual illustration of the proposed intensity scan context. Left ﬁgure: original point cloud is decomposed into
subspace based on geometry characteristics. Right ﬁgure: derived intensity scan context by intensity projection on the
subspace.
intensity readings vary for different objects [6], e.g., retro- Hence to increase the computational efﬁciency, the inten-
reﬂective material such as metal plate usually returns high sity information is interpreted as global descriptor in this
value and concrete returns low value. In Fig. 2, we show work. Inspired by scan context [21], [22] and shape context
an example from KITTI dataset [18] for demonstration. [23], we introduce intensity scan context that can efﬁciently
The point cloud and image present the same place and integrate both geometry and intensity characteristics into a
the intensity structure is interpreted as color in the ﬁrst global signature.
image. We pick 3 landmarks including car, road sign and Denote the intensity reading η, geometry reading [x,y,z],
P
building respectively and highlight them with red rectangles number of points n, a LiDAR scan is deﬁned as =
{ ··· }
in both Fig. 2 (a) and Fig. 2 (b). It is observed that the road p ,p , ,p with each point p = [x ,y ,z ,η ]
1 2 n k k k k k
sign is highly distinguishable with low energy loss while in the local Cartesian coordinate. Each point p can be
k
the building structure (concrete) returns medium intensity. converted into polar coordinate, but only in x-y plane, so
Moreover, the reﬂectance is consistent within object. that (cid:112)
However,theintensitychannelisnoisysinceitisaffected p =[ρ ,θ ,z ,η ],
k k k k k
by not only target surface characteristics (e.g., roughness,
surface reﬂectance), but also acquisition geometry (e.g., ρk = xk2+yk2, (2)
y
distance) and instrument effects (e.g., transmitted energy) θ =arctan k.
k x
[6]. Hence calibration is necessary in order to reduce the k
disturbancebyotherfactors.Similarto[19],[8],inthiswork The point cloud is then segmented by equally diving polar
wecalibratetheintensityreadingηr withamappingfunction coordinateinazimuthalandradialdirectionsintoNs sectors
ϕ: and Nr rings. Each segment is represented by:
ηcal =ϕ(ηr,d), (1) S { ∈P|i·L ≤ (i+1)·L
= p max ρ < max,
where d is the distance reading. The mapping function ϕ ij k · Nr k ·Nr (3)
describes the inﬂuence of distance on the received energy j 2π − ≤ (j+1) 2π − }
π θ < π ,
and can be collected based on ofﬂine experiment. Besides N k N
s s
remapping, the intensity return from 3D LiDAR is an eight- ∈ | | ∈ | | | |
where i [1,N ], j [1,N ] and the symbol [1,N ]
byteinteger,e.g.,VelodyneVLP-16.Itisre-scaledinto[0,1] { ·s·· } r
represents 1,2, ,N . The point cloud is then divided
as a ﬂoat number for convenience. ×
into N N subspace. As discussed before, the intensity
In the pratical applications, some pre-processing of the s r
reading is often consistent for the same object. Since the
LiDAR scan is necessary in order to remove the redun-
each subspace is much smaller than whole point cloud, we
dant information. Based on observation, the LiDAR noise
canassumethattheintensityreadingdoesnotvarytoomuch.
increases with distance. Therefore, the LiDAR reading is
Henceforeachsubspace,acodingfunctionκcanbeapplied
ﬁrstlyﬁlteredbysettingadistancethresholdLmaxtoremove to reduce the intensity dimension. It is deﬁned as:
unreliable points. Moreover, the ground points are often not
S
signiﬁcant for place recognition so that they are optimized η =κ( )
ij ij
by the method similar to [20] in advance. (4)
= max η .
∈S k
pk ij
B. Intensity Scan Context S ∈∅
Note that if (i.e., no scan data), η =0. Up to this
ij ij
Extraction of local descriptors is often computationally point, we can generate the intensity scan context Ω by:
expensive since we need to identify local geometry char-
acteristics such as local norm for every single key-point. Ω(i,j)=η . (5)
ij
2097
Authorized licensed use limited to: Heriot-Watt University. Downloaded on September 22,2020 at 01:08:50 UTC from IEEE Xplore.  Restrictions apply. The global signature Ω is a 2D matrix that reveals both
geometry and intensity distribution of the environment. We
pick a LiDAR scan from KITTI dataset [18] for illustration
in Fig. 3. The left ﬁgure is the point cloud from top down
viewwiththesubpointcloudspaceS markedinredcolor.
ij
Therightﬁgureisthecodedintensityscancontextwitheach
pixel calculated by intensity coding function.
C. Place Re-identiﬁcation
P
Place recognition targets to match current place with
n
the previously visited places from the historical database
D {P P ··· P }
= , , , − . As more places visited, the scale
1 D2 n 1
of database inevitably increases so that the computational
cost grows accordingly. To reduce the computational cost,
in this section we propose a two-stage hierarchical intensity
scan context retrieval strategy that makes use of fast binary
operation to speed up the process of place re-identiﬁcation.
1) Fast Geometry Re-identiﬁcation: Most of 3D descrip-
tors are histogram based such as unique shape context
(USC) [4], ISHOT [8], etc. Matching between histograms
can be slow in practice since mathematical operation is
Fig. 4: Autonomous warehouse robot platform used for our
inevitable, e.g., multiplication of ﬂoat numbers. In compar-
experiment.
ison, binary operation (or logical operation) achieves much
faster speed than those mathematical operation. Inspired by
[24], we introduce an efﬁcient binary operation geometry
2) IntensityStructureMatching: Thesecondstagemainly
re-identiﬁcation for fast indexing. Given an intensity scan
identiﬁes the intensity similarity between two intensity scan
context Ω, its geometry(cid:40)distribution onIthe local coordinate context Ωq and Ωc by column-wise comparison. Let vq and
can be represented as a binary matrix : i
vc be the ith column of Ωq and Ωc, the score can be found
i
by taking cosine distance: (cid:88)
I false, if Ω(x,y)=0
(x,y)= (6)
true, otherwise 1 Ns−1 vq·vc
ϕ (Ωq,Ωc)= ((cid:107) (cid:107)i ·(cid:107)i (cid:107)). (9)
For a query intensity scan context Ωq, a candidaIte inItensity i Ns i=0 viq vic
scan context Ωc and their binary transform q, c, the
Similar to geometry matching, we have to correct viewing
geometry similarity can be derived as:
angle change. Since the viewing angle change k is already
I I
I I XOR( q, c) identiﬁed from geometry re-identiﬁcation, we compare Ωq
ϕg( q, c)= |Iq| , (7) and Ωc where Ωq is Ωq shifted by kth column. This alsok
k
signiﬁcantly reduces the computational cost because com-
| |
where x isthetotalnumberofelementsinxandXOR(x,y) parison of intensity involves mathematical operation rather
refers to element-wise exclusive OR operation between ma- than logical operation. The ﬁnal score is calculated as:
trix x and y. Because the column vector in intensity scan
context represents azimuthal direction, the rotation of laser Φ (Ωq,Ωc)=ϕ (Ωq,Ωc). (10)
i i k
scan becomes column shift in intensity scan context [21].
The unmatched pair can be also ﬁltered out by setting an
Hence for place recognition, the viewing angle change can
beinterpretedascolumn-shiftsofΩ.Therefore,todetectthe empirically determined threshold i.
view-point change, the ﬁnal score is calculated by:
D. Consistency Veriﬁcation
I I I I
Φg( q, c)= ∈m| ax | ϕg( iq, c), (8) As discussed in previous section, global descriptor is
i [1,Ns] a highly simpliﬁed representation of original point cloud.
I I
where q is q shifted by ith column. In the meantime, Hence it is inevitable to have some features ignored, which
i I
we can identify the best matched q with column-shifts of can lead to false positive. Therefore it is necessary to check
k
k that can be used to correct viewing angle change. The consistency before closing the loop.
unmatched pair can be ﬁltered out by setting an empirically 1) Temporal consistency check: In a SLAM system, it is
determined threshold  . In experiments we ﬁnd that binary observed that the occurrence of single loop closure often
g
matching only costs 0.5 ms on a desktop computer that is implies high similarity on the neighbour LiDAR scans since
very computationally efﬁcient. thesensorfeedbackiscontinuousintime[25].Wecanverify
2098
Authorized licensed use limited to: Heriot-Watt University. Downloaded on September 22,2020 at 01:08:50 UTC from IEEE Xplore.  Restrictions apply. (a) Loop not detected with visual place recognition. (b) Loop identiﬁed with the proposed approach.
Fig. 5: Local experiment result of the proposed algorithm.
the loop closure b(cid:88)y measuring the temporal consistency: pointcloudlibrary(PCL)[27]andtherobottrajectory(front-
end SLAM) are collected from the fusion of both wheel
P P 1 N I I odometer and LiDAR odometer. To illustrate, we simulate a
P( , )= (Φ ( − , − )+Φ (Ω − ,Ω − )),
m n N g m k n k i m k n k common task where the robot leaves the docking station to
k=1
(11) fetch materials and come back in a reverse direction. Our
whereN isthenumberofframesincludedfortemporalcon- approach is compared with vision based approach tested
sistencyveriﬁcation.Notethatincaseofreversevisit(view- based on a front-mounted camera. In particular, we use
ing angle change detected in geometry re-identiﬁcation), DBoW2 [10] which is implemented in ORB SLAM [11].
I I
− becomes accordingly. The loop candidate can The result is shown in Fig. 5 with the estimated trajectory
m i m+i
be accepted by taking threshold ξ on the ﬁnal temporal plottedingreencolor.ForDBoW2,thereversevisitchanges
consistency score. the view angle signiﬁcantly so that there is not enough
2) Geometricalconsistency: Thegeometricalconsistency similarity to conclude loop closure. Hence the trajectory
veriﬁestherawscan-to-scansimilarity.Similarto[8],FPFH collides with shelves according to the ﬁnal result, which is
features are extracted and matched to get an initial guess of incorrect. In comparison, our proposed method is able to
therigidtransformmatrix.Startingfromthisinitialestimate, identifytherevisitedplace.Byapplyingglobaloptimization,
iterative closest point (ICP) [26] is applied to ﬁnd minimal the resultant trajectory and the mapping result is much more
distance error between the query scan and candidate scan. reasonable.Thisisduetotheviewangleinvariantpropertyof
our proposed two-stage hierarchical Intensity Scan Context
IV. EXPERIMENTEVALUATION
based retrieval.
A. Experiment Setup
C. Evaluation on Public Dataset
In this section we present preliminary results from our
proposed method, including both indoor warehouse robot To further demonstrate the performance of the proposed
navigationandoutdoorautonomousdrivingexperiments.The method, we test the algorithm on KITTI dataset which
proposed method is implemented in C++ and is integrated is commonly used for place recognition. KITTI dataset is
to robot operating system (ROS) on an Intel NUC mini collected from an autonomous car equipped with various
computer. For indoor warehouse navigation, an autonomous perception systems including front-mounted cameras, Velo-
guided vehicle (AGV) equipped with Velodyne VLP-16 and dyne HDL-64E LiDAR, GPS, etc. The recorded scenarios
Intel Realsense r200 is used for the experiment. As shown are challenging for loop closure detection due to similar ar-
inFig.4,therobotisdevelopedforwarehousemanipulation chitectures and dynamic environment. The proposed method
tasks such as packaging and transportation which require is tested with multiple recordings such as sequence 00, 02
high accuracy on long-term localization. The maximum
speed of AGV is 1 m/s. For outdoor autonomous driving,
KITTIdataset[18]isusedforevaluation.Theproposedplace Parameter Description Value
recognitionapproachisusedtoreducelocalizationdriftsand Lmax Maximumradius 50
improve the mapping accuracy. A list of parameters used in Ns Numberofsectors 20
the experiment is shown in Table I. Nr Numberofrings 60
g Geometrymatchingthreshold 0.9
B. Experiment on Autonomous Robot i Intensitymatchingthreshold 0.92
N Framesusedintemporalconsistencycheck 5
The robot is tested in real warehouse environment con- ξ Temporalconsistencycheckingthreshold 1.8
sisting of operating machines, shelves, human, etc. LiDAR TABLE I: Parameter List.
odometer is estimated based on feature points matching via
2099
Authorized licensed use limited to: Heriot-Watt University. Downloaded on September 22,2020 at 01:08:50 UTC from IEEE Xplore.  Restrictions apply. （a） （b） （c）
Fig. 6: Loop closure detection result. (a) KITTI sequence 00. (b) KITTI sequence 02. (c) KITTI sequence 05.
and 05. Sequence 00 and 05 are commonly used for place istestedwithlocalexperiment.TheresultsarelistedinTable
recognition since most of loop closure places are forward II.Comparetovisionbasedapproach,ourapproachachieves
visited. Sequence 02 contains both forward and reverse visit competitiveprecisionandrecallrateonbothsequence00and
and is considered more challenging for place recognition. sequence 05. On more challenging dataset sequence 02, our
Recall rate and precision are collected from each test. approach achieves much higher recall rate. This is because
Precision is the percentage of successful pairing and recall insequence02visionbasedapproachfailstoidentifythere-
rateisthereportedlooppairsagainsttotallooppairs.Higher versevisitsothattherecallratedropssigniﬁcantly.However,
recall rate can signiﬁcantly reduce drifting error and higher false positive is reported for LiDAR based approach such
recallprecisioncanpreventwrong registrationofpointsinto as our method and [21]. The failure case comes from non-
map. The number of total loop closure is collected based on residential area where both sides of roads are trees so that
GPS result since the test-ﬁeld is large in scale. The results thegeometryandintensitycharacteristicsareverylimitedfor
of our proposed approach can be found in Fig. 6, with the place recognition. Compared to Lidar based approach such
GPS trajectory plotted from light to dark with time going. as [21], [17], [24], we achieve both higher recall precision
Each of the loop closure place detected is marked with red andrecallrateacrossallthreedatasets.Inthemeanwhile,the
color. proposed intensity scan context only costs 1.2 ms for each
The results are also compared with existing works such query which is very efﬁcient in practice.
as [21], [17], [24] that are commonly used in SLAM ap-
V. CONCLUSION
plications. We also include the state-of-the-art vision based
In this paper, we presented a robust loop closure de-
place recognition methods such as [10] for comparison. The
tection approach by integrating both geometry and inten-
experiment results of [21], [17], [24] are collected from
sity information. Existing works on LiDAR based loop
respectivepaperduetounavailabilityofsourcecodeand[10]
closure detection mainly leverage on the geometric-only
descriptor and ignore intensity reading. Inspired by the
Dataset Approaches Precision(%) RecallRate(%) recent researches on LiDAR intensity, we argue that the
intensity information can be effective for place recognition
Kim[21] 100 87
and propose a global 3D descriptor named intensity scan
sequence00 GLAROT3D[17] 86 40
context. To reduce the computational cost, an efﬁcient two-
Cieslewski[24] 92 80
stagehierarchicalintensityscancontextretrievalisproposed,
Ga´lvez-Lo´pez[10] 100 92
consisting of fast binary-operation based geometry indexing
Proposed 100 90.2
and intensity structure re-identiﬁcation. It costs only 1.2 ms
Kim[21] 90 73
for each query on a normal computer in practice. Thorough
sequence02 Ga´lvez-Lo´pez[10] 100 80.6
experiments have been conducted including local run test
Proposed 98 91
with autonomous warehouse robot and public dataset test
Kim[21] 100 90
to evaluate our proposed method. The results show that our
sequence05 Cieslewski[24] 93 60 proposed method achieves competitive recall precision and
GLAROT3D[17] 80 80 recall rate compared to existing state-of-the-art methods.
Ga´lvez-Lo´pez[10] 100 87.6
REFERENCES
Proposed 100 91.2
[1] A. Angeli, D. Filliat, S. Doncieux, and J.-A. Meyer, “A fast and
TABLE II: Comparison with existing methods. incremental method for loop-closure detection using bags of visual
words,”IEEETransactionsonRobotics,pp.1027–1037,2008.
2100
Authorized licensed use limited to: Heriot-Watt University. Downloaded on September 22,2020 at 01:08:50 UTC from IEEE Xplore.  Restrictions apply. [2] R.B.Rusu,N.Blodow,andM.Beetz,“Fastpointfeaturehistograms [15] R.Dube´,D.Dugas,E.Stumm,J.Nieto,R.Siegwart,andC.Cadena,
(fpfh)for3dregistration,”in2009IEEEInternationalConferenceon “Segmatch: Segment based place recognition in 3d point clouds,”
RoboticsandAutomation,2009,pp.3212–3217. in2017IEEEInternationalConferenceonRoboticsandAutomation
[3] G. D. Tipaldi and K. O. Arras, “Flirt-interest regions for 2d range (ICRA),2017,pp.5266–5272.
data,” in 2010 IEEE International Conference on Robotics and Au- [16] M. Bosse and R. Zlot, “Place recognition using keypoint voting in
tomation,2010,pp.3616–3622. large 3d lidar datasets,” in 2013 IEEE International Conference on
[4] F.Tombari,S.Salti,andL.DiStefano,“Uniqueshapecontextfor3d RoboticsandAutomation,2013,pp.2677–2684.
datadescription,”inProceedingsoftheACMworkshopon3Dobject [17] D.L.Rizzini,“Placerecognitionof3dlandmarksbasedongeometric
retrieval,2010,pp.57–62. relations,”in2017IEEE/RSJInternationalConferenceonIntelligent
[5] C. Wang, J. Yuan, and L. Xie, “Non-iterative slam,” in Advanced RobotsandSystems(IROS),2017,pp.648–654.
Robotics (ICAR), 2017 18th International Conference on. IEEE, [18] A.Geiger,P.Lenz,C.Stiller,andR.Urtasun,“Visionmeetsrobotics:
2017,pp.83–90. The kitti dataset,” The International Journal of Robotics Research,
[6] A.Kashani,M.Olsen,C.Parrish,andN.Wilson,“Areviewoflidar vol.32,no.11,pp.1231–1237,2013.
radiometric processing: From ad hoc intensity correction to rigorous [19] J. Levinson and S. Thrun, “Unsupervised calibration for multi-beam
radiometriccalibration,”Sensors,vol.15,no.11,pp.28099–28128, lasers,”inExperimentalRobotics. Springer,2014,pp.179–193.
2015. [20] T. Shan and B. Englot, “Lego-loam: Lightweight and ground-
[7] K. P. Cop, P. V. Borges, and R. Dube´, “Delight: An efﬁcient de- optimized lidar odometry and mapping on variable terrain,” in 2018
scriptor for global localisation using lidar intensities,” in 2018 IEEE IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems
InternationalConferenceonRoboticsandAutomation(ICRA),2018, (IROS),2018,pp.4758–4765.
pp.3653–3660. [21] G. Kim and A. Kim, “Scan context: Egocentric spatial descriptor
[8] J. Guo, P. V. Borges, C. Park, and A. Gawel, “Local descriptor for forplacerecognitionwithin3dpointcloudmap,”in2018IEEE/RSJ
robust place recognition using lidar intensity,” IEEE Robotics and International Conference on Intelligent Robots and Systems (IROS),
AutomationLetters,vol.4,no.2,pp.1470–1477,2019. 2018,pp.4802–4809.
[9] A. Glover, W. Maddern, M. Warren, S. Reid, M. Milford, and [22] G. Kim, B. Park, and A. Kim, “1-day learning, 1-year localization:
G. Wyeth, “Openfabmap: An open source toolbox for appearance- Long-termlidarlocalizationusingscancontextimage,”IEEERobotics
basedloopclosuredetection,”in2012IEEEinternationalconference andAutomationLetters,vol.4,no.2,pp.1948–1955,2019.
onRoboticsandautomation(ICRA),2012,pp.4730–4735. [23] S. Belongie, J. Malik, and J. Puzicha, “Shape matching and object
[10] D.Ga´lvez-Lo´pezandJ.D.Tardos,“Bagsofbinarywordsforfastplace recognition using shape contexts,” IEEE Transactions on Pattern
recognition in image sequences,” IEEE Transactions on Robotics, Analysis&MachineIntelligence,no.4,pp.509–522,2002.
vol.28,no.5,pp.1188–1197,2012. [24] T.Cieslewski,E.Stumm,A.Gawel,M.Bosse,S.Lynen,andR.Sieg-
[11] R. Mur-Artal and J. D. Tardo´s, “Orb-slam2: An open-source slam wart,“Pointclouddescriptorsforplacerecognitionusingsparsevisual
systemformonocular,stereo,andrgb-dcameras,”IEEETransactions information,”in2016IEEEInternationalConferenceonRoboticsand
onRobotics,vol.33,no.5,pp.1255–1262,2017. Automation(ICRA),2016,pp.4830–4836.
[12] X. Gao, R. Wang, N. Demmel, and D. Cremers, “Ldso: Direct [25] M.J.MilfordandG.F.Wyeth,“Seqslam:Visualroute-basednaviga-
sparseodometrywithloopclosure,”in2018IEEE/RSJInternational tionforsunnysummerdaysandstormywinternights,”in2012IEEE
ConferenceonIntelligentRobotsandSystems(IROS),2018,pp.2198– InternationalConferenceonRoboticsandAutomation. IEEE,2012,
2204. pp.1643–1649.
[13] A.Anoosheh,T.Sattler,R.Timofte,M.Pollefeys,andL.VanGool, [26] P.J.BeslandN.D.McKay,“Methodforregistrationof3-dshapes,”
“Night-to-day image translation for retrieval-based localization,” in inSensorfusionIV:controlparadigmsanddatastructures,vol.1611.
2019 International Conference on Robotics and Automation (ICRA), InternationalSocietyforOpticsandPhotonics,1992,pp.586–606.
2019,pp.5958–5964. [27] R.B.RusuandS.Cousins,“3Dishere:PointCloudLibrary(PCL),”in
[14] T. Naseer, M. Ruhnke, C. Stachniss, L. Spinello, and W. Burgard, IEEEInternationalConferenceonRoboticsandAutomation(ICRA),
“Robustvisualslamacrossseasons,”in2015IEEE/RSJInternational Shanghai,China,May9-132011.
ConferenceonIntelligentRobotsandSystems(IROS),2015,pp.2529–
2535.
2101
Authorized licensed use limited to: Heriot-Watt University. Downloaded on September 22,2020 at 01:08:50 UTC from IEEE Xplore.  Restrictions apply. 