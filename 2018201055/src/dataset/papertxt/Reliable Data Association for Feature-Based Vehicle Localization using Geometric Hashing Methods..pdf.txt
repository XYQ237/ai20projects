2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
Enabling Topological Planning with Monocular Vision
∗ ∗ ∗
Gregory J. Stein , Christopher Bradley , Victoria Preston , and Nicholas Roy
Abstract—Topologicalstrategiesfornavigationmeaningfully
reduce the space of possible actions available to a robot,
allowing use of heuristic priors or learning to enable com-
putationally efﬁcient, intelligent planning. The challenges in
estimating structure with monocular SLAM in low texture
or highly cluttered environments have precluded its use for
topological planning in the past. We propose a robust sparse
map representation that can be built with monocular vision
and overcomes these shortcomings. Using a learned sensor, we
estimatehigh-levelstructureofanenvironmentfromstreaming
imagesbydetectingsparse“vertices”(e.g.,boundariesofwalls)
and reasoning about the structure between them. We also
estimate the known free space in our map, a necessary feature
for planning through previously unknown environments. We
showthatourmappingtechniquecanbeusedonrealdataand
is sufﬁcient for planning and exploration in simulated multi-
agent search and learned subgoal planning applications.
I. INTRODUCTION
Autonomous navigation is a ubiquitous problem in the
ﬁeld of mobile robotics. In order to reduce the number of
actions available to an agent, it is often easier to describe
the problem, and subsequently generate plans, using the
languageoftopology.Incaseswherethemapisfullyknown,
Fig. 1. Map Building with Real Monocular Camera Data: Here we
recent work in the ﬁeld leverages topological constraints
show that our procedure builds a sparse map of an environment which is
for a variety of different objectives, such as multi-agent partiallyobservedbyarobotequippedwithamonocularcamera(greenline
search though known maps [1], in which robotic agents are istherobotpath).Alearnedsensorisprovidedpanoramicimages(topof
panels(a)–(c))andreturnsalistofverticeswhichrepresentcornersin
constrained to navigate through different homology classes
the world, in addition to estimates of the edges that exist between them.
to search more efﬁciently. Byfusingobservationsbetweenframes,wecanrejectdetectionerrors(red
circles) and extract the most likely map (solid black lines and circles).
Intelligent decision-making, particularly in the context of
Known(shadedwhiteregions)andunknown(shadedgrayregions)spaceis
navigating unknown space, can be modelled as a Partially additionallytracked,whichcanbeusedbyatopologicalplanner.
ObservableMarkovDecisionProcess(POMDP)[2].Topology
the maps autonomous agents use to represent their environ-
provides a means of specifying high-level actions available
ment. Monocular, feature-based methods have proven to be
totheagent(e.g.,“goleftaroundthebuilding”or“enterun-
effective tools for localization and navigation [7] and have
known space through this boundary”), allowing the planner
shown promise in building occupancy maps sufﬁcient for
to reduce the computational challenges of the optimization
explorationofsmallenvironments[8],[9].Theseapproaches
problem implicit in the POMDP. Planning under topological
build sparse point-clouds of visual features, and bin them
constraints reduces the space of actions, making it easier to
into voxels to represent obstacles which a robot could use
plan [3], [4] or encode heuristic or learned priors [5], [6].
for path planning. However, such approaches have limita-
Critically however, to plan with topological constraints, a
tions when planning with topological constraints in general;
robot must be able to extract them from its environment.
spurious detections can block potential paths, and a lack
The ubiquity of cameras and the richness of information
of detections in featureless regions (e.g., a white wall in a
vision provides makes monocular images an attractive can-
hallway) can lead a robot to believe free space exists where
didate for both informing high-level autonomy and building
it does not. Resolving the discrepancies between the point
cloud and the true underlying geometry of an environment
*Equalcontribution.
All authors are with the Computer Science and Artiﬁcial Intelligence is a challenge for such systems. Furthermore, due to the
L{aboratory, Massachusetts Institute of Technolog}y, in Cambridge, USA. high dimensionality of point-cloud-based SLAM, robustly
gjstein, cbrad, vpreston, nickroy @mit.edu
eliminating erroneous features is an open problem [7].
This work was supported by the Ofﬁce of Naval Research and Marc
Steinberg under the PERISCOPE MURI Contract #N00014-17-1-2699.
Planning with topological constraints necessitates a repre-
Theirsupportisgratefullyacknowledged.V.Prestonacknowledgessupport
byaNDSEGFellowship. sentation which is robust to the types of failures described:
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 1667
Authorized licensed use limited to: Heriot-Watt University. Downloaded on September 21,2020 at 17:09:10 UTC from IEEE Xplore.  Restrictions apply. allobstaclesmustbeaddedsothatdeadendscanbedetected homology—atopologicalproperty—toconstrainpossibleac-
andplannedaround,whilespuriousobstacles(thatmayblock tions the agent can take (e.g., “go left around the building”).
paths or add unnecessary options) should be eliminated. Storingasinglepointpercontiguousobstacleisaminimally
Speciﬁcallyfortheexplorationproblem,evenstate-of-the-art sufﬁcient representation for computing the homology class
workviamonocularvisionacknowledgesthechallenges(and of a trajectory [14]. Of interest in this paper is planning
failures)ofextractingfrontiersfromtheirdensemapreliable through unknown environments during map construction,
enough to enumerate the different ways the agent can enter for which relative homology [15] considers only partially
unknownspaceduringplanning[8].Relatedly,mostplanning revealed obstacles. Frontiers are boundaries between free
strategies that leverage homology or frontiers rely on ideal and unknown space, and constrains planning to relative
sensors [5], [10] or specialized hardware, like a laser range homologyclass.Frontier-basedplanningrestrictstrajectories
ﬁnder, to build a map and generate plans [11], [12]. to pass through the frontier of interest, thus trajectories are
During exploration and navigation, planning with a map guaranteedtohaveuniquerelativehomology.Touseplanners
may be done via computation of a visibility graph [13], based on relative homology (or map frontiers), we need a
which is a minimal, optimal representation for planning in representation that tracks both obstacles and free space.
a known environment and can be extracted directly from a The Gap Navigation Tree [16], [17] is a mapping tech-
map.Mapsgeneratedusingsparsefeaturescontainclutter or nique for building a sparse, minimal, tree-structured graph
voids of features (e.g., in low-texture regions), which yield of the world by detecting gaps, discontinuities in depth,
visibility graphs that are not useful for planning. Recent and maintaining a list of unexplored branches for later
progressindeeplearningforcomputervisionaffordsoppor- exploration. Gaps store the high-level actions available to
tunitiestoovercomethesefaultscomparativelyeasilybyonly the robot, yet their practical utility is limited by sensor
addingsparsefeatureswherenecessaryfordevelopinguseful noise [18] and, due to its tree structure, Gap Navigation
graphsforplanning,withoutrelianceonsurfacetexturesand is limited to simply-connected environments. Moreover, gap
by ignoring clutter or outlying detections. locations are viewpoint-dependent, creating challenges for
The contribution of this paper is to extend recent work on data association and robust map construction. To overcome
planning with topological constraints to maps constructed these challenges, we approximate the environment as a
with monocular vision, with an emphasis on planning polygon, so that the location of gaps become viewpoint
through unknown environments. In this work, we present independent,enablingreliabledataassociation.Assuch,our
a novel map representation built from sparse detections sensor detects polygonal vertices in view of the agent; by
from monocular vision, which is robust to noise in a way localizing vertices and estimating the presence of walls or
that enables navigation and exploration using topological other impassible obstacles that connect them, our agent is
constraints in both simulated and real-world environments. capable of reconstructing the polygonal representation of its
From our representation, we compute a visibility graph environment as it travels (see Sec. III).
corresponding to the sparse, clutter-free structure of the Our map representation consists of the vertices and edges
known environment from which we can efﬁciently navigate that deﬁne obstacles and an estimate of the free space
using topological constraints. To build our map, we use a observed by the robot. Planning within this representation
Convolutional Neural Network (CNN) to detect the edges of is straightforward via computation of a visibility graph [13],
locally visible structure, which are fused to estimate regions which is minimal and optimal in a known polygonal en-
ofknownandunknownspace,andtrackobstaclesthatdeﬁne vironment. For exploration tasks, the representation of free
topological constraints. space can be used for frontier-based planning by extracting
We ﬁrst study multi-agent search; our method succeeds frontiers from the difference between map edges and the
in 99% of trials across two simulated environments, demon- boundaryoffreespace.Additionally,wecaneasilycompute
strating the robustness of our procedure (Sec. IV). We addi- the relative homology class of a trajectory from our repre-
tionally show that our representation enables visual learned sentation by tracking partially revealed obstacles, making it
subgoal planning, demonstrating that our map is sufﬁciently sufﬁcient for planning with topological constraints.
stable to enable learning-based selection of topological ac-
tionsforimprovedgoal-directednavigation(Sec.V).Finally, III. PROBABILISTICMAP-BUILDINGFROMLEARNED
we show that our work extends beyond simulation by build- MONOCULARSENSING
ing maps in three real-world environments (Sec. VI). Our agent is equipped with a learned sensor (Sec. III-A)
that returns a list of detected vertices in view of the agent
II. PLANNINGWITHTOPOLOGICALCONSTRAINTS and, for each detection, the likelihood an obstacle exists
In this section, we examine requirements of topological betweenitandeachofitsneighbors(e.g.,ifthevertexandits
planners and motivate our map representation. In order to neighborareendpointsofasharedwall).Welearntoidentify
efﬁciently navigate through both known and unknown envi- where the robot can and cannot travel in order to explore
ronments,topologicalconstraintscanbeemployedtoreduce its environment, and do this by detecting the boundaries
the space of actions available to a robot. Two trajectories of untraversable obstacles. Our map representation therefore
are said to be in different homology classes if the area stores three types of information: (a) vertices, each a cluster
theyboundcontainsobstacles;planningapproachesmayuse of vertex detections; (b) edges, connections between two
1668
Authorized licensed use limited to: Heriot-Watt University. Downloaded on September 21,2020 at 17:09:10 UTC from IEEE Xplore.  Restrictions apply. and negative detections may also be introduced. In order to
handle sensor noise, we deﬁne a probabilistic model from
which we can assess the likelihood of a proposed map; the
posterior likelihood of maps is discussed in Sec. III-C. To
propose and evaluate the posterior over maps, we sample
over the set of polygonal vertices and their most likely
edges, then compute free space in the presence of noise.
This discussion is included in Sec. III-B.
A. Vertex Detection with Convolutional Neural Networks
In order to detect vertices, we train a CNN to estimate the
vertex and two edge likelihood terms. The network takes a
×
128 512pixelRGBpanoramicimageandreturnsthe3like-
×
lihoodsovera32 128grid,whereeachpointcorrespondsto
arange-bearingcoordinatefromthesensor1.Weuseafully-
convolutional encoder-decoder network structure composed
Fig.2. SearchinSimulatedOfﬁcePark:Hereweshowthreerepresen-
of blocks. Each encoder block consists of 2 convolution
tativesearchscenariosinasimulatedoutdoorenvironment(a)populated ×
with buildings and trees. In (b)–(d) the progress of multiple agents is layerswith3 3kernels,followedbyabatchnormoperation
×
shown,wheretheagentsareconstrainedtopursueuniquehomologyclasses. and a ReLU activation function, and terminated by a 2 2
Wesuccessfullyidentifyandmapallbuildingsin99of100simulatedtrials,
max-pooling operation. Decoder blocks are similar, however
illustratingtherobustnessofourtechnique.SeeSec.IVformoredetails.
theﬁnalconvolutionallayerhasstride2(sothattheoutputis
vertices which form an untraversable boundary; and a (c) upsampled)andthemax-poolingoperationiseliminated.The
known space polygon, a polygon deﬁning what space has network consists of 5 encoder blocks, with output channels
been observed to be free. [64,64,128,128,256], and 3 decoder blocks, with output
a) Vertices: Each vertex detection in an observation channels[128,64,64].Theoutputoftheﬁnaldecoderlayeris
×
z(i) is compared to the vertices that exist in the map in passedthroughaﬁnal1 1convolutionallayerwith3output
order to determine how each detection should be associated. channels, corresponding to the 3 likelihoods. A weighted
Either a detection is matched with an existing vertex, or cross-entropy with an empirically chosen positive-weight 8
it is added as a new vertex to the map with 2D pose is used as the loss function for the vertex likelihood, so
and covariance. The Mahalanobis distance [18] is used to that the sparse positive detections are not overwhelmed by
associate vertex detections between observations in order the negative background. The edge likelihoods each have a
to cluster detections into vertices. We additionally apply a sigmoid cross-entropy loss, yet are masked by the training
mutual exclusion constraint [19] such that no two vertex vertex label so that loss is only non-zero where a vertex
detections from a single observation are associated with the exists. The edge likelihood loss is weighted by a factor
same vertex. Successful associations update the position and of 1/16 versus the vertex likelihood so that the network
covariance of vertices using a Kalman ﬁlter. prioritizeswhetheravertexexistsbeforetryingtoestimateits
b) Edges: Edges between vertices impose topological properties. The Adam optimizer is used to train the network
constraints necessary for planning. However, directly detect- for 100k steps, with an initial learning rate of 2.5 and a
ingthestructuralconnectivityofanenvironmentusingvision learning rate decay of 0.5 every 10k steps.
is a challenging problem [20]. For each vertex detection
B. Generating Proposal Maps
our sensor instead returns a likelihood that an untraversable
obstacle exists between each of its covisible neighbors (an Associating detected vertices across many observations
“edge likelihood”). For an observation z(i), we average results in a set of potential vertices, of which only a subset
these likelihoods for each pair of detections to deﬁne the may appear in the ﬁnal map. By iteratively including and
probability an edge exists between the vertices; over many removing potential vertices, and the walls connected to
observations we average these probabilities to determine them, we explore the space of possible maps via sampling2.
whether or not an edge exists in the map. Computingthemapalsorequirescomputingtheknownspace
c) KnownSpacePolygon: Eachsensorobservationz(i) polygon and, as we change the vertices and edges included
reveals a star-shaped region surrounding the agent s(z(i)). in the map, the known space must be updated. To construct
The function s constructs a polygo(cid:83)nal region by sorting the this polygon for a given set of vertices and edges, we ﬁrst
vertex detections by the angle at which they were observed. computeahypotheticalobservationz byraycastingagainst
h
Theunionofthesepolygons,S = Nobss(z(i))forallsensor the proposed structure (zh is what the robot would see if the
observations N deﬁnes the spaceir=e0vealed by the agent. proposed structure were accurate). However, this detection
obs
When the sensor is perfect, the map representation can does not include unobserved obstacles. As such, we instead
be built directly from the descriptions provided. In prac-
1Torecoveradiscretesetofverticesfromthegridbasedoutputproduced
tice however, the sensor is noisy—both vertex and edge
bytheCNN,weuseapeak-detectionprocedurewithathresholdof0.5.
detections will not only be imperfect, but false positive 2Werandomlysampleoververticesuniformly.
1669
Authorized licensed use limited to: Heriot-Watt University. Downloaded on September 21,2020 at 17:09:10 UTC from IEEE Xplore.  Restrictions apply. compute a conservative estimate of known space S when
c
proposing maps: the intersection of the polygons computed
(cid:91) (cid:91)(cid:16) (cid:17)
from both the real and hypothetical sensor observation:
Nobs Nobs ∩
S = S(i) = s(z(i)) s(z(i)) (1)
c c h
i=0 i=0
Without this conservative approach, the agent may incor-
rectlymarkunobservedspaceasexplored,leadingtoincom-
plete exploration.
WeprobabilisticallyacceptproposedmapsusinganMCMC
criteria and the map posterior. After a ﬁxed number of
samples(50sampleswereusedforallexperimentspresented
in this work) we return the maximum likelihood map. In
general, sampling requires iterating over the inclusion of
walls in the proposed map. In practice, we use a heuristic to
determinewhetherawallexists:wecomputeanapproximate
marginal edge likelihood by computing individual observa-
tions of the edge likelihood using co-visible vertices in the
real detections (rather than the conservative detections). We
Fig.3. SearchinSimulatedIndoorEnvironment:Hereweshowthree
havefoundthisheuristicsufﬁcientforeffectivemap-building
representativesearchscenariosinasimulatedindoorenvironmentpopulated
and planning and considerably reduces the search space. withrandomclutter(a).In(b)theprogressofmultipleagentsisshown,
where the agents are constrained to enter unknown space via a unique
C. Deﬁning the Posterior Over Maps frontier.Inpanels(b)-(d),justasintheOfﬁceParktrialsinFig.2,multi-
{ } agentteamscompleteexplorationinroughlyhalfthetime(onaverage)that
GivenN observations z(i) 1≤i≤N,theposteriordistribu- singleagentsuse.
tion over maps m can be expanded using Bayes Rule:
(cid:80) detections, which do not have an associated real vertex
| ···
logP(mz(1), ,z(N))= detection, are assigned a uniform prior over edge likelihood.
| − ··· Our posterior codiﬁes a number of behaviors we would
N logP(z(i) m)+logP(m) logP(z(1), ,z(N))
i=0 expect to see during our map-building process. First, the
(2)
posterior allows us to omit false positive vertices in a
wherewehaveassumedthatthesensorobservationsarei.i.d. principled manner. Since false positive vertices can result
··· |
to factor logP(z(1), ,z(N) m). The ﬁnal term in Eq. (2), in unwanted false negative observations as the robot travels,
the likelihood of a sensor measurement, is a normalizer and it ultimately makes it more likely that the vertex does not
can be ignored. The prior distribution over maps, P(m), is appear in the underlying geometry. Second, the existence of
intractably difﬁcult to obtain in practice. However, we can a wall depends on both the detected edge likelihood and
instead incorporate heuristic priors about the map, namely its ability to occlude vertices. By occluding portions of the
biases against the addition of new vertices and walls, which map, the edges also inﬂuence the number of false positive
mitigates the impact of false positive detections. and false negative detections recorded during the evaluation
To evaluate the likelihood of each observation given a of map likelihood.
proposed map, we compare the sensor observation against
the conservative space estimate for individual observations IV. MULTI-AGENTSEARCH
S(i). When evaluating the likelihood of a sensor observation Todemonstratethereliabilityofourapproach,wetestour
c |
P(z(i) m), we reason about the likelihood of the vertices map-building procedure with multi-agent search, in which
| |
P (z(i) m) and edges P (z(i) m) independently. The con- agents must be able to maintain an accurate estimate of
v e
tribution from the vertices depends only on the number of observed space in order to reach an unseen goal. Multi-
false positive and false negative detections: any real vertex agentsearchstronglybeneﬁtsfromplanningwithtopological
detections that do not appear in S(i) are considered false constraints, so that each agent is encouraged to explore
c
positive detections and any vertices within S(i) that were different regions of space and reveal the environment more
c
not detected by the sensor are considered false negatives. quickly [1]. The challenges in detecting features in tex-
|
Therefore, P (z(i) m)=RNFPRNFN, where R ,R are the tureless regions and accurately resolving depth in cluttered
v FP FN FP FN
rates of false positive/negative detections and N ,N are regions make traditional monocular SLAM approaches im-
FP FN
the number of observed false positive/negative detections. practical for this task [8], [9].
|
The likelihood contribution from the edges P (z(i) m) We conduct 200 simulated experiments across two envi-
e
also makes use of the conservative space estimate S(i). For ronments we created in the Unity Game Engine [21]: (1) an
c
each vertex our sensor returns the likelihood it is connected Indoor Environment, a small labyrinth of rooms connected
via an obstacle to each of its covisible neighbors. The bycorridors,and(2)anOfﬁcePark,anoutdoorenvironment
averageofthesedeﬁnestheedgesinourmap.Falsenegative ofrandomlyplacedbuildingsandtrees.Trainingdataisgen-
1670
Authorized licensed use limited to: Heriot-Watt University. Downloaded on September 21,2020 at 17:09:10 UTC from IEEE Xplore.  Restrictions apply. TABLEI
PERFORMANCEMETRICS(AVERAGED)FORTHESEARCHTASK
IndoorEnvironment OfﬁcePark
Success Steps IoU Success Steps IoU
Multi-agent 99/100 137.6 0.979 100/100 128.0 0.990
Single-agent 99/100 230.6 0.979 97/100 254.1 0.990
erated via simulated travel through a subset of environments
not included in the test set. To plan with multiple agents, a
joint planner encourages the agents to minimize the net cost
ofenteringunknownspaceyetviadifferentfrontiers,thereby
ensuring that agents select trajectories in different relative
homology classes. Map construction with multiple agents is
straightforward: observations are received in sequence from
each of the three agents and added to the map in turn. Since
the probabilistic model treats each measurement as i.i.d., no
modiﬁcations are necessary to our map-building procedure.
We report performance in terms of coverage—the agent’s
map of the world should match the underlying map. As
such, we use Intersection over Union (IoU) between the
ﬁnal reconstructed map and the true map as a measure of
coverage3. Examples of our results are shown in Figs. 2
and 3, and a data table summarizing the results of all
simulated trials—100 randomized maps for each simulated Fig.4. OptimisticandLearnedSubgoalPlanninginSimulatedGuided
Mazes: Our map representation is used by an optimistic planner and a
environment—are in Table I. Of our 200 trials, 198 succeed
learned subgoal planner (LSP) that are compared in 200 simulated guided
in completely exploring their environment, achieving an mazeenvironmentsinwhichgreen“breadcrumbs”leadtothegoal(a).In
IoU above 0.95. The two remaining trials miss a single panel (b), snapshots of the two planners in progress are shown, wherein
the optimistic planner is greedily searching nearby paths and the learned
wall and the agents became stuck in place. This results in
subgoal planner is pursuing frontiers marked with dashed lines (red are
identical input images fed to our model, which will produce unlikelytoreachthegoal,greenaremorelikelytoreachthegoal).Panels
identical outputs. This violates the i.i.d. assumption of our (d, e) show completed missions for each planner. As shown in panel
(c),robotsusingthelearnedsubgoalplannertravelatotalof25.6%less
sensor model and the maximum likelihood map may not
distancethanthoseusingtheoptimisticplanner.Thisresulthighlightsthat
matchtheunderlyingenvironment.Thisdoesnotoccuroften ourmethodisnotonlysuitablefornavigation,itisstableenoughtosupport
and, despite this occasional limitation, our representation is LSP.
sufﬁcient for mapping and planning in unknown simulated between free and unknown space, previous work on LSP
environments with monocular vision, even in the presence relied on a planar laser scanner to build an occupancy grid
of sensor noise. Data from the 200 multi-agent trials can map. We demonstrate here that our map representation from
be found in Table I; we show performance alongside an monocular vision is sufﬁciently reliable to enable LSP, thus
additional 200 trials from single-agent planning, reinforcing showing that vision can be used for both enumerating the
that imposing topological constraints with our map enables high-level actions available to the agent during exploration
more efﬁcient search of unknown environments. and deciding between these actions.
V. VISUALLEARNEDSUBGOALPLANNING TheLSPexperimentsrequirethreefrontierproperties[6]—
Our ﬁnal simulated experiments demonstrate that our the likelihood a frontier will lead to the goal, the expected
representation enables Learned Subgoal Planning (LSP) [6] costofreachingthegoal(ifthegoalcanbereached,andthe
without the need for specialized hardware. LSP involves expectedcostofexploration(ifthegoalcannotbereached)—
estimating properties of contiguous boundaries between free thatweestimatealongsidethevertexandedgelikelihoodsby
and unknown space (frontiers), including the likelihood the addingoutputchannelstoournetworkarchitecturedescribed
goal can be reached through the boundary of interest, and in Sec. III-A. A few other changes to the neural network
uses these properties to approximate the expected cost of are made when estimating frontier properties. The frontiers
topological actions. In [6], the authors showed that their can overlap in range-bearing space, so we add an additional
approach is a computationally tractable means of computing decoder block to create a higher-resolution output. To ease
expected cost and demonstrated improvements for planning initialization of this larger network, we found it more effec-
through different types of environment. Because the learn- tive to use leaky-ReLU for the convolution layer activations.
ing procedure relies upon stable, well-deﬁned boundaries The goal location, needed to estimate frontier properties,
is encoded into a 2-channel image and appended to the
3WecomputeIoUdifferentlyintheoutdoorenvironment,sinceweshould input of the third encoder block; each pixel corresponds to
not expect that the agent should mark all space around the buildings as [r sin(θ ),r cos(θ )],wherer istherelativerangeandθ
free.Instead,wecomputeIoUwithrespecttotheplacementofthebuilding g g g g g g
is the relative angle of the goal versus the bearing of the
obstacles. Mapping is perfect when all buildings are detected and in the
correctlocationsandthatnospuriousbuildingsareadded. pixel of interest. See [6] for a complete discussion of the
1671
Authorized licensed use limited to: Heriot-Watt University. Downloaded on September 21,2020 at 17:09:10 UTC from IEEE Xplore.  Restrictions apply. a) Bld. 36-2, Fig. 5(a): Our ﬁrst environment was
a simply-connected hallway intersection. Despite a high
numberoffalse-positivedetections(redcircles),wecorrectly
built the underlying map, capturing the high-level structure
and building a single contiguous boundary.
b) Bld. 6C Atrium, Fig. 5(b): This environment con-
sisted of one large loop and a small annex with large
windowsandglasswalls.Althoughwewereabletocorrectly
map the central loop, we failed to accurately represent
the annex, where light glare saturated the images. Notably,
during the trial, the robot at one point had an accurate
representation of the annex (highlighted in Fig. 5(b)), yet
Fig.5. Additional Real-World Mapping Trials:Beyondthereal-world
the challenges in detection overwhelmed and resulted in
resultshowninFig.1,weshowheremappingtrialsonreal-worlddatain
twodifferentenvironments.In(a),therobotsuccessfullybuiltamapwhich many false positive detections (red circles) in this region.
accurately represents the underlying environment. In (b), despite a small
c) Bld. E52-2, Fig. 1: This environment consisted of
alcovewithchallenginglightingforwhichnotrainingdatawascollected,the
learnedsensorwasabletoaccuratelybuildmuchofthisatriumenvironment. two loops and several branching hallways populated with
clutter (benches, chairs) and challenging surfaces (glass and
frontier properties.
texturelesswalls).Inthesetrialsweshowthatweareableto
To evaluate performance, we created a visually-oriented
consistentlybuildamapwhich,qualitatively,wellrepresents
variant of the guided maze environment from [6]. Each
the underlying structure of the environment. Quantitatively,
randomly-generated map is a minimum spanning tree maze,
10 of 11 trials built the two loops in the environment
yet the path between the start and goal is marked by a green
completely and accurately. In the unsuccessful trial, there
path on the ground. An agent using the learned subgoal
wasafailureofdataassociationintherightmostloop;where
planning algorithm should learn that the highlighted path
there should be 1 vertex, there were 3 rejected vertices (red
indicates actions that are more likely to lead to the goal
circles). As compared to a state-of-the-art monocular SLAM
and preferentially pursue those. Vision therefore provides a
package [25] for panoramic input, highlighted in Fig. 1,
strong signal for the correct path.
our learned sensor performed robustly in predominantly
We conducted 400 simulated trials in our guided maze
textureless regions.
environments: 200 in which the agent builds the map and
uses the LSP algorithm to select the lowest expected cost
action, and 200 using a naive baseline that plans as if all VII. CONCLUSION
unknown space is free. All 400 trials reach the unseen goal,
speakingtothereliabilityofourmap-buildingandnavigation In this work, we present a sparse map that enables plan-
procedure.Resultsshowingaside-by-sidecomparisonofthe ningwithtopologicalconstraintsusingmonocularvision.We
LSP procedureversusthenaivebaselineareshowninFig.4. introduce a learned sensor to detect vertices from panoramic
Asexpected,thelearnedplannermatchesoroutperformsthe images and estimate the presence of large obstacles that
baseline planner in nearly all trials (25.6% reduction in net connect them in order to extract a polygonal representation
cost), highlighting that our representation is not only able of the environment that includes only high-level structure.
to support navigation, but also is stable enough to enable Our map also includes an estimate of known free space,
learning the properties necessary for LSP. whichweusetodeﬁnetopologicalconstraints—e.g.,homol-
ogy and frontier constraints—for planning through unknown
environments. Upon completion, our procedure yields a
VI. MAPPINGWITHREALMONOCULARCAMERADATA
polygonal map which is sufﬁcient for optimal navigation
Todemonstrateourlearnedsensorandmappingprocedure via computation of a visibility graph. Our results motivate
on real data, a teleoperated robot equipped with a Ricoh future work that generalizes to novel environments, extends
Theta S [22] panoramic camera and Hokuyo LIDAR [23] toactionsbeyondthedomainofnavigation,andincorporates
(used to generate training data) was driven in three distinct localization within our representation.
indoorenvironmentsontheMITcampus.Wedrovetherobot Wedemonstratetheutilityandrobustnessofourrepresen-
through our target environments multiple times (fewer than tation in both multi-agent search and visual learned subgoal
10 traversals for each environment; on the order of 10k planning.Insimulatedtrials,weshowthatourrepresentation
images) and trained a sensor for each environment. We gen- can be used to efﬁciently search unknown space. Moreover,
erated ground truth occupancy grids with Cartographer [24] we demonstrate that our map is sufﬁciently stable to enable
and hand-built the polygonal map of each environment with visual learned subgoal planning. Through trials in several
clutter removed to pair with localized images for training. representativeindoorenvironments,wefurthershowthatour
Thetrainingandtestingsetsweremutuallyexclusive,though map can be generated from real, noisy visual data under
the generalization performance of our planner to unseen conditions in which other methods relying on sparse visual
environments is the subject of future work. features have been shown to fail.
1672
Authorized licensed use limited to: Heriot-Watt University. Downloaded on September 21,2020 at 17:09:10 UTC from IEEE Xplore.  Restrictions apply. REFERENCES [25] S. Sumikura, M. Shibuya, and K. Sakurada, “OpenVSLAM: A ver-
satile visual SLAM framework,” in Proceedings of the 27th ACM
[1] V.Govindarajan,S.Bhattacharya,andV.Kumar,“Human-robotcol- InternationalConferenceonMultimedia,2019.
laborativetopologicalexplorationforsearchandrescueapplications,”
inDistributedAutonomousRoboticSystems,2016,pp.17–32.
[2] L.P.Kaelbling,M.L.Littman,andA.R.Cassandra,“Planningand
acting in partially observable stochastic domains,” Artiﬁcial Intelli-
gence,vol.101,no.1-2,pp.99–134,1998.
[3] S. Kim, S. Bhattacharya, R. Ghrist, and V. Kumar, “Topological
explorationofunknownandpartiallyknownenvironments,”inIEEE
InternationalConferenceonIntelligentRobotsandSystems,2013,pp.
3851–3858.
[4] J.Park,S.Karumanchi,andK.Iagnemma,“Homotopy-baseddivide-
and-conquerstrategyforoptimaltrajectoryplanningviamixed-integer
programming,” IEEE Transactions on Robotics, vol. 31, no. 5, pp.
1101–1115,2015.
[5] R. Korb and A. Scho¨ttl, “Exploring unstructured environment with
frontier trees,” Journal of Intelligent and Robotic Systems, vol. 91,
no.3,pp.617–628,Sep.2018.
[6] G.J.Stein,C.Bradley,andN.Roy,“Learningoversubgoalsforefﬁ-
cientnavigationofstructured,unknownenvironments,”inConference
onRobotLearning,2018,pp.213–222.
[7] C.Cadena,L.Carlone,H.Carrillo,Y.Latif,D.Scaramuzza,J.Neira,
I.Reid,andJ.J.Leonard,“Past,present,andfutureofsimultaneous
localization and mapping: Toward the robust-perception age,” IEEE
TransactionsonRobotics,vol.32,no.6,pp.1309–1332,2016.
[8] L. von Stumberg, V. Usenko, J. Engel, J. Stu¨ckler, and D. Cremers,
“FrommonocularSLAMtoautonomousdroneexploration,”inIEEE
EuropeanConferenceonMobileRobots. IEEE,2017,pp.1–8.
[9] C. Mostegel, A. Wendel, and H. Bischof, “Active monocular local-
ization: Towards autonomous monocular exploration for multirotor
MAVs,”inIEEEInternationalConferenceonRoboticsandAutoma-
tion. IEEE,2014,pp.3848–3855.
[10] B. Tovar, S. M. La Valle, and R. Murrieta, “Optimal navigation
and object ﬁnding without geometric maps or localization,” in IEEE
InternationalConferenceonRoboticsandAutomation,vol.1. IEEE,
2003,pp.464–470.
[11] B.Yamauchi,“Afrontier-basedapproachforautonomousexploration,”
inComputationalIntelligenceinRoboticsandAutomation,1997,pp.
146–151.
[12] Y. Tian, K. Liu, K. Ok, L. Tran, D. Allen, N. Roy, and J. How,
“SearchandrescueundertheforestcanopyusingmultipleUAS,”in
InternationalSymposiumonExperimentalRobotics,2018.
[13] T. Lozano-Pe´rez and M. A. Wesley, “An algorithm for planning
collision-freepathsamongpolyhedralobstacles,”Communicationsof
theACM,vol.22,no.10,pp.560–570,1979.
[14] S. Bhattacharya, M. Likhachev, and V. Kumar, “Topological con-
straints in search-based robot path planning,” Autonomous Robots,
vol.33,no.3,pp.273–290,2012.
[15] F. T. Pokorny, K. Goldberg, and D. Kragic, “Topological trajectory
clustering with relative persistent homology,” in IEEE International
ConferenceonRoboticsandAutomation. IEEE,2016,pp.16–23.
[16] B. Tovar, R. Murrieta-Cid, and S. M. LaValle, “Distance-optimal
navigation in an unknown environment without sensing distances,”
IEEETransactionsonRobotics,vol.23,pp.506–518,2007.
[17] B. Tovar, L. Guilamo, and S. M. LaValle, “Gap navigation trees:
Minimal representation for visibility-based tasks,” in Algorithmic
FoundationsofRoboticsVI. Springer,2005,pp.425–440.
[18] L.MurphyandP.Newman,“Usingincompleteonlinemetricmapsfor
topologicalexplorationwiththegapnavigationtree,”inInternational
ConferenceonRoboticsandAutomation,2008,pp.2792–2797.
[19] M.MontemerloandS.Thrun,“Simultaneouslocalizationandmapping
with unknown data association using FastSLAM,” in IEEE Interna-
tionalConferenceonRoboticsandAutomation,vol.2,Sep.2003,pp.
1985–1991vol.2.
[20] W.N.GreeneandN.Roy,“Flame:Fastlightweightmeshestimation
using variational smoothing on Delaunay graphs,” in International
ConferenceonComputerVision,2017,pp.4696–4704.
[21] UnityTechnologies,“Unitygameengine,”https://unity3d.com,2019.
[22] Ricoh Theta S, “Ricoh Theta S Panoramic Camera,”
https://theta360.com/en/about/theta/s.html,2019.
[23] Hokuyo, “Hokuyo UTM-30LX Scanning Laser Rangeﬁnder,”
https://www.hokuyo-aut.jp/search/single.php?serial=169,2019.
[24] W.Hess,D.Kohler,H.Rapp,andD.Andor,“Real-timeloopclosure
in2DLIDARSLAM,”inIEEEInternationalConferenceonRobotics
andAutomation,May2016,pp.1271–1278.
1673
Authorized licensed use limited to: Heriot-Watt University. Downloaded on September 21,2020 at 17:09:10 UTC from IEEE Xplore.  Restrictions apply. 