2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
Enabling Robots to Understand Incomplete Natural Language Instructions
Using Commonsense Reasoning
∗
Haonan Chen1 , Hao Tan1, Alan Kuntz2, Mohit Bansal1, Ron Alterovitz1
Abstract—Enabling robots to understand instructions pro-
vided via spoken natural language would facilitate interaction
betweenrobotsandpeopleinavarietyofsettingsinhomesand Pour me some water
workplaces. However, natural language instructions are often
1. Understand the language
missing information that would be obvious to a human based
2. Observe the environment
on environmental context and common sense, and hence does
not need to be explicitly stated. In this paper, we introduce 4. Conduct 
Language-Model-based Commonsense Reasoning (LMCR), a To which? the action
new method which enables a robot to listen to a natural To
language instruction from a human, observe the environment
around it, and automatically ﬁll in information missing from 3. Infer with 
the instruction using environmental context and a new com- common sense
monsense reasoning approach. Our approach ﬁrst converts an
instruction provided as unconstrained natural language into a
formthatarobotcanunderstandbyparsingitintoverbframes.
Ourapproachthenﬁllsinmissinginformationintheinstruction
byobservingobjectsinitsvicinityandleveragingcommonsense
reasoning. To learn commonsense reasoning automatically, our Fig. 1: Common sense in instruction understanding. A person
approach distills knowledge from large unstructured textual gives an instruction “pour me some water” but the robot cannot
corpora by training a language model. Our results show carryouttheactionwithoutknowingwheretopourthewater.After
the feasibility of a robot learning commonsense knowledge scanningtheenvironment,therobotusescommonsenseknowledge
automatically from web-based textual corpora, and the power to determine the missing parameters and successfully perform the
of learned commonsense reasoning models in enabling a robot action.
to autonomously perform tasks based on incomplete natural
language instructions. a new approach which enables a robot to listen to a natural
languageinstructionfromahuman,observetheenvironment
I. INTRODUCTION around it, automatically resolve missing information in the
instruction, and then autonomously perform the speciﬁed
Natural language is inherently unstructured and often
task.
reliant on common sense to understand, which makes it
The core problem we are addressing is enabling a robot
challenging for robots to correctly and precisely interpret
to understand incomplete natural language instructions with
natural language. Consider a scenario in a home setting in
the help of commonsense reasoning, particularly handling
which a robot is holding a bottle of water and there are
cases in which an argument of the instruction’s verb is
scissors, a plate, some bell peppers, and a cup on a table
missing.Solvingthisproblemrequirestwosteps:(1)identify
(see Fig. 1). A human gives an instruction, “pour me some
if and how an instruction is incomplete, and (2) complete
water”, to the robot. This instruction is incomplete from the
the instruction using knowledge of the objects in the robot’s
robot’s perspective since it does not specify where the water
environment.
should be poured, but for a human, it might be obvious that
For the ﬁrst step (identifying incomplete instructions),
thewatershouldbepouredintothecup.Arobotthathasthe
we parse the natural language instruction into a structured
common sense to automatically resolve such incompleteness
representation, referred to as a verb frame. A verb frame is
innaturallanguageinstructions,justashumansdointuitively,
atuplecontainingapredicate(i.e.,averborverbphrase)and
will allow humans to interact with it more naturally and
a set of semantic roles and their associated content [18]. For
increase its overall usefulness. To this end, we introduce
example, LMCR automatically parses the instruction “pour
Language-Model-based Commonsense Reasoning (LMCR),
me some water” to the verb frame (pour, Theme: water,
Destination: ?), where “pour” is the predicate, “water”
ThisresearchwassupportedinpartbytheU.S.NationalScienceFoun-
dation(NSF)underAwardCCF-1533844,DARPAMCSGrantN66001-19- and “?” are arguments that help complete the meaning
2-4031,GoogleFocusedResearchAward,andARO-YIPAwardW911NF- of a predicate, and “Theme” and “Destination” are
18-1-0336.
semantic roles which specify the underlying relationship
1Department of Computer Science, University of North Carolina at
Chapel Hill, Chapel Hill, NC 27599, USA. {haonanchen, haotan, between arguments and the predicate. The empty tag ?
}
mbansal, ron @cs.unc.edu indicates that the argument of Destination is missing.
2Robotics Center and School of Computing, University of Utah, Salt
Under such a representation, incomplete instructions can be
LakeCity,UT84112,USA.adk@cs.utah.edu
∗Towhomcorrespondenceshouldbeaddressed. easily identiﬁed as not all roles in the verb frame are ﬁlled
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 1963
Authorized licensed use limited to: Heriot-Watt University. Downloaded on September 22,2020 at 01:07:33 UTC from IEEE Xplore.  Restrictions apply. Language-Model-based 
Inputs Output
Commonsense Reasoning (LMCR)
Predicate-
Speech 
Argument 
Recognition
Parsing
Audio pour water
Pour me some water ( , Theme: ,
Destination:?)
Incomplete 
verb frame
Complete 
Object list Language  verb frame Motion 
Detection Model 
Planning
Reasoning
RGB-D Image bell pepper  (pour,
plate Theme:water,
cup Destination:?)
…
Motions
Fig. 2: The components of a robot with LMCR.
with content from the instruction. The task of resolving taskbasedonincompleteinstructions,enablingmorenatural
incompleteinstructionsthenbecomesﬁllingthemissingrole human-robot interaction.
with objects in the environment.
For the second step (completing an incomplete instruc- II. RELATEDWORK
tion),wenotethatpeoplearemorelikelytoomitinformation Reasoning using commonsense knowledge to understand
from an instruction if it is obvious to the listener, so the incomplete natural language instruction has been studied in
correct role ﬁller should be the one that yields a complete a variety of contexts. For example, Bolt et al. [3] presented
verb frame with the highest probability among all possible a robotic system that could leverage deictic reference or
combinations.Inspiredbythis,LMCRusesaneuralnetwork pointing gestures to understand human instructions in a
based language model, which acts as a probability distri- situated human-robot interaction setting. Recent years have
bution over sequences of words. After training on textual seen systems like Prac [26] and RoboBrain [34] that have
corporacontainingdescriptionsofcommonhouseholdtasks, theabilitytoleverageworldknowledgetounderstandnatural
our language model is able to assign higher probabilities languageinstructions.However,thesesystemstendtorelyon
to candidate verb frames that correspond to more com- graph-based knowledge representations. For example, Prac
mon complete instructions, such as (pour, Theme: water, considered a similar commonsense reasoning problem as
Destination:cup).Thislanguagemodel,combinedwith ours,aimingatinferringthemostprobableexecutableaction
limiting the missing arguments to objects in the robot’s in a given context, but the knowledge is encoded in a Prac
vicinity, enables the robot to automatically ﬁll in missing knowledge base, which is constructed from manually anno-
information in natural language instructions via common tated clauses found in natural language recipes. LMCR, by
sense. contrast, uses a neural network language model and is based
Weincorporatetheabovelanguageunderstandingpipeline on the intuition that world knowledge is implicitly encoded
into a robot as shown in Fig. 2. The robot gets instructions in textual corpora. The idea is adapted from recent works in
and environmental information via the Speech Recognition neural language models such as ELMo [30], OpenAI GPT
and Detection modules respectively, processes the inputs [32], and BERT [9], using a pre-trained language model
via LMCR, and executes the speciﬁed task via the Motion to improve the performance of various downstream applica-
Planning module. A video of an LMCR-enabled robot is tions, including commonsense reasoning. These applications
providedinSupplementaryMaterials.Wealsoquantitatively show that neural network language models are well suited
evaluate LMCR on a human-annotated dataset collected as to encoding and extracting knowledge that exists in large
part of this work. We compile a novel dataset as existing language corpora.
datasets on commonsense reasoning such as [5, 31, 40] Tounderstandnaturallanguageinstructions,arobothasto
consist mostly of general-purpose verbs and nouns and are extract a semantically meaningful representation of natural
not aimed speciﬁcally at robot manipulation applications, language and ground it to the perceptual elements and
making them unsuitable for evaluating our method. In this actions in its environment. This process is referred to as
work we focus on kitchen assistance tasks (e.g., blend, languagegrounding[20].Severalapproacheshavebeenpro-
pour, sprinkle), but the same pipeline can be extended to posedforlanguagegrounding,whichcanbebroadlydivided
otherscenariosgivenrelevanttrainingdata.Theresultsshow into probabilistic models [14, 16, 27, 28] and deterministic
that incorporating commonsense knowledge via a language models [22, 23, 37, 38]. These approaches seek to ﬁnd
modelapproachenablesarobottounderstandandperforma an intermediate representation in order to bridge natural
1964
Authorized licensed use limited to: Heriot-Watt University. Downloaded on September 22,2020 at 01:07:33 UTC from IEEE Xplore.  Restrictions apply. O
language and machine commands. To bridge this gap, the the labels (e.g., ‘apple’, ‘banana’) of detected objects in
A
probabilistic models employ a probabilistic graphical model a testing scenario come from this vocabulary .
approach, while the deterministic models employ a frame- The problem we want to study is to translate the possibly
likestructure.Ourproposedmodelfallsintothedeterministic incomplete input instruction W into a complete verb frame
modelcategory.However,therelatedworkmentionedabove f with all arguments ﬁlled in, while the detected object
O
does not consider grounding unstated concepts with the list help with ﬁlling in the missing arguments. Thus the
help of commonsense world knowledge. Recently, due to robot’s motion planner can execute this verb frame later.
the advancement of deep neural networks, several works Below,weﬁrstdescribeourapproachforidentifyingmissing
use sequence learning and reinforcement learning to directly arguments using verb frames (Sec. III-B). We focus on
map text to actions, skipping the need for an intermediate the case where the human-provided instruction is missing
representation of instructions [2, 7, 17, 35, 39]. However, one of the two roles. We then introduce our approach to
they either consider only navigation tasks, or a simple simu- completing an incomplete verb frame using common sense
lated environment, where the possible actions are limited. In via a neural-network based language model (Sec. III-C),
contrast, our method generalizes to any task domain as we which will enable the robot to plan a motion to accomplish
can easily extend the set of our verb frame representations the desired task (Sec. III-D).
by adding more frames to our training corpus.
B. Identifying Incomplete Instructions
Affordance can be deﬁned as knowledge of an object’s
functionality, and understanding affordances is crucial for To identify if and how an instruction is incomplete, we
a robot to recognize human activities, interact with the parse the natural language instruction into a sequence of
environment, and achieve its goals [5]. Previous research verbframes.ThePredicate-ArgumentParsingmoduleinFig.
on affordance can be primarily divided into two categories, 2 takes the sequence of tokens W and outputs a sequence
namely,visualaffordanceandsemanticaffordance.Ourwork of verb frames as input. We use an off-the-shelf semantic
iscloselyrelatedtosemanticaffordance[5,42],whichseeks role labeling (SRL) model [13] to parse the sentence into
to model the possible actions that can be conducted on an verb frames, which provide us with a predicate-argument
object. However, these works only model single verb-object structure. Since some arguments may be missing from the
A
pairs. We extend the dependency by using verb frames, instruction, we augment the vocabulary to include an
which allows us to make inferences on object affordances empty token, which is used to indicate a missing argument.
conditioned on both the predicate and other roles. Using parsed verb frames, an incomplete instruction can be
identiﬁed as one having an empty token for one of its roles.
III. METHOD The problem of resolving an incomplete instruction then
A. Problem Deﬁnition
becomes ﬁlling the missing role with an object from the
The robot receives a spoken instruction from the user as environment.
input. Our Speech Recognition module, shown in Fig. 2,
C. Completing an Incomplete Instruction Using Common
transcribes the audio of spoken language into text, which
Sense
we specify as a sequence of K tokens representing words,
{ }
W = w ,w ,...,w . We use Google Cloud API [11] Given an incomplete verb frame f with one missing role
1 2 K O
for the transcription. The robot also receives input from its and a list of objects in the robot’s environment, we
RGB-D sensors. Our Detection module in Fig. 2 detects formalize the task of commonsense reasoning as ﬁnding
instances of certain classes of objects and their positions in the most proper roll ﬁller and outputting a complete verb
theinputRGB-Dimage.Thismodulecanbeimplementedby frame. This problem can be further treated as ranking a list
anobjectdetector,suchasMaskR-CNN[12].TheDetection of complete verb frames, as we can easily iterate over the
O O
module outputs a list of relevant objects in the vicinity of objectlist tocreateallpossiblecandidateverbframesthat
the robot, along with their associated positions. are feasible in the current environment. Thus, we implement
We represent actions that the robot can perform using commonsense reasoning as a scoring function g(f) where f
verbframes.Followingtheconventionintheframesemantic isacompleteverbframe.Andfromthelistofcandidateverb
parsing literature [8, 15], we deﬁne a verb frame as f = frames we pick the one with the highest score as the output
(v,r ,a ,...,r ,a ), where v denotes the predicate and verbframe.Werefertothescoreasaplausibilityscore.The
1 1 N N
r and a denote the i’th role and its argument, respectively. job of the commonsense reasoning method is then to deﬁne
i i ∈V V
The predicate v represents an action, where is the set the scoring function g.
of actions that the robot can perform (e.g., “pour”, “brush”, To compute the plausibility score, we note that people are
as summarized in the left column of Table I for our robot). more likely to omit information from an instruction if it is
We focus on predicates (actions) that take 2 arguments, so obvious to the listener, so the correct role ﬁller should be
wesimplifytheverbframestoitstwo-argumentspeciﬁcation the one that yields a complete verb frame with the highest
f = (v,r ,a ,r ,a ). In our work, r are drawn from a probabilityamongallpossiblecombinations.Tothisend,we
1 1 2 2 i
ﬁxed, pre-deﬁned set of role labels and are a function of usealanguagemodel(LM)whosegoalistopredicttheprob-
the predicates. At the same time, each a is drawn from a abilityofawordsequence(weassumeawordsequencewith
A i
ﬁxedvocabulary(i.e.,asetofwords) .Inourexperiments, higher probability to appear is more plausible). A language
1965
Authorized licensed use limited to: Heriot-Watt University. Downloaded on September 22,2020 at 01:07:33 UTC from IEEE Xplore.  Restrictions apply. model factorizes the probability according to the chain rule. E. ComparisonMethodsforCommonsenseReasoningEval-
{ }
Using u ,u ,...,u to denote an entire sentence with T uation
1 2 T
tokens, the chain rule can be written as,
(cid:89) Asdescribedabove,LMCRgivesascoretoeachcomplete
verb frame f =(v,r ,a ,r ,a ) in a generated list, and the
1 1 2 2
T | framewiththehighestscoreischosenastheoutput.Weuse
p(u1,u2,...,uT)= p(ut u1,u2,...,ut−1), (1) g(f) to denote the scoring function. In Sec. V, we compare
t=1 the scoring function of our method LMCR against those of
| co-occurrence,Word2Vec,andConceptNet,describedbelow.
where p(u u ,u ,...,u − ) is the conditional probability
t 1 2 t 1 a) Co-occur: Thisistheshorthandof“co-occurrence”.
of the word u given the previous words. In the Language
t Chao et al. [5] used co-occurrence in a textual corpus to
Model Reasoning module of our work, we model this con-
determine the relatedness of a verb-object pair. We extend
ditional probability using a recurrent neural network (RNN)
this to determine the relatedness of a verb frame, which is
[21].
deﬁned as
Following the recent progress in the study of language
models, we also tried other advanced pre-trained language g (f)=cooccur(v,a )+cooccur(a ,a ), (2)
cooccur 2 1 2
models such as ELMo [30] and BERT [9]. However, we
where cooccur(x,y) denotes the total normalized co-
empiricallydidnotﬁndasigniﬁcantdifferencebetweenthese
occurrencescoreofxandy inthetrainingtextcorpora.This
different language models of our approach, so we take the ∗
is computed by count(x,y)/(count(x) count(y)) where
simplest RNN-based LM as our model here.
count(x) and count(y) are the occurrences of x and y
Note that the language model operates on a sequence of
individually in the corpus and count(x,y) is the count of
words, but verb frames are a structured representation of
x and y co-occurring in the same sentence.
language. We thus need to serialize the candidate complete
b) Word2Vec: Chao et al. [5] also used Word2Vec as
verb frames into a sequence of words, a process known as
oneoftheiraffordanceminingmethods.Similarly,weextend
linearization[10,19].Weproposetwolinearizationmethods
it to work on verb frames by deﬁning the scoring function
in this work. The ﬁrst is to concatenate the predicate and all
as
arguments directly, i.e., to treat (v,a ,a ) as a sequence. −
1 2 g (f)= (dist(v,a )+dist(a ,a )), (3)
This results in unnatural sounding word sequences. The word2vec 2 1 2
second is to make a more natural sentence from the frame where dist(x,y) denotes the Euclidean distance of word
using a rule-based approach. With these two approaches, embeddings of x and y. We use GloVe embeddings [29]
(pour, Theme: water, Destination: cup) is converted for this comparison.
to pour water cup with the former approach and pour water c) ConceptNet: Systems like PRAC and RoboBrain
to the cup with the latter one. We refer to the LM trained use knowledge graphs and conduct probabilistic inference
andtestedwiththeformerapproachasframe-basedLMand on the graph for instruction completion. Similarly, we use
the latter one as sentence-based LM. For both, the sequence ConceptNet [36], which is a large scale common sense
format needs to be consistent during training and inference knowledge graph. We use the relatedness score provided by
to get the best performance. As our training corpus contains the ConceptNet API [6], and compute the score for a frame
natural language sentences, we can use them to train the f as
sentence-based LM directly, while frame-based LM requires g (f)=rel(v,a )+rel(a ,a ), (4)
ConceptNet 2 1 2
predicate-argument parsing on the entire training corpus as
a pre-processing step. where rel(x,y) denotes the ConceptNet relatedness score
[36] of x and y.
IV. DATASETS
D. Motion Planning for a Complete Verb Frame
a) Training Data for the Language Model: The train-
The motion planner takes as input a complete verb frame ing data for LMCR’s language model comes from textual
O
f and the positions of relevant objects in and computes corpora, which can be treated as the knowledge source
a motion for the robot that executes the task speciﬁed by of the method. We use YouCook2 [41] and Now You’re
∈V
the verb frame. For each v , we deﬁne a motion planner Cooking (NYC) [25] as training corpora. YouCook2 is a
parameterizedbyitsarguments.Inourimplementation,each large instructional video dataset designed to facilitate video
motion planner is deﬁned by a series of waypoints for the captioning research. The cooking steps for each video are
end-effector.Eachwaypointisdeﬁnedinacoordinatesystem annotated with temporal boundaries and described by im-
O
relative to the positions of a task-relevant object in [4], perative English sentences, resulting in around 14,000 raw
which enables the robot to plan motions that are robust to descriptionsofcookingactions.NYCcontainsover150,000
the movement of the objects in the environment. Reaching recipes,eachcontainingastep-by-stepdescriptionofhowto
these waypoints in sequence executes the action. We use execute the recipe. Although NYC is much larger in size,
the motion planning toolkit MoveIt! [24] to compute the it contains unrelated information such as ingredient lists and
movement of the robot arm given the relative waypoints. comments,whichismoresimilartowhatwecangetdirectly
1966
Authorized licensed use limited to: Heriot-Watt University. Downloaded on September 22,2020 at 01:07:33 UTC from IEEE Xplore.  Restrictions apply. −
Question Preparation Annotation Scenario Construction randomlypickoneframefromthepositiveandk 1frames
from the negative subset (we restrict one correct answer in a
Vocabulary
test scenario for the convenience of evaluation), keeping the
Positive Subset Scenarios
abpopwlel,, .(.p.our, water, bowl), predicateandoneoftheargumentsthesameandvaryingthe
broccoli, (pour, water, cup),
…towel, CVoermbp Flertaem es Plausibility Scores ... otherargument.Inthisway,wecanconstructatestscenario
umbrella … … Ambiguous Subset (pour, water, ?) with k candidates, where one of them is plausible based on
… (pour, water, apple) 2.8 ... cup,
(pour, water, apple) (pour, water, bowl) 4.0 (pour, water, apple), bird, the human annotation.
IVnecrobm Fprlaemte es ((…((…ppppoooouuuurrrr,,,,    wwwwaaaatttteeeerrrr,,,,    bbtuoormwowbcelcrl))oelllia)) (…((…pppooouuurrr,,,   wwwaaattteeerrr,,,   btuormwobcecrl)oelllia))4…31…...406 N(..p.eogura, twiavteer ,S touwbesl)e,t bcktaooebiyonlbke,o,ard, andIntohuerneuxmpebreimr eonftsc,awndeidvaatreys tkhetoplacurseiabtielittyestthrsecsehnoaldrioλs
((bblleenndd,,  bcourtnte,r ?, )?) Average .(.p.our, water, book), with different difﬁculties. A larger λ brings more ambigu-
…(…((psspporruiinnr,kk wlleea,,  tsweara,l tt?,e )r?,) ?) …ppIoonuusrr  twwraautteecrr  tttiooo  ttnhhees  abpopwlle Dif potho yeuor c uwh atehtfei nsr kato yi tst hp..eo. saspipbllee (..p.our, water, umbrella), opulasusfirbaimliteys)(iwnthoicthheepvoesnitihvuemaanndsnaergeatnivoet ssuubrseetasb.oAutlatrhgeeirr
pour water to the broccoli Strongly agree
…pour water to the towel ANgerueteral k introduces more candidates in a single scenario. In both
p…our water to the umbrella SDtirsoanggrelye disagree cases, the test scenarios become more challenging.
Fig. 3: Human judgment collection and scenario preparation. V. RESULTS
a) Comparison with Other Methods: Based on the
TABLE I: Overall accuracy and accuracy per predicate for
collectedhumanjudgmentdataset,wecomparetheproposed
λ=1.0 and k =6. LMCR approach1 with other baseline methods Co-occur,
Word2Vec,andConceptNet,describedabove,aswellaswith
Random Word2Vec LMCR (Ours)
Cooccur ConceptNet a uniform random choice (Random). Each method deﬁnes a
scoringfunctiong(f)givenacompleteverbframef.Givena
blend 0.18 0.46 0.66 0.66 0.88
brush 0.18 0.58 0.50 0.26 0.86 testscenariocontainingkcandidateverbframes,asuccessful
dip 0.10 0.42 0.20 0.40 0.50 predictiongivesthehighestscoretothegroundtruth,namely
dump 0.24 0.68 0.64 0.58 0.54 the one sampled from the positive subset. We consider 11
ﬁll 0.20 0.62 0.46 0.42 0.94 verbs listed in the leftmost column of Table I, vary the
fry 0.18 0.86 0.48 0.62 0.66
plausibility threshold λ and the number of objects in the
heat 0.06 0.74 0.40 0.70 0.64
pour 0.08 0.74 0.56 0.52 0.60 list k to create scenarios with various difﬁculties, and report
rub 0.04 0.54 0.38 0.40 0.50 the accuracy (success rate). Table I gives the overall and per
season 0.16 0.58 0.50 0.84 0.84 predicateaccuracywithk =6andplausibilitythresholdλ=
sprinkle 0.20 0.58 0.54 0.36 0.78 1.0,andFig.4AandFig.4Bshowtheresultswhenvaryingλ
Overall 0.15 0.62 0.48 0.52 0.70 and k respectively. LMCR performs consistently better than
other methods when considering all actions (predicates), for
allvariationsofkandλ,althoughsomemethodsshowbetter
by crawling web data. In our experiment, we deliberately
performance on speciﬁc individual predicates. The results
keepthis extraneousinformationin ordertodetermine ifthe
suggests that, overall, LMCR better encodes the type of
languagemodelcandistillcommonsenseknowledgerequired
commonsense reasoning we are addressing in this work.
in human-robot interaction from noisy textual corpora.
b) Comparison under Different Training Settings: We
b) Testing Data Based on Human Judgments: In order
also compare several different ways of training the language
to quantitatively evaluate LMCR’s commonsense reasoning
model(LM)usedbytheLanguageModelReasoningmodule
for robotic assistance instructions, we created a new human-
of LMCR. To do so we train the LM with two different
generated dataset, since existing datasets on commonsense
linearization strategies, namely, frame-based (Frame) and
reasoning[5,31,40]arenotspeciﬁctoourdomainofﬁlling
sentence-based(Sent.)linearization.WealsotraintheLMon
in missing information in instructions for robotic assistance
different combinations of training corpora, YouCook2 data
tasks. We show our data collection process in Fig. 3. We
only (YouCook2), Now You’re Cooking data only (NYC),
provided human annotators on Amazon Mechanical Turk
and the combination of the two (All data). Results for these
(AMT)[1]withsentencesrepresentingcompleteverbframes
comparisons are shown in Fig. 4C and Fig. 4D which vary
andaskedthemtogiveaplausibilityratingforeachofthem,
λ and k respectively. As shown in the results, sentence-
scaling from 1 (most implausible) to 5 (most plausible) (see
based LM generally performs better than frame-based LM.
the ﬁgure for examples). We collected 5 annotations for all
Wesuspectthisisduetothefactthattheformerisend-to-end
complete verb frames in our dataset.
trainedwhilethelatterrequiresgeneratingtrainingdatafrom
In our experiments, for each predicate we split the verb
an upstream predicate-argument parser, whose errors may
frames into positive, ambiguous, and negative subsets using
propagate to the training process of the frame-based LM.
a plausibility threshold λ. Namely, for a complete verb Also, the parser cannot generate a frame for predicates that
frame with an average plausibility rating s¯, it is included
−
in the positive subset if s¯ > 5 λ, the negative subset if 1Thelanguagemodelhereisthesentence-basedLMtrainedonboththe
s¯ < 1+λ, and the ambiguous subset otherwise. We then YouCook2andNowYou’reCookingdataset.
1967
Authorized licensed use limited to: Heriot-Watt University. Downloaded on September 22,2020 at 01:07:33 UTC from IEEE Xplore.  Restrictions apply. A B
C D
Fig. 4: Accuracy results for LMCR. We evaluate the ability of LMCR to correctly ﬁll in missing information for scenarios of varying
difﬁculty,bychangingtheplausibilitythresholdλandnumberofobjectsperscenariok.(A)Overallaccuracywithλchangingfrom0.6
to 2.0 and k = 6 for LMCR and baseline methods. (B) Overall accuracy with k changing from 4 to 40 and λ = 1.0 for LMCR and
baseline methods. (C) Overall accuracy with λ changing from 0.6 to 2.0 and k =6 for different language model training settings. (D)
Overall accuracy with k changing from 4 to 40 and λ=1.0 for different language model training settings. There are 500 test scenarios
in all settings.
are not in its verb vocabulary, even though these relatively c) Real Robot Experiment: We deploy LMCR on a
rare predicates can be beneﬁcial when learning others. For Baxter robot [33], a research robotics platform with two
example, “scatter some salt on the beef” would help with 7 degree of freedom arms, and demonstrate its ability to
the learning for “spread” and “sprinkle” as they can be successfully accomplish intended tasks given incomplete
synonyms.Whilethesentence-basedLMcantakeadvantage spoken instructions in different scenarios. A video of the
of this,the frame-basedschema losesthis informationin the LMCR-enabledrobotinactionisprovidedinSupplementary
training data, since “scatter” is not among the 11 verbs we Materials.
consider. For sentence-based LM, the performance of the VI. CONCLUSION
models trained with NYC and all data (YouCook2+NYC) In this work, we presented a robot that can detect when a
are similar, and both are better than the model trained on human instruction is incomplete and automatically resolve it
YouCook2alone.Forframe-basedLM,thealldatayieldsthe by observing the environment and making inferences based
best performance. Although NYC is noisier than YouCook2, on commonsense world knowledge. The use of a neural
the former still brings positive input to the language model language model in capturing the commonsense knowledge
training, since it is much larger than the latter. This suggests allows us to leverage online textual corpora and train the
that the language model is robust to the noise in the training model with little manual intervention. We demonstrate the
data on the commonsense reasoning task considered in this effectiveness of our algorithm both by measuring the align-
paper. These results demonstrate that a human annotated ment with human judgments and on a physical robot. In
dataset, such as YouCook2, is not necessarily better than a future work, we plan to investigate the robustness of the
recipe-based dataset, although it may still be helpful. Based entire system against error in each module, consider verb
ontheaboveanalyses,weusethesentence-basedLMtrained frames with a varying number of missing arguments, and
on all data when comparing with the other commonsense use dialogue when LMCR cannot make a conﬁdent decision
reasoning approaches in the previous section. about ﬁlling in missing information.
1968
Authorized licensed use limited to: Heriot-Watt University. Downloaded on September 22,2020 at 01:07:33 UTC from IEEE Xplore.  Restrictions apply. REFERENCES structions,”TheInternationalJournalofRoboticsResearch,vol.35,
no.1-3,pp.281–300,2016.
[1] AmazonMechanicalTurk,https://www.mturk.com/.
[23] D.K.Misra,K.Tao,P.Liang,andA.Saxena,“Environment-driven
[2] V. Blukis, D. Misra, R. A. Knepper, and Y. Artzi, “Mapping
lexiconinductionforhigh-levelinstructions,”inProceedingsofthe
navigationinstructionstocontinuouscontrolactionswithposition-
53rdAnnualMeetingoftheAssociationforComputationalLinguis-
visitation prediction,” in Conference on Robot Learning, 2018,
ticsandthe7thInternationalJointConferenceonNaturalLanguage
pp.505–518.
Processing(Volume1:LongPapers),vol.1,2015,pp.992–1002.
[3] R. A. Bolt, “Put-that-there: Voice and gesture at the graphics
interface,”3,vol.14,ACM,1980. [24] Moveit!https://moveit.ros.org/.
[4] C.Bowen,G.Ye,andR.Alterovitz,“Asymptotically-optimalmotion [25] Now you’re cooking recipe dataset, http://www.ffts.com/
planning for learned tasks using time-dependent cost maps,” IEEE recipes.htm,FoodforThoughtSoftware,2013.
[26] D.NygaandM.Beetz,“Cloud-basedprobabilisticknowledgeser-
Trans.AutomationScienceandEngineering,vol.12,no.1,pp.171–
vicesforinstructioninterpretation,”inRoboticsResearch,Springer,
182,Jan.2015.
2018,pp.649–664.
[5] Y.-W.Chao,Z.Wang,R.Mihalcea,andJ.Deng,“Miningsemantic
[27] R.Paul,J.Arkin,D.Aksaray,N.Roy,andT.M.Howard,“Efﬁcient
affordances of visual object categories,” in 2015 IEEE Conference
groundingofabstractspatialconceptsfornaturallanguageinterac-
onComputerVisionandPatternRecognition(CVPR),IEEE,2015,
tion with robot platforms,” The International Journal of Robotics
pp.4259–4267.
Research,2018.
[6] ConceptNet5 API, https://github.com/commonsense/
[28] R.Paul,J.Arkin,N.Roy,andT.Howard,“Groundingabstractspa-
conceptnet5/wiki/API.
tial concepts for language interaction with robots,” in Proceedings
[7] A.Das,S.Datta,G.Gkioxari,S.Lee,D.Parikh,andD.Batra,“Em-
ofthe26thInternationalJointConferenceonArtiﬁcialIntelligence,
bodiedquestionanswering,”inProceedingsoftheIEEEConference
AAAIPress,2017,pp.4929–4933.
onComputerVisionandPatternRecognition(CVPR),vol.5,2017.
[29] J.Pennington,R.Socher,andC.Manning,“Glove:Globalvectors
[8] D. Das, D. Chen, A. F. Martins, N. Schneider, and N. A. Smith,
forwordrepresentation,”inProceedingsofthe2014conferenceon
“Frame-semanticparsing,”Computationallinguistics,vol.40,no.1,
empiricalmethodsinnaturallanguageprocessing(EMNLP),2014,
pp.9–56,2014.
pp.1532–1543.
[9] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-
[30] M.E.Peters,M.Neumann,M.Iyyer,M.Gardner,C.Clark,K.Lee,
trainingofdeepbidirectionaltransformersforlanguageunderstand-
andL.Zettlemoyer,“Deepcontextualizedwordrepresentations,”in
ing,”arXivpreprintarXiv:1810.04805,2018.
Proceedingsofthe2018ConferenceoftheNorthAmericanChapter
[10] K.FilippovaandM.Strube,“TreelinearizationinEnglish:Improv-
oftheAssociationforComputationalLinguistics:HumanLanguage
ing language model based approaches,” in Proceedings of Human
Technologies(NAACL:HLT),NewOrleans,Louisiana,USA,2018.
LanguageTechnologies:The2009AnnualConferenceoftheNorth
[31] L.Pylkka¨nenandB.McElree,“AnMEGstudyofsilentmeaning,”
AmericanChapteroftheAssociationforComputationalLinguistics,
Journalofcognitiveneuroscience,vol.19,no.11,pp.1905–1921,
Companion Volume: Short Papers, Association for Computational
2007.
Linguistics,2009,pp.225–228.
[32] A.Radford,K.Narasimhan,T.Salimans,andI.Sutskever,“Improv-
[11] Google, Google cloud speech-to-text, https : / / cloud .
inglanguageunderstandingbygenerativepre-training,”2018.
google.com/speech-to-text/.
[12] K.He,G.Gkioxari,P.Dolla´r,andR.Girshick,“MaskR-CNN,”in [33] Rethink Robotics, Baxter research robot, www .
2017 IEEE International Conference on Computer Vision (ICCV), rethinkrobotics.com/baxter-research-robot/,
2013.
IEEE,2017,pp.2980–2988.
[34] A. Saxena, A. Jain, O. Sener, A. Jami, D. K. Misra, and H. S.
[13] L.He,K.Lee,M.Lewis,andL.Zettlemoyer,“Deepsemanticrole
Koppula, “Robobrain: Large-scale knowledge engine for robots,”
labeling:Whatworksandwhat’snext,”inProceedingsofthe55th
arXivpreprintarXiv:1412.0691,2014.
Annual Meeting of the Association for Computational Linguistics
[35] P.Shah,M.Fiser,A.Faust,J.C.Kew,andD.Hakkani-Tur,“Fol-
(Volume1:LongPapers),vol.1,2017,pp.473–483.
lownet: Robot navigation by following natural language directions
[14] S. Hemachandra, F. Duvallet, T. M. Howard, N. Roy, A. Stentz,
withdeepreinforcementlearning,”arXivpreprintarXiv:1805.06150,
andM.R.Walter,“Learningmodelsforfollowingnaturallanguage
2018.
directions in unknown environments,” in 2015 IEEE International
[36] R. Speer, J. Chin, and C. Havasi, “Conceptnet 5.5: An open
Conference on Robotics and Automation (ICRA), IEEE, 2015,
multilingual graph of general knowledge,” in Thirty-First AAAI
pp.5608–5615.
ConferenceonArtiﬁcialIntelligence,2017.
[15] K. M. Hermann, D. Das, J. Weston, and K. Ganchev, “Seman-
[37] B. J. Thomas and O. C. Jenkins, “Roboframenet: Verb-centric
tic frame identiﬁcation with distributed word representations,” in
semantics for actions in robot middleware,” in Robotics and Au-
Proceedings of the 52nd Annual Meeting of the Association for
tomation (ICRA), 2012 IEEE International Conference on, IEEE,
Computational Linguistics (Volume 1: Long Papers), vol. 1, 2014,
2012,pp.4750–4755.
pp.1448–1458.
[38] J.Thomason,A.Padmakumar,J.Sinapov,N.Walker,Y.Jiang,H.
[16] T.M.Howard,S.Tellex,andN.Roy,“Anaturallanguageplanner
Yedidsion,J.Hart,P.Stone,andR.J.Mooney,“Improvinggrounded
interface for mobile manipulators,” in 2014 IEEE International
naturallanguageunderstandingthroughhuman-robotdialog,”arXiv
Conference on Robotics and Automation (ICRA), IEEE, 2014,
preprintarXiv:1903.00122,2019.
pp.6652–6659.
[39] X.Wang,Q.Huang,A.Celikyilmaz,J.Gao,D.Shen,Y.-F.Wang,
[17] M.Janner,K.Narasimhan,andR.Barzilay,“Representationlearning
W.Y.Wang,andL.Zhang,“Reinforcedcross-modalmatchingand
forgroundedspatialreasoning,”TransactionsoftheAssociationof
self-supervised imitation learning for vision-language navigation,”
ComputationalLinguistics,vol.6,pp.49–61,2018.
arXivpreprintarXiv:1811.10092,2018.
[18] D. Jurafsky and J. H. Martin, Speech and language processing.
[40] T. Warren, E. Milburn, N. D. Patson, and M. W. Dickey, “Com-
PearsonLondon,2014,vol.3.
prehending the impossible: What role do selectional restriction
[19] I.Konstas,S.Iyer,M.Yatskar,Y.Choi,andL.Zettlemoyer,“Neural
violations play?” Language, cognition and neuroscience, vol. 30,
AMR: Sequence-to-sequence models for parsing and generation,”
no.8,pp.932–939,2015.
in Proc. Annual Meeting of the Association for Computational
[41] L. Zhou, C. Xu, and J. J. Corso, “Towards automatic learning of
Linguistics(Volume1:LongPapers),vol.1,2017,pp.146–157.
proceduresfromwebinstructionalvideos,”inAAAIConferenceon
[20] C. Matuszek, “Grounded language learning: Where robotics and
ArtiﬁcialIntelligence,2018.
nlp meet (early career spotlight),” in Proceedings of the 27th
[42] Y. Zhu, A. Fathi, and L. Fei-Fei, “Reasoning about object affor-
International Joint Conference on Artiﬁcial Intelligence (IJCAI),
dancesinaknowledgebaserepresentation,”inEuropeanconference
Stockholm,Sweden,2018.
[21] T.Mikolov,M.Karaﬁa´t,L.Burget,J.Cˇernocky`,andS.Khudanpur, oncomputervision,Springer,2014,pp.408–424.
“Recurrent neural network based language model,” in Eleventh
Annual Conference of the International Speech Communication
Association,2010.
[22] D. K. Misra, J. Sung, K. Lee, and A. Saxena, “Tell me dave:
Context-sensitivegroundingofnaturallanguagetomanipulationin-
1969
Authorized licensed use limited to: Heriot-Watt University. Downloaded on September 22,2020 at 01:07:33 UTC from IEEE Xplore.  Restrictions apply. 