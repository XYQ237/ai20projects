2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
Beyond Top-Grasps Through Scene Completion
Jens Lundell, Francesco Verdoja and Ville Kyrki
Abstract—Current end-to-end grasp planning methods pro-
pose grasps in the order of seconds that attain high grasp
success rates on a diverse set of objects, but often by con-
strainingtheworkspacetotop-grasps.Inthiswork,wepresent
amethodthatallowsend-to-endtop-graspplanningmethodsto
generatefull six-degree-of-freedomgrasps usinga singleRGB-
D view as input. This is achieved by estimating the complete
shape of the object to be grasped, then simulating different
viewpoints of the object, passing the simulated viewpoints to (a)
(b)
an end-to-end grasp generation method, and ﬁnally executing
theoverallbestgrasp.Themethodwasexperimentallyvalidated
on a Franka Emika Panda by comparing 429 grasps generated
bythestate-of-the-artFullyConvolutionalGraspQualityCNN,
both on simulated and real camera images. The results show
statistically signiﬁcant improvements in terms of grasp success
rate when using simulated images over real camera images,
especially when the real camera viewpoint is angled. Code
and video are available at https://irobotics.aalto.ﬁ/beyond-top-
grasps-through-scene-completion/.
I. INTRODUCTION
Robotic grasping has undergone a paradigm shift from
analytical methods toward data-driven ones. Deep learning
is the major driving force behind the shift and has given
rise to a diverse set of methods [1]–[8]. These methods
typicallyreachhighgraspsuccessrates(oftenabove90%)on
awidevarietyofobjectswhilekeepingthetotalcomputation
time in the order of seconds, surpassing analytical methods (c)
by a large margin. However, to reach such a performance
the grasp planning problem is usually constrained to the Fig. 1: (a) shows a point-cloud of a cluttered scene acquired
generationoftop-graspswithfourdegrees-of-freedom(dof): with a real camera while (b) shows a simulated top-view of
one orientation and three translations. Top-grasps are good thesamescenebutshapecompleted.(c)showsthatwithour
if the camera perceiving the environment is perpendicular method, successful grasps can be generated from approach
to the plane supporting the target. However, as shown in directions different from the camera viewpoint.
this work, once the camera views a scene from an angle the
performance drops. In such situations the grasping methods
Fig. 1, such a solution enables a robot to grasp objects from
need to propose grasps in full six dof space to allow a robot
directions different from the one of the real camera.
to approach objects from any possible direction.
One viable option to achieve full 6 dof grasping with cur- TO this end, we present a grasping pipeline that uses
rent state-of-the-art grasping methods is to mount a camera the state-of-the-art Fully Convolutional Grasp Quality CNN
on the robot itself and have it scan the scene from multiple (FC-GQ-CNN) [2] to propose grasps. The pipeline ﬁrst
viewpoints. However, not only is such an approach slow segments a point-cloud of the scene into objects. Then the
as the robot needs to ﬁrst plan where to move and then shape of each object is estimated and placed in a physics
physically move there but also the robot might self-occlude simulator. In the context of this work, we refer to this as
the view of the camera, rendering the method useless. A scenecompletion.Insidethephysicssimulator,asetofdepth
novelalternative,whichisstudiedinthiswork,istosimulate images are sampled from different viewpoints and fed to the
different viewpoints of the object to be grasped and feed grasp proposal method generating a set of grasp candidates.
these to the methods proposing 4 dof grasps. As shown in The grasp candidate with the highest score is then executed
on the robot.
ThisworkwassupportedbytheStrategicResearchCouncilatAcademy Theproposedgraspingpipelineisexperimentallyvalidated
ofFinland,decision314180.
onaFrankaEmikaPandabybenchmarkingitagainstgrasps
J.Lundell,F.VerdojaandV.KyrkiarewithSchoolofElectricalEngineer-
ing,AaltoUniversity,Finland.{firstname.lastname}@aalto.fi proposedonrealdepthimagesonsingleobjectgraspingand
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 545
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:55:51 UTC from IEEE Xplore.  Restrictions apply. graspinginclutter.Theresultsof429graspsonsingleobject The GQCNN was later improved in [2] through the use of
grasping show statistically signiﬁcant improvement in terms on-policy data and a fully convolutional network structure
of higher grasp success rate when planning is performed called FC-GQ-CNN. The state-of-the-art FC-GQ-CNN was
on simulated depth images compared to planning solely on fasterthanGQCNNwhilesamplingabout5000xmoregrasps
real camera images. Similar results were also evident when and was thus used to generate grasps in this work.
grasping in cluttered scenes. Another line of research in end-to-end data-driven grasp
Themain contributionsof thispaper are:(i) anovel grasp planningisReinforcemntLearning(RL)[17]–[20]wherethe
planning pipeline that enables existing 4 dof end-to-end goalistolearnthesensor-to-graspmapdirectlythroughtrial
methodstoproposefull6dofgrasps,(ii)amethodtodensely and error on the robot. Models learned with RL can attain a
sample simulated depth images, and (iii) an empirical evalu- high grasp success rate without any hand-labeled data-sets,
ation of the proposed method against state-of-the-art on real but the extensive interaction time needed to learn the model,
hardware, presenting a statistically signiﬁcant improvement whichcanbemonthsonphysicalrobots[19],isabottleneck.
in terms of higher grasp success rate using the proposed Althoughsomeworkhavereducedthereal-worldinteraction
method. byusingsimulation[20],thelearnedmodelsstillneedsﬁne-
tuning on physical hardware to reach similar grasp success
II. RELATEDWORKS rate as methods that uses supervision [1], [2].
To date, many grasping methods rely fully or in part A major limitation in most end-to-end data-driven grasp
on deep learning. Some methods only use deep learning planning works is that the planned grasp is only from the
to extract additional information about objects with e.g., viewpoint of the camera, which effectively constrains the
shape completion [9], [10] or tactile information [11] and graspstoasubsetofthecomplete6dofworkspace(typically
then use analytical methods to plan the actual grasp [12], 4 dof grasps are considered). The work presented here lifts
while others employ data-driven grasp planning in an end- this limitation by shape completing the real objects, placing
to-end fashion to generate grasps directly from images [1]– them into a physics simulator, and from there sampling
[8]. We will review both shape completion and end-to-end differentviewpointsoftheobject.Ourmethodhenceenables
data-driven grasp planning as both are vital parts of our standardend-to-enddata-drivengraspplanningmethodsthat
grasping pipeline. suggest grasps from only one camera viewpoint to generate
full 6 dof grasps from other directions such as the back of
A. Deep Shape Completion
the object.
Inthecontextofshapecompletionfromincompletepoint-
clouds, most recent improvements come from the adoption III. PROBLEMFORMULATION
ofdeeplearning.Forinstance,differentworkshaveexplored Inthiswork,weaddresstheproblemofgraspingunknown
tailored network structures [9], [13], [14], semantic object objects lying on a supporting surface with a robotic arm
classiﬁcation to aid the reconstruction [15], the integration equipped with a parallel-jaw gripper. Information about the
of other sensing modalities such as tactile information [11], sceneisobtainedbyaRGB-Dcamerawhoseposeisarbitrary
or the exploitation of the network uncertainty [10]. but known relative to the robot.
In the context of robotics grasping, [9], [11] and [10] Formally, let x = (c,O) denote a state representing the
∈ R
are the most interesting as they not only focus on shape environment, where c 6 is the camera pose, and the
{ O }
reconstruction quality but also on grasping accuracy. In set O = ( ,p ) N contains the properties of the N
i i i=0 ∈ R
this work, we make use of our previous shape completion objects to be grasped, described by their pose p 6 and
O
network [10] to complete objects but instead of planning model . The state is partially observable as O can only
grasps with analytical methods—which is computationally be indirectly and incompletely observed using the RGB-
expensive—we turn to data-driven grasp planning. D camera. The camera produces a 2.5D point-cloud y =
{y }H×W ∈RH×W which can be represented as a H ×W
B. End-To-End Data-Driven Grasp Planning i i=1 +
depth image, assuming known camera intrinsic parameters.
∈R
The general interest in end-to-end data-driven grasp plan- Let g 6 denote a parallel-jaw grasp, described by the
∈ { }
ning came after the pioneering work by Saxena et al. [16] 6D pose of the gripper center point and S(g,x) 0,1
where they trained a logistic regression model to directly be a binary-valued grasp success metric indicating, e.g.,
predict good grasping points from a monocular image. To force closure. Assuming a joint distribution p(S,g,x,y),
E |
train the logistic regressor they used a large amount of let Q(g,y) = [S g,y] be the expected value of the
syntheticallylabeledimagesofobjectsandthecorresponding metric given a grasp g and a point-cloud y. The quality
grasping location. Q is intractable in most real cases. Therefore, it is modeled
The use of synthetic data to train the sensor-to-grasp map in data-driven grasp planning methods with a learned para-
was later used in a wide variety of similar methods [1]–[8]. metric model Q with parameters θ. The parametric model
θ
For instance, Mahler et al. [1] used a data-set containing Q is typically optimized either with supervised learning on
θ
millions of synthetic antipodal top-grasps on a wide variety synthetic [1] or real grasping data [4], or with RL [19].
of objects to train a Grasp Quality CNN (GQCNN) that Then, given a point-cloud y obtained from a known
generatesagraspfromadepthimageintheorderofseconds. camera pose c, the goal of most data-driven grasp planning
546
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:55:51 UTC from IEEE Xplore.  Restrictions apply. Input point-cloud Filtering Segmentation Scene completion Grasp sampling Grasp execution
Fig. 2: The proposed grasping pipeline
∗
methods is to ﬁnd a grasp g such that: The pipeline is agnostic to the segmentation algorithm
∗ employed, but the assumption is that, after the segmentation
g =argmaxQ (g,y), (1)
∈G θ step, each region in R contains points belonging to a
g
G different object to be grasped. The next step is to estimate
where is a set of grasp candidates. However, such a
each object’s properties through scene completion.
formulation only accounts for grasps approaching the scene
from the same direction as the camera, which is usually
B. Scene Completion
looking at it from top, leading to only top-grasps being
proposed. Scene completion refers to the process of both shape
Instead, we propose to extend this framework to allow completing each object in the scene and then placing them
object grasping from any direction, even those not directly inaphysicssimulatoraccordingtotheirindividualestimated
seen by the camera. For this, we need a function xˆ =C(y) pose. Shape completion refers to reconstructing the shape of
as an estimate of the full state x from the point-cloud y. an object from partial information about it in the form of a
Practically, this means understanding how many and what point-cloud. More precisely, a shape completion algorithm
O
objects are in the scene (i.e., segmentation), and then for estimates ( ,p) given a point-cloud r. To shape complete
each object proposing a model O and a pose p (i.e., shape objectsweusedthepre-trainedfullyconvolutionalhour-glass
completion). Once a state estimate xˆ is available we want to shapedDeepNeuralNetwork(DNN)proposedin[10]whose
evaluatethequalityofgraspsapproachingfromanydirection input is a voxel grid of the point-cloud captured from the
and execute the ﬁrst kinetically feasible one with highest cameraandoutputisacompletedvoxelgrid.Thecompleted
quality. Our proposed pipeline to achieve this is described voxel grid is post-processed into a mesh by merging it
next. with the input point-cloud and running the marching cube
algorithm [21].
IV. GRASPINGPIPELINE The DNN in [10] also included dropout layers throughout
The grasping pipeline shown in Fig. 2 consist of: (i) ﬁl- that were active during run-time to generate a set of shape
tering and segmenting the real objects, (ii) shape complet- samples representing, through Monte Carlo sampling, the
ing each segment and add them to a physics simulator, shape uncertainty. In this work, we also generate shape
(iii) generate and rank grasps from different viewpoints of samples but average them together to get a mean shape,
the objects, (iv) execute the best ranked grasp on the real effectivelyignoringtheshapeuncertainty.Althoughitwould
robot. be possible to deactivate the dropout layers at run-time and
only consider a point estimate of the shape, the beneﬁt of
A. Segmentation
using the mean shape is that it is smoother, removing sharp
Thesceneinthepoint-cloudcontainsanunknownnumber
artefacts on the shape which many end-to-end data-driven
of objects lying on a table. We ﬁrst remove points that are
grasp planning methods often rank as stable grasp points.
part of the background indicated by their distance exceeding ∈
For each region r R we generate, through shape
a threshold, as well as points that belong to the supporting Oi
completion,objects( ,p ).Together,allobjectsrepresents
surface identiﬁed using the known pose of the camera with i i
an estimate xˆ of the real environment state x. The state
respect to the robot. We are then left with a ﬁltered point-
estimate xˆ, containing all objects represented as meshes, are
cloud y¯ containing only the view of the objects to be
subsequently placed in a physics simulator. The next step is
segmented.
then to sample grasps over the state estimate.
Given the point-cloud y¯, we deﬁne an N-region segmen-
{ }
tation as a partition R = ri Ni=1 of the points of y¯. More C. Grasp Sampling
precisely, the regions must satisfy the following constraints:
∀ ∈ ∃ ∈ | ∈ Toobtaingraspcandidatesfromalldirections,wepopulate
∀y∈ y¯ ((cid:64)r(cid:48) ∈R \y{ }r|); ∩ (cid:48) (cid:54) ∅ (2) a scene in a physic simulator according to the state estimate
r R ( r R r r r = ). xˆ.Giventhepopulatedscene,werenderndepthimagesY =
These constrains enforce that all points in the point-cloud y¯ y , y ,...,y of the objects from different viewpoints. To
1 2 n
have to belong in a region but no point can belong in two densely sample the scene we propose the sampling scheme
regions. visualizedinFig.3,whichistoapproximateaspherearound
547
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:55:51 UTC from IEEE Xplore.  Restrictions apply. 90°
45°
30°
Fig. 3: The proposed dodecahedron sampling scheme. The
object in yellow is lying on the blue plane. The sampled
viewpoints (represented as red circles) are the midpoints of
eachfaceintheupper-halfofthedodecahedron(bestviewed
in color).
Fig. 4: The three camera viewpoints for single object grasp-
ing.
theworkspacewithadodecahedronandusethemidpointsof
each face in the upper half as the viewpoints. This amounts
to n = 6 viewpoints in total and includes the top-view of 2) Is it beneﬁcial to simulate angled viewpoints instead of
the object that most end-to-end data-driven grasp planning top-views only?
methods are trained on. Of course, other sampling strategies
In order to provide justiﬁed answers to these questions,
can be devised to obtain an higher or lower number of
weconductedtwoseparateexperiments.Theﬁrstexperiment
viewpoints, as desired.
evaluates grasp success rate on single object grasping while
Next, we add noise to each simulated depth image y
i the second one evaluates the clearance rate in cluttered
to make them more similar to ones acquired from physical
scenes.
cameras. The reason for adding noise is because many end-
to-end data-driven grasp planning methods [1], [2], [5] use
synthetic data to train Qθ and adding artiﬁcial noise mimics A. Experimental Setup
depthimagesacquiredfromphysicalcameraswhich,inturn,
improves the sim-to-real transfer. Similar to [1], we added To perform the experiments we used the Franka Emika
both multiplicative and additive noise to each viewpoint Panda robot and a Kinect 360° camera to capture the input
resulting in the noisy depth image yˆ = αy + , where point-clouds as shown in Fig. 4. We used an Aruco marker
∼ i i
α Γ(k,s) is a Gamma random variable modeling depth- [23]fortheextrinsiccalibrationofthecamera.Onceapoint-
proportionalnoise,andisapixel-wisezero-meanGaussian cloud was captured, it was segmented, shape completed and
noiseasexplainedin[22]withbandwidthlandmeasurement ﬁnallyplacedintoaphysicalrenderingofthescenewiththe
varianceσmodelingadditivenoise.Experimentsveriﬁedthat same transformation as in the real world. For segmentation
adding noise to the depth images made the grasps more weusedtheregiongrowingmethodinPCLandforphysical
robust. renderingMuJoCo[24].Forthezero-meanGaussiannoise
G
Grasps are then generated on the set of noisy depth wesetσ =0.001andkernelbandwidthl=6.Forthedepth-
i ∗
images yˆ . The grasp g that achieves the highest utility proportional noise α, modeled as a Gamma distribution, we
i
among all candidates from all viewpoints is considered the set k =5000, s=0.0002.
best and, if physically reachable, is executed on the real In both experiments we tested three different methods
robot. Formally, the best grasp is all using a pre-trained FC-GQ-CNN which is trained to
∗ recognize stable top-grasps from depth images [2]. The
g = argmax Q (g,yˆ ). (3)
∈G θ i ﬁrst method, which is the baseline, generates grasps with
g i,i=1,...,n the FC-GQ-CNN on real depth images captured from a
Kinect 360° camera. We benchmarked this against our
V. EXPERIMENTS method with two different sampling schemes for simulating
depthimages:oneusedthecompletedodecahedronsampling
The two main questions we wanted to answer in the
method described in Section IV-C while the other sampled
experiments were:
a depth image from a top-down view only. Henceforth we
1) Whatistheimpactofgeneratinggraspsfromsimulated refer to the baseline method as FC-GQ-CNN, ours with the
depth images as opposed to real ones on grasp success dodecahedronsamplingasSimulatedAll-Grasps(SAG),and
and object clearance rate? ourwithtop-downsamplingasSimulatedTop-Grasps(STG).
548
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:55:51 UTC from IEEE Xplore.  Restrictions apply. 2
which the object lies prevents the object from slipping and
5
reduces the chance of it falling over. Although top-grasps
13
seem more robust to external perturbations, we hypothesize
6
that the performance difference between STG and SAG
10 11
couldbereducedifFC-GQ-CNNwasalsotrainedonangled
viewpoints.
Finally, Fig. 6 shows clearly that STG performs better
1 3 4 7 8 9 12 than average on all objects except for object 3 while SAG is
above average on 7 out of the 13 objects. The performance
Fig. 5: The 13 individually numbered objects used in the
ofFC-GQ-CNN,ontheotherhand,isworsethanaverageon
experiment. All objects, except 6 and 13, are from the YCB
10objectswithanover20%worsethanaverageperformance
object set.
on objects 2, 3, and 12. One possible reason FC-GQ-CNN
performs poorly on those objects is that grasping them from
B. Single Object Grasping
anangledviewpointismuchharderthangraspingthemfrom
For single object grasping we compared FC-GQ-CNN the top.
with and without simulated depth images on the 13 objects
C. Grasping in Clutter
shown in Fig. 5. To study the effect of the camera viewing
angleongraspperformance,werantheexperimentsonthree In this experiment we studied the clearance rate of each
different angles towards the grasping plane (30°, 45° and method in a cluttered scene, meaning that the objective
90°)allshowninFig.4.Forthe30°and45°viewingangles was to remove as many objects as possible within a given
theobjectswereplacedinﬁvedifferentorientations:0°,72°, grasping budget. The grasping budget was set to 12 grasps
144°,216°and288°,whileforthe90°viewingangle,which and the objects we chose to use were 4, 6, 7, 8, 10 and 12
corresponds to a top-down view, we only placed the objects in Fig. 5 as these represent different shapes and sizes. To
at a 0° orientation. In total this setup amounts to 143 grasps generate a cluttered scene, the objects were placed in a box
per method. thatwasshakenandemptiedontoatable.Anexamplescene
To evaluate if a grasp was successful, the robot moved isshowninFig.1.Thephysicalcameraperceivingthescene
to the planned grasp pose, closed its ﬁngers, and moved the was set to 45°. To evaluate if an object was successfully
armupward20cm.Then,thearmmovedbacktothestarting removed from the scene we used the same procedure as in
±
position, and once there rotated the hand 90° around the the single object grasping experiments except the last step
lastjoint.Agraspwassuccessfuliftheobjectwaswithinthe to rotate the gripper was excluded for speed.
gripperforthiswholeprocedureandunsuccessfulifdropped. The experimental results are presented in Table II. These
The experimental results for the different methods, which results show a clear improvement in average clearance rate
are analyzed for statistical difference with a one sided using STG and SAG over FC-GQ-CNN. For instance, STG
Wilcoxon signed-rank test, are presented in Table I. Over removed all objects in 9 out of 10 scenes and for the one
all viewing angles, the average grasp success rate is higher scene it did not clear only one object was left. SAG, on
with the proposed STG and SAG compared to the base- the other hand, managed to clear 5 out of 10 scenes while
line FC-GQ-CNN (p < 0.0001, p < .05). This result FC-GQ-CNN cleared 2 out of 10 scenes.
stems from the fact that the performance of FC-GQ-CNN Forsceneswherenotallobjectswerecleared,theaverage
deteriorates heavily when moving from a top-down view clearance rate were 83.33% for STG, 76.68% for SAG,
to an angled view. For instance, the relative performance and 47.9% for FC-GQ-CNN. In these cases, FC-GQ-CNN
drop for FC-GQ-CNN from a 90° viewing angle to a 45° manged to remove more than half of the objects in only
is -42.22% and to 30° the drop is -28.89%. This is much 2 out of the 8 scenes. SAG and STG, on the other hand,
higher compared to the performance drop for SAG, which removed more than half of the objects in all scenes.
is only -5% and -10%. The performance drop for STG is Based on the presented results, we demonstrated that it
even less with -4.4% and -2.2% when moving from a 90° is also beneﬁcial to generate grasps from other viewing
viewing angle to a 45° and 30° respectively. Together, these angles when removing objects in cluttered scenes. Together,
results show the importance of simulating depth images if both the result on single object grasping and grasping in
the viewing angle of the real camera is not 90°. clutter demonstrate that the performance of FC-GQ-CNN, a
AnotherinterestingresultfromTableIisthatSTG,which state-of-the-art end-to-end data-driven 4 dof grasp planning
simulates only top-down views, outperforms SAG which, method, deteriorates heavily when viewing the scene from
in addition to simulating a top-down view, also simulates an angled viewpoint. However, through the use of shape
from angled viewpoints. One reason STG achieved a higher completion and simulated viewpoints this is no longer the
grasp success rate than SAG was that in many cases when case.
an angled grasp was executed the gripper either tilted the
object over or if the gripper decided to grasp a corner of VI. CONCLUSIONS
an object the object simply slipped out of the gripper. Such We presented a grasping pipeline that enables end-to-
situations were not common for top-grasps as the surface on end data-driven grasp planning methods which previously
549
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:55:51 UTC from IEEE Xplore.  Restrictions apply. TABLE I: Average grasp success rate on different viewing angles with test statistics and p-values of pair-wise one sided
Wilcoxon signed-rank test between methods.
Viewingangle FC-GQ-CNN STG SAG FC-GQ-CNNvsSTG FC-GQ-CNNvsSAG SAGvsSTG
90°(top-down) 69.23% 69.23% 61.54% – – –
45° 40.00% 66.15% 58.46% T=52,p<.0005*** T=144,p<.05* –
30° 49.23% 67.69% 55.38% T=144,p<.05* – –
Averagesuccessrate 46.85% 67.13% 57.34% T=450,p<.0001*** T=768,p<.05* –
PlanningTime(s) 2.3 2.0 12.3
ShapeCompletionTime(s) – 27.6 27.6
ct FC-GQ-CNN STG SAG
e
bj
o 4
40 3
per 33. 78
e 7.
at 2
r
ucess 20 16.67 16.67 16.67 3.89 16.66
s 4 1 3 3 3
verage 8.33 25 25 8.3 22.225.56 2.785.552.78 8.33 8.33 16.67 2.782.785.55 19.44 5.56 8.38.34 11.115.555.55 11.115.555.55 25 8.3 8.33 8.3
a −−− −− −−−− 0− −− 0−− − − −0
– 0
e
at
r
s
s
e
cc −
u 20
s
p
s
a
Gr 1 2 3 4 5 6 7 8 9 10 11 12 13
Objects
Fig. 6: Grasp success rate per object for each method minus the average success rate per object on each of the 13 objects
used in the experiment.
TABLE II: Results on the cluttered scene
is not spent on shape completion but on the post-processing
of the completed voxel grid which could be improved with
FC-GQ-CNN STG SAG
betterhardwareandoptimizedimplementation.Anotherlim-
Averageclearancerate(%) 58.33 98.33 88.33 itationisthattheaccuracyofshapecompletionisconditional
PlanningTime(s) 1.86 2.7 12.84 on successful segmentation. The analytical region growing
ShapeCompletionTime(s) – 47.53 60.23 segmentation method used here is known to perform poorly
in highly cluttered scenes [25]. Thus, the segmentation
method would need to be replaced in such cases.
only generated 4 dof top-grasps from a single depth image
In conclusion the work presented here demonstrates that
to generate full 6 dof grasps from simulated viewpoints.
planning full 6 dof grasps brings signiﬁcant advantages over
The key component was the use of shape completion to
4 dof grasps. This, in turn, poses new interesting research
model a partly observed object and place it into a physics
questions.Forinstance,insteadofsimulatingdifferentview-
simulatortosimulatedepthimagesfrommultipleviewpoints.
points of the shape completed object, is it maybe better to
We used FC-GQ-CNN to generate grasps and compared the
plan directly on the object itself using, e.g., mesh neural
6 dof grasps generated with our pipeline to the 4 dof grasps
networks[26]?Or,isitpossibletogeneratefull6dofgrasps
proposed from a depth image captured by a real camera
directly from real depth image, removing the need for shape
on both single object grasping and grasping in clutter. The
completion? These questions pave way for interesting future
single object grasping results show that generating full 6
research avenues.
dof grasps leads to a statistical signiﬁcant improvement in
termsofhighergraspsuccessrate.Majorimprovementswere
ACKNOWLEDGMENT
alsoprominentforgraspinginclutterwhengenerating6dof
grasps opposed to 4 dof ones. We gratefully acknowledge the support of NVIDIA Cor-
Despite the good results, shape completion is a major poration with the donation of the Titan Xp GPU used for
computational bottleneck. Most computation time, however, this research.
550
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:55:51 UTC from IEEE Xplore.  Restrictions apply. REFERENCES [22] J.Mahler,S.Patil,B.Kehoe,J.VanDenBerg,M.Ciocarlie,P.Abbeel,
andK.Goldberg,“Gp-gpis-opt:Graspplanningwithshapeuncertainty
[1] J.Mahler,J.Liang,S.Niyaz,M.Laskey,R.Doan,X.Liu,J.A.Ojea,
using gaussian process implicit surfaces and sequential convex pro-
and K. Goldberg, “Dex-net 2.0: Deep learning to plan robust grasps gramming,” in 2015 IEEE international conference on robotics and
withsyntheticpointcloudsandanalyticgraspmetrics,”arXivpreprint automation(ICRA). IEEE,2015,pp.4919–4926.
arXiv:1703.09312,2017.
[23] S.Garrido-Jurado,R.Mun˜oz-Salinas,F.J.Madrid-Cuevas,andM.J.
[2] V. Satish, J. Mahler, and K. Goldberg, “On-policy dataset synthesis
Mar´ın-Jime´nez,“Automaticgenerationanddetectionofhighlyreliable
for learning robot grasping policies using fully convolutional deep ﬁducialmarkersunderocclusion,”PatternRecognition,2014.
networks,” IEEE Robotics and Automation Letters, vol. 4, no. 2, pp.
[24] E. Todorov, T. Erez, and Y. Tassa, “Mujoco: A physics engine for
1357–1364,2019. model-basedcontrol,”in2012IEEE/RSJInternationalConferenceon
[3] U. R. Aktas, C. Zhao, M. Kopicki, A. Leonardis, and J. L. Wyatt, IntelligentRobotsandSystems,2012.
“Deepdexterousgraspingofnovelobjectsfromasingleview,”arXiv
[25] M. Danielczuk, M. Matl, S. Gupta, A. Li, A. Lee, J. Mahler, and
preprintarXiv:1908.04293,2019.
K.Goldberg,“Segmentingunknown3dobjectsfromrealdepthimages
[4] J.Varley,J.Weisz,J.Weiss,andP.Allen,“Generatingmulti-ﬁngered using mask r-cnn trained on synthetic data,” in 2019 International
robotic grasps via deep learning,” in 2015 IEEE/RSJ International Conference on Robotics and Automation (ICRA). IEEE, 2019, pp.
ConferenceonIntelligentRobotsandSystems(IROS). IEEE,2015.
7283–7290.
[5] E. Johns, S. Leutenegger, and A. J. Davison, “Deep Learning
[26] Y. Feng, Y. Feng, H. You, X. Zhao, and Y. Gao, “Meshnet: Mesh
a Grasp Function for Grasping under Gripper Pose Uncertainty,” neural network for 3d shape representation,” in Proceedings of the
arXiv:1608.02239[cs],2016. AAAIConferenceonArtiﬁcialIntelligence,2019.
[6] C. Choi, W. Schwarting, J. DelPreto, and D. Rus, “Learning Object
Grasping for Soft Robot Hands,” IEEE Robotics and Automation
Letters,2018.
[7] K.Bousmalis,A.Irpan,P.Wohlhart,Y.Bai,M.Kelcey,M.Kalakrish-
nan,L.Downs,J.Ibarz,P.Pastor,K.Konoligeetal.,“Usingsimulation
anddomainadaptationtoimproveefﬁciencyofdeeproboticgrasping,”
in2018IEEEInternationalConferenceonRoboticsandAutomation
(ICRA),2018.
[8] J.Tobin,L.Biewald,R.Duan,M.Andrychowicz,A.Handa,V.Kumar,
B. McGrew, A. Ray, J. Schneider, P. Welinder et al., “Domain
randomization and generative models for robotic grasping,” in 2018
IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems
(IROS),2018.
[9] J.Varley,C.DeChant,A.Richardson,J.Ruales,andP.Allen,“Shape
completionenabledroboticgrasping,”inIEEE/RSJInternationalCon-
ferenceonIntelligentRobotsandSystems,2017,pp.2442–2447.
[10] J. Lundell, F. Verdoja, and V. Kyrki, “Robust Grasp Planning Over
UncertainShapeCompletions,”in2019IEEE/RSJInternationalCon-
ference on Intelligent Robots and Systems (IROS). Macau, China:
IEEE,Nov.2019.
[11] D. Watkins-Valls, J. Varley, and P. Allen, “Multi-Modal Geometric
Learning for Grasping and Manipulation,” arXiv:1803.07671 [cs],
2018.
[12] A.Sahbani,S.El-Khoury,andP.Bidaud,“Anoverviewof3dobject
graspsynthesisalgorithms,”RoboticsandAutonomousSystems,2012.
[13] X.Han,Z.Li,H.Huang,E.Kalogerakis,andY.Yu,“High-Resolution
ShapeCompletionUsingDeepNeuralNetworksforGlobalStructure
andLocalGeometryInference,”in2017IEEEInternationalConfer-
enceonComputerVision(ICCV). IEEE,2017,pp.85–93.
[14] B. Yang, H. Wen, S. Wang, R. Clark, A. Markham, and N. Trigoni,
“3d object reconstruction from a single depth view with adversarial
learning,” in Proceedings of the IEEE International Conference on
ComputerVision,2017,pp.679–688.
[15] A. Dai, C. R. Qi, and M. Nießner, “Shape completion using 3d-
encoder-predictorcnnsandshapesynthesis,”inProc.ComputerVision
andPatternRecognition(CVPR),IEEE,2017.
[16] A.Saxena,J.Driemeyer,andA.Y.Ng,“RoboticGraspingofNovel
ObjectsusingVision,”TheInternationalJournalofRoboticsResearch,
2008.
[17] D. Quillen, E. Jang, O. Nachum, C. Finn, J. Ibarz, and S. Levine,
“Deep Reinforcement Learning for Vision-Based Robotic Grasp-
ing: A Simulated Comparative Evaluation of Off-Policy Methods,”
arXiv:1802.10264[cs,stat],2018.
[18] L. Pinto and A. Gupta, “Supersizing Self-supervision: Learning to
Graspfrom50kTriesand700RobotHours,”arXiv:1509.06825[cs],
2015.
[19] S.Levine,P.Pastor,A.Krizhevsky,andD.Quillen,“LearningHand-
Eye Coordination for Robotic Grasping with Deep Learning and
Large-ScaleDataCollection,”arXiv:1603.02199[cs],2016.
[20] S. James, P. Wohlhart, M. Kalakrishnan, D. Kalashnikov, A. Irpan,
J.Ibarz,S.Levine,R.Hadsell,andK.Bousmalis,“Sim-to-realviasim-
to-sim: Data-efﬁcient robotic grasping via randomized-to-canonical
adaptation networks,” in Proceedings of the IEEE Conference on
ComputerVisionandPatternRecognition,2019,pp.12627–12637.
[21] W. E. Lorensen and H. E. Cline, “Marching cubes: A high resolu-
tion 3d surface construction algorithm,” in ACM siggraph computer
graphics,vol.21,no.4. ACM,1987,pp.163–169.
551
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:55:51 UTC from IEEE Xplore.  Restrictions apply. 