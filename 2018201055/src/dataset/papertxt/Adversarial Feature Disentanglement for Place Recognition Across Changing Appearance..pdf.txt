2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
Efﬁcient Bimanual Manipulation Using Learned Task Schemas
∗
Rohan Chitnis Shubham Tulsiani Saurabh Gupta Abhinav Gupta
MIT Computer Science and Artiﬁcial Intelligence Laboratory, Facebook Artiﬁcial Intelligence Research
ronuchit@mit.edu, shubhtuls@fb.com, saurabhg@illinois.edu, gabhinav@fb.com
Abstract—We address the problem of effectively composing
skills to solve sparse-reward tasks in the real world. Given a
set of parameterized skills (such as exerting a force or doing a
topgraspatalocation),ourgoalistolearnpoliciesthatinvoke
these skills to efﬁciently solve such tasks. Our insight is that
for many tasks, the learning process can be decomposed into
learning a state-independent task schema (a sequence of skills
to execute) and a policy to choose the parameterizations of the
skillsinastate-dependentmanner.Forsuchtasks,weshowthat
explicitly modeling the schema’s state-independence can yield
signiﬁcant improvements in sample efﬁciency for model-free
reinforcementlearningalgorithms.Furthermore,theseschemas
canbetransferredtosolverelatedtasks,bysimplyre-learning
the parameterizations with which the skills are invoked. We
ﬁnd that doing so enables learning to solve sparse-reward
tasksonreal-worldroboticsystemsveryefﬁciently.Wevalidate
our approach experimentally over a suite of robotic bimanual
Fig. 1: We learn policies for solving real-world sparse-reward
manipulation tasks, both in simulation and on real hardware.
bimanual manipulation tasks from raw RGB image observations.
See videos at http://tinyurl.com/chitnis-schema.
We decompose the problem into learning a state-independent task
schema (a sequence of skills to execute) and a state-dependent
I. INTRODUCTION policythatappropriatelyinstantiatestheskillsinthecontextofthe
environment. This decomposition speeds up learning and enables
Let us consider the task of opening a bottle. How should
transferring schemas from simulation to the real world, leading to
a two-armed robot accomplish this? Even without knowing
successfultaskexecutionwithinafewhoursofreal-worldtraining.
the bottle geometry, its position, or its orientation, one can TheARtagsareonlyusedforautomatingtherewardcomputation;
answer that the task will involve holding the bottle’s base ourmodelisnotgivenanyobjectposeinformation.Topleft:Lifting
withonehand,graspingthebottle’scapwiththeotherhand, an aluminum tray. Top right: Rotating a T-wrench. Bottom left:
Opening a glass jar. Bottom right: Picking up a large soccer ball.
and twisting the cap off. This “schema,” the high-level plan
of what steps need to be executed, only depends on the task
a discrete set of generic skills parameterized by continuous
and not on the object’s geometric and spatial state, which
arguments. Examples of skills include exerting a force at a
only inﬂuence how to parameterize each of these steps (e.g.,
location or moving an end effector to a target pose. Thus,
deciding where to grasp, or how much to twist).
the action space is hybrid discrete-continuous [10]: at each
Reinforcement learning methods provide a promising ap-
timestep, the agent must decide both 1) which skill to use
proach for learning such behaviors from raw sensory in-
and 2) what continuous arguments to use (e.g., the location
put [1], [2], [3]. However, typical end-to-end reinforcement
to apply force, the amount of force, or the target pose to
learningmethodsdonotleveragetheschematicsoftasks,and
move to). The sample inefﬁciency of current reinforcement
instead aim to solve tasks by learning a policy, which would
learning methods is exacerbated in domains with these large
involve inferring both the schema and the parameterizations,
searchspaces;evenbasictaskssuchasopeningabottlewith
asafunctionoftherawsensoryinput.Theseapproacheshave
twoarmsarechallengingtolearnfromsparserewards.While
led to impressive successes across domains such as game-
one could hand-engineer dense rewards, this is undesirable
playing [1], [4], [5], [6] and robotic control tasks [2], [7],
asitdoesnotscaletomorecomplicatedtasks.Weaskafun-
[8],[9],butareknowntohaveveryhighsamplecomplexity.
damental question: can we use the given skills to efﬁciently
For instance, they require millions of frames of interaction
learn policies for tasks with a large policy search space, like
to learn to play Atari games, or several weeks’ worth of
bimanual manipulation, given only sparse rewards?
experience to learn simulated control policies, which makes
them impractical to train on real hardware. Our insight is that for many tasks, the learning process
can be decomposed into learning a state-independent task
In this work, we address the problem of learning to per-
schema(sequenceofskills)andastate-dependentpolicythat
formtasksinenvironmentswithasparserewardsignal,given
choosesappropriateparameterizationsforthedifferentskills.
*WorkdoneduringaninternshipatFacebookAIResearch. Such a decomposition of the policy into state-dependent
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 1149
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 12:23:06 UTC from IEEE Xplore.  Restrictions apply. and state-independent parts simpliﬁes the credit assignment
problemandleadstomoreeffectivesharingofexperience,as
data from different instantiations of the task can be used to
improvethesamesharedskills.Thisleadstofasterlearning.
This modularization can further allow us to transfer
learned schemas among related tasks, even if they have
differentstatespaces.Forexample,supposewehavelearned
a good schema for picking up a long bar in simulation,
wherewehaveaccesstoobjectposes,geometryinformation,
etc. We can then reuse that schema for a related task
such as picking up a tray in the real world from only
raw camera observations, even though both the state space
and the optimal parameterizations (e.g., grasp poses) differ
Fig. 2: In bimanual manipulation tasks, a schema is a sequence
signiﬁcantly. As the schema is ﬁxed, policy learning for of skills for each arm to execute. We train in simulation both a
this tray pickup task will be very efﬁcient, since it only state-independent model for predicting this schema and a state-
requires learning the (observation-dependent) arguments for dependent neural network φ for predicting its continuous param-
eters,viareinforcementlearning.Ourexperimentsshowthatusing
each skill. Transferring the schema in this way enables
astate-independentschemapredictorforthesetasksmakestraining
learningtosolvesparse-rewardtasksveryefﬁciently,making
signiﬁcantly more efﬁcient. To solve real-world tasks, we transfer
it feasible to train real robots to perform complex skills. See schemas learned in simulation, and only optimize φ.
Figure 2 for an overview of our approach.
Wevalidateourapproachoverasuiteofroboticbimanual efﬁcient robotic learning was ﬁrst developed in the early
manipulationtasks,bothinsimulationandonrealhardware. 1990s [14], [15]. More recent techniques include learning
We give the robots a very generic library of skills such from model ensembles [16] and utilizing domain random-
as twisting, lifting, and reaching. Even given these skills, ization [17], [18], [19], in which physical properties of a
bimanualmanipulationischallengingduetothelargesearch simulatedenvironmentarerandomizedtoallowlearnedpoli-
spaceforpolicyoptimization.Weconsiderfourtaskfamilies: ciestoberobust.However,asthesemethodsdirectlytransfer
laterallifting,picking,opening,androtating,allwithvarying the policy learned in simulation, they rely on the simulation
objects,geometries,andinitialposes.Alltaskshaveasparse being visually and physically similar to the real world. In
binary reward signal: 1 if the task is completed, and 0 contrast, we only transfer one part of our learned policy —
otherwise. We empirically show that a) explicitly modeling the skill sequence to be executed — from simulation to the
schema state-independence yields large improvements in real world, and allow the associated continuous parameters
learning efﬁciency over the typical strategy of conditioning to be learned in the real-world domain.
the policy on the full state, and b) transferring learned Temporal abstraction for reinforcement learning. The
schemas to real-world tasks allows complex manipulation idea of using temporally extended actions to reduce the
skills to be discovered within only a few hours (<10) of sample complexity of reinforcement learning algorithms has
trainingonasinglesetup.Figure1showssomeexamplesof been studied for decades [20], [21], [22], [23]. For instance,
real-world tasks solved by our system. work on macro-actions for MDPs [23] attempts to build a
II. RELATEDWORK hierarchicalmodelinwhichtheprimitiveactionsoccupythe
lowest level, and subsequently higher levels build local poli-
Searchinparameterizedactionspaces.Anagentequipped
cies, each equipped with their own termination conditions,
with a set of skills parameterized by continuous arguments
thatmakeuseofactionsatthelevelbelow.Morerecentwork
must learn a policy that decides both which skills to use
seekstolearnthesehierarchies[24],[25],butsuccesseshave
and what continuous arguments to use for them. Param-
largely been limited to simulated domains due to the large
eterized action MDPs (PAMDPs) [10] were constructed for amount of data required. In our work, we propose a model
this exact problem setting. Recent work has addressed deep
thatmakesskillselectionindependentofstate,enablingreal-
reinforcement learning for PAMDPs [11], [12], by learning world robotic tasks to be solved via transfer.
policies that output both the discrete skill and continuous
parameterselectionsateachtimestep.Incontrast,wepropose III. APPROACH
amodelthatbakesinstate-independenceofthediscreteskill Givenasetofparameterizedskills,weaimtosolvesparse-
selection, and show that this assumption not only improves reward tasks by learning a policy that decides both which
learning efﬁciency, but also is experimentally useful. A skill to execute and what arguments to use when invoking
separate line of work learns control policies for steps in a it. Our insight is that, for many tasks, the same sequence
policy sketch [13], which can be recombined in novel ways of skills (possibly with different arguments) can be used
to solve new task instances; however, this work does not to optimally solve different instantiations of the task. We
considerthediscretesearchaspectoftheproblem,aswedo. operationalize this by disentangling the policy into a state-
Transfer learning for robotics. The idea of transferring a independent task schema (sequence of skills) and a state-
learned policy from simulation to the real world for more dependentpredictionofhowtoparameterizetheseskills.We
1150
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 12:23:06 UTC from IEEE Xplore.  Restrictions apply. ﬁrst formallydeﬁne ourproblem setup,and thenpresent our
model for leveraging the state-independence of schemas to
learn efﬁciently. Finally, we describe how our approach also
allowstransferringschemasacrosstasks,lettinguslearnreal-
world policies from raw images by reusing schemas learned
for related tasks in simulation.
Problem Setup. Each task we consider is deﬁned as a
parameterizedactionMarkovdecisionprocess(PAMDP)[10],
[26] with ﬁnite horizon T. The reward for each task is
a binary function indicating whether the current state is
an element of the set of desired goal conﬁgurations, such
as a state with the bottle opened. The learning objective,
therefore,istoobtainapolicyπthatmaximizestheexpected
Fig. 3: Top: A baseline policy architecture for solving tasks with
proportion of times that following it achieves the goal. Note X
a discrete set of skills , each parameterized by a vector vx of
thatthisisaparticularlychallengingsetupforreinforcement
continuous values. The policy for a task is a neural network with
learningalgorithmsduetothesparsityoftherewardfunction. weightsφthatpredicts1)logitsoverwhichskilltouseand2)mean
X
The agent is given a discrete library of generic skills , andvarianceofaGaussianovercontinuousargumentvaluesforall
∈X skills.Tosampleanaction,weﬁrstsampleaskillbasedonthelog-
where each skill x is parameterized by a corresponding
its, then select the corresponding subset of continuous arguments,
vector vx of continuous values. Examples of skills can andﬁnallysampleargumentvaluesfromthoseGaussians.Bottom:
includeexertingaforceatalocation,movinganendeffector Our proposed policy architecture, which leverages the assumption
to a target pose, or rotating an end effector about an axis. thattheoptimalschemaisstate-independent.Thekeydifferenceis
A
Let denote the action space of the PAMDP. An action thattheneuralnetworkonlypredictsadistributionover×co|Xnt|inuous
a ∈ A is a tuple (cid:104)x,vx(cid:105), indicating what skill to apply as argument values, and we train a state-independent T array
of logits over which skill to use at each timestep. In the text, we
well asthe correspondinXg parameterization. A schema x¯ is a discuss how to update these logits.
sequenceofT skillsin ,wherex¯=x ,x ,...,x captures
1 2 T
×|X|
the sequence of skills but not their continuous parameters. taskedwithpredictingtheskillarguments.TheT array
∗
Assumption. We assume that the optimal schema x¯ is oflogitsandtheneuralnetwork,takentogether,representthe
state-independent: it depends only on the task, not on the policy π, as depicted in Figure 3 (bottom).
state and its dynamics. This implies that the same schema Learning Schemas and Skill Arguments. The weights φ
is optimal for all instantiations of a task, e.g. different
of the neural network can be updated via standard policy
geometries and poses of objects. We note that this is a valid
gradient methods. Let τ denote a trajectory induced by
assumption across many tasks of interest, since the skills
following π in an episode. The objective we wish to maxi-
themselves can be appropriately chosen to be complicated ≡ E
mize is J(φ) [r(τ)]. Policy gradient methods such as
and expressive, such as stochastic, closed-loop control poli- τ
REINFORCE [27] leverage the likelihood ratio trick, which
cies for guiding an end effector. ∇ E ∇
says that J(φ) = [r(τ) logπ(τ)], to tune φ via
φ τ φ
Modular Policies. The agent must learn a policy π that, gradient ascent. When estimating this gradient, we treat the
∈ X
at each timestep, infers both which skill x to use (a current setting of the array of logits as a constant.
× |X|
discrete choice) and what continuous arguments vx to use. Updating the logits within the T array can also
Whatisagoodformforsuchapolicy?Asimplestrategy, be achieved via policy gradients; however, since there is
which we use as a baseline and depict in Figure 3 (top), no input, and because we have sparse rewards, the pol-
wouldbetorepresentπviaaneuralnetwork,withweightsφ, icy optimization procedure is quite simple. Let ϕtx be
thattakesthestateasinputandhasatwo-headedoutput.One the logit for time t and skill x. Given trajectory τ =
(cid:104) (cid:105)
head predicts logits that represent a categorical distribution s ,x ,vx0,s ,x ,vx1,...,s :
X •0 0 0 1 1 1 T
over the skills , while the other head predicts a mean and If τ achieves the goal, i.e. r(τ) > 0, increase ϕ for
tx
varianceofaGaussiandistributionovercontinuousargument each timestep t and skill x taken at that timestep.
•
values for all skills. To sample an action, we can sample Ifτ doesnotachievethegoal,i.e.r(τ)=0,decreaseϕ
∈X tx
x fromthelogitspredictedbytheﬁrsthead,thensample for each timestep t and skill x taken at that timestep.
argumentsusingthesubsetofmeansandvariancespredicted The amount by which to increase or decrease ϕ is
tx
by the second head that correspond to vx. absorbed by the step size and thus gets tuned as a hyper-
However, this does not model the fact that the optimal parameter. See Algorithm 1 for full pseudocode.
schema is state-independent. To capture this, we need to SchemaTransferAcrossTasks.Sincewehavedisentangled
remove the dependence of the discrete skill selection on the the learning of the schema from the learning of the skill ar-
×|X|
inputstate.Thus,weproposetomaintainaseparateT guments within our policy architecture, we can now transfer
×|X|
array, where row t is the logits of a categorical distribution the T array of logits across related tasks, as long as
over which skill to use at time t. Note that T is the horizon the skill spaces and horizons are equal. Therefore, learning
of the MDP. In this architecture, the neural network is only for a new task can be made efﬁcient by reusing a previously
1151
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 12:23:06 UTC from IEEE Xplore.  Restrictions apply. TaskFamily Object(Sim) Objects(Real) SchemaDiscoveredfromLearninginSimulation
laterallifting bar aluminumtray,rollingpin,heavybar,plasticbox 1)L:topgrasp,R:topgrasp2)L:lift,R:lift
picking ball soccerball 1)L:topgrasp,R:go-topose2)L:no-op,R:go-topose3)L:lift,R:lift
opening bottle glassjar,waterbottle 1)L:topgrasp,R:sidegrasp2)L:twist,R:no-op
rotating corkscrew T-wrench,corkscrew 1)L:go-topose,R:sidegrasp2)L:go-topose,R:no-op3)L:rotate,R:no-op
TABLEI:Taskfamilies,objectconsideredinsimulation,objectsconsideredinrealworld,andschemas(forleftandrightarms)discovered
by our algorithm in simulation. Schemas learned in simulation for a task family are transferred to multiple objects in the real world.
M
Algorithm TRAIN-POLICY( ,α,β) architecture,andallowsustolearnbimanualpoliciesonreal
M
1 Input: , an MDP as deﬁned in Section III. robotsinlessthan10hoursoftraining.Weﬁrstdescribethe
2 Input: α and β, step sizes. experimental setup, then discuss our results.
3 Initialize neural network weights φ.
×|X| A. MuJoCo Experimental Setup
4 Zero-initialize T array of logits ϕ .
tx
5 while not done do Environment. For all four task families, two Sawyer robot
D ← (cid:104) (cid:105)
6 batch of trajectories τ = s ,x ,vxt arms with parallel-jaw grippers are placed at opposing ends
t Mt t
obtained from running policy π in . ofatable,facingeachother.Asingleobjectisplacedonthe
∇ ← D
7 J(φ) POLICYGRADIENT(π, ) table,andthegoalistomanipulatetheobject’sposeinatask-
φ← ∇
8 φ φ+α J(φ) // Or Adam [28]. speciﬁcway.Laterallifting(bar):Thegoalistoliftaheavy
φ ∈D
9 for each trajectory τ do and long bar by 25cm while maintaining its orientation. We
10 for each skill x used in τ do vary the bar’s location and density. Picking (ball): The goal
t
11 if τ achieves the goal then istoliftaslippery(lowcoefﬁcientoffriction)ballvertically
←
12 ϕ ϕ +α by 25cm. The ball slips out of the gripper when grasped
tx tx
else by a single arm. We vary the ball’s location and coefﬁcient
← −
13 ϕ ϕ β of friction. Opening (bottle): The goal is to open a bottle
tx tx
implemented as two links (a base and a cap) connected by
Algorithm 1: Training policies π that explicitly model the a hinge joint. If the cap is twisted without the base being
×|X|
state-independenceofschemasviaaT arrayoflogits held in place, the entire bottle twists. The cap must undergo
over what skill to use at each timestep.
aquarter-rotationwhilethebasemaintainsitspose.Wevary
thebottle’slocationandsize.Rotating(corkscrew):Thegoal
learnedschema,sincewewouldonlyneedtotraintheneural is to rotate a corkscrew implemented as two links (a base
networkweightsφtoinferskillargumentsforthatnewtask. andahandle)connectedbyahingejoint,likethebottle.The
Importantly, transferring the schema is reasonable even handlemustundergoahalf-rotationwhilethebasemaintains
when the tasks have different state spaces. For instance, its pose. We vary the corkscrew’s location and size.
one task can be a set of simulated bimanual bottle-opening
Skills. The skills we use are detailed in Table II, and
problems in a low-dimensional state space, while the other
the search spaces for the skill parameters are detailed in
involveslearningtoopenbottlesintherealworldfromhigh-
Table III. Note that because we have two arms, we actually
dimensionalcameraobservations.Asthestatespacescanbe
need to search over a cross product of this space with itself.
different, it follows immediately that the tasks can also have
State and Policy Representation. Experiments conducted
different optimal arguments for the skills.
in the MuJoCo simulator [29] use a low-dimensional state:
IV. EXPERIMENTS proprioceptive features (joint positions, joint velocities, end
We test our proposed approach on four robotic bimanual effector pose) for each arm, the current timestep, geome-
manipulation task families: lateral lifting, picking, opening, try information for the object, and the object pose in the
and rotating. Table I lists the different objects that we world frame and each end effector’s frame. The policy is
consideredforeachone.Thesetaskfamilieswerechosenbe- represented as a 4-layer MLP with 64 neurons in each layer,
causetheyrepresentachallenginghybriddiscrete-continuous ReLU activations, and a multi-headed output for the actor
search space for policy optimization, while meeting our and the critic. Since object geometry and pose can only be
requirement that the optimal schema is independent of the computed within the simulator, our real-world experiments
state. We show results on these tasks both in simulation and will instead use raw RGB camera images.
on real Sawyer arms: schemas are learned in simulation by Training Details. We use the Stable Baselines [30] im-
training with low-dimensional state inputs, then transferred plementation of proximal policy optimization (PPO) [31],
as-is to visual inputs (in simulation as well as in the real though our method is agnostic to the choice of policy
world),forwhichweonlyneedtolearnskillarguments.Our gradient algorithm. We use the following hyper-parameters:
experimentsshowthatourproposedapproachissigniﬁcantly Adam [28] with learning rate 0.001, clipping parameter 0.2,
more sample-efﬁcient than one that uses the baseline policy entropy loss coefﬁcient 0.01, value function loss coefﬁcient
1152
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 12:23:06 UTC from IEEE Xplore.  Restrictions apply. Skill AllowedTaskFamilies ContinuousParameters
topgrasp laterallifting,picking,opening (x,y)position,z-orientation
sidegrasp opening,rotating (x,y)position,approachangle
go-topose picking,rotating (x,y)position,orientation
lift laterallifting,picking distancetolift
twist opening none
rotate rotating rotationaxis,rotationradius
no-op all none
TABLE II: Skills, allowed task families, and skill parameters.
Parameter RelevantSkills SearchSpace(Sim) SearchSpace(Real)
Fig. 4: To predict continuous arguments given an image input, we
[-0.1,0.1]x/y/zoffset leveragethefactthatalltasksrequirelearningtwospatialarguments
(x,y)position grasps,go-topose locationontablesurface
fromobjectcenter for each arm: an (x, y) location along the table surface. To learn
z-orientation topgrasp [0,2π] [0,2π] efﬁciently,weuseafullyconvolutionalarchitecture[33].Webegin
− −
approachangle sidegrasp [ π,π] [ π,π] bypassingtheimagethroughtheﬁrstfourlayersofanImageNet-
2 2 2 2
orientation go-topose [0,2π]r/p/yEulerangles [0,2π]r/p/yEulerangles pretrained VGG16 [34]; each laye×r is 2 convolutions followed by
convertedtoquat convertedtoquat a max-pool. This gives us 512 8 8 maps. The second-last layer
× ×
distancetolift lift [0,0.5] [0,0.5] convolves with 16 2 2 ﬁlters with stride 2, giving 16 8 8
[-0.1,0.1]x/yoffset maps. Finally, the last layer convolves with (2+4+2p) ﬁlters
rotationaxis rotate locationontablesurface
fromobjectcenter with stride 1, where p is the number of non-spatial parameters
rotationradius rotate [0,0.2] [0,0.2] for the task. The ﬁrst two dimensions give each arm’s probability
×
distribution(responsemap)overan8 8discretizationofthetable
TABLEIII:Parametersearchspacesforsimulationandrealworld.
surface locations, the next four dimensions give offsets on these
Insimulation,unliketherealworld,wehaveaccesstoobjectposes,
locations, and the ﬁnal 2p dimensions give values for each arm’s
so we can constrain some search spaces for efﬁciency. Positions,
remaining arguments (orientations, distances, etc.). Upper right:
distances,andlengthsareinmeters.Rotationaxisisalwaysvertical.
Example response maps learned by this model for each arm, for
therollingpinandsoccerballtasks.TheARtagsareonlyusedin
0.5, gradient clip threshold 0.5, number of steps 10, number ourexperimentsforautomatingtherewardcomputationtomeasure
of minibatches per update 4, and number of optimization success; our model is not given any object pose information.
epochs4.OurimplementationbuildsontheSurrealRobotics
Suite [32]. Training is parallelized across 50 workers. The
time horizon T =3 in all tasks.
B. Real-World Sawyer Experimental Setup
Environment. Our real-world setup also contains two
Sawyer robot arms with parallel-jaw grippers placed at
opposing ends of a table, facing each other. We task the
robots with manipulating nine common household objects
that require two parallel-jaw grippers to interact with. We
consider the same four task families (lateral lifting, picking,
opening and rotating), but work with more diverse objects
(suchasarollingpin,soccerball,glassjar,andT-wrench),as
detailed in Table I. For each task family, we use the schema
discovered for that family in simulation, and only learn the
continuous parameterizations of the skills in the real world.
See Figure 1 for pictures of some of our tasks.
Skills. The skills and parameters are the same as in simu-
lation (Table II), but the search spaces are less constrained
(Table III) since we do not have access to object poses.
State and Policy Representation. The state for these real- Fig. 5: Learning curves for simulated tasks, with training paral-
× lelized across 50 workers. Each curve depicts an average across
world tasks is the 256 256 RGB image obtained from an
5 random seeds. By using a policy architecture that leverages the
overhead camera that faces directly down at the table. To
state-independence of the optimal schema (orange), we are able to
predict the continuous arguments, we use a fully convolu- achievesigniﬁcantgainsinsamplecomplexityacrossallfourtasks,
tional spatial neural network architecture [33], as shown in over the baseline architecture that predicts skills and arguments
Figure 4 along with example response maps. conditioned on the state (blue). If an oracle tells us the perfect
schema, and we only need to learn the arguments for those skills,
Training Details. We use PPO and mostly the same hyper- thenofcourse,learningwillbeextremelysample-efﬁcient(green).
parameters, with the following differences: learning rate
1153
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 12:23:06 UTC from IEEE Xplore.  Restrictions apply. Fig. 6: Learning curves for training from images in simulation,
with training parallelized across 50 workers. Each curve depicts Fig. 7: Learning curves for the real-world tasks. We stop training
an average across 5 random seeds. When learning visual policies, whenthepolicyreaches90%averagesuccessrateoverthelast100
transferring schemas trained on low-dimensional states is crucial. episodes.Ittakesaround6-8hourstoexecute2000skills,somost
policiesarelearnedinaround4-10hours.Bytransferringschemas
0.005, number of steps 500, number of minibatches per learned in simulation, we can train robots to solve sparse-reward
update 10, number of optimization epochs 10, and no paral- bimanual manipulation tasks from raw camera images.
lelization. We control the Sawyers using PyRobot [35].
depends only on the task, not the (dynamic) state. The skills
C. Results in Simulation may themselves be parameterized closed-loop policies.
Please see the supplementary video for examples of
Figure 5 shows that our policy architecture greatly im-
learned behavior on the real-world tasks.
proves the sample efﬁciency of model-free reinforcement
learning. In all simulated environments, our method learns
V. FUTUREWORK
the optimal schema, as shown in the last column of Table I.
Much of the difﬁculty in these tasks stems from sequenc- In this work, we have studied how to leverage state-
ing the skills correctly, and so our method, which more independent sequences of skills to greatly improve the sam-
effectively shares experience across task instantiations in its ple efﬁciency of model-free reinforcement learning. Fur-
attempt to learn the task schema, performs very well. thermore, we have shown experimentally that transferring
Before transferring the learned schemas to the real-world sequences of skills learned in simulation to real-world tasks
tasks, we consider learning from rendered images in simu- enablesustosolvesparse-rewardproblemsfromimagesvery
lation, using the architecture from Figure 4 to process them. efﬁciently, making it feasible to train real robots to perform
Figure 6 shows the impact of transferring the schema versus complex skills such as bimanual manipulation.
re-learningitinthismorerealisticsimulationsetting.Wesee An important avenue for future work is to relax the as-
that when learning visual policies, transferring the schemas sumptionthattheoptimalschemaisopen-loop.Forinstance,
learnedinthetaskswithlow-dimensionalstatespacesiscriti- one could imagine predicting the schema via a recurrent
caltoefﬁcienttraining.Theseresultsincreaseourconﬁdence mechanism, so that the decision on what skill to use at time
that transferring the schema will enable efﬁcient real-world t + 1 is conditioned on the skill used at time t. Another
training with raw RGB images, as we show next. interesting future direction is to study alternative approaches
to training the state-independent schema predictor. Finally,
D. Results in Real World wehopetoinvestigatetheideaofinferringtheschemasfrom
human demonstrations of a task, such as those in videos.
Figure 7 shows our results on the nine real-world tasks,
with schemas transferred from the simulated tasks. We can
ACKNOWLEDGMENTS
see that, despite the challenging nature of the problem
(learning from raw camera images, given sparse rewards), We thank Dhiraj Gandhi for help with the experimental
our system is able to learn to manipulate most objects in setup. Rohan is supported by an NSF Graduate Research
around 4-10 hours of training. We believe that our approach FellowshipthroughhiscapacitywiththeMassachusettsInsti-
canbeusefulforsample-efﬁcientlearninginproblemsother tute of Technology. Any opinions, ﬁndings, and conclusions
thanmanipulationaswell;alloneneedsistodeﬁneskillsap- expressedinthismaterialarethoseoftheauthorsanddonot
propriatefortheenvironmentsuchthattheoptimalsequence necessarily reﬂect the views of our sponsors.
1154
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 12:23:06 UTC from IEEE Xplore.  Restrictions apply. REFERENCES [23] M.Hauskrecht,N.Meuleau,L.P.Kaelbling,T.Dean,andC.Boutilier,
“Hierarchical solution of Markov decision processes using macro-
[1] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. actions,”inProceedingsoftheFourteenthconferenceonUncertainty
Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski in artiﬁcial intelligence. Morgan Kaufmann Publishers Inc., 1998,
et al., “Human-level control through deep reinforcement learning,”
pp.220–229.
Nature,vol.518,no.7540,p.529,2015.
[24] K. Frans, J. Ho, X. Chen, P. Abbeel, and J. Schulman, “Meta
[2] S. Levine, C. Finn, T. Darrell, and P. Abbeel, “End-to-end training learningsharedhierarchies,”inInternationalConferenceonLearning
of deep visuomotor policies,” The Journal of Machine Learning Representations,2018.
Research,vol.17,no.1,pp.1334–1373,2016.
[25] O. Nachum, S. S. Gu, H. Lee, and S. Levine, “Data-efﬁcient hier-
[3] T.P.Lillicrap,J.J.Hunt,A.Pritzel,N.Heess,T.Erez,Y.Tassa,D.Sil- archical reinforcement learning,” in Advances in Neural Information
ver, and D. Wierstra, “Continuous control with deep reinforcement ProcessingSystems,2018,pp.3303–3313.
learning,” in International Conference on Learning Representations, [26] M. L. Puterman, Markov Decision Processes: Discrete Stochastic
2016. DynamicProgramming. JohnWiley&Sons,1994.
[4] H.VanHasselt,A.Guez,andD.Silver,“Deepreinforcementlearning
[27] R. J. Williams, “Simple statistical gradient-following algorithms for
with double Q-learning,” in Thirtieth AAAI conference on artiﬁcial connectionist reinforcement learning,” Machine learning, vol. 8, no.
intelligence,2016.
3-4,pp.229–256,1992.
[5] Z. Wang, T. Schaul, M. Hessel, H. Van Hasselt, M. Lanctot, and
[28] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimiza-
N.DeFreitas,“Duelingnetworkarchitecturesfordeepreinforcement tion,”inInternationalConferenceonLearningRepresentations,2015.
learning,”inInternationalConferenceonMachineLearning,2016.
[29] E. Todorov, T. Erez, and Y. Tassa, “MuJoCo: A physics engine for
[6] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, model-basedcontrol,”in2012IEEE/RSJInternationalConferenceon
D.Silver,andK.Kavukcuoglu,“Asynchronousmethodsfordeeprein- IntelligentRobotsandSystems. IEEE,2012,pp.5026–5033.
forcementlearning,”inInternationalconferenceonmachinelearning,
[30] A. Hill, A. Rafﬁn, M. Ernestus, A. Gleave, R. Traore, P. Dhariwal,
2016,pp.1928–1937.
C. Hesse, O. Klimov, A. Nichol, M. Plappert, A. Radford, J. Schul-
[7] S.Gu,E.Holly,T.Lillicrap,andS.Levine,“Deepreinforcementlearn-
man,S.Sidor,andY.Wu,“Stablebaselines,”https://github.com/hill-
ing for robotic manipulation with asynchronous off-policy updates,”
a/stable-baselines,2018.
in 2017 IEEE international conference on robotics and automation
[31] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,
(ICRA). IEEE,2017,pp.3389–3396. “Proximal policy optimization algorithms,” arXiv preprint
[8] A.Nagabandi,G.Kahn,R.S.Fearing,andS.Levine,“Neuralnetwork arXiv:1707.06347,2017.
dynamics for model-based deep reinforcement learning with model-
[32] L. Fan, Y. Zhu, J. Zhu, Z. Liu, O. Zeng, A. Gupta, J. Creus-Costa,
freeﬁne-tuning,”in2018IEEEInternationalConferenceonRobotics
S.Savarese,andL.Fei-Fei,“SURREAL:Open-sourcereinforcement
andAutomation(ICRA). IEEE,2018,pp.7559–7566. learning framework and robot manipulation benchmark,” in Confer-
[9] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust enceonRobotLearning,2018.
region policy optimization,” in International conference on machine
[33] J.Long,E.Shelhamer,andT.Darrell,“Fullyconvolutionalnetworks
learning,2015,pp.1889–1897. forsemanticsegmentation,”inProceedingsoftheIEEEconferenceon
[10] W. Masson, P. Ranchod, and G. Konidaris, “Reinforcement learning computervisionandpatternrecognition,2015,pp.3431–3440.
withparameterizedactions,”inThirtiethAAAIConferenceonArtiﬁcial
[34] K. Simonyan and A. Zisserman, “Very deep convolutional networks
Intelligence,2016. for large-scale image recognition,” in International Conference on
[11] M.HausknechtandP.Stone,“Deepreinforcementlearninginparam- LearningRepresentations,2015.
eterizedactionspace,”2016.
[35] A.Murali,T.Chen,K.V.Alwala,D.Gandhi,L.Pinto,S.Gupta,and
[12] E.Wei,D.Wicke,andS.Luke,“Hierarchicalapproachesforreinforce-
A.Gupta,“Pyrobot:Anopen-sourceroboticsframeworkforresearch
ment learning in parameterized action space,” in 2018 AAAI Spring andbenchmarking,”arXivpreprintarXiv:1906.08236,2019.
SymposiumSeries,2018.
[13] J.Andreas,D.Klein,andS.Levine,“Modularmultitaskreinforcement
learningwithpolicysketches,”inProceedingsofthe34thInternational
ConferenceonMachineLearning-Volume70. JMLR.org,2017,pp.
166–175.
[14] Y.Davidor,GeneticAlgorithmsandRobotics:Aheuristicstrategyfor
optimization. WorldScientiﬁc,1991,vol.1.
[15] E.Gat,“Ontheroleofsimulationinthestudyofautonomousmobile
robots,” in AAAI-95 Spring Symposium on Lessons Learned from
ImplementedSoftwareArchitecturesforPhysicalAgents,1995.
[16] I.Mordatch,K.Lowrey,andE.Todorov,“Ensemble-CIO:Full-body
dynamic motion planning that transfers to physical humanoids,” in
2015 IEEE/RSJ International Conference on Intelligent Robots and
Systems(IROS). IEEE,2015,pp.5307–5314.
[17] J.Tobin,R.Fong,A.Ray,J.Schneider,W.Zaremba,andP.Abbeel,
“Domain randomization for transferring deep neural networks from
simulation to the real world,” in 2017 IEEE/RSJ International Con-
ferenceonIntelligentRobotsandSystems(IROS). IEEE,2017,pp.
23–30.
[18] K.Bousmalis,A.Irpan,P.Wohlhart,Y.Bai,M.Kelcey,M.Kalakrish-
nan,L.Downs,J.Ibarz,P.Pastor,K.Konoligeetal.,“Usingsimulation
anddomainadaptationtoimproveefﬁciencyofdeeproboticgrasping,”
in2018IEEEInternationalConferenceonRoboticsandAutomation
(ICRA). IEEE,2018,pp.4243–4250.
[19] F.SadeghiandS.Levine,“CAD2RL:Realsingle-imageﬂightwithout
asinglerealimage,”inRobotics:ScienceandSystems,2017.
[20] M. Asada, S. Noda, S. Tawaratsumida, and K. Hosoda, “Purposive
behavior acquisition for a real robot by vision-based reinforcement
learning,”Machinelearning,vol.23,no.2-3,pp.279–303,1996.
[21] L. Chrisman, “Reasoning about probabilistic actions at multiple lev-
els of granularity,” in AAAI Spring Symposium: Decision-Theoretic
Planning,1994.
[22] P. Dayan and G. E. Hinton, “Feudal reinforcement learning,” in
Advances in neural information processing systems, 1993, pp. 271–
278.
1155
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 12:23:06 UTC from IEEE Xplore.  Restrictions apply. 