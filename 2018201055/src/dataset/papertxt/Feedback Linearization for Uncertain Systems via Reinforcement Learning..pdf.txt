2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
Cooperative Perception and Localization for Cooperative Driving
Aaron Miller1, Kyungzun Rim1, Parth Chopra2, Paritosh Kelkar2, and Maxim Likhachev1
Abstract—Fully autonomous vehicles are expected to share
the road with less advanced vehicles for a signiﬁcant period
of time. Furthermore, an increasing number of vehicles on the
road are equipped with a variety of low-ﬁdelity sensors which
providesomeperceptionandlocalizationdata,butnotatahigh
enough quality for full autonomy. In this paper, we develop a
perception and localization system that allows a vehicle with
low-ﬁdelity sensors to incorporate high-ﬁdelity observations
from a vehicle in front of it, allowing both vehicles to operate Fig. 1: An L2 vehicle engaged in cooperative sensing and
with full autonomy. The resulting system generates perception
planning with an L4 vehicle. The unoccluded L4 sensor
and localization information that is both low-noise in regions
range is shown in blue, and the L2 sensor range in green.
covered by high-ﬁdelity sensors and avoids false negatives in
areas only observed by low-ﬁdelity sensors, while dealing with
latency and dropout of the communication link between the
two vehicles. At its core, the system uses a set of Extended concernedspeciﬁcallywiththeprocessoffusingthepercep-
Kalman ﬁlters which incorporate observations from both vehi-
tion and localization measurements and not the other parts
cles’sensorsandextrapolatethemusinginformationaboutthe
of the system.
road geometry. The perception and localization algorithms are
evaluated both in simulation and on real vehicles as part of a There are several challenges involved in such a system.
full cooperative driving system. First, the bandwidth of a V2V link is not high, meaning that
raw sensor information cannot be shared and fused in the
I. INTRODUCTION
same way that sensors would be fused on a single vehicle.
Vehiclesarecurrentlybeingdevelopedwithvaryinglevels
Second, there is some latency associated with the network
of driver assistance and autonomy capabilities. There are
communication. At highway speeds (approximately 55mph
already cars on the road today that have some ability to
and above), this means that measurements of each observed
sensetheirsurroundingsandprovidedriverassistancebutare
vehicle must be extrapolated separately and accurately for
unabletodriveautonomouslywithoutconstanthumansuper-
robustresults.Andﬁnally,thesystemmustbeabletohandle
vision.ThesecarsaredesignatedasLevel2,orL2,vehicles.
a drop in communication or in quality of perception data
Meanwhile, Level 4 (L4) vehicles are fully autonomous and
from the L4. If another vehicle cuts between the L2 and the
do not require human supervision in the areas in which
L4, or if the L4 goes out of range of the L2, there must
they are approved to drive. Such vehicles are currently in
be some amount of time where the L2 is able to operate
development, but are not yet available, and will take a long
autonomously before the driver is able to take over.
time to replace existing cars on the road even once they are
We present a cooperative perception and localization sys-
available. For a long period of time, the road will be shared
tem which deals with these issues. Our system shares ob-
by L4, L2, and lower capability vehicles.
served vehicles (“tracks”) and their associated uncertainties
Standardsarealreadyinplaceforvehicle-to-vehicle(V2V)
along with localization estimates and uncertainties. These
communication, allowing vehicles to communicate wire-
estimatesarematchedtovehiclesobservedbytheL2,allow-
lessly,albeitatlowbandwidthandlimitedrange.Becauseof
ingeliminationoffalsenegativesduetoocclusionorlimited
this,itisdesirableforalimitednumberofvehicleswithhigh-
sensor range. The associated measurements are then fused
ﬁdelity sensors (L4 vehicles) to be able to share information
togetherinanExtendedKalmanﬁltertogivehigh-ﬁdelityes-
from their own perception systems with less capable (L2)
timatesoftrackedvehiclestatesatthecurrenttime.Because
vehicles. This allows an L2 vehicle to achieve “Affordable
of this EKF architecture, the system degrades gracefully
Autonomy through Cooperative Sensing & Planning” (Fig-
over time if communication drops out, producing reliable
ure 1), where an L2 vehicle is able to operate autonomously
perception until the driver is able to take over. The system
without expensive sensors by receiving perception informa-
is validated both in a simulator and on physical test vehicles
tion and a suggested trajectory from an L4 vehicle. The
equipped with typical L2- and L4-capable sensor suites.
L2 vehicle then fuses the perception information from both
vehicles, the L4 vehicle and itself, and uses it to generate II. RELATEDWORK
a safe plan which follows the L4 vehicle but also avoids
There has been a good deal of work in the cooperative
obstacles that might not be visible to the L4. This paper is
driving space. One set of approaches has come primarily
from a connected vehicles perspective with less emphasis
1TheRoboticsInstitute,CarnegieMellonUniversity,Pittsburgh,PA,USA
2HondaR&DAmericas,Inc,AnnArbor,MI,USA on sensing. Many of these approaches were demonstrated
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 1256
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 11:46:37 UTC from IEEE Xplore.  Restrictions apply. in the Grand Cooperative Driving Challenge [1]. This re-
quiredcompetitorstodemonstratevariousconnectedvehicle
behaviors, such as platooning, using shared localization in-
formation from the connected vehicles, but did not require
interaction with vehicles only observed by sensors.
Some work has also been done to fuse perception data
from multiple vehicles. Several works have done this us-
ing occupancy grids [2] [3] [4] or raw point clouds [5].
Theseapproachessuccessfullycombinestaticobstaclesfrom
multiple vehicles, eliminating false negatives and mitigating
occlusions. They do have some ability to deal with dynamic
obstacles either at low speed or low latency. However, due
to the loss of representation of individual vehicles and the
lack of velocity information, these approaches cannot fuse
observations of other vehicles at highway speeds while tol-
Fig. 2: Sensor layouts of the two test vehicles. The L2 is
erating latency in communication.
above and the L4 is below.
Other approaches include using Probability Hypothesis
Density (PHD) ﬁlters [6], which are able to represent uncer-
taintybothinlocationsofobservedobjectsandinthenumber
B. Mathematical Formulation
ofobservedobjects.However,theseapproachesrequirehigh
communication bandwidth and latency is not considered. We start by denoting the L2 vehicle state by xL2 and
(cid:2) (cid:3) t
Rauchetal.[7]dofuseindividualvehiclesbyrepresenting the L4 vehicle state by xL4. Similarly, vehicles other than
t
them as point clouds, using a single point with uncertainty the L2 or the L4 have states xi. The state of each vehicle
t
for each corner of each observed vehicle. This successfully is a vector x = x y θ v ω T, where x, y, and θ
fuses tracks with lower bandwidth requirements, but suffers are the pose of the center of the rear axle, v is the signed
from higher estimation errors than expected and does not speed, and ω is the angular velocity. For our cooperative
consider latency compensation necessary for operation at localizationalgorithm,wewillomittheangularvelocity(and
highway speeds. equivalently, wheel angle) because it is considered to be an
On the localization side, [8] uses multiple sensors to de- internal part of the state, not observed by the localization
cidetheglobalpositionofaMicroAerialVehicle(MAV).In system. Then, the state of the whole environment can be
this paper, as they use sensors having different frequencies, summarized as E =(xL2,xL4,x1,x2,...,xN).
t t t t t t
they process delayed measurements and integrate them with Several types of measurements are assumed to be avail-
aMultiSensorFusionExtendedKalmanFilter.Thismethod able, and all are assumed to be normally distributed. First,
succeeds in compensating for delayed measurements and lo- each vehicle has some localization system which provides
calizing the MAV in both indoor and outdoor environments. measurements
However, the error of this approach is too high to apply to
∼N
a vehicle running at high speed. zL2 =xL2+εL2,εL2 (0,ΣL2)
t t t t ∼N t (1)
Ourapproachfocusesontheabilitytooperateathighway zL4 =xL4+εL4,εL4 (0,ΣL4)
t t t t t
speeds while dealing with limited network bandwidth and
latency. with independent Gaussian noise εL2 and εL4, respectively.
t t
The perception system is assumed to both have some
III. FORMULATION false negatives and have some error in estimated state of the
tracked vehicles. These false negatives can have a variety
A. Sensor Suites
of causes, the most common being occlusions and limited
The approach we present is general and can be applied sensor range. Formally, we assume there is some probabil-
regardless of the types of sensors on each vehicle. With ity pL2(xi,E ) that track i is undetected by the L2 vehi-
that being said, the domain for the experiments is highway cle’sFpNercetptiotn system at time t, and similarly probability
driving. In our experiments, the L4 vehicle is equipped with pL4(xi,E ) that track i is undetected by the L4 vehicle at
high-resolution sensors typical of an L4 test vehicle today - tiFmNe t,twhetre the subscript FN stands for false negative. For
Lidar and Radar covering 360 degrees around the vehicle as vehicleswithstatesxi whicharedetectedattimet,wehave
well as cameras for perception, and an RTK GPS for local- measurements t
ization. The L2 vehicle, on the other hand, is equipped only
∼N
with Radar and a forward-facing camera for perception and zi,L2 =xi+εi,L2,εi,L2 (0,Σi,L2)
t t t t t (2)
a lower resolution GPS-INS system for localization. Each zi,L4 =xi+εi,L4,εi,L4 ∼N(0,Σi,L4),
vehicle is equipped with a DSRC radio for V2V communi- t t t t t
cation. Diagrams showing similar vehicles to the ones that for vehicles detected by the L2 and L4, respectively, again
we tested on are shown in Figure 2. with independent Gaussian noise εi,L2 and εi,L4.
t t
1257
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 11:46:37 UTC from IEEE Xplore.  Restrictions apply. ·
IV. PERCEPTIONMETHODS where f(;µ,Σ) is the PDF (probability density function)
of the multivariate normal distribution and xˆij is the mean
The overall algorithm uses a set of Extended Kalman Fil- t
of the estimate formed by fusing measurements zi,L2
ters (EKFs), one for each tracked vehicle. The state of each t
and zj,L4. This represents the negative log-likelihood of
EKFisreinitializedeachtimeanewsetoftracksisavailable t
detecting both vehicles as well as measuring them at the
from the L4 vehicle; during normal operation, this measure-
given positions. It should be noted that E is unknown. In
mentisdelayedbycommunicationlatency,andduringadrop t
our experiments, a constant pL2 and pL4 were sufﬁcient;
in communication this measurement is delayed even more. FN FN
however, it would also be possible to use an approximation
Wereinitializethestateinsteadoffusingthewholestreamof
of E and a more sophisticated function for p .
measurementsfromtheL4becausetheL4perceptionsystem t FN
Once a cost is assigned to each matching, the problem
is assumed to have its own ﬁltering scheme, and the mea-
is solved by the Hungarian algorithm [9] [10] [11], which
surement we receive is assumed to contain all information
ﬁndsthebestcostmatchingincubictimeinthetotalnumber
from older measurements made by the L4’s sensors. More
of measurements from both vehicles. An example matching
recentmeasurementsfromtheL2perceptionsystemarethen
problem,anditsformulationasabipartitegraphtobesolved
fused in these EKFs to get a current estimate of the tracked
by the Hungarian algorithm, is shown in Figure 3.
vehicles’states.Forthistobepossible,wemustknowwhich
trackobservedbytheL2correspondstoeachtrackobserved B. EKF per Tracked Vehicle
bytheL4,aswellaswhichtrackswereonlyobservedbyone
Each tracked vehicle has its own EKF with state space
ofthetwovehicles.Findingthiscorrespondenceisnontrivial,
[x,y,θ,v,ω]T, where x,y, and θ are the pose in SE(2), v is
especiallywhencovariancesofestimatesaren’tuniform.The
the signed speed, and ω is the angular velocity. The angular
overallperceptionalgorithm(showninAlgorithm1)thenhas
velocity is not expected to be measured; instead, we use the
two essential components: a MATCH function which takes roadcurvatureandvehiclespeedv tocalculateapriorforω.
twosetsoftracksandreturnsasetofpairingsbetweenthem,
The PREDICT function assumes constant speed and angular
andasetofEKFswhichmakeupthePREDICTandUPDATE velocity to extrapolate the given set of states to the desired
functions used to extrapolate and fuse the individual track
time, while adding process noise. The UPDATE function is
states. It uses these to extrapolate the current set of tracks
thenthestandardEKFupdatewhichtakestheﬁlterstateand
St−1|t−1 to a timestamp t when a measurement is available, covariance (xˆ,Σˆ) and measurement (z,Σ) and returns the
producing the set St|t−1, then associate the measurements new state and covariance. Because the L2 perception system
with existing tracks and update them to produce the set St|t, isassumedtohavesomeinternalﬁlteringschemethatwould
beforerepeatingtheprocessuntilthecurrenttimeisreached.
causesequentialmeasurementstobehighlycorrelated,wedo
This algorithm handles drops in communication with no not fuse every measurement we get from the L2 perception
further modiﬁcations - when no new messages are being system;instead,wesubsampledowntoafrequencyatwhich
received,itisstillabletotakethelastreceivedmeasurement sequential measurements are not highly correlated.
from the L4 vehicle and iteratively fuse more recent mea-
surementsfromtheL2;asthetimesincethecommunication V. LOCALIZATIONMETHODS
failure increases, the output gradually becomes noisier and A. Estimation of Global Position of L2
closer to the L2’s raw perception output.
TogetamoreaccurateglobalpositionestimatefortheL2
vehicle, the measurements from the L4 vehicle, which have
A. Matching Tracks between Vehicles
a higher resolution, are fused with measurements from the
Each measurement must be associated correctly with a L2 vehicle. We have two measurements of the L2 vehicle
measurementfromtheothervehicle;caseswhereavehicleis
not observed by both the L2 and the L4 should be correctly
Algorithm 1 Fuse Perception
identiﬁed. This is formulated as a minimum-weight bipartite
matching problem, where a measurement can be matched tL4 = Time of last available measurement from the L4
either to a measurement from the other vehicle or left un- T = Time o{f last avail}able{measuremen}t from the L2
mfroamtchtehde.loAg-lcioksetlihisooadssoofctihateedmewasituhreemacehntoufndtheresthe,atcmomaticnhg. SfotrL4t|t=L4t=L4+xˆit1L,4,..Σˆ.,itTL4do= zit,LL44,Σit,LL44
Fleofrt aunmmeaatscuhreedm,etnhtezcito,vstfriosm−vlnehpivFclNe(zvit,∈v,{ELt2).,LF4o}r tahaptaiisr SMt|t=−1M=∅ATPCRHE(DSItC|tT−(1S,t−{z1|jtt,−L12,,Σt)jt,L2}Mj=t1)
of measurements (zi,L2,Σi,L2) and (zj,L4,Σj,L4) that are St|t =
matched to each othetr, thetcost is t t for match in M do
if match is a false negative from one vehicle then
− −
C(zi,L2,zj,L4)= ln(1 pL2(zi,L2,E )) Add onemeasurement (xˆi| − or zj,L2) to S |
t t FN t t tt 1 t tt
− − else
ln(1 pL4(zj,L4,E ))
− FN t t (3) Add UPDATE(xˆi| − ,zj,L2) to S |
lnf(zi,L2;xˆij,Σi,L2) tt 1 t tt
−lnf(ztj,L4;xˆtij,Σtj,L4), return PREDICT(ST|T, tnow)
t t t
1258
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 11:46:37 UTC from IEEE Xplore.  Restrictions apply. zL2 zL4
z1 z2
z3 zL4 z2 z3 FN FN
∞ − ∞
zL2 12 19964 7
− ∞
z1 12 3595 1647 7
∞ ∞
FN 7 0 0
∞ ∞
FN 7 0 0
∞ ∞
FN 7 0 0
Fig. 3: Example toy matching problem. The scenario is
shown in the top right; tracks and localization from the L4
vehicle (with their covariances in x and y) are shown in
blue, and tracks and localization from the L2 are shown in Fig. 4: Diagram of measurements and state estimates of the
yellow. In the representation of the matching problem as a L2vehicle.tmeansthetimewhenweprocessmeasurements
bipartite graph (on the left), each blue node represents a and “current time” is when our algorithm ﬁnishes.
localization measurement, black nodes represent perception
measurements, and red nodes represent false negatives. The
leftcolumnshowstheL2localizationandonetrackdetected vehicle relative to the L4 vehicle. εL2 is the measurement
by the L2, while the right column shows the L4 localization noiseofthesensorswhichmeasurethteglobalpositionofthe
and two tracks detected by the L4 perception system. Each L2 vehicle. We use an Extended Kalman Filter modiﬁed by
edgerepresentsanallowedconnection-noedgebetweentwo usingthemeasurementfromtheL4vehiclezL−2 tocalculate
nodesisthesameasinﬁnitecost.Blackedgesareassociated t τ
xˆL−2 in the prediction step instead of using the previous
with pairs of measurements and have costs given by the t τ
step’s L2 vehicle estimate xˆL−2 , because all measurements
function C. Orange edges represent assigning a −detection are assumed to come from ﬁltte1rs which already account for
as undetected by the other vehicle and have costs lnpvFN. all past information.
Green edges allow FN nodes to be matched with each other
To estimate the L2 vehicle state, we ﬁrst calculate zL−2 ,
for 0 cost, which is necessary for this to be a well-deﬁned t τ
where τ is a time latency between measured time and time
bipartitematchingproblem.Thetableofedgecostsisshown
whenweprocessmeasurements,byaddingmeasurementsof
in the bottom right; edges forming the optimal matching are
the global position of the L4 vehicle and the L2 vehicle’s
in bold.
position relative to the L4 vehicle.
−
zL−2 =zL−4 +zL−2 L4
position. The ﬁrst measurement is received from GPS-INS t τ t τ t τ − (6)
=xL−2 +εL−4 +εL−2 L4
on the L2 vehicle; the second measurement is received from t τ t τ t τ
the L4 perception system, from which we calculate the L2 As zL−2 is also a normal distribution, we use the mean of
vehicle location by adding the global position measurement t τ
zL−2 as xˆL−2 . In the prediction step, we predict the L2
of the L4 vehicle to the measurement of the L2 vehicle t τ t τ
vehicle state xˆL|2− by using the system model and xˆL−2 .
position relative to L4 vehicle. These measurements from tt τ t τ
In the update step, we calculate the state estimate xˆL2 by
the L4 vehicle are delayed by time τ. To use these two t
updating the predicted state with the measurement from the
measurements, we deﬁne the dynamical system
  L2 vehicle zL2.
∼N t
xLt2 =AtxLt−2τ +εt,εt (0,Qt) B. Compensating for Latency with Prediction
1 0 0 cos(θ)τ
(4)
0 1 0 sin(θ)τ We compensate the measurement from the L2 vehicle
A = ,
t 0 0 1 0 which has a lower resolution with measurements received
0 0 0 1 from the L4 vehicle. As in Figure 4, when there is latency
where ε is the process noise, and measurement models in the communication between the L2 vehicle and the L4
t
vehicle, we compensate for the L4 global position and the
∼N
zL−4 =xL−4 +εL−4 ,εL−4 (0,ΣL−4 ) L2-L4 relative distance measurements by extrapolation. Ex-
t τ t τ t τ t τ t τ
− − − − ∼N −
zL−2 L4 =xL−2 L4+εL−2 L4,εL−2 L4 (0,ΣL−2 L4) (5) trapolation is implemented with the same algorithm used in
t τ t τ t τ t τ t τ
the Kalman Filter prediction step using the model in Eq. 4.
zL2 =xL2+εL2,
t t t However,ifthelatencyislongerthan1second,weonlyuse
where εL−4 is the measurement noise of the sensors for the themeasurementfromtheL2vehicle,becausepredictingthe
t τ −
globalpositionoftheL4vehicle.εL−2 L4 isthemeasurement statewithmeasurementsfromtheL4vehicledoesnotreduce
t τ
noise of the sensors which measure the position of the L2 the resulting localization error.
1259
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 11:46:37 UTC from IEEE Xplore.  Restrictions apply. VI. EXPERIMENTSANDRESULTS
We tested our perception and localization system both
in simulation and on real vehicles. Testing in simulation
allowed us to study how the system responds to various
amountsoflatencyandtogatherstatisticsonhowthesystem
performs. On the real vehicle, we are able to gather some
statistics as well. However, we rarely have ground truth
positions for the tracked vehicles, so we are primarily con-
cerned with demonstrating that the perception system gener-
ates smooth output which eliminates false negatives. In the
Fig. 5: RMS and 99th-percentile errors between the ground
following subsections, we present results from testing per-
truth track locations and output of the fused perception sys-
ception and localization in simulation and on real vehicles.
tem.Positionerrorissplitintoon-track(blue)andcross-track
(orange) error. The horizontal axis shows the time since the
A. Simulation
lastmeasurementfromtheL4perceptionsystem;theaverage
Experimentsinsimulationweredoneinahighwaydriving error increases with time, but remains near or below the L2
scenario using the VTD simulator. The road is a divided perception system on its own up to 1s of latency.
highwaywithaspeedlimitof60mph(26.8m/s)andmoderate
to heavy trafﬁc traveling at approximately that speed. The
simulator generates perception from both the L2 and L4
vehicles by applying limited sensor range to each (200m for
the L4 and 100m for the L2), along with Gaussian noise
on the poses and speeds of all observed vehicles (.12m and
.5m/s on position and velocity respectively on the L4, and
.25m and .5m/s on the L2). Similarly, the simulator adds
Gaussiannoisetothelocalizationavailabletothesystemfor
Fig. 6: Change in perception error with varying amounts of
each vehicle (.01m on the L4 and .1m on the L2). Random
noiseinsimulation,withL4andL2sensornoiseincreasedin
latency is also injected into the communication between the
proportiontoeachother.AsinFigure5,blueandorangeare
vehicles; the typical delay is around 100ms, but there are
on-track and cross-track error respectively. Error increases
random spikes of higher latency as well.
linearly with sensor noise, as would be expected.
1) Perception: First, the RMS error and the 99th per-
centile error of the fused perception output is shown in
Figure 5. It remains in a high-quality range (on the level of
athighwayspeed,butinsteadatapproximately12m/sonthe
theL4perceptionsystem)foralltypicallatencies;forlonger
test track and up to 18m/s on the public road.
delays from the last measurement, as caused by drops in
1) Perception: The perception system was tested on sev-
communication, the error increases, but remains in a usable
eral scenarios on the test track and a public road, with other
rangeforlongenoughthatthesystemcancontinueoperating
vehiclesineachscenario.Theperceptionsystemsuccessfully
until the system is able to ask the driver to take control.
matchedthevehiclesfromtheL2andL4perceptionsystems
The rate of mismatched tracks is also low; in experiments
correctly, eliminating false negatives (which were due to
insimulation,only0.017%ofmeasurementswereincorrectly
occlusions and sometimes to blind spots). An example de-
associated between the L2 and L4 perception systems.
tectionisshowninFigure9.Becausewedidnothaveaccess
2) Localization: We evaluate the performance of our lo- to ground truth poses or velocities for most of the tracked
calization system by using the RMS error. The error is cal- vehicles,wedonotstudytheaccuracyofthepredictedtracks
culatedbycomparingtheestimatedlocationwiththeground on the real data. However, we do have one example of the
truth location. We have 2 error distributions in Figure 8: system tracking a vehicle equipped with an RTK to provide
the measurement location error is for location estimated by ground truth position; this is shown in Figure 7, where the
measurementsfromonlytheL2vehicleandupdatedlocation system handles false negatives from both the L2 and L4
error is for cooperative localization. perceptionsystemsatdifferenttimesandgeneratesasmooth
Theaverageoftheupdatedlocationerroris0.074m,which output trajectory close to the ground truth.
is lower than the average of the measurement location error, 2) Localization: Forthelocalizationsystem,wegetmea-
0.127m. The updated location error is small enough to lo- surements of the L4 vehicle global location zL−4 from the
calize the car at highway speeds. RTK GPS on the L4 vehicle and the global potsiτtion of the
L2 vehicle zL2 from GPS-INS on the L2 vehicle. In our
B. Real vehicles t
tests, we use an RTK mounted on the L2 to simulate the
−
Experiments were run on data from real vehicles on a test position of the L2 vehicle relative to the L4 vehicle zL−2 L4
t τ
track and on a public road. The test track is similar to a instead of the L4 perception system because the L4 did not
divided highway with three lanes. Vehicles were not driving detecttheL2vehicleconsistently.Weaddwhitenoisetothe
1260
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 11:46:37 UTC from IEEE Xplore.  Restrictions apply. Fig. 7: The trajectory, both raw and fused, of a vehicle tracked by the perception system. The vehicle is traveling from left
torightwhilechanginglanes.Thefusionsystemsuccessfullydealswithfalsenegativesfrombothperceptionsystems,while
alsohavinglesserrorthantheL2systemalone.BecausethetrackedvehiclealsohasanRTK,wecanusethisasgroundtruth.
Fig. 10: Distributions of the cross-track error and on-track
error of real localization data. Measured location error is
Fig. 8: Distributions of localization error in simulation. The
the error of measurements from the L2 vehicle’s GPS-INS.
bluehistogramshowstheRMSerrorofthefusedL2vehicle
(cid:112) Updated location error is the error of fused estimates.
location. The red histogram shows the RMS error of the
L2 localization measurement before fusion. RMS error is
deﬁned as the total displacement x2+y2.
ﬁltering measurements from both while dealing with latency
and drops in communication. This approach was demon-
stratedsuccessfullybothinsimulationandondatafromreal
vehicles as part of a full cooperative driving system.
Possible issues with this perception approach include
tracksthataremissedduetoocclusionincrowdedareaswith
high sensor uncertainty; because we model the probability
Fig. 9: An example detection on the real vehicle. The image offalsenegativesandthesensoruncertaintyasuniformover
ontheleftisfromacameraintheL4vehicle(notthecamera space, and independent of occlusions, this can cause chal-
usedfordetection).Ontheright,thedetectedvehicleisseen lenges in matching the observations from the L2 and the L4
in pink in front of the L4 (in blue) and the L2 (in green). correctly.Futureworkcoulduseamoresophisticatedmodel
forp ,oramorecomplexsensormodel,whichaccountsfor
FN
this and makes the matching more robust. Similarly, future
−
zL−2 L4 with a standard deviation of 0.02m. For the ground work could investigate the use of non-Gaussian distributions
t τ
truthlocation,weusemeasurementsfromanRTKontheL2. to model the errors in both perception and localization more
We evaluate our performance with the cross-track error, accurately. This work was also focused on highway driving
which is the error in the y direction in the local frame of with one L2 vehicle following one L4 vehicle; different
the L2 vehicle. The right histogram in Figure 10 shows that scenarios and sets of vehicles are potential extensions.
RMSerrorsarereducedwhenweupdatemeasurementswith
information from the L4 vehicle. As there is a lot of noise
in the on-track direction in GPS-INS data, the error of the ACKNOWLEDGEMENTS
updated estimates is large. However, the cross-track error is
small enough, which is more important. This research was funded by Honda R&D Americas, Inc.
The contents of this paper reﬂect the views of the authors,
VII. DISCUSSIONANDCONCLUSION
whoareresponsibleforthefactsandtheaccuracyofthedata
We presented approaches for cooperative perception and presented herein. The contents do not necessarily reﬂect the
localizationbetweentwovehicles,capableofcombiningand ofﬁcial views of Honda R&D Americas, Inc.
1261
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 11:46:37 UTC from IEEE Xplore.  Restrictions apply. REFERENCES [9] H. W. Kuhn, “The Hungarian method for the assignment problem,”
NavalResearchLogisticsQuarterly,vol.2,no.1-2,pp.83–97,1955.
[1] J.Ploeg,E.Semsar-Kazerooni,A.I.M.Medina,J.F.C.M.deJongh, [10] R. Jonker and T. Volgenant, “Improving the Hungarian assignment
J. van de Sluis, A. Voronov, C. Englund, R. J. Bril, H. Salunkhe, algorithm,” Operations Research Letters, vol. 5, no. 4, pp. 171–175,
A´. Arru´e, A. Ruano, L. Garc´ıa-Sol, E. van Nunen, and N. van de Oct.1986.
Wouw,“CooperativeAutomatedManeuveringatthe2016GrandCo- [11] A.Bewley,Z.Ge,L.Ott,F.Ramos,andB.Upcroft,“Simpleonline
operativeDrivingChallenge,”IEEETransactionsonIntelligentTrans- and realtime tracking,” in 2016 IEEE International Conference on
portationSystems,vol.19,no.4,pp.1213–1226,Apr.2018. ImageProcessing(ICIP),Sept.2016,pp.3464–3468.
[2] W.Liu,S.W.Kim,Z.J.Chong,X.T.Shen,andM.H.Ang,“Motion
planning using cooperative perception on urban road,” in 2013 6th
IEEEConferenceonRobotics,AutomationandMechatronics(RAM),
Nov.2013,pp.130–137.
[3] S.Saxena,I.K.Isukapati,S.F.Smith,andJ.M.Dolan,“Multiagent
Sensor Fusion for Connected & Autonomous Vehicles to Enhance
NavigationSafety,”p.13.
[4] F. Camarda, F. Davoine, and V. Cherfaoui, “Fusion of evidential
occupancy grids for cooperative perception,” in 2018 13th Annual
ConferenceonSystemofSystemsEngineering(SoSE),June2018,pp.
284–290.
[5] S.Kim,B.Qin,Z.J.Chong,X.Shen,W.Liu,M.H.Ang,E.Frazzoli,
and D. Rus, “Multivehicle Cooperative Driving Using Cooperative
Perception:DesignandExperimentalValidation,”IEEETransactions
on Intelligent Transportation Systems, vol. 16, no. 2, pp. 663–680,
Apr.2015.
[6] J. Gan, M. Vasic, and A. Martinoli, “Cooperative multiple dynamic
objecttrackingonmovingvehiclesbasedonSequentialMonteCarlo
ProbabilityHypothesisDensityﬁlter,”in2016IEEE19thInternational
ConferenceonIntelligentTransportationSystems(ITSC),Nov.2016,
pp.2163–2170.
[7] A. Rauch, S. Maier, F. Klanner, and K. Dietmayer, “Inter-vehicle
objectassociationforcooperativeperceptionsystems,”in16thInter-
nationalIEEEConferenceonIntelligentTransportationSystems(ITSC
2013),Oct.2013,pp.893–898.
[8] S. Lynen, M. W. Achtelik, S. Weiss, M. Chli, and R. Siegwart, “A
robust and modular multi-sensor fusion approach applied to MAV
navigation,”in2013IEEE/RSJInternationalConferenceonIntelligent
RobotsandSystems,Nov.2013,pp.3923–3929.
1262
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 11:46:37 UTC from IEEE Xplore.  Restrictions apply. 