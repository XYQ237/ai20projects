2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
Towards Noise Resilient SLAM
Anirud Thyagharajan1, Om Ji Omer1, Dipan Mandal1, Sreenivas Subramoney1
Abstract—Sparse-indirect SLAM systems have been dom-
inantly popular due to their computational efﬁciency and
photometric invariance properties. Depth sensors are critical
to SLAM frameworks for providing scale information to the
3D world, yet known to be plagued by a wide variety of
noise sources, possessing lateral and axial components. In this
work, we demonstrate the detrimental impact of these depth
noise components on the performance of the state-of-the-art
sparse-indirect SLAM system (ORB-SLAM2). We propose (i)
Map-Point Consensus based Outlier Rejection (MC-OR) to
counter lateral noise, and (ii) Adaptive Virtual Camera (AVC)
to combat axial noise accurately. MC-OR utilizes consensus
information between multiple sightings of the same landmark
Fig. 1: A comparison of trajectory plots between state-of-the-art
to disambiguate noisy depth and ﬁlter it out before pose
ORB-SLAM2[28](inred),ourmethod(inblue)andgroundtruth
optimization. In AVC, we introduce an error vector as an
trajectory(ingreen)fortheTUMfr2 cokesequence,whichisfound
accuraterepresentationoftheaxialdeptherror.Weadditionally
to be critically affected by lateral depth noise in Figure 2.
propose an adaptive algorithm to ﬁnd the virtual camera
location for projecting the error used in the objective function
utilize only cameras. Cameras prove to be cost-effective for
of the pose optimization. Our techniques work equally well
the rich source of information that they provide, with RGB-
for stereo image pairs and RGB-D input directly used by
sparse-indirect SLAM systems. Our methods were tested on D cameras providing depth estimates as well. While there
the TUM (RGB-D) and EuRoC (stereo) datasets and we show are multiple commodity RGB-D sensors available, they are
that they outperform existing state-of-the-art ORB-SLAM2 by impactedbyvariousnoiseissuescausedbythemeasurement
2-3x, especially in sequences critically affected by depth noise.
setup and the surface properties [20], [31]. Though generic
I. INTRODUCTION outlier removal based SLAM systems have been proposed
[46], [4], [17], [34], [33], [41], [42], they either require
SLAM is a fundamental building block in several mobile
signiﬁcantcomputeorarenotrobustenoughindealingwith
autonomous systems [9], [1], [3]. SLAM methods are clas-
the noise.
siﬁed [3], [13] as either (i) sparse/dense, indicating whether
a select set of points or all the pixels in the image are used Though monocular cameras can perform visual odometry,
for processing and (ii) direct/indirect, indicating whether the they suffer from scale, drift and initialization issues [28],
system works directly on the measured quantities or not. [35] as they do not have a direct perception of depth. Active
Beyond being computationally economical than their dense depth sensors (eg. Kinect V2) have been preferred over
counterparts [27], sparse methods have evolved over the passive ones for a wide variety of use cases and have been
years to be more robust and accurate as well. In addition, studied extensively [20], [38], [31], [37], [47]. These works
dense implementations have inherent bias due to geometric alsofocussedonmodellingtheuncertaintyofdepthsensors;
priors and assumptions, leading to reduced long-term accu- [20] modelled depth measurement from disparity through
racy [13]. Eventhough directapproaches do not need feature mathematical formulation, while Nguyen et al. [31] empir-
extraction and can be useful for denser reconstructions as ically derived a noise model for the Kinect and applied it
well,theysufferfromartifactscreatedbyassumingasurface totheKinectFusionsystem[29].Utilizingsuchfundamental
reﬂectance model. These are further impacted by rolling models of sensor characteristics, there have been only a few
shutter, auto gain and auto-exposure artifacts if not modeled extensions to apply them to tackle depth noise problems
properly [27], [13]. Also, [44] highlighted that feature- in stereo/RGB-D SLAM; using Gaussian mixture models
basedmethodsdemonstrateawiderangeofphotometricand with Kalman ﬁlters [11], probabilistic line ﬁtting solutions
geometric invariance as compared to direct methods. Due to [36], along with weighting schemes to denoise depth using
these reasons, sparse-indirect SLAM implementations have residuals [46]. There have been various works in monocular
been adopted widely [18], [8], [21], [28]. SLAM using inverse depth parameterization [5], [30], [14].
While information arising from multiple diverse sensor Though direct noise models are not employed, volumetric
modalities can be fused together [26], [24], [32], [25] to depth fusion models also exist [19], [29], [48], [49], [7].
provide the required estimates, there is a rising trend to Estimating depth error is crucial for SLAM performance.
Strasdat et al. [39] proposed a virtual right stereo camera
1All authors are with Processor Architecture Research Lab, Intel
{ onto which the depth error is projected, which was also
Labs. anirud.thyagharajan, om.j.omer, dipan.mandal,
sreenivas.subramoney}@intel.com adopted by ORB-SLAM2 [28]. It uses a Bundle Adjustment
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 72
Authorized licensed use limited to: University of New South Wales. Downloaded on September 19,2020 at 10:33:44 UTC from IEEE Xplore.  Restrictions apply. (BA) based optimization framework for minimizing the 3D
geometric error between the estimated landmark in 3D
coordinates (Map-Point [27]) and 3D feature (obtained by
back-projecting 2D feature of the landmark from a camera
viewpointusingestimateddepth),helpinginprovidingnoise
resistant poses to an extent. However, the optimization pro-
cessisunlikelytodetectscenarioswherenoisydepthaffects
featurecorrespondences,whichcouldseverelyimpactSLAM
Fig. 2: RGB-D images showing outliers for the TUM fr2 coke
performance. [36], [15], [12], [43] have explored denoising sequence. Most of the outliers (in red) occur along the depth
methodstomitigateafewlimitations,butincorporatingnoise boundaries, typically showing the impact of lateral depth noise.
resilience inherently into the SLAM system is always bene-
ﬁcial, especially since heavy pre-processing is expensive.
In our work, we pick cues from Nguyen et al. [31]
to formulate the components of depth noise (lateral and
axial). We propose two novel techniques addressing both
categories of depth noise to architect noise-resilient SLAM
systems. We show that these techniques, individually and
cumulatively,improveSLAMperformanceonallsequences,
obtaining upto 2-3x improvement over state-of-the-art on Fig. 3: Failure case for the static virtual camera
those critically affected by depth noise.
1. We highlight limitations with state-of-the-art sparse pose gets affected by outliers. Since outlier detection itself
indirect SLAM solutions in handling depth-noise. depends on the estimated camera pose, in several scenarios
2. To the best of our knowledge, we are the ﬁrst to bad features may get annotated as inliers and good features
present a notion of dynamic centroid of 3D features and couldbediscardedasoutliers,badlyimpactingthereliability
propose Map-point Consensus based Outlier Rejection to of the estimated pose. Some solutions use RANSAC [22],
detectandﬁlteroutnoiseaffectedlandmarksand3Dfeatures [10]toiterativelypickrandomsetsoffeatures,tracknumber
through a novel hierarchical decision ﬂow. of outliers in each set and choose the set with the lowest
3. We also propose a novel technique, Adaptive Virtual number of outliers, but such methods are computationally
Camera, which dynamically identiﬁes the virtual camera demanding and non-deterministic.
location per feature, and propose a depth error term to Lack of Consensus: ORB-SLAM2 [28] detects outlier
accurately estimate the depth noise. We extend this error features on a per-frame basis assuming the feature in the
term in both x and y directions, as opposed to only in the anchorframe(cameraviewpointfromwherethelandmarkis
baseline direction [28]. ﬁrstperceived)tobepristine.However,itispossiblethatthe
feature in the anchor frame (and possibly in few subsequent
II. MOTIVATION ones as well) is observed incorrectly, while in most of the
Monocular vision based SLAM suffers from scale, drift viewpointslateritisobservedaccurately.Duetothelackofa
and initialization issues especially when the camera is sub- consensus based approach, a potential good feature could be
jected to pure rotation [35] or low camera motion along the discardedasanoutlier.Also,insomescenarios,eventhough
projectionplane.Therefore,depthcuesareessentialforgood a bad feature is identiﬁed as an outlier in the majority of
SLAM performance. However, all commodity depth sensors frames, it could be treated as an inlier in a few frames, thus
suffer from noisy measurements. Nguyen et al. [31] cited affecting the estimated pose for those frames.
two major components of depth noise (i) axial noise: error Correspondence Issues at Boundaries: Since the depth
in depth along the depth axis, (ii) lateral noise: depth of estimate of a feature-point is inﬂuenced by neighborhood
thecurrentpixelbeinginﬂuencedbyneighboringpixels.We regionduetolateralnoise,pointsonthedepthboundariesare
highlightkeyissueswithstate-of-the-artsolutionsindealing generally very deviant from their true locations. As shown
with noise which severely impacts SLAM performance: in Figure 2, a landmark in a foreground object might be
Outlier Detection & Rejection: ORB-SLAM2 [28] clas- perceived as if it is located in the background and vice
siﬁes features into inliers and outliers during BA based on versa. For tracking a feature across viewpoints, a motion-
theerrorbetweenthemap-pointandthe3Dfeature.Itgauges model[8]isgenerallyemployedtopredictthesearchregion.
the pose estimation ﬁdelity using the inlier-outlier ratio. If In the presence of lateral depth noise, such predictions are
theratioispoor,theprocessisreiteratedwithadditionalcues plausibletofallshortoftheirtruelocations,leadingtoeither
(like matching more features), or tracking is considered as tracking failure or latching onto wrong correspondences.
failed for that camera viewpoint. Similarly, [13] identiﬁes SLAMmethodssuchas[28],[13],whichdonothaveoutlier
outliers based on photometric error during the joint opti- rejection to prune out such features prior to the optimization
mization process and rejects them in the next iterations. In process, are likely to be adversely affected.
such solutions where outlier detection occurs either as part Static Virtual Camera: Indirect methods such as [8],
of or at the end of the optimization routine, the estimated [28],[18],[21]useprojectionsofthe3Dgeometricerroronto
73
Authorized licensed use limited to: University of New South Wales. Downloaded on September 19,2020 at 10:33:44 UTC from IEEE Xplore.  Restrictions apply. the image plane as error terms for the optimization process. Feature-level metrics (FL): These metrics are used to
This favors feature points nearer to the camera to have a score 3D features individually. The FL metrics portray how
stronger contribution towards pose estimation as compared consistent the 3D feature is with rest of the data. The FL
to the farther ones. In indirect methods, 3D error projection metrics aid in providing a speciﬁc understanding of the
cannot capture error along depth axis, as all points on the adherent and deviant 3D projections of the landmark and
depth axis would have the same projection. To mitigate this, this helps to prune the bad projections individually.
(cid:107) − (cid:107)
ORB-SLAM2 [28] followed [39] to use a virtual camera MCE = M X (2)
FL,1,i (cid:107) − i(cid:107)
at a ﬁxed baseline distance on the x-axis from the actual MCE = G X (3)
FL,2,i i
cameratoprojectthe3Derrorontothevirtualcameraimage Ifmajorityofthe3Dfeaturesofaparticularframehavehigh
plane as a means to reﬂect the depth error component. We FL metrics, it is very likely that the camera pose is inaccu-
ﬁnd this method to be inaccurate as the static virtual camera rate, providing essential cues in performance indication.
cannot capture the depth error for all 3D error orientations.
Cluster-levelmetrics(CL):Thesemetricsareusedtoscore
AsshowninFigure3,whenthe3DfeatureD,virtualcamera theclusterasawhole.Theﬁrsttwometrics(Equations4and
(cid:48)
C andthehithertoestimatedmap-pointM arecollinear,the 5) represent the mean values of the FL metrics (Equations 2
3D error projection would be zero. In such scenarios, a non-
and 3), while Equation 6 conv(cid:88)eys how close the map-point
zero3Derrorvectorwouldbetreatedaszerodeptherrorwith
is to the statistical mean of the depth data.
a static virtual camera causing an inherent bias for certain
N
1
depth error orientations, thus severely affecting performance MCE = (cid:88)MCE (4)
CL,1 N FL,1,i
of SLAM methods. Direct methods [13], [14] do not suffer
i=1
fromthisphenomenonastheyoperatewithphotometricerror 1 N
MCE = MCE (5)
instead of geometric error. CL,2 N FL,2,i
i=(cid:107)1 − (cid:107)
III. METHOD MCECL,3 = M G (6)
TheCLmetricsprovideanotionofconsensus;denotinghow
A. Map-point Consensus based Outlier Rejection (MC-OR)
consistentmultiplesightingsofalandmarkareandingeneral
We propose the concept of a moving centroid G amongst howwell-observedthelandmarkis.Iftheyarehigh,itmeans
3D features of the landmark which is updated after every that the landmark is bad to consider as an interest point.
camera viewpoint. Using G, we introduce distance metrics We group these metrics based on similarity and subject
to draft a hierarchical outlier rejection scheme, that weeds
similar metrics to the same thresholds: (i) distances between
out both individual aberrant points and entire groups of
M, F and their average (Equations 2 and 4), (ii)
points. As seen in Figure 4, for a generic SLAM system 3D,X
distances between G, F and their average (Equations 3
in the world coordinate, let LX be a l{andmark which }is and 5), and (iii) distanc3eDs,bXetween G and M (Equation 6).
tracked across N camera views, C = C1,C2,...CN ; Thecorrespondingthresholdsareτ ,τ andτ . For
this landmark is perceived from these camera views at 2D M,F G,F M,G
{ } Algorithm 1 Map-point Consensus based Outlier Rejection
features F = x ,x ...x . By backprojecting these
2D,X 1 2 N ←
23DD fperaotjuercetsioFns usin=g t{hXeir,dXept.h..dXi, w}e, roebftearirnedthtoe asestthoef LC←{Ncu1m,cb2e,rco3f..fceLa←}ture-clusters
cluster of land3mD,aXrk LX. 1The2SLAMNsystem maintains an τfoMr,iF←←,τCG1,Fto,τLMd,oG thresholds
estimated representation of the landmark, the map-point M. cF ← [i]
c.getFeaturesInCluster()
F
forf in do
f.computeFeatureLevelMetrics()
endfor
c.computeClusterLevelMetrics()
endfor
selectedfeatures=[]
←
fori 1toLdo
←C
c [i]
←
kCL,1,kCL,2,kCL,3 c.getCLMetrics()
ifkCL,1>τM,F orkCL,2>τG,F orkCL,3>τM,G then
continue
endif
F ←
c.getFeaturesInCluster()
F
forf in do
←
kFL,1,kFL,2 f.getFLMetrics()
ifkFL,1>τM,F orkFL,2>τG,F then
(cid:88) continue
Fig.4:DescriptionoftheMC-ORschemeinamulti-viewscenario endif
selectedfeatures.append(f)
1 N endfor
G= X (1) endfor
N i
i=1
For outlier rejection, we propose the following Map-point fresh frames, the camera poses are assumed to be initialized
Consensus Error (MCE) metrics: by the SLAM framework, utilizing modules like motion-
74
Authorized licensed use limited to: University of New South Wales. Downloaded on September 19,2020 at 10:33:44 UTC from IEEE Xplore.  Restrictions apply. Pose 
New  Pose  MC-OR Estimation Pose Fidelity
Frame Initialization
AVC
Bad
Correspondence  No Refinement 
refinement with 
Done?
additional cues
Skip Frame
Yes
Fig.5:PlacementofMC-ORandAVCinagenericSLAMsystem.
models,PnPmodels,ICPetc.asviewedinORB-SLAM2and Fig. 6: Adaptive Virtual Camera Setup
DSO. SLAM methods like ORB-SLAM2 conduct multiple
2) Our Proposal: As discussed in Section II (Figure
pose-reﬁnements iteratively, based on certain optimization
3), the static virtual camera has biases for certain error
performance indicators. As indicated in Figure 5, our outlier
orientations. To circumvent this, we propose the following:
rejection method is designed to act before every run of
(i) the position of the virtual camera should be adjusted
the pose optimization block. The outlier rejection process
adaptively per landmark on the camera plane (both x & y
is detailed in Algorithm 1. After computing the FL and
direction)atconstantbaselinefromcamerainsteadofalways
CL metrics, a hierarchical approach is adopted wherein the
positioning to right of the camera, (ii) instead of projecting
aberrant clusters are ﬁrst culled using the CL metrics and
3D geometric error onto virtual camera’s image plane, only
for the remnant clusters, the individual FL observations are
the component along depth axis should be projected (iii)
culled using the FL metrics. Only those features that pass
since virtual camera can be located anywhere on camera
both the tests enter the pose optimization framework.
plane, projection error term should be capturing projection
The running centroid serves as an estimate of the map-
error in bot−h−x−→and y directions. We ﬁrst identify 3D error
point prior to the optimization, thus helping in pre-ﬁltering (cid:48) (cid:2) (cid:3)(cid:91)
component D M along depth axis CM using Equations 8,
observationsbeforetheoptimization.Evenbetweenmultiple
9, as sh−o−w−→n in Figure 6. −−→
runsoftheoptimizationforaframe,MC-ORensuresthecen- (cid:48) (cid:107) (cid:107)(cid:91)
troid to be dynamically updated. Constraining the distance CD = X−D−(cid:48)−→YD(cid:48)−−−Z→D(cid:48) T−−=−→CD CM (8)
(cid:48) − (cid:48)
between M and G ensures the map-point to be close to the −−−→ D M =CM −C−−D→ (9)
(cid:48)
statistical mean of the 3D features with relatively accurate NotethatDD and3DerrorvectorDM willhavethesame
depth. Where M is updated based on cumulative projective projectionontoimageplaneofC.Toﬁnd−−th−→elocationofthe
errors across other features as well, G is affected only by virtualcameraC(cid:48),wetakecomponentofDD(cid:48)paralleltothe
the depth of that particular feature and thus provides a pin- image plane of the camera C as per Equation 10. We place
pointed qualitative assessment of the feature-cluster. thevirtualcameraC(cid:48)atabaselinedistancebfromthecamera
B. Adaptive Virtual Camera C using Equation 11 to maximize the error projection. Here
nˆ representsthenormaltoimageplane.Notethatthelocus
1) Prior Art: Consider the camera located at C(0,0,0), ofcC(cid:48) is a c−−ir−c→le o(cid:2)f radius−b−c−(cid:3)→entred−a−t−→C.
viewing a landmark L , projected onto the image plane as (cid:48) (cid:48)− (cid:48)
X DD =DD (DD .nˆ )nˆ (10)
tThhee23DDfefeaatuturerexo2fDt,hehalvainndgmaardkepisthfodu,nadssbhyobwanckin-pFroigjeucrtein6g. C(cid:48) =protjx,nˆcty 0 T−=−(cid:48)−b→D(cid:91)D(cid:48)procj,nˆcc (11)
x using d as D(X ,Y ,Z ). The estimated location of As per our proposal, the error D M−−i−s→projected onto the
2D D D D (cid:48)
the landmark, namely the map-point is M(X ,Y ,Z ). virtual camera’s image plane. Since D M is collinear with
M M M
Let D and M be projected onto the image plane as C,alltheviewpointsfromwhichtheprojectionofthisvector
(cid:48)
(x ,y ) and (x ,y ) respectively using camera param- would be zero, will be on the line joining C, D and M.
D D M M (cid:48)
eters (f ,f ,c ,c ). In previous works [28], [39], they use Since C is located b distance−−a−w→ay from C, this provides
astaticvxirtuyalxcamyeralocatedatabaselinebalongthex-axis a guarantee that a non-zero (cid:107)D(cid:48)M(cid:107) would always have a
to the right of the original camera, such that it is rec−t−iﬁ−→ed non-zeroprojectionontotheimageplaneofC(cid:48).Additionally,
with respect to theoriginal camera. The error vector DM we also choose an optimal position of C(cid:48) as observed in
is projected onto the image plane of the virtual camera for Equation11.WeconstructtheerrorequationusingEquations
optimization. Theirerror vector is represented by: 7, 9 and 11, including error projections on both x and y
−
xM −xD directionsonthevirtualcamera’simageplane.Weminimize
eORB = − yM −yD − (7) a least-squares error formulation to solve for the camera
(x fxb) (x fxb) poses and map-point locations during the pose estimation
Since depth noise growMs witZhMdepth, nDormaZlDizing the error phase (Figure 5).   
 −   − 
with z helps in preventing disproportionate contributions  x x   x x 
 M − D   M − D 
fromnear/farobservations,hencethe3Derrorsareprojected.
y y y y
− M− D − M− D
The projective errors across all the features are accumulated eAVC = (x fxtx) (x fxtx) = (fxtx fxtx)
and minimized in a least-squares BA formulation [28], [7], M − ZM − M− ZD(cid:48) ZD(cid:48) − ZM
(y fyty) (y fyty) (fyty fyty)
[45].InORB-SLAM2,outliersareﬁlteredpost-optimization M ZM M ZD(cid:48) ZD(cid:48) Z(M12)
and are removed from consequent optimizations.
75
Authorized licensed use limited to: University of New South Wales. Downloaded on September 19,2020 at 10:33:44 UTC from IEEE Xplore.  Restrictions apply. IV. EVALUATION
A. Experimental Setup
Datasets: We evaluate our algorithms on both stereo and
RGB-D data; we utilize the TUM RGB-D dataset [40] and
the EuRoC stereo dataset [2], consisting of 6DoF ground
truthposeandsequenceswithvaryingdifﬁculty.Tohighlight
noise affected sequences and beneﬁts with our methods, we
group the TUM and EuRoC sequences into two categories,
basedonATERMSE(m)scoreswithORB-SLAM2:(i)easy,
having scores <0.10 m, (ii) hard, otherwise.
Implementation Details: While we implement both the
proposed techniques on ORB-SLAM2 [28], a state-of-the-
art SLAM implementation, we ﬁnd both techniques can
generalize well to other SLAM implementations. We com-
Fig.7:Showstheoutliers(inred)rejectedbyourschemeonframes
pare both techniques (individually and cumulatively) with
of the fr3 walking rpy dataset before pose optimization.
baseline ORB-SLAM2, DVO [19], BundleFusion [7] and
EexlahRsibteidictFuuchisingioghnv[i4na9rdi]ae,ttieDornSmsOina[ic1sr3mo]s,:sOWrKueVnsIﬁS[n6[d2].3O]DaRenpBde-nSSdLViAnOgM[12u6p]ot.no # Inliers,Outliers2400000
whether keyframe-BA in the local mapping has ﬁnished or ORBSLAM Inliers ORBSLAM Outliers MC Inliers MC Outliers
1
nkiseosytu,fertsah,meweterapscyoknsiecn,hgrcotahnuriezsaiendaglmliOanydRenBteo-rtSmLoibAntiMasimn2.tthhTeroeamdmosistttiogoacptoetimmtphilzeeestdee Outlier Ratio0.50 Frames ORBSLAM Outlier Ratio MC Outlier Ratio
before processing the next frame. Secondly, the algorithm Fig. 8: Comparison for ORB-SLAM2 with and without MC-OR;
lines indicate the outlier ratio comparison (lesser is better); bar
uses multiple RANSACs within solving PnP models for
charts show certain instances of inlier and outlier counts.
relocalization/pose initialization, causing high variations in
SLAMperformance.Totacklethis,werunmultiple(n=10) featuresthathavehighlateraldepthnoiseandremovingthem
runs of the same conﬁguration to squash variability, and prior to pose estimation; utilizing consensus information
report mean ATE RMSE [40] for pose evaluation. amongst other sightings of the same landmark to cull depth
observations that stray from the rest of the compact cluster.
B. Map-point Consensus based Outlier Rejection Invariance to multiple runs: MC-OR also aids in re-
The thresholds τ , τ and τ were varied ducing the variation of pose-optimization over n = 10 runs
M,F G,F { M,G }
over a discrete parameter space 0.1,0.3,0.5,0.7,0.9 ; upto10x,asshowninFigure9.WeﬁndthatMC-ORreduces
while 0.9 was lenient (allowing noisy points), a thresh- ORB-SLAM2’s dependency on RANSAC while selecting a
old of 0.1 proved to be too strict for highly noisy se- good set of inliers and consistently provides high ﬁdelity
quences like fr3 walking rpy. Through exhaustive search, features, thus reducing the variability.
while different parameter optima were observed across
C. Adaptive Virtual Camera (AVC)
datasets, we observed optimal SLAM performance for
{ } { }
τ ,τ ,τ = 0.7,0.7,0.5 across sequences, and Invariance to parameter b: We expect AVC to be
M,F G,F M,G
used this for generating results. resilient with baseline b, as it truly represents depth noise.
Pose Evaluation on MC-OR: As shown in Table III Thus,wecomparetheperformanceofAVCwithbaseORB-
(Column ‘w/ MC-OR’), MC-OR signiﬁcantly improves over SLAM2 on varying b. In ORB-SLAM2, this parameter is
ORB-SLAM2 for hard and performs on par or marginally taken as approximately 8 cm for the Kinect Sensor. Table I
better for easy sequences. We ﬁnd signiﬁcant improvement shows that AVC is relatively tolerant to the choice of b. We
for hard sequences due to elimination of bad features,
alongwithretaininggoodfeatures,whichaidsinmaintaining
the performance for easy sequences. Figure 7 demonstrates
instances along depth boundaries where MC-OR detects in-
consistenciesbetweentheRGBanddepthimagesonaccount
of lateral noise as described in Section II.
Inlier-Outlier Analysis: Figure 8 compares the inlier-
outlier balance of the BA framework post outlier-rejection.
MC-OR’s outlier ratio is consistently lesser than that of
ORB-SLAM2;whilethenumberofinliersreducesbyasmall
amount or stays the same, the number of outliers reduces Fig. 9: Comparison of ATE variation over several runs of ORB-
considerably. Thus, MC-OR is effective in identifying depth SLAM2 with and without MC-OR on TUM datasets
76
Authorized licensed use limited to: University of New South Wales. Downloaded on September 19,2020 at 10:33:44 UTC from IEEE Xplore.  Restrictions apply. pick b=9 cm for generating all subsequent results. TABLE III: Comparison of ATE RMSE on TUM and EuRoC
datasets;‘-’denotesresultswerenotpublished,‘F’denotesfailure
TABLE I: Comparison of ATE (m) for ORB-SLAM2 with and cases; ‘fr3 w ’ and ‘fr s ’ indicate the freiburg3 walking and
without AVC under variations in baseline, b on fr3 walking rpy. freiburg3 sitting sequences respectively.
TUM ORB- w/ w/ w/MC- DVO BF EF
b(m) 0.01 0.03 0.05 0.07 0.09 0.11 0.13 Sequence SLAM2 MC-OR AVC OR+AVC [19] [7] [49]
w/oAVC 0.60 0.59 0.65 0.58 0.83 0.56 0.72 fr1desk 0.023 0.014 0.015 0.015 0.021 0.016 0.020
w/AVC 0.30 0.42 0.37 0.30 0.27 0.34 0.26 fr1plant 0.018 0.014 0.014 0.013 0.028 - 0.022
fr1rpy 0.022 0.018 0.019 0.018 0.021 - 0.025
fr1xyz 0.010 0.009 0.009 0.010 0.011 - 0.011
Pose Evaluation on AVC: As observed in Table III fr1ﬂoor 0.016 0.013 0.013 0.012 - - F
fr1desk2 0.031 0.023 0.023 0.022 0.046 - 0.048
(Column ‘w/ AVC’), AVC performs signiﬁcantly better than fr1room 0.079 0.044 0.046 0.043 0.053 - 0.068
OfoRrBth-eSrLeAstM.T2hfisoraihdasrdussteoquceonnccelusdaentdhadtoAesVmCairsgainbalellytobtertutleyr Easy fffrrr122tdxeeydszdky 000...000510514 000...000300294 000...000300393 000...000300183 000...000311478 0.0--11 000..00.080439
fr3ssphere 0.023 0.016 0.017 0.016 - - -
represent depth error and eliminate the effect of axial depth fr3srpy 0.020 0.018 0.019 0.017 - - -
noisebetterthanexistingsystems,instrumentalinenhancing fr3sstatic 0.009 0.007 0.008 0.007 - - -
fr3sxyz 0.009 0.008 0.009 0.009 - - -
pose estimation performance. fr3ofﬁce 0.010 0.009 0.009 0.008 0.035 0.022 0.017
Correlation Analysis: Table II demonstrates how closely fr3nst 0.023 0.019 0.019 0.018 0.018 0.012 0.016
fr1360 0.215 0.107 0.164 0.105 0.083 - 0.108
the AVC error term represents depth noise as compared fr2coke 0.415 0.188 0.152 0.106 - - F
d
to that of ORB-SLAM2. eORB,z indicates the projected Har ffrr33wwrsptaytic 00..735637 00..308166 00..206176 00..108158 -- -- --
virtual error component in ORB-SLAM2, while e EuRoCSe- ORB- w/ w/ w/MC- DSO SVO OKVIS
AVC,zx quence SLAM2 MC-OR AVC OR+AVC [13] [16] [23]
and e denote the x-y virtual error components in EuR/MH01 0.037 0.035 0.035 0.035 0.050 0.040 0.330
AVC,zy
eAVC (Equation 12); eBASE,z denotes the z-normalized 3D EEuuRR//MMHH0023 00..004480 00..003399 00..003398 00..003366 00..015800 00..005600 00..327500
eornrotrheeBorAigSiEn,alwchaimleer(a∆’sxi,m∆ayg)eaprleanteh.eO2RDB-pSroLjAecMtiv2e’sesrtraotrics Easy EEEuuuRRR///MVV11H000125 000...000586385 000...000486371 000...000486371 000...000486061 000...111121000 000...100244000 000...301994040
EuR/V103 0.088 0.074 0.072 0.062 0.660 0.070 0.210
TABLE II: Correlation Scores between error terms EuR/V201 0.066 0.062 0.060 0.057 0.040 0.050 0.090
{e ,e ,e } and {e ,(∆x,∆y)} EuR/V202 0.060 0.056 0.056 0.053 0.190 0.090 0.170
ORB,z AVC,zx AVC,zy BASE,z d EuR/MH04 0.124 0.070 0.060 0.045 2.500 0.170 0.270
eORB,z eAVC,zx eAVC,zy Har EuR/V203 0.280 0.270 0.213 0.189 1.160 0.790 0.230
eBASE,z 0.25 0.72 0.76 TABLE IV: Comparison of frames processed per second (fps) for
(∆x,∆y) (0.96,0.03) (0.10,0.12) (0.12,0.12)
single-threaded ORB-SLAM2 on the TUM Dataset; on an Intel
Xeon (R) E5-2667 3.2 GHz machine.
virtualcameraerrordoesnotrepresentthez-normalizederror
well, rather showing a high correlation with ∆x, showing Sequence #Frames ORB-SLAM2 w/MC-OR w/AVC
redundancyinerrorrepresentation.Onthecontrary,theAVC fr1360 744 12.17 15.16 13.70
fr1xyz 792 5.89 11.62 7.76
error terms are well correlated with eBASE,z and relatively fr2coke 2472 5.59 22.00 13.01
independent of (∆x,∆y), thus presenting new information. fr3wrpy 866 2.31 4.24 8.38
fr3wstatic 717 3.06 7.47 4.50
D. Combined AVC & MC-OR
V. CONCLUSIONS
Pose Evaluation: Since our proposals deal with distinct WepresentednovelmethodsMC-ORandAVCtoimprove
types of depth noise, we also conducted experiments with ﬁdelity of pose estimation under noisy depth scenarios. In
both AVC and MC-OR enabled in Table III (Column ‘w/ contrast to previous methods, MC-OR tracks features over
AVC+MC-OR’) to understand if they complement each multiple frames and based on consensus prevents (i) the
other. For hard sequences, our methods provide signiﬁcant removal of good features with depth inaccuracies in initial
improvements individually and cumulatively. For easy se- frames, and (ii) latching onto bad features inconsistent with
quences, these methods maintain or improve SLAM per- most frames, contributing to improved pose estimation. Our
formance incrementally. Figure 1 shows a comparison of consensusbasedapproachisalsoapplicabletoidentifymov-
trajectories, showing that our methods are able to aid ORB- ing objects in dynamic scenes which would otherwise de-
SLAM2 in localizing the camera more accurately. gradeperformanceofSLAMmethods.AVCtargetstocorrect
Time Complexity Analysis: As shown in Table IV, both the axial depth error measurement for optimization methods
MC-ORandAVCimprovecomputationalperformancealong asusedinstate-of-the-artindirecttechniques.Lookingahead,
with overall pose accuracy. Rejecting noisy depth outliers it would be interesting to study the impact of our work
through MC-OR results in heavily reducing the number of on other kinds of dense/fusion SLAM systems; expecting
error terms for the SLAM graph-solver to process. In case a positive impact due to the intuitive and generalizable
of AVC, eventhough the dimensionality of e increases reasoning behind our contributions.
AVC
from 3 to 4, the time proﬁle of BA is known to be highly
REFERENCES
dominated by Cholesky factorization and linear equation
[1] T. Bailey and H. Durrant-Whyte. Simultaneous localization and
solver.Moreover,weobservethatAVCinducestheoptimizer
mapping (slam): Part ii. IEEE Robotics & Automation Magazine,
to converge quicker across frames. 13(3):108–117,2006.
77
Authorized licensed use limited to: University of New South Wales. Downloaded on September 19,2020 at 10:33:44 UTC from IEEE Xplore.  Restrictions apply. [2] M.Burri,J.Nikolic,P.Gohl,T.Schneider,J.Rehder,S.Omari,M.W. navigation. InIntelligentRobotsandSystems(IROS),2013IEEE/RSJ
Achtelik, and R. Siegwart. The euroc micro aerial vehicle datasets. InternationalConferenceon,pages3923–3929.IEEE,2013.
The International Journal of Robotics Research, 35(10):1157–1163, [25] D. Martins, K. van Hecke, and G. de Croon. Fusion of stereo and
2016. stillmonoculardepthestimatesinaself-supervisedlearningcontext.
[3] C.Cadena,L.Carlone,H.Carrillo,Y.Latif,D.Scaramuzza,J.Neira, In2018IEEEInternationalConferenceonRoboticsandAutomation,
I. Reid, and J. J. Leonard. Past, present, and future of simultaneous ICRA 2018, Brisbane, Australia, May 21-25, 2018, pages 849–856,
localization and mapping: Toward the robust-perception age. IEEE 2018.
TransactionsonRobotics,32(6):1309–1332,2016. [26] H.P.Moravec. Sensorfusionincertaintygridsformobilerobots. AI
[4] L. Carlone and G. C. Calaﬁore. Convex relaxations for pose graph magazine,9(2):61,1988.
optimization with outliers. IEEE Robotics and Automation Letters, [27] R.Mur-Artal,J.M.M.Montiel,andJ.D.Tardos.Orb-slam:aversatile
3(2):1160–1167,2018. andaccuratemonocularslamsystem.IEEETransactionsonRobotics,
[5] J.Civera,A.J.Davison,andJ.M.Montiel.Inversedepthparametriza- 31(5):1147–1163,2015.
tion for monocular slam. IEEE transactions on robotics, 24(5):932– [28] R. Mur-Artal and J. D. Tardo´s. Orb-slam2: An open-source slam
945,2008. systemformonocular,stereo,andrgb-dcameras. IEEETransactions
[6] I.Cvisic,J.Cesic,I.Markovic,andI.Petrovic. Soft-slam:Computa- onRobotics,33(5):1255–1262,2017.
tionallyefﬁcientstereovisualslamforautonomousuavs. Journalof [29] R. A. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux, D. Kim,
FieldRobotics,2017. A. J. Davison, P. Kohi, J. Shotton, S. Hodges, and A. Fitzgibbon.
[7] A.Dai,M.Nießner,M.Zollho¨fer,S.Izadi,andC.Theobalt. Bundle- Kinectfusion: Real-time dense surface mapping and tracking. In
fusion: Real-time globally consistent 3d reconstruction using on-the- Mixedandaugmentedreality(ISMAR),201110thIEEEinternational
ﬂy surface reintegration. ACM Transactions on Graphics (TOG), symposiumon,pages127–136.IEEE,2011.
36(4):76a,2017. [30] R.A.Newcombe,S.J.Lovegrove,andA.J.Davison. Dtam:Dense
[8] A. J. Davison, I. D. Reid, N. D. Molton, and O. Stasse. Monoslam: trackingandmappinginreal-time. InComputerVision(ICCV),2011
Real-timesinglecameraslam.IEEETransactionsonPatternAnalysis IEEEInternationalConferenceon,pages2320–2327.IEEE,2011.
&MachineIntelligence,(6):1052–1067,2007. [31] C. V. Nguyen, S. Izadi, and D. Lovell. Modeling kinect sensor
[9] M. G. Dissanayake, P. Newman, S. Clark, H. F. Durrant-Whyte, noise for improved 3d reconstruction and tracking. In 3D Imaging,
and M. Csorba. A solution to the simultaneous localization and Modeling, Processing, Visualization and Transmission (3DIMPVT),
map building (slam) problem. IEEE Transactions on robotics and 2012 Second International Conference on, pages 524–530. IEEE,
automation,17(3):229–241,2001. 2012.
[10] P. Dong, X. Ruan, J. Huang, X. Zhu, and Y. Xiao. A rgb-d slam [32] K.Park,S.Kim,andK.Sohn. High-precisiondepthestimationwith
algorithm combining orb features and bow. In Proceedings of the the3dlidarandstereofusion.In2018IEEEInternationalConference
2nd International Conference on Computer Science and Application on Robotics and Automation, ICRA 2018, Brisbane, Australia, May
Engineering,page118.ACM,2018. 21-25,2018,pages2156–2163,2018.
[11] I. Dryanovski, R. G. Valenti, and J. Xiao. Fast visual odometry and [33] M.PﬁngsthornandA.Birk. Simultaneouslocalizationandmapping
mappingfromrgb-ddata. InRoboticsandAutomation(ICRA),2013 with multimodal probability distributions. The International Journal
IEEEInternationalConferenceon,pages2305–2310.IEEE,2013. ofRoboticsResearch,32(2):143–171,2013.
[12] T. Edeler, K. Ohliger, S. Hussmann, and A. Mertins. Time-of-ﬂight [34] M. Pﬁngsthorn and A. Birk. Generalized graph slam: Solving local
depth image denoising using prior noise information. In Signal andglobalambiguitiesthroughmultimodalandhyperedgeconstraints.
Processing (ICSP), 2010 IEEE 10th International Conference on, TheInternationalJournalofRoboticsResearch,35(6):601–630,2016.
pages119–122.IEEE,2010. [35] C.Pirchheim,D.Schmalstieg,andG.Reitmayr.Handlingpurecamera
[13] J. Engel, V. Koltun, and D. Cremers. Direct sparse odometry. IEEE rotation in keyframe-based slam. In Mixed and Augmented Reality
transactionsonpatternanalysisandmachineintelligence,40(3):611– (ISMAR), 2013 IEEE International Symposium on, pages 229–238.
625,2018. IEEE,2013.
[14] J. Engel, T. Scho¨ps, and D. Cremers. Lsd-slam: Large-scale direct [36] P.F.Proenc¸aandY.Gao.Probabilisticrgb-dodometrybasedonpoints,
monocularslam. InEuropeanConferenceonComputerVision,pages lines and planes under depth uncertainty. Robotics and Autonomous
834–849.Springer,2014. Systems,104:25–39,2018.
[15] K. Essmaeel, L. Gallo, E. Damiani, G. De Pietro, and A. Dipanda`. [37] H. Sarbolandi, D. Leﬂoch, and A. Kolb. Kinect range sensing:
Temporal denoising of kinect depth data. In Signal Image Technol- Structured-light versus time-of-ﬂight kinect. Computer vision and
ogy and Internet Based Systems (SITIS), 2012 Eighth International imageunderstanding,139:1–20,2015.
Conferenceon,pages47–52.IEEE,2012. [38] J.Smisek,M.Jancosek,andT.Pajdla. 3dwithkinect. InConsumer
[16] C.Forster,Z.Zhang,M.Gassner,M.Werlberger,andD.Scaramuzza. depthcamerasforcomputervision,pages3–25.Springer,2013.
Svo: Semidirect visual odometry for monocular and multicamera [39] H. Strasdat, A.J. Davison, J. M. Montiel,and K. Konolige. Double
systems. IEEETransactionsonRobotics,33(2):249–265,2017. window optimisation for constant time visual slam. In Computer
[17] M.HsiaoandM.Kaess.Mh-isam2:Multi-hypothesisisamusingbayes Vision(ICCV),2011IEEEInternationalConferenceon,pages2352–
treeandhypo-tree.In2019InternationalConferenceonRoboticsand 2359.IEEE,2011.
Automation(ICRA),pages1274–1280.IEEE,2019. [40] J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cremers. A
[18] H. Jin, P. Favaro, and S. Soatto. Real-time 3d motion and structure benchmark for the evaluation of rgb-d slam systems. In Intelligent
of point features: a front-end system for vision-based control and RobotsandSystems(IROS),2012IEEE/RSJInternationalConference
interaction. In Computer Vision and Pattern Recognition, 2000. on,pages573–580.IEEE,2012.
Proceedings. IEEE Conference on, volume 2, pages 778–779. IEEE, [41] N.Su¨nderhauf. Robustoptimizationforsimultaneouslocalizationand
2000. mapping. PhDthesis,TechnischenUniversitatChemnitz,2012.
[19] C.Kerletal. Dvo-slam. InIROS,2013. [42] N.Su¨nderhaufandP.Protzel. Switchableconstraintsvs.max-mixture
[20] K.KhoshelhamandS.O.Elberink.Accuracyandresolutionofkinect modelsvs.rrr-acomparisonofthreeapproachestorobustposegraph
depth data for indoor mapping applications. Sensors, 12(2):1437– slam. In 2013 IEEE International Conference on Robotics and
1454,2012. Automation,pages5198–5203.IEEE,2013.
[21] G.KleinandD.Murray. Paralleltrackingandmappingforsmallar [43] C. Sweeney, G. Izatt, and R. Tedrake. A supervised approach to
workspaces.InMixedandAugmentedReality,2007.ISMAR2007.6th predictingnoiseindepthimages.
IEEE and ACM International Symposium on, pages 225–234. IEEE, [44] P.H.TorrandA.Zisserman. Featurebasedmethodsforstructureand
2007. motion estimation. In International workshop on vision algorithms,
[22] G. H. Lee, F. Fraundorfer, and M. Pollefeys. Rs-slam: Ransac pages278–294.Springer,1999.
samplingforvisualfastslam.InIntelligentRobotsandSystems(IROS), [45] R. Wang, M. Schwo¨rer, and D. Cremers. Stereo dso: Large-scale
2011IEEE/RSJInternationalConferenceon,pages1655–1660.IEEE, direct sparse visual odometry with stereo cameras. In International
2011. ConferenceonComputerVision(ICCV),volume42,2017.
[23] S. Leutenegger, P. Furgale, V. Rabaud, M. Chli, K. Konolige, and [46] O.Wasenmu¨ller,M.D.Ansari,andD.Stricker.Dna-slam:Densenoise
R. Siegwart. Keyframe-based visual-inertial slam using nonlinear awareslamfortofrgb-dcameras. InAsianConferenceonComputer
optimization.ProceedingsofRobotisScienceandSystems(RSS)2013, Vision,pages613–629.Springer,2016.
2013. [47] O.Wasenmu¨llerandD.Stricker.Comparisonofkinectv1andv2depth
[24] S. Lynen, M. W. Achtelik, S. Weiss, M. Chli, and R. Siegwart. images in terms of accuracy and precision. In Asian Conference on
A robust and modular multi-sensor fusion approach applied to mav ComputerVision,pages34–45.Springer,2016.
78
Authorized licensed use limited to: University of New South Wales. Downloaded on September 19,2020 at 10:33:44 UTC from IEEE Xplore.  Restrictions apply. [48] T. Whelan, M. Kaess, M. Fallon, H. Johannsson, J. Leonard, and
J.McDonald. Kintinuous:Spatiallyextendedkinectfusion. 2012.
[49] T. Whelan, R. F. Salas-Moreno, B. Glocker, A. J. Davison, and
S. Leutenegger. Elasticfusion: Real-time dense slam and light
source estimation. The International Journal of Robotics Research,
35(14):1697–1716,2016.
79
Authorized licensed use limited to: University of New South Wales. Downloaded on September 19,2020 at 10:33:44 UTC from IEEE Xplore.  Restrictions apply. 