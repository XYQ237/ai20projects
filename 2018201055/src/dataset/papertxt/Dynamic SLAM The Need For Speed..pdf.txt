2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
Real-Time UAV Path Planning for
Autonomous Urban Scene Reconstruction
Qi Kuang1, Jinbo Wu1, Jia Pan2 and Bin Zhou13
Abstract—Unmanned aerial vehicles (UAVs) are frequently algorithms. We estimate the heights of the buildings in the
usedforlarge-scalescenemappingandreconstruction.Howev- scene, based on which we build the coarse scene model.
er, in most cases, drones are operated manually, which should
Then we predict the coverage of the scene and determine
be more effective and intelligent. In this article, we present
the details that need close-ups. After that we are able to get
a method of real-time UAV path planning for autonomous
urbanscenereconstruction.Consideringtheobstaclesandtime the high-resolution images and reconstruct the urban scene
costs, we utilize the top view to generate the initial path. Then ofﬂine.
we estimate the building heights and take close-up pictures The main contributions of this work can be summarized
that reveal building details through a SLAM framework. To
as follows:
predict the coverage of the scene, we propose a novel method
whichcombinesinformationonreconstructedpointcloudsand • areal-timeUAVpathplanningsystem,whichconsiders
possible coverage areas. The experimental results reveal that the factors of time cost, obstacle avoidance, details,
the reconstruction quality of our method is good enough. Our
coverage and enables to plan the path on-the-ﬂy within
method is also more time-saving than the state-of-the-arts.
one navigation pass;
• an efﬁcient path initialization with camera orientation
I. INTRODUCTION
method for building coarse scene model and predicting
On account of the fact that drones can provide more
the coverage based on the top view of the scene;
abundant views, they are the ideal capturing tools for large-
• an effective coverage prediction method which com-
scale urban scene mapping and reconstruction which has
bines information on reconstructed point clouds and
a role to play in urban planning and construction [1], [2],
possible coverage areas.
[3]. In order to get the satisfactory reconstruction result-
s, drones are supposed to capture plenty of images that II. RELATEDWORKS
densely cover the scene. Drones are usually under manual
We divide the discussions of related works into the fol-
control nowadays. However, due to the lack of intuitional
lowing two subsections.
reconstruction results, it often needs more than one ﬂight.
Several existing ground control softwares make drones ﬂy
A. Aerial 3D Scanning and Scene Reconstruction
along zigzag or circular paths [4], which makes drones
capture images automatically. But they need users to deﬁne There have been a large variety of works on exploring
someﬂightparametersinadvance,whichresultsintheﬁxed unknown environment with the development of robotics.
camera orientation and more time cost. Recent studies focus Autoscanning systems are equipped with robots to scan
ontheintelligentﬂightpathplanning[5],[6],[7],whichcan indoor scenes [8], [9], [10], [11]. While exploring large-
getdesiredreconstructionresults,buttheyusuallyneedmore scale outdoor scenes in GPS-denied environments, drones
than one navigation pass. are often used to capture images and videos, and then build
In this paper, we address a real-time UAV path planning the map of the scene and localize itself in the map through
system that makes drones capture images required for urban the Simultaneous Localization and Mapping (SLAM) tech-
scenereconstruction.Meanwhile,oursystemconsidersmany niques. [12]proposedaSLAMsystemusingorbfeaturesfor
factors that are time cost, obstacle avoidance, scene details monocular,stereoandRGB-Dcameras,includingmapreuse,
and coverage. First we extract the layout from the top view loop closing, and relocalization capabilities. [13] proposed
of the scene. Subsequently we calculate the initial path a visual odometry method based on a novel, highly accurate
with camera orientation through traditional path planning sparse and direct structure and motion formulation, which
combines a fully direct probabilistic model (minimizing a
*ThisworkwassupportedinpartbyNationalNaturalScienceFoundation photometric error) with consistent, joint optimization of all
ofChina(U1736217and61932003),NationalKeyR&DProgramofChina
model parameters.
(2019YFF0302902), and Pre-research Project of the Manned Space Flight
(060601). The methods based on SLAM techniques focus on real-
1Qi Kuang, Jinbo Wu and Bin Zhou are with State Key Laboratory time reconstruction, while there are many existing meth-
of Virtual Reality Technology and Systems, School of Computer Sci-
{ ods which obtain high quality 3D reconstructed models
ence and Engineering, Beihang University, Beijing, China kuangqi,
}
wujinbo, zhoubin @buaa.edu.cn ofﬂine. Beneﬁting from the theory of structure from motion
2JiaPaniswiththeDepartmentofComputerScience,theUniversityof (SfM)[14]andmulti-viewstereoreconstruction(MVS)[15],
HongKong,HongKong,Chinajpan@cs.hku.hk
plenty of works extend structure from motion to a variety of
3BinZhouiswithPengChengLaboratory,Shenzhen,China.
BinZhouisthecorrespondingauthor. ﬁelds, such as mapping [16], rescue [17] and medicine [18].
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 1156
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 12:59:33 UTC from IEEE Xplore.  Restrictions apply. (a) (b) (c) (d)
Fig.2. Initialization.Thescenelayoutisshownin(a).WeusetheDijkstra
algorithm to ﬁnd the shortest paths passing all nodes (b). Then the scene
Fig.1. Illustrationoftheproposedframework.SeeSec.IIIformoredetails.
layout can be turned into connected graph (c). Finally we calculate the
shortestpathpassingthroughalledges(d).
Besides, there are also many publicly available algorithm-
s for reconstruction ofﬂine, like MVE [19], SMVS [20] reconstruction result based on the sparse partial information
and COLMAP [21], or commercial softwares, such as of the point cloud. Therefore we estimate the heights of the
Pix4D [22], Agisoft Photoscan [23] and Altizure [24]. buildings in the scene at ﬁrst. Then we are able to build the
coarse scene model with the scene layout and the estimated
B. View Selection and Path Planning
building heights. According to the coarse model and sparse
One of the essential topics for 3D reconstruction is the pointcloudwecangetthecoveragepredictionbycalculating
view selection problem [25], [26], [27]. [28] utilized an the conﬁdence map [31]. Moreover, we also consider the
integer programming method to solve the view planning structural details that need close-up views to make the ﬁnal
problem and proved that it is an NP-Complete problem. 3Dmodelmorecomplete.Allthepathplanningstepsareon
[29] proposed a method which optimizes the next-best-view the ﬂy. Due to the limitation of battery capacity, drone ﬂight
(NBV) for reconstruction based on greedy selection. [30] time is limited. Therefore, in the path planning process, we
proposed to learn a better utility function based on a 3D need to estimate the ﬂight time to determine whether the
convolutional neural network that predicts the usefulness of path can be planned.
future viewpoints.
Afterthat,thedronecancaptureaseriesofhighresolution
Currently, lots of studies on 3D reconstruction in robot images. A detailed and complete model of the urban scene
community focus on view and path planning, which makes can be obtained by some SfM algorithms ofﬂine.
robots explore and reconstruct unknown environments more
efﬁciently. [5] formulated the viewpoint selection as a sub- IV. PATHPLANNING
modular optimization problem and leveraged submodularity
A. Initialization
todevelopacomputationallyefﬁcientmethodforgenerating
scanningtrajectories. [6]introducedamethodthatefﬁciently Weutilizethetopviewofthesceneastheinputandexpect
computes a set of viewpoints and trajectories while limiting toobtaintheinitialﬂighttrajectory.Forthesakeofsafety,the
the total travel distance of the quadcopter for high-quality path above the building is not encouraged, since the height
3D reconstructions in outdoor environments. [7] developed ofthebuildingisunknown.Thustheﬂighttrajectorieswhich
a continuous optimization approach using heuristics and traversethefacadesofbuildingsaresupposedtobegenerated
applied it to the problem of path planning for urban scene by our system, in contrast to traditional aerial path planning
reconstruction. These methods all need coarse meshes as considering forward and side overlap [32].
prior information, thus more than one navigation pass is First, we extract the layout of the scene through a deep
indispensable. [31] presented a fast MVS algorithm which learning method, which is shown in Fig. 2a. We employ
enables online model reconstruction and quality assessment Mask R-CNN [33] to get the result of instance segmenta-
to determine the NBVs on the ﬂy. The UAV also needs an tion. However, we use simple geometric shapes to represent
initial ﬂight. In comparison, our proposed system explores buildingsforsimplifyingtheproblem.Sometimesthemanual
and reconstructs the urban scene within one ﬂight. adjustment for the scene layout is required due to the
obstructions in the scene, like the vegetation, pedestrian and
III. SYSTEMOVERVIEW
vehicle.
In this section, we describe the proposed framework of Thedesiredtrajectoryissupposedtobecontinuoussothat
UAV path planning for urban scene reconstruction as shown a drone can follow it. Thus we need to turn the layout graph
in Figure 1. into a connected graph in the next step. First we consider
We use a quadrotor with a pan-and-tilt camera as our each building as a node, and then connect the nodes with
platform. First we generate the initial ﬂight trajectory based edges. Each edge represents the ﬂight trajectory between
on the top view of the scene. We extract the scene layout buildings. Then we calculate the minimum distance w as
i
and calculate the camera orientation under proper geometric the weight of the edge i to make the time cost minimum.
constraint. Then we utilize the SLAM framework to obtain After the weighted graph is constructed, we can ﬁnd the
sparsepointcloudoftheurbanscene.Duetothelimitationof shortest paths passing all nodes in this graph using classical
computingcapability,weneedtopredictthedensecomplete Dijkstra algorithm as shown in Fig. 2b. Then we restore the
1157
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 12:59:33 UTC from IEEE Xplore.  Restrictions apply. (a) (b)
Fig. 3. Geometric constraint. We ﬁrst assign an initial height to all
buildings, which is shown in (a). (b) denotes the geometric constraint in
theinitializationprocess.
Fig.4. Thepipelineofheightestimationandcoverageprediction.
nodes to the building and get the connected graph as shown
in Fig. 2c. After that the goal is changed to ﬁnd the shortest
all the buildings in the scene based on the scene layout and
path passing through all edges as shown in Fig. 2d, while
make the initial trajectory height adjustable. The pipeline is
some edges may be passed more than once. Thus we can
shown in Fig. 4.
formulate the task as the Chinese postman problem [34].
In order to estimate the building heights online, we
To get the executable ﬂight trajectory, the camera orien-
introduce a real-time SLAM framework to reconstruct the
tation must be taken into consideration. We encourage the
sparse point cloud of the urban scene. Nevertheless, since
camera to be tilted downwards since it can observe top
the monocular camera cannot obtain the depth information,
and side faces of buildings simultaneously. The positional
the reconstructed point cloud has a lot of noise. There are
relationshipbetweenthecameraandthebuildingisasshown
alsoproblemsofscaleuncertaintyandscaledrift.Soweuse
in Fig. 3.
the scene layout that has been extracted before to estimate
We ﬁrst assign an initial height value to all buildings. We
the heights of the buildings rapidly with a uniﬁed scale.
set the horizontal safe distance a and vertical safe distance
First we extract the Scale Invariant Feature Transform
b to avoid collisions between drones and obstacles. The
(SIFT) features from top and side views of the building.
length w and h denote the top face w and side face h
p p Then we are able to match them by FLANN based matcher
projectedontotheimagingplane,whichcouldbeformulized
and calculate the homography transformation to determine
as follows:
the position of the roof in the frame. We only count the 3D
− − −
sin[π (α+β) (π fov)] sinα points that are reconstructed by the roof. The height of the
2 = , (1) (cid:88) (cid:88)
c w building can be estimated as:
p
where fov denotes the angle of ﬁeld of view. α and β can · 1 M − 1 N
berepresentedbya,b,andotherparametersusingthelawof h=s ( max(z ) min(z )). (3)
M m N n
cosines, which is shown in Fig. 3b. The following formula
m=1 n=1
can be obtained in a similar way: Here,sdenotesthescalebetweenthetopviewandtherecon-
− structed 3D points, which can be calculated by dividing the
sin(π fov) sinβ
2 = . (2) distance in the top view by the distance of the reconstructed
c h
p 3D points. M and N denote the number of the points. z is
Inordertogetgoodreconstructionquality,topfaceandside the value of the point on the z axis.
face both should be captured. In other words, the ratio of
C. Coverage Prediction
w to h should be equal to the ratio of w to h. So we
p p
can ﬁnd the relationship between a and b by (1) and (2). While planning the path for reconstruction, one of the
Withtheparametersabove,wecancalculatethepitchofthe most important things is reconstruction quality evaluation.
viewinganglethatiscontrolledbythepan-tilt.Theyawthat Previous researches mostly compute the coverage based on
is controlled by the rotation of the drone can be calculated the reconstructed models or real-time dense reconstruction.
by the initial path so that the camera is always shooting the Hence they need an initial ﬂight to reconstruct a coarse 3D
building. model or the high computational overhead, which makes it
impossible to calculate on the ﬂy. As our task is to generate
B. Height Estimation
the trajectory within one pass, we introduce a lightweight
In the process of initialization we assign a ﬁxed height SLAM framework to generate sparse point cloud, based on
to all buildings. However, in reality, the heights of different which we predict the coverage of the whole urban scene in
buildings, even the disparate parts of the same building vary real time.
from each other. If the distance c from the drone to the First we obtain the coarse model using the extracted
building is large, the length w and h will be very small scene layout and estimated building heights. This process
p p
according to (1) and (2), which will affect the imaging and is like the way of computer-aided design modeling, which
reconstruction quality. Therefore we estimate the heights of designs 2D sketches ﬁrst and then generates 3D models by
1158
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 12:59:33 UTC from IEEE Xplore.  Restrictions apply. (a) (b)
(a) (b)
Fig.5. Weevaluatethecoveragefromtwoaspects,existingcoveredcells
Fig. 6. Path planning strategies used in our system. (a) shows the
(a)andpotentialcoveredcells(b).
paths passing by side faces and top faces of buildings, and (b) shows the
immediatecompletionCi anddelayedcompletionCd.
assigningheights.Duetothereconstructedsparse3Dpoints,
we evaluate the coverage from two aspects, existing and center of these cells. In order to save time, we calculate the
potential covered regions, which is shown in Fig. 5. time cost C of two parts between two points, p and p , and
k l
We divide the current observed surface of the coarse choosetheonewhichtakesshortertimeasshowninFig.6a.
model into several cells. If the number of 3D points n It can be formulized as:
p
in a cell exceeds the threshold n , the cell can be seen
th C(p ,p )=min[C (p ,p ),C (p ,p )], (6)
as existing covered, because it must be observed in several k l s k l t k l
frames according to the reconstruction principle of SLAM. where C denotes the time cost of the path which passes
s
However, because of the reason of the sparse 3D points, by side faces of buildings. The paths are all like that in the
not all the points on the surface are reconstructed. Therefore initialization phase because the heights of the buildings are
we predict the potential coverage using the position of the unknown.C denotesthetimecostofthepathovertopfaces
t
camera which can be obtained when the positions of 3D of buildings as the heights have been already estimated.
points are calculated. When we plan the path for completion, we apply two
Besides, we also expect to provide G(g), a measurement strategies, namely immediate completion C and delayed
i
ofthedistancefromtheobservedsurfacetothecamera.Too completion C , which is shown in Fig. 6b. For immediate
d
long shooting distance will make the content unclear and completion, the current point p and point p need to be
cu a
affect the quality of reconstruction. While if the distance is considered.Sincethepointtobejoinedmustbeintheregion
tooclose,theimagematchingisdifﬁcultandprolongedwith that has been passed through, it is necessary to calculate the
the lack of global information. Therefore the metric can be round trip time. As for delayed completion, we should take
represented by a Gaussian distribution. However, measuring into consideration point p and the closest point p to it in
a cl
thedistancedirectlyisnotappropriate,becausethelensfocal thefollowingpath.ThuswewillﬁndthepathP tominimize
length, camera orientation and other factors will affect the the total time cost T using
result. So we adopt the proportion of squares in the picture { }
argminT =argmin min[2C (p ,p ),C (p ,p )] . (7)
to do that: i cu a d cl a
P P
−
G(g)= √1 exp(−(λSSgf 1)2). (4) D. Detail & Close-Up
2πσ 2σ2 When an urban scene is reconstructed, there are many
Here, σ represents the rate of change of the metric and details that the drone cannot capture during aerial photog-
λ denotes the expected ratio of the square area S to the raphy. As a result, many details, such as water tanks, stairs,
g
screen area S . For each cell g, we deﬁne the conﬁdence sills, are in the reconstruction of large-scale scenes. So we
f
measurement E(g) as: (cid:88) encourage drones to capture more architectural details that
need the close-ups, as time allows. When the 3D points are
− m+∆m · reconstructed through the SLAM method, the points with
E(g)=[I(n n )+ I(t )sin(t )] G(g), (5)
p th i i largergradientshavehighprobabilityofbeingselected.Thus
−
i=m ∆m more dense points mean more details may be included. We
where I denotes the step function, m means the current use a ﬁlter with a radius D to determine the region where
frame,and∆mmeansthenumberofframesassociatedwith the number of points exceeds the threshold N . We will
dense
thecurrentcell.Theﬁrsttermmeasurestheexistingcovered estimate the path optimization time. If time allows, we will
cell. The second term encourages to observe the cell from addaclose-upofthedetails,thatis,reducethedistancefrom
multiple views. ti is the angle between the camera and the the UAV to the building.
plane of the cell.
V. EXPERIMENTS
Oncetheconﬁdencemapiscomputed,westarttoplanthe
path for complement in real time. The cells with very low In this section we verify our system in both synthetic and
conﬁdencevalueswillbetreatedasunobservedcells.Weget real-worldurbanenvironments.Wealsoevaluatetheruntime
the point p that needs to be added to the path based on the and accuracy of our system.
a
1159
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 12:59:33 UTC from IEEE Xplore.  Restrictions apply. (a) (b) (c) (d) (e) (f)
Fig.7. TheMallexample:topview(a),initialpath(b),heightestimation(c),coverageprediction(d),detaildetermination(e)andﬁnalreconstruction
result(f).
Fig. 8. Reconstruction results of the two synthetic scenes. For each scene, the ﬁrst column represents the aerial mapping results, the second column
representstheinitialresults,andthelastoneshowstheoptimizedresults.
A. Synthetic Scene
We evaluate our system on two synthetic urban scenes,
namely Mall and Urban City. We import these scenes into
Unreal Engine 4, a real-time rendering engine, for image
capture.WealsoemployAirsim[35],asimulatorfordrones,
to simulate the ﬂight process and plan the UAV ﬂight path.
AsshowninFig.7,thelayoutofthesceneisextractedby
Mask R-CNN ﬁrst. It is required to know the speciﬁc scale (a) (b)
forﬂightpathplanning.Wesettheheightofallthebuildings
Fig.9. Resultsofthereconstructionerrorwhichvarieswiththenumber
as 30 m and calculate the initial path. We obtain the aerial ofviews(a)andtime(b).
images during the ﬂight via ROS [36]. Then we utilize a
lightweight SLAM method [13] and take the low-resolution
×
images (912 608) as its input. After that we can estimate fullycaptured.Theheightofthedronemustbemuchhigher
the heights of the buildings and build the conﬁdence map than the maximum height of the building in order to avoid
of the scene. Besides we are able to determine the details obstaclesandobtainmorecomprehensivesceneinformation.
by calculating the point cloud density. As we can obtain the Butitalsobringstheproblemoflowreconstructionaccuracy,
×
high-resolution images (5472 3648) during the ﬂight, we which is shown in Tab. I. To measure the error, we sample
reconstruct the scene using COLMAP. some points on the reconstructed model and ﬁnd the closest
To compare the results with the synthetic data, we ﬁrst distance points on the ground truth model. Finally we take
align the reconstructed points to the ground truth model. We the root mean square error as the result.
select several feature points to align them roughly. Then we The scene completeness is calculated according to [31].
use the interactive closest points (ICP) method [37] as a Because of the different scales of the two scenes, we
ﬁnal reﬁnement. Finally we are able to visualize the error calculate the ﬁrst scene completeness with 0.1 m distance
by calculating the distance between the sampling points of thresholdwhilethesecondonewith0.2m.Thecompleteness
reconstructed model and ground truth model. of Urban City is lower because of many complex objects
We compare our method with traditional aerial mapping suchastrees,streetlights,andmore.Theresultsdemonstrate
methodthatisusedastheinitializationinseveralstudies[7]. that our system is more efﬁcient on account of the shorter
The forward and side overlap is set as 80. We also present distance and the smaller error.
the result of the initialization in this paper, namely initial Moreover,weverifytherelationshipamongreconstruction
method. Fig. 8 presents the reconstruction results of the two error, the number of views and time by experiments. Fig. 9
syntheticscenes.Becausethecameraorientationisﬁxedwith shows that the reconstruction error decreases as the view
theaerialmappingmethod,thesideofthebuildingcannotbe and time increase. The optimized reconstruction error is the
1160
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 12:59:33 UTC from IEEE Xplore.  Restrictions apply. TABLEI TABLEII
RESULTSOFTHETWOSYNTHETICSCENES. RESULTSOFTHEREAL-WORLDSCENESWITHAREASIZE,MAXIMUM
ALTITUDE,NUMBEROFVIEWS,FLIGHTDISTANCEANDTOTALTIME.
Mall UrbanCity
Method Dist. Error Comp. Dist. Error Comp.
Downtown
(m) (cm) (%) (m) (cm) (%) Method Area MaxAlt View Dist Time
AerialMapping 1559 5.57 60.29 3494 15.17 34.39
(m2) (m) (m) (min)
Initial 772 5.32 64.17 1277 14.78 41.20
AerialMapping 55,118 25 406 3042 71
Optimized 1011 5.11 66.78 1445 13.98 43.02
Initial 34,402 20 351 2026 48
Optimized 34,402 20 378 2254 55
ConstructionSite
AerialMapping 7,752 20 144 883 22
Initial 4,128 17 121 672 17
Optimized 4,128 17 132 704 19
TABLEIII
COMPARISONOFTIMECOSTANDRECONSTRUCTIONERROR.
Mall
Method 1st 2nd Coarse Match Dense Total Error
(min) (min) (min) (min) (min) (min) (cm)
AerialMapping – 36.60 – 1.67 111.03 161.48 5.57
Smith[7] 20.38 22.56 65.54 16.90 130.68 302.65 5.10
Ours – 23.73 – 1.44 145.4 185.77 5.11
Fig.10. Reconstructionresultsofreal-worldscenes,includingDowntown
andConstructionSite.
synthetic scene Mall as an example. We compare the time
cost and the reconstruction error of the three methods in
Tab. III. Fig. 11 shows the planned path of these methods.
Asoursystemenablesthedronetoﬂywithinonenavigation
pass, the ﬂight time of the drone and the time of building
the coarse model are much less.
(a) (b) (c)
Besides, unlike previous view planning methods which
Fig. 11. The ﬂight path with camera orientation of the three methods: generate the path connecting optimal views, our drone path
aerialmapping(a),Smith(b)andours(c).
is more in line with the way to explore the unknown
environment. Our method is more time-saving in the fea-
ture matching process, because it only needs to match the
sameastheinitialerrorﬁrst,becausethereisnooptimization
adjacent frames instead of exhausted matching. However,
due to the time constraints. However, as the view and time
sincethedronelacksofpriorinformationofthecoarsescene
increase, the optimized reconstruction error can be smaller.
model during ﬂight, more views are required, which results
B. Real-World Scene in the longer time at the dense reconstruction stage.
We further verify our path planning system in two real- The results show that our method can achieve the same
world urban scenes. We select one large-scale Downtown reconstructionaccuracyasthestate-of-the-art.Sinceweonly
withvariousbuildingsandonesmall-scaleConstructionSite. need one ﬂight, the total duration can be greatly reduced.
We set the initial height as 25 m for downtown and 20 m
VI. CONCLUSIONSANDFUTUREWORK
for the last one. The UAV ﬂight speed is 1 m/s. However,
In this study, we present a real-time UAV path plan-
it takes longer when drones actually ﬂy. Because the drone
ning system for urban scene reconstruction. Technically,
needs to slow down when turning and taking photos, for
we propose a coverage prediction method combining the
better shooting results. Since many commercial drones can
information about reconstructed point clouds and potential
record GPS information, we use Altizure, a software which
coverage areas. The experimental results have demonstrated
can make use of GPS, to complete the ﬁnal reconstruction.
thatthereconstructionqualityofourmethodisgoodenough
Fig. 10 shows the ﬁnal reconstruction results of the two
in both synthetic scenes and real-world scenes. Our method
scenes. Tab. II presents the details of these scenes. The
is more time-saving than the state-of-the-arts. Although the
aerial mapping method requires the drone to ﬂy the farthest
proposed system is effective, it has a few limitations. Some
and takes the longest time. Our system proves to be more
lower parts of the buildings cannot be well reconstructed. It
effective since we need fewer views and less time.
mightbeimprovedbyaddinggroundrobots.Somereﬂective
C. Comparison of Efﬁciency materials and shadows also affect reconstruction results, so
We next compare our system with previous studies about reconstruction with different materials might be considered
UAV path planning for reconstruction. Here we take the in the future.
1161
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 12:59:33 UTC from IEEE Xplore.  Restrictions apply. REFERENCES Processing,vol.9784. InternationalSocietyforOpticsandPhotonics,
2016,p.97840V.
[1] J. Everaerts et al., “The use of unmanned aerial vehicles (uavs) [19] S. Fuhrmann, F. Langguth, N. Moehrle, M. Waechter, and M. Goe-
for remote sensing and mapping,” The International Archives of the sele,“Mveanimage-basedreconstructionenvironment,”Computers&
Photogrammetry, Remote Sensing and Spatial Information Sciences, Graphics,vol.53,pp.44–53,2015.
vol.37,no.2008,pp.1187–1192,2008. [20] F. Langguth, K. Sunkavalli, S. Hadap, and M. Goesele, “Shading-
[2] O.EsraﬁlianandD.Gesbert,“3dcitymapreconstructionfromuav- aware multi-view stereo,” in European Conference on Computer
basedradiomeasurements,”inGLOBECOM2017-2017IEEEGlobal Vision. Springer,2016,pp.469–485.
CommunicationsConference. IEEE,2017,pp.1–6. [21] J.L.SchonbergerandJ.-M.Frahm,“Structure-from-motionrevisited,”
in Proceedings of the IEEE Conference on Computer Vision and
[3] M. Faessler, F. Fontana, C. Forster, E. Mueggler, M. Pizzoli, and
PatternRecognition,2016,pp.4104–4113.
D. Scaramuzza, “Autonomous, vision-based ﬂight and live dense 3d
[22] C.Strecha,W.VonHansen,L.VanGool,P.Fua,andU.Thoennessen,
mapping with a quadrotor micro aerial vehicle,” Journal of Field
“Onbenchmarkingcameracalibrationandmulti-viewstereoforhigh
Robotics,vol.33,no.4,pp.431–450,2016.
resolution imagery,” in 2008 IEEE Conference on Computer Vision
[4] S.SiebertandJ.Teizer,“Mobile3dmappingforsurveyingearthwork
andPatternRecognition. Ieee,2008,pp.1–8.
projectsusinganunmannedaerialvehicle(uav)system,”Automation
[23] L. Agisoft and R. St Petersburg, “Agisoft photoscan,” Professional
inconstruction,vol.41,pp.1–14,2014.
Edition,vol.7,2014.
[5] M.Roberts,D.Dey,A.Truong,S.Sinha,S.Shah,A.Kapoor,P.Han-
[24] Altizure.[Online].Available:https://www.altizure.com
rahan,andN.Joshi,“Submodulartrajectoryoptimizationforaerial3d
[25] V. Sequeira, J. M. Goncalves, and M. I. Ribeiro, “Active view
scanning,” in Proceedings of the IEEE International Conference on
selectionforefﬁcient3dscenereconstruction,”inProceedingsof13th
ComputerVision,2017,pp.5324–5333.
InternationalConferenceonPatternRecognition,vol.1. IEEE,1996,
[6] B. Hepp, M. Nießner, and O. Hilliges, “Plan3d: Viewpoint and
pp.815–819.
trajectory optimization for aerial multi-view stereo reconstruction,”
[26] S.Shen,“Accuratemultipleview3dreconstructionusingpatch-based
ACMTransactionsonGraphics(TOG),vol.38,no.1,p.4,2018.
stereoforlarge-scalescenes,”IEEEtransactionsonimageprocessing,
[7] N. Smith, N. Moehrle, M. Goesele, and W. Heidrich, “Aerial path
vol.22,no.5,pp.1901–1914,2013.
planning for urban scene reconstruction: A continuous optimization
[27] J.L.Scho¨nberger,E.Zheng,J.-M.Frahm,andM.Pollefeys,“Pixel-
methodandbenchmark,”inSIGGRAPHAsia2018TechnicalPapers.
wiseviewselectionforunstructuredmulti-viewstereo,”inEuropean
ACM,2018,p.183.
ConferenceonComputerVision. Springer,2016,pp.501–518.
[8] M.Krainin,B.Curless,andD.Fox,“Autonomousgenerationofcom-
[28] W.R.Scott,G.Roth,andJ.-F.Rivest,“Viewplanningforautomated
plete3dobjectmodelsusingnextbestviewmanipulationplanning,”
three-dimensional object reconstruction and inspection,” ACM Com-
in2011IEEEInternationalConferenceonRoboticsandAutomation.
putingSurveys(CSUR),vol.35,no.1,pp.64–96,2003.
IEEE,2011,pp.5031–5037.
[29] J. I. Va´squez and L. E. Sucar, “Next-best-view planning for 3d ob-
[9] B. Charrow, G. Kahn, S. Patil, S. Liu, K. Goldberg, P. Abbeel, jectreconstructionunderpositioningerror,”inMexicanInternational
N. Michael, and V. Kumar, “Information-theoretic planning with ConferenceonArtiﬁcialIntelligence. Springer,2011,pp.429–442.
trajectory optimization for dense 3d mapping,” in Proceedings of [30] B.Hepp,D.Dey,S.N.Sinha,A.Kapoor,N.Joshi,andO.Hilliges,
Robotics:ScienceandSystems,vol.11. Rome,2015. “Learn-to-score: Efﬁcient 3d scene exploration by predicting view
[10] L.Liu,X.Xia,H.Sun,Q.Shen,J.Xu,B.Chen,H.Huang,andK.Xu, utility,” in Proceedings of the European Conference on Computer
“Object-aware guidance for autonomous scene reconstruction,” ACM Vision(ECCV),2018,pp.437–452.
TransactionsonGraphics(TOG),vol.37,no.4,p.104,2018. [31] R. Huang, D. Zou, R. Vaughan, and P. Tan, “Active image-based
[11] S. Dong, K. Xu, Q. Zhou, A. Tagliasacchi, S. Xin, M. Nießner, and modeling with a toy drone,” in 2018 IEEE International Conference
B.Chen,“Multi-robotcollaborativedensescenereconstruction,”ACM onRoboticsandAutomation(ICRA). IEEE,2018,pp.1–8.
TransactionsonGraphics(TOG),vol.38,no.4,p.84,2019. [32] J. Ferna´ndez-Hernandez, D. Gonza´lez-Aguilera, P. Rodr´ıguez-
[12] R. Mur-Artal and J. D. Tardo´s, “Orb-slam2: An open-source slam Gonza´lvez, and J. Mancera-Taboada, “Image-based modelling from
systemformonocular,stereo,andrgb-dcameras,”IEEETransactions unmannedaerialvehicle(uav)photogrammetry:aneffective,low-cost
onRobotics,vol.33,no.5,pp.1255–1262,2017. toolforarchaeologicalapplications,”Archaeometry,vol.57,no.1,pp.
[13] J.Engel,V.Koltun,andD.Cremers,“Directsparseodometry,”IEEE 128–145,2015.
transactions on pattern analysis and machine intelligence, vol. 40, [33] K. He, G. Gkioxari, P. Dolla´r, and R. Girshick, “Mask r-cnn,” in
no.3,pp.611–625,2017. ProceedingsoftheIEEEinternationalconferenceoncomputervision,
[14] S.Ullman,“Theinterpretationofstructurefrommotion,”Proceedings 2017,pp.2961–2969.
oftheRoyalSocietyofLondon.SeriesB.BiologicalSciences,vol.203, [34] H. A. Eiselt, M. Gendreau, and G. Laporte, “Arc routing problems,
no.1153,pp.405–426,1979. parti:Thechinesepostmanproblem,”OperationsResearch,vol.43,
[15] R. Hartley and A. Zisserman, Multiple view geometry in computer no.2,pp.231–242,1995.
vision. Cambridgeuniversitypress,2003. [35] S. Shah, D. Dey, C. Lovett, and A. Kapoor, “Airsim: High-
[16] S. K. Nouwakpo, M. A. Weltz, and K. McGwire, “Assessing the ﬁdelity visual and physical simulation for autonomous vehicles,”
performanceofstructure-from-motionphotogrammetryandterrestrial in Field and Service Robotics, 2017. [Online]. Available:
lidar for reconstructing soil surface microtopography of naturally https://arxiv.org/abs/1705.05065
vegetated plots,” Earth Surface Processes and Landforms, vol. 41, [36] M. Quigley, K. Conley, B. Gerkey, J. Faust, T. Foote, J. Leibs,
no.3,pp.308–322,2016. R. Wheeler, and A. Y. Ng, “Ros: an open-source robot operating
[17] A. Martell, H. A. Lauterbach, A. Nuchtcer, et al., “Benchmarking system,”inICRAworkshoponopensourcesoftware,vol.3,no.3.2.
structurefrommotionalgorithmsofurbanenvironmentswithapplica- Kobe,Japan,2009,p.5.
tionstoreconnaissanceinsearchandrescuescenarios,”in2018IEEE [37] P.J.BeslandN.D.McKay,“Methodforregistrationof3-dshapes,”
International Symposium on Safety, Security, and Rescue Robotics inSensorfusionIV:controlparadigmsanddatastructures,vol.1611.
(SSRR). IEEE,2018,pp.1–7. InternationalSocietyforOpticsandPhotonics,1992,pp.586–606.
[18] S. Leonard, A. Reiter, A. Sinha, M. Ishii, R. H. Taylor, and
G.D.Hager,“Image-basednavigationforfunctionalendoscopicsinus
surgeryusingstructurefrommotion,”inMedicalImaging2016:Image
1162
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 12:59:33 UTC from IEEE Xplore.  Restrictions apply. 