2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
Stereo Visual Inertial Odometry with Online Baseline Calibration 
Yunfei Fan, Ruofu Wang, and Yinian Mao 
Abstract— Stereo-vision devices have rigorous requirements  method is to formulate stereo camera extrinsic parameters 
for extrinsic parameter calibration. In Stereo Visual Inertial  (rotation and translation) into the set of state variables and 
Odometry (VIO), inaccuracy in or changes to camera extrinsic  model the relevance between feature reprojection error and 
parameters may lead to serious degradation in estimation  stereo extrinsic parameters in update Jacobian, so that the 
performance. In this manuscript, we propose an online cali- stereo extrinsic parameters can be calibrated online as part of 
bration method for stereo VIO extrinsic parameters correction.  the state estimation. To accelerate the self-calibration pro-
In particular, we focus on Multi-State Constraint Kalman 
cess, the initial covariance of stereo extrinsic parameters is 
Filter (MSCKF [1]) framework to implement our method. The 
set to a large value. In addition, during the initial phase of 
key component is to formulate stereo extrinsic parameters as 
the estimation, the threshold of the outlier rejection rule 
part of the state variables and model the Jacobian of feature 
based  on  stereo  extrinsic  constraint  on  the  algorithm 
reprojection error with respect to stereo extrinsic parameters 
frontend is relaxed to avoid too many inliers being mistak-
as sub-block of update Jacobian. Therefore we can estimate 
enly taken out. 
stereo  extrinsic  parameters  simultaneously  with  inertial 
measurement unit (IMU) states and camera poses. Experi- Using EuRoC dataset and real-world outdoor dataset, we 
ments  on  EuRoC  dataset  and  real-world  outdoor  dataset  compare the proposed scheme with other state-of-the-art 
demonstrate that the proposed algorithm produce higher posi-
stereo VIO algorithm, specifically S-MSCKF. The experi-
tioning accuracy than the original S-MSCKF [2], and the noise 
ments show that, without calibration errors, the proposed 
of camera extrinsic parameters are self-corrected within the 
method  performs  similarly  to  S-MSCKF.  Besides,  when 
system. 
artificial noises are involved in the calibrated parameters, the 
I.  INTRODUCTION  proposed scheme can achieve rapid self-calibration and out-
performs S-MSCKF in position estimation.  
 In recent years, high-precision positioning technologies 
have progressed significantly, propelling the advancements  The rest of this paper is organized as follows: Section II 
in multiple application scenarios such as autonomous driv- introduces  related  works.  Section  III  introduces  system 
ing, robotics and unmanned aerial vehicles (UAVs), and  framework and derives analytical formulations. Section IV 
augmented and virtual reality (AR and VR). In outdoor en- compares experimental results of the proposed scheme with 
vironments, GNSS such as GPS and RTK can be employed.  those of VINS-Fusion [3,4] and S-MSCKF using EuRoC 
In indoor and GPS-denied environments, Lidar and visual  dataset and real-world outdoor datasets collected by UAVs 
SLAM can be used. For applications that are limited by de- as well as by a handheld device. Finally, the conclusions are 
vice size and weight requirements, the applicable positioning  summarized in section V. 
technology is rather limited in the absence of GPS. Since 
II.  RELATED WORK 
VIO only requires IMU and one or two camera modules to 
estimate ego-motion, it is naturally suitable for such scenar- The current scholarly works in VIO could be roughly di-
ios. It has been reported that stereo-vision VIO system can  vided into loosely-coupled [5,6] and tightly-coupled [1- 4,7] 
improve the overall estimation accuracy over single-vision  methods.  Tightly-coupled  methods  put  IMU  information 
VIO system (S-MSCKF [2], VINS-Fusion [3,4]). A good  into state variables and optimize with vision information 
stereo calibration ensures the epipolar lines of stereo images  simultaneously, which is a mainstream direction currently. 
being  parallel,  which  is  the  foundation  for  most  stereo  Tightly-coupled methods can be divided further into fil-
matching algorithms. However, in stereo VIO systems, the  ter-based and optimization-based. 
estimation accuracy heavily depends on camera extrinsic 
VIO methods based on non-linear optimization utilize all 
parameters  calibration.  With  a  poor  calibration  or  slight 
measurements,  including  IMU  measurements  and  visual 
changes in camera parameters during operation, stereo VIO 
measurements, to find the optimal state variables to mini-
positioning accuracy will drop sharply. Even with rigid and 
mize the measurement error. Stereo VIO based on non-linear 
bulky frames, most stereo cameras cannot ensure that extrin-
optimization includes OKVIS [7], VINS-Fusion [3,4], etc. 
sic parameters are unchanged during long course of opera-
Both OKVIS and VINS-Fusion perform online estimation of 
tions. Within this context, an accurate calibration algorithm  
extrinsic parameters between the IMU and each camera, 
that is robust to changes in camera extrinsic parameters is 
separately. However, due to the large number of state varia-
highly desired. 
bles, even the current mainstream sliding window based 
In this paper, we propose a stereo VIO algorithm with  VIO methods using non-linear optimization have a consid-
online calibration to overcome the above issues. The core  erable demand for computational resource, and it is still dif-
ficult to run in real time on embedded platforms. 
 
This work was supported by the Meituan-Dianping Group.  Filter based VIO methods are mainly based on Extended 
Yunfei Fan and Yinian Mao are with the Meituan-Dianping Group,  Kalman Filter (EKF) [1]. Generally, IMU is used for predic-
Beijing, China (e-mail: {fanyunfei | maoyinian}@meituan.com).  tion,  while  visual  information  is  used  for  update.  They 
Ruofu Wang is with University of Southern California, Los Angeles, 
achieve almost the same level of accuracy as optimiza-
CA 90007 USA (e-mail: ruofuwan@usc.edu) 
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 1084
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 11:45:09 UTC from IEEE Xplore.  Restrictions apply. XB =5 pC 	 vC 	 θE 	bE 	bE 	 θE 	 pC 	 θE 	 pC 9
’ , ’ , " , , , 67 , " , 68 , 67 & 1
" " " ’ . / " 67 67 68
tion-based methods using relatively low computational re-
 
sources. Thus they can run in real time on embedded plat- x=xG+xC
forms.  S-MSCKF  [2]  is  one  of  filter-based  stereo  VIO 
(3) 
frameworks, which only estimates the extrinsic parameters  {I}
between IMU and left camera online. In order to achieve  Except for quaternions, other states can be used with 
real-time performance, our method is also based on MSCKF  standard addq+iti=veδ erq+ro⨂r (q+Ke.g. δ q+ =51	 ). θtEh9e extended addi-
framework.  tive error of" quatern"ion "is defin"ed in [140]" (in, this paper, qua-
ternion erro’r is def’ined ’in fram’e   , sLee’ details in [11]) 
P Hansen et al. [8] and Yonggen Ling et al. [9] proposed 
approaches to estimate stereo extrinsic parameters online. 
They are all based on epipolar geometric constraints for  RN q+O= R R,  =(1−$ θE0 ) RS                    (4) 
online self-calibration of stereo extrinsic. However, because  similarly, the" extend"ed a"dditive erro"r of ro"tation matrix is 
pure vision-based methods cannot self-calibrate the baseline  defined:  ’ ’ ’ ’ × ’
fully, they can only achieve estimation of stereo extrinsic 
parameters with 5-DOF, while the length of the baseline  ,                     (5) 
cannot be estimated. Therefore, we use the IMU and the 
B.  State Propagation 
cameras jointly, to self-calibrate the 6-DOF stereo extrinsic 
parameters online.  Similar to EKF state propagation, MSCKF framework 
uses IMU data to propagate states. The difference is state 
III.  MSCKF ALGORITHM FRAMEWORK  augmentation at theq+ ̇m(to)m=en1t ΩoNf ωne(wt) Oimq+a(gte) arrival. As can be 
seen from [1], The time 2evolution of IMU states are de-
A.  State defiXniti=on$  p 	 v 	 q+ 	b 	b 0 " "
Following the de’fin&iti’on &of "M,SCK,F in, [11], IMU state is  scribed below:  ’ ḃ (t)=n (t)’
defined below:"  " " ’ . /
E E /v̇ (t)= Za/(t)  
’ ’
3     4                     (1)  ḃ ("t)=n 	(t)
 
In this paper, different from [1, 2], both extrinsic param- 	 p.̇ (t)= Zv.(t
eXter=s   and   of stereo VIO system shown in Fig. 1, are  ’ ’  
added into IMU states and calibrated online. The extended  a " " {G}
I	M	"			U5 sptate	s vare 	deq+fin	bed: 	b  	 q+ 	 p 	 q+ 	 p 91 ω=$ω’		ω 		ω 0  
’  "& ’ "& ’" , ., 	{/G,}67" , "{I}67& 6678 , 67 68& \ ] ^{,I} )                                (6) 
{C } {C } C where   represents the body accele0ration− ωin fraωme  . 
C p 3 v 4     (23)  −[ω] ω
Ω(ω)=_  reprce,se[nωts] an=guelaωr veloci0ty of −IMωUf ex-
4 ’ {G} ’ 4×1	 q+ −ω 0 ^ ]
In these expressions, "  and  "  are the global and in- pressed in frame  × . And:  −ω ω 0
{G} {I} " , × ^ \
ertial frame respectively,  b and  b are fram’e of    ω a ] \
and   respectively.   and   are position and veloc-
 (7) 
ity of IMU expressed in  , res/pectively..   repre- g g
q+ {I}
sents the rotation from   to   (in this paper, quaternion 
obe{yCs J}PL rulesp). The 6v7"ectors   and  C are the biases of   and   aωre  t=he ωg+yrbosc+opne  and  accelerometer 
t{hI}e m3eqa+sured a"npg6u7lar velocity and linear 3acceleration from  measuremenats s=epaRrNategq+lyON. Igan−ored/g tOh+e e/bffe+ctsn of the planet’s 
the I6M7"U, separ"at6eE7ly.   represents the rotatiq+on from    rotation, they are given" by ’[1]:  ’
to  , and   3is th{eC p}osition of  {C b}a66s78ed on pframe  {G} ’g g ’  . .
 (  and  C  are the3 rotation a{nCd }tr4ansq+lation6 7of pex-
68                  (8) 
trinsic parameter 4  respectively). Fina3lly, 68  repre6s7ents 
tEhe rotation from frame   to frame  6, 7and   6iq+8s  where   is graq+Kv̇i=tatioΩn(aωil )acqc+Ke,lebKṙat=ion0, ex,pressed in frame 
th4e pospition of   based on frame   (  and  6678   of I.M AUp psltyatiensg  cEa’"qn.  b	(e6 vG)o4L̇ bin=ta EiRnqeN.’d "(:q+8K O), caG/o+ntingujo×u4s dynamic model 
are  rthe6se7p re6oc8ttaivtieolny .a Wnde  ttrraenastl asttXieorneo o ef xsttreirnesoic e pxatrrianmsiect eprasr aasm eter   bK̇ (t)’="0 ’" ,pĠ =’ vG   
" ’ ’
and   later).  aG =.a −bKj×,4ωi =ω "−bK "
  
The EKF error-state of   is defined accordingly: 
g . g /
BẊ =F,XB    +   Gn                        (9) 
moreover,  ,  continuous  dy-
" " "
namic model of IMU error-state is defined by: 
 
Figure 1.  Structure  diagram  of  sensor,  and  definition  of  extrinsic                                  (10) 
parameters 
1085
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 11:45:09 UTC from IEEE Xplore.  Restrictions apply. n =$		n 		n 		n 		n 0
XB =_ θE 	 pC c ,j=(0,…,N−1)
" /, l,/ ., l,. , 6(cid:128)7 , ’ , , C
67 ’ 67
(cid:128) (cid:128) 3
where  0 I 0  is 0the  sys0tem  n0oise.  It 
⎡ ⎤ where,  P P  represents 
dtreicFpee=sn dF⎢⎢⎢s a 00nojjdn××  jjGth	 e00th jjIa××Mtjj aU	p−p nReoaN−ri’s "[iqje+Kωni× O c]jE,h[qaaG.r ](a×1c	0−te)rR 0aisNrjte’×i" cq+KjgsOi.v ,Fe	−ni0n Ijjba××ylljj:y 	,  00thjj××e qqm⎥⎥⎥	a- tchoev aerriraonrc oef P ijst hd ae=ufigJnPmedeP nxb(cid:133),|tyxeP:d=   caz=mPxJe|xPra  JPL4. ,M{oreover, augmented 
⎣0 0 0 0 0 0 ⎦ L4 LL
j×j j×j × j×j j×j j×q ,
							4	p×	j			4	p×	j						4	p	×	j							4	p	×	j				4p	×	j			4	p×	q 	 XBL4 x|x LL x|x              (18) 
⎡0 0 0 I ⎤ Note that  67 J= are the augmented co-
(cid:128)
⎢0j×j 0j×j −RNj×q+KjO 0j×j ⎥ varia0nce w0ith respect toR Sjth augm0entedI state0, and J0 is the Ja-
G=⎢⎢−0Ij×j 		00j×j		 00 ’" ,		00j×j⎥⎥									(11)	 cob(cid:135)i		aInj ×ojf 		0j× jw		−ithRS re6s$’7pepGct t0o t	h	0ej e×r(cid:136)r	o	0rj-s×tjat		e vjRS×ecjt		o0rj. ×((cid:136)(cid:137)|(cid:136))(cid:138)
⎣⎢⎢00jj××jj 0Ijj××jj 00jj××jj 00jj××jj⎦⎥⎥ j×j j×j ’" , " 67 ×   j×(cid:136) j×j ’" , j×((cid:136)(cid:137)|(cid:136))
j×j j×j j×j j×j           
(12)
4L×j 4L×j 4L×j 4L×j
(19) 
E
Φ=I+Fδt																		 	 C. State Update 
Following Euler integration [4] of Eq. (10), discrete-time  (cid:139) q+S	i	m4pilar (cid:141)to [2], (cid:139)we q+c	a	n pfor(cid:141)mulate the reprojection of fea-
system matrix	 i	s Pgi=veΦn PbyΦ:  +(ΦG)Q(ΦG) δt	 ttue6rr’(cid:140)7se s f’rom 6e(cid:140)7 mstpelroeyoe. dD 6’i(cid:140)8fifne r’etnht6i s(cid:140)8f ropmap e[2r ], itsh ec eaxlitCbrirnasteicd  paornalminee-. 
(13)
, , 4
 and   are ith left and right camera 
Moreover, the propagation of covariance is given by:  q+ = q+⨂ q+ p = p +RN q+O ∙ p
P P pose at the same time instance respectively. Employing the 
,
P =z {													 	 s6t8ereo e6x8trins6i7c, the ’pose of t’he right zGca6m7era  67 can be eas-
P""x|x P"6x|x il’y der6iv7ed in’ terms 6o8f the l6e7ft cam(cid:144)e’ra augme6n8ted( e.g. 
In this paper, covariance structure is defined as: 
x|x , (cid:143)
"6x|x 66x|x ,  ).  The 
uG XS
(14) reprojection of ste(cid:144)reo measuremen0t,   i⎛n 6it7h po⎞se is defined 
as:   ⎛vG(cid:143),3⎞ ⎛ 4 ⎞⎜ (cid:140)YS(cid:144)⎟
cchoavnaSgriineac netcPh eet  phpreooP xspce|au =4gro|arxfetΦ i=nosPtln iz dmsPit"nPa6e"gtΦxt"eh x| x,|wo,o4d+iΦ|fn:x  d,(IoΦMwGΦU,P )6 wQPP6"p(e6xΦr x|oxc|xGpa{an)	g, 	afδ	otti	rom	n	u 	lda	ot	ee stnPh’et    zG(cid:143)(cid:144)_=SX⎝⎜⎜u	vGG(cid:143)(cid:143)(cid:143)(cid:144)(cid:144)(cid:144),,,344XS⎠⎟⎟	=XS⎝⎜c0(cid:153)L(cid:140)7×(cid:152)S(cid:128)L (cid:153)L(cid:140)84×(cid:152)S(cid:128)L⎠⎟⎝⎜⎜666(cid:140)8(cid:140)7(cid:140)8XSYS(cid:144)(cid:144)(cid:144)⎠⎟⎟      (20) 
""x|4|x ""x|x (P15")"  C 6(cid:140)(cid:155) (cid:144) 6(cid:140)(cid:155) (cid:144) 6(cid:140)(cid:155) (cid:144)
"6
x
where,  ,  and  66 
Note that   is the coordinate of jth feature in 
represents covariance of IMU states.   represents covari-
ance of IMU states with respect to pose of cameras.    frame { } in ith camerra p=osze of− slzGiding window (k=0,1 rep-
represents covariance of pose of augmented cameras.  resents left and right (cid:144),xcamer(cid:144),ax resp(cid:144),exctively). Measurement 
(cid:143) (cid:143) (cid:143)
residual is defined as:  
When  a  new  image  arrives,  current  state  of  system 
shouldXS be= a5uXSgm	e	XSnted 		(XSin th		iSXs pap	e··r·,	 SXwe a9ugment the left                 (21) 
camera state si,milarl,y to [,2]). In,cluding a,ug,mented states, 
We can formulate least-squares system to optimize the coor-
the extexndXSed s=t"at5e6s7 q6+Ka77r,e	 ’dpGe6fi87n,e9d, a6,Cs}7j:=  (0,16,…(cid:127)7 ,N) derirnoart eosf o jfrt h(cid:144)f ,exfae=tauturzer(cid:144)se,x.  oS−beszeGe (cid:144)d,rxve≈atatiiHolsn (cid:144)i, xniXBn [ 1+it3h] H.c (cid:144)aT,xmh’eepCnra,  t+phoen sre(cid:144)e, xpinro jselicdtiionng  
67 ’ 67         (15)  (cid:143) H (cid:143)=$0(cid:143) (cid:159)	𝐻 	0 (cid:160) (cid:160) 0 (cid:143)
(cid:128) E (cid:128) 3 window is deri(cid:144)v,3ed as:    (cid:140) (cid:140) (cid:128)
H (cid:159)=$0 L×	(𝐻L¡|	0(cid:136)¢) 4	𝐻L	×0(cid:136)(⁄¥¢¥4) 0
where  3 q+K = q+K⨂ q+K  represents  (cid:144),4 (cid:140)     (22) 
tshice  ppaorsaem oeft earu gm’pG eannted=d I 6Mc’7’apUmG es+rta6aR 7t"e(s:" .q+K ’ I)"t, is∙ d"peGrived from extrin- r H(cid:159)(cid:144),3(cid:159)(cid:140) HL(cid:159)(cid:144),×4L4 L L×(cid:136)¢ j L𝐻×(cid:136)(⁄𝐻¥¢¥4 )𝐻 r(cid:143)(cid:144),3
67 " ’ 67 (cid:144),4 (cid:140) (cid:140)  
  (cid:143) 4 L j
where,   and   represents the Jacobian of   and 
         (16)  𝐻 =	5𝐽 5 pG 9 			−𝐽 RS9
Hence, inXB Er=ror5 XBSta	t	eBX Kal	m	XBan F	i	lXBter (E	·S··K	XBF [129]) framework,   with respect to e§r,3ror“-7state. And  §,3“, 7 ,   are de-
error-state of syst,em (in,cludin,g aug,mented c,am,eras) is de- rived respective𝐻4ly =by	:5 ¢𝐽 5« pG¤' ×9 	−𝐽¢ ’«RS9
fined by: x " 67 67 67 67 §,4 “8 §,468
7 8 } (cid:127) L ¢ « ¤' × ¢ 67  
      (17) 
 
1086
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 11:45:09 UTC from IEEE Xplore.  Restrictions apply. 𝐻 =	5	𝐽 RS5 pG 9 	−𝐽 RS9
§,468 “7 §,4“8
𝐽 j 𝐽 ¢ 67 « ¤' × ¢ ’«
§,3 §,4
¢ 1¢ ZK 					0		− XS   formed with the stereo extrinsic containing initial noise to 
𝐽 = e6(cid:155) 6(cid:155) f,(k=0,1) show the robustness and the validity of the proposed algo-
where  §,‹ an›d  ZK ﬂ ar	e	0 (cid:140)d	e		(cid:144)f		ineZKd 	a−s：(cid:140) YS(cid:144)	 rithm. All of the following algorithms run on Intel i9-9900k 
¢ 6(cid:155) L 6(cid:155) 6(cid:155) (3.6GHZ) desktop platform. 
(cid:140) (cid:144) (cid:140) (cid:144) (cid:140) (cid:144) H
A.  Dataset 
(cid:144)  
H
(cid:160) EuRoC dataset is a visual-inertial dataset [18] produced 
(cid:140) (cid:144) by ASL team of ETH. Collected by UAV, the dataset in-
(cid:159)
Similar  to  original  S-MSCKF  [2],   represents  the(cid:140)  cludes stereo images of 20 FPS and IMU data of 200 Hz. It 
also provides ground truth trajectories from Leica MS50 
Jacobian with respect to the error of feature coordinate.   
lidar and Vicon motion capture system. The dataset consists 
represents the Jacobian with respect to error-state. The co𝐸re  of three scenarios and 11 sequences. Five of them are ran-
point in this paper is, different with S-MSCKF, in the Jaco- domly selected for comparison. 
bian of rep𝐸rojection error in right cam𝐸era with respect to4 
error-state, the sub-Jacobian of the reprojection error in right  Our large scale dataset includes 30 Hz stereo images and 
n
camera with4 respect to the error-state of 4stereo extrinsic    500 Hz IMU collected by Mynteye S1030 camera shown in 
(cid:144)
Fig. 2. The cameras is calibrated by Kalibr toolkit [19], and 
is a non-zero block. (cid:143)It just models the reprojection error with 
the  ground  truth  is  collected  by  5Hz  GPS  of  UBLOX 
respect to  . During state update, the   will be calibrated 
online iteratrive=ly.z  − zGrep≈reHsenXBts+ obHservpCati+onn noise of jth fea- NEO-M8N.  
ture in ith pose(cid:144). We(cid:144) can (cid:144)stack (cid:144)Eq. (22(cid:144))’ of all th(cid:144)e observations  Our real-world dataset contains two scenes. The first da-
with respect to the same featu(cid:159)re:   (cid:160) (cid:160)(cid:128) taset is the outdoor flight scene of UAV at 10m, 25m and 
30m altitude. The horizontal trajectory distances are 1km, 
H
        (23)  1.1km and 2.1km separately, and the trajectories look like 
(cid:144)
As EKF(cid:160) state variables are formulated regardless of fea- rectangles. The second dataset is an outdoor hand-held scene 
ture coorrdin=ateVs, rwe= cVan (pzro−jezGct) E≈q.V (2H3) XBin+to nthe left null  with a total distance of 1.5km. Therefore, all sequences are 
from large scale scenes. Fig. 3 shows sample images of the 
space of  (cid:144) , an,d (cid:144)margi,nal(cid:144)ize th(cid:144)e form,ul(cid:144)a oHf fe(cid:144)natu=re Verrnor 
† (cid:159) † self-collected dataset. 
[14]:  (cid:144) (cid:144) , (cid:144)
(cid:160) † B.  RMSE comparison 
     (24) 
RMSE, root mean square error, is a popular measure-
where, V represents the left null space of  ,  . 
ment to evaluate estimation accuracy. In the experiment, we 
Hence, Eq. (24) becomes the same as standard EKF update,  compare  proposed  method  with  S-MSCKF  and 
and QR decomposition can be employed to accelerate the 
VINS-Fusion. The former is also based on MSCFK frame-
standard EKF update [1].  work which cannot estimate stereo extrinsic online. The 
   Similar  to  original  S-MSCKF  [2],  the  Observability  latter is an optimization based stereo VIO, which can esti-
mate extrinsic between IMU and every camera. For fairness, 
Constrained EKF [15] is applied in our method for main-
taining the consistency of the filter. And the strategy of fea- we turn off the loop closure mode of VINS-Fusion. 
ture update also comes from S-MSCKF.  In some cases, as the proposed algorithm has a relatively 
large initial threshold in frontend, we only compare trajecto-
D. Vision Frontend 
ries after 30s. 
In our implementation, for efficiency, FAST [16] corners 
are extracted as landmarks. Similar to [2-4], the KLT optical 
flow algorithm [17] is employed in feature matching of front 
and rear frames, as well as left and right frames. In stereo 
matching, essential matrix constraint is used to eliminate 
outliers. Different from [2-4], since stereo extrinsic parame-
ters are calibrated online in this work, the stereo extrinsic 
parameters used in the frontend will also be time-varying. 
Since the initial extrinsic parameters may be inaccurate, the 
outlier rejection algorithm may incorrectly remove inliers 
during the initial phase of system start-up. Therefore, the 
constraint of outlier rejection using essential matrix relation 
should  be  weakened  during  the  initial  period  of  system 
startup to prevent serious errors. After the system runs for a 
period of time (i.e. 30 seconds in this paper), the essential 
matrix constraint could be set to the normal threshold. 
IV.  EXPERIMENTS 
×
Two experiments are performed to evaluate the proposed   
algorithm.  Firstly,  we  compare  our  method  with 
Figure 2.  The  device  we  used  for  our  dataset.  It  contains  oblique 
state-of-the-art stereo VIO [2-4] on EuRoC dataset and a 
top-down global shutter stereo camera( AR0135, 30Hz) with 752 480 
large scale dataset. Secondly, another experiment is per-
resolution and it contains a build-in IMU ( ICM20602, 500Hz ). 
1087
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 11:45:09 UTC from IEEE Xplore.  Restrictions apply. 1)  In EuRoC dataset 
In Table 1, when the initial stereo extrinsic is normal, 
VINS-Fusion performs the best, and our method performs 
similarly to original S-MSCKF. Although VINS-Fusion has 
higher accuracy, it consumes more computational resource 
because of too many variables optimized at the same time 
(see the detail comparison in [2]), and the average CPU load     
on our machine is about 146%. On the contrary, the pro- (a)                      (b) 
posed method is filter-based. Thus, it has the advantage of 
both high efficiency and lightweight. The average CPU load 
of our method is only about 57%, which is less than 1/2 of 
VINS-Fusion.  Our  method  is  similar  with  S-MSCKF  in 
terms of CPU load. 
In Table 2, as expected, benefited from the online stereo 
   
extrinsic calibration, the estimation results with the initial  (c)                     (d) 
noise in stereo extrinsic of our algorithm and VINS-Fusion 
Figure 3.  Sample images of large scale dataset. (a) and (b) are the images 
are not degraded significantly compared with no noise situa-
when the UAV is flying on 30 meters height. (c) and (d) are the images 
tions.  However,  without  stereo  extrinsic  estimation, 
when the device is held by hand. 
S-MSCKF performs badly or even diverges.  
TABLE I.   RMSE (m) comparison with normal initial stereo extrinsic. 
2)  In large scale environment 
For EuRoC dataset, only trajectories after 30s were considered. 
In large scale environment, the estimation accuracy of 
proposed  method  is  similar  with  VINS-Fusion,  both  of  Data sequences  VINS-Fusion  S-MSCKF  Our Method 
which are superior to S-MSCKF. Especially in the handheld 
MH_03  0.080  0.211  0.223 
data (Fig. 4), because there is no stereo baseline estimation, 
the estimated scale of S-MSCKF has a large error. It indi- MH_04  0.110  0.373  0.315 
cates that in the large scale environment, online stereo ex- V1_03  0.129  0.260  0.195 
trinsic estimation is crucial for scale estimation. 
V2_01  0.079  0.110  0.091 
Similarly, with stereo extrinsic containing initial noise, 
V2_02  0.035  0.139  0.163 
the proposed algorithm and VINS-Fusion still work well in 
most cases, but S-MSCKF diverges. Hence, the robustness  UAV_10m  2.935  4.417  3.737 
of our method is validated. 
UAV_25m  4.068  4.674  4.768 
C.  Stereo extrinsic estimation result 
UAV_30m  15.976  19.328  13.866 
As can be seen from Table 3, with different perturbations 
Hand_Held  14.223  41.969  9.480 
to X and Y direction of the initial stereo extrinsic parameters, 
our method can converge to be approximately  the same as 
the off-line calibration results. Specifically, for errors in  TABLE II.   RMSE (m) comparison with bad initial stereo extrinsic 
(added 2 deg. error in Z axis for rotation and 5mm error in baseline for 
translation in X or Y axis, most of the final errors are limited 
translation). Only trajectories after 30s were considered, as it took time to 
to below 0.5 mm. For errors in any direction of rotation, the  estimate appropriate stereo extrinsic for filter-based method. 
final error is controlled under 0.1 degree. It should be noted 
Data sequences  VINS-Fusion  S-MSCKF  Our Method 
that in Z axis of translation, all final errors are around 5 mm, 
including the case with normal initial stereo extrinsic pa- MH_03  0.087  -  0.302 
rameters. An intuitive explanation is that the Z axis of stereo 
MH_04  0.102  1.659  0.337 
camera device is aligned with UAV heading direction in 
EuRoC dataset. Almost all the time UAV moves towards the  V1_03  0.195  0.585  0.235 
heading direction, which leads to a bigger error in the esti-
V2_01  0.154  0.724  0.127 
mated offset along the depth direction. 
V2_02  0.067  0.454  0.165 
Fig. 5 shows that, with different initial artificial perturba-
tions, the estimated translation in X axis and rotation in yaw  UAV_10m  2.950  -  4.930 
direction between two cameras change with time. The figure  UAV_25m  4.076  -  11.213 
shows that in about 30 seconds the estimated translation in X 
UAV_30m  -  -   - 
axis converges, and in 5 seconds the estimated rotation in 
yaw converges. These results indicate that the proposed al- Hand_Held  18.610  -  24.861 
gorithm can effectively estimate the stereo extrinsic in a 
timely manner. 
1088
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 11:45:09 UTC from IEEE Xplore.  Restrictions apply.  
Figure 4.  The estimated trajectories with good initial stereo extrinsic in 
hand held outdoor environment aligned to Google Map. VINS-Fusion (red), 
S-MSCKF (blue), our method (yellow) and GPS (green). 
 
V.  CONCLUSION 
𝐸 (b) 
In this paper, we have presented an approach fo𝐸r online  Figure 5.  With different initial artifical perturbations, estimated baseline 
estimation  of  stereo  extrinsic  parameter4s  based  on  (translation in X axis) and rotation in yaw between two cameras changing 
S-MSCKF framework. The key component of our fo4rmula- with time compared with offline calibration results using V2_02_medium 
tion is that the stereo extrinsic parameter   is explicitly  data of EuRoC. (a) shows the translation and (b) shows the rotation. 
included in state variables, and the model between   error 
TABLE III.   Given different artificial initial perturbations, the final 
and feature reprojection error is formulated. The resulting 
estimation errors of translation (mm) and rotation (Euler Angles in degree) 
stereo VIO system significantly reduces the dependency on  between two cameras compared to the offine calibration ground truth. 
accurate offline stereo calibration. At the same time, the  V2_02_medium data sequence of EuRoC is used in this experiment. 
robustness and accuracy of the system are improved. Based 
Errors in 
on the experiments using EuRoC and real-world datasets,  translation  -10 mm  -7 mm  0 mm  +7 mm  +10 mm 
our scheme significantly outperforms the original S-MSCKF 
X  0.127  0.295  0.547  0.442  0.838 
when there are perturbations to camera parameters. Espe-
cially, given inaccurate extrinsic parameters, our method can  Y  0.248  -0.026  -0.204  -0.020  -0.040 
converge to an accurate estimation of extrinsic parameters 
Z  5.425  5.720  5.253  5.547  5.554 
over a few dozens of seconds. Since our method is fil-
Errors in 
ter-based, the computational requirement is much lower than  -3 deg  -1.5 deg  0 deg  +1.5 deg  +3 deg 
rotation 
those of optimization-based methods (e.g. VINS-Fusion), 
Roll  0.096  0.097  0.093  0.096  0.087 
without significantly degrading the accuracy and robustness 
of the algorithm.   Pitch  -0.078  -0.077  -0.080  -0.080  -0.086 
In future work, we will focus on real-time evaluation of  Yaw  0.027  0.026  0.027  0.025  0.026 
the certainty of stereo extrinsic parameters. 
REFERENCES 
[1]  A. I. Mourikis and S. I. Roumeliotis, “A multi-state constraint 
kalman filter  for  vision-aided  inertial  navigation,”  inPro-
ceedings 2007 IEEE International Conference on Robotics and Au-
tomation, pp. 3565–3572,2007. 
[2]  K. Sun, K. Mohta, B. Pfrommer, M. Watterson, S. Liu, Y. Mulgaon-
kar,C. J. Taylor, and V. Kumar,  “Robust stereo visual inertial 
odometry forfast autonomous flight,” IEEE Robotics and Automation 
Letters, vol. 3,pp. 965–972, April 2018.[3]  M. Brossard, S. Bon-
nabel, and J. 
[3]  T. Qin, J. Pan, S. Cao, and S. Shen, “A general optimization-based 
framework for local odometry estimation with multiple sensors”, 
arXiv preprint arXiv:1901.03638, 2019. 
[4]  T. Qin and S. Shen, “Online temporal calibration for monocular 
visual-inertial systems”, in 2018 IEEE/RSJ International Conference 
on Intelligent Robots and Systems (IROS), IEEE, 2018, pp. 3662–
3669. 
[5]  S. Weiss, M. W. Achtelik, S. Lynen, M. Chli, and R. Siegwart, “Re-
al-time onboard visual-inertial state estimation and self-calibration of 
  MAVs in unknown environments,” Robotics and Automation (ICRA), 
(a)  2012 IEEE International Conference on, pp. 957-964, 2012. 
[6]  S. Shen, Y. Mulgaonkar, N. Michael, and V. Kumar, “Multi-sensor 
fusion for robust autonomous flight in indoor and outdoor environ-
ments with a rotorcraft MAV,” Robotics and Automation (ICRA), 
2014 IEEE International Conference on, pp. 4974-4981, 2014.   
1089
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 11:45:09 UTC from IEEE Xplore.  Restrictions apply. [7]  Stefan Leutenegger, Simon Lynen, Michael Bosse, Roland Siegwart 
and Paul Timothy Furgale. Keyframe-based visual-inertial odometry 
using nonlinear optimization. The International Journal of Robotics 
Research, 2015. 
[8]  Hansen P, Alismail H, Rander P, et al. Online continuous stereo 
extrinsic parameter estimation[C]//2012 IEEE Conference on Com-
puter Vision and Pattern Recognition. IEEE, 2012: 1059-1066. 
[9]  Ling Y, Shen S. High-precision online markerless stereo extrinsic 
calibration[C]//2016 IEEE/RSJ International Conference on Intelli-
gent Robots and Systems (IROS). IEEE, 2016: 1771-1778. 
[10] N. Trawny and S. Roumeliotis, “Indirect Kalman filter for 6D pose 
estimation,” University of Minnesota, Dept. of Comp. Sci. & Eng., 
Tech. Rep, vol. 2, 2005. 
[11] Li, M. and Mourikis, A. I. (2011). Consistency of EKF-based visu-
al-inertial odometry. Technical report, University of California Riv-
erside. www.ee.ucr.edu/∼mourikis/tech reports/VIO.pdf. 
[12] J. Sola`, “Quaternion kinematics for the error-state kalman filter,” 
CoRR, vol. abs/1711.02508, 2017. 
[13] L. Clement, V. Peretroukhin, J. Lambert, and J. Kelly. The battle for 
filter supremacy: A comparative study of the multi-state constraint 
kalman filter and the sliding window filter. In Computer and Robot 
Vision (CRV), 2015 12th Conference on, pages 23–30, 2015. 
[14] Y. Yang, J. Maley, and G. Huang, “Null-space-based marginalization: 
Analysis and algorithm,” in Proc. IEEE/RSJ International Conference 
on Intelligent Robots and Systems, Vancouver, Canada, Sep. 24-28, 
2017, pp. 6749-6755. 
[15] J. A. Hesch, D. G. Kottas, S. L. Bowman, and S. I. Roumeliotis, 
“Observability-constrained vision-aided inertial navigation,” Univer-
sity of Minnesota, Dept. of Comp. Sci. & Eng., MARS Lab, Tech. 
Rep, vol. 1, 2012. 
[16] M. Trajkovic ́ and M. Hedley, “Fast corner detection,” Image and 
vision computing, vol. 16, no. 2, pp. 75–87, 1998. 
[17] B. D. Lucas and T. Kanade, “An iterative image registration tech-
nique with an application to stereo vision (ijcai),” Proceedings of the 
7th International Joint Conference on Artificial Intelligence 
(IJCAI ’81), pp. 674–679, April 1981. 
[18] Burri M, Nikolic J, Gohl P, et al. The EuRoC micro aerial vehicle 
datasets[J]. The International Journal of Robotics Research, 2016, 
35(10): 1157-1163. 
[19] Rehder J, Nikolic J, Schneider T, et al. Extending kalibr: Calibrating 
the extrinsics of multiple IMUs and of individual axes[C]//2016 
IEEE International Conference on Robotics and Automation (ICRA). 
IEEE, 2016: 4304-4311. 
 
1090
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 11:45:09 UTC from IEEE Xplore.  Restrictions apply. 