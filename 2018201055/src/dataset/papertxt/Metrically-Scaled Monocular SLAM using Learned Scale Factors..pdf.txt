2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
Hierarchical 6-DoF Grasping with Approaching Direction Selection
Yunho Choi, Hogun Kee, Kyungjae Lee, JaeGoo Choy, Junhong Min, Sohee Lee, and Songhwai Oh
Abstract—In this paper, we tackle the problem of 6-DoF
graspdetectionwhichiscrucialforrobotgraspingincluttered
real-world scenes. Unlike existing approaches which synthesize
6-DoF grasp data sets and train grasp quality networks with
input grasp representations based on point clouds, we rather
take a novel hierarchical approach which does not use any 6-
DoF grasp data. We cast the 6-DoF grasp detection problem
as a robot arm approaching direction selection problem using
the existing 4-DoF grasp detection algorithm, by exploiting a
fully convolutional grasp quality network for evaluating the
quality of an approaching direction. To select the best ap-
proaching direction with the highest grasp quality, we propose
an approaching direction selection method which leverages
a geometry-based prior and a derivative-free optimization
method.Speciﬁcally,weoptimizethedirectioniterativelyusing Fig. 1: The grasp detection method proposed in this paper enables 6-DoF
thecrossentropymethodwithinitialsamplesofsurfacenormal grasping by selecting the best approaching direction for 4-DoF grasping.
directions.Ouralgorithmefﬁcientlyﬁndsdiverse6-DoFgrasps Theproposedmethodespeciallyyieldshighgraspsuccessratesforadense
by the novel way of evaluating and optimizing approaching clutterofobjectsasshowninthisﬁgure.
directions. We validate that the proposed method outperforms
other selection methods in scenarios with cluttered objects in Toovercomethelimitationof4-DoFgrasping,itisnatural
a physics-based simulator. Finally, we show that our method
to pursue a 6-DoF grasp which consists of the 3D position
outperformsthestate-of-the-artgraspdetectionmethodinreal-
and 3D orientation of the gripper, giving the robot the
world experiments with robots.
maximum ﬂexibility to select the best grasp. Planning a 6-
I. INTRODUCTION DoF grasp is a much harder problem, since the search space
Grasp detection is one of the most long-studied problems for a 6-DoF grasp pose is larger than that of a 4-DoF grasp
in robot manipulation due to its utility for various robotic and a 6-DoF grasp depends more on the entire geometry of
applications from industry to service robots [1], [2]. In this the object while we only have a partial observation from a
problem, a robot observes a novel object and determines the single view even with possible occlusions. Most of recent
position and orientation of its gripper to pick up the object. methods which deal with 6-DoF grasp detection [9]–[11]
Therobotreliesonlyonthepartialobservationoftheobject, proposetheirowngrasprepresentationsbasedonobjectpoint
which makes the problem challenging. To make matters clouds.Withthe6-DoFgrasprepresentation,theysynthesize
worse, the object and gripper’s geometry, surface frictions, a grasp data set to train a grasp quality network which are
massdistribution,andkinematicfeasibilityaffectthestability moredemandingcomparedtothegenerationof4-DoFgrasp
ofgrasps.Consequently,manyapproachesdealingwithgrasp samples.
detection simplify the problem by assuming a top-down In order to take advantages of 6-DoF grasping by using
grasp with a four-dimensional action space [3]–[8] , which only 4DoF grasping data and a simpler grasp detection
consists of 3D positions and a rotation angle about the model,werathertakeahierarchicalapproachwhichnaturally
gripper axis. Especially, recent data-driven approaches [3], extends the 4-DoF grasp detection problem into the 6-DoF
[4] which predict the best grasp pose from a depth image grasp detection problem. First of all, we assume a depth
haveshownsuccessfulgeneralizationperformance.However, sensor attached to a robot arm as shown in Figure 1. This
these4-DoFgraspingmethodsdonotgeneralizewellforthe is inspired by the fact that humans actively observe an
arbitraryposeofanobjectandclutteredobjects,andthisfact object and manipulate the object with the best approaching
makes them unsuitable for unstructured environments. direction. Then, we cast the 6-DoF grasp detection problem
into the approaching direction selection problem for 4-DoF
Y. Choi, K. Lee, J. Choy, H. Kee and S. Oh are with the De-
grasp detection, which is our ﬁrst contribution. We propose
partment of Electrical and Computer Engineering and ASRI, Seoul Na-
tional University, Seoul 08826, Korea (e-mail: {yunho.choi, kyungjae.lee, a novel method for solving the grasp approaching direction
} { }
jaegu.choy @rllab.snu.ac.kr, hogunkee,songhwai @snu.ac.kr).J.Minand selection problem which are comprised of three main parts:
S. Lee are with Samsung Electronics Co., Suwon 16677, Korea (e-mail:
{ } (1) how to deﬁne a good approaching direction; (2) how
junhong1.min, ssohee.lee @samsung.com). This work was supported in
partbyInstituteofInformation&CommunicationsTechnologyPlanning& to generate approaching direction candidates; and (3) how
Evaluation(IITP)grantfundedbytheKoreagovernment(MSIT)(No.2019- to improve approaching direction candidates. To deﬁne a
0-01371,DevelopmentofBrain-InspiredAIwithHuman-LikeIntelligence)
good approaching direction, we utilize a fully convolutional
and by a research grant from Samsung Electronics Co., Ltd (No. 0418-
20190018). graspqualitynetworktrainedwith4-DoFgraspdata[4].We
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 1553
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 14:19:05 UTC from IEEE Xplore.  Restrictions apply. assumethatthe4-DoFgraspqualitynetworkhassuccessfully datasetsarecollectedviaeitherhuman-labeled[5],[8],self-
learned the relation between a depth image and qualities supervised [6], [7], or synthetic ways [3]. They commonly
of 4-DoF grasps within that image. Then, we evaluate the represent the grasp with a simple 3-DoF oriented rectangle
qualityofanapproachingdirectionwithtop-kgraspqualities in the image [17] or 4-DoF representation where the grasp
predicted from the depth image seen from the chosen ap- depth is added, facilitating the grasp data collection from
proaching direction, where we propose to estimate the depth depth images. However, these simpliﬁed rectangular repre-
image by transforming the point cloud, rather than moving sentations of grasps can limit the grasp on arbitrary shapes,
the depth sensor every time. and impose a restriction on the workspace a robot.
The second and third parts are related with selecting Mostofthesemodel-freemethodsdonotdirectlygenerate
the best approaching direction whose predicted top-k grasp grasp poses, but rather consist of two cascaded parts, grasp
qualities are maximal. Our another main contribution lies candidate sampling and grasp quality evaluation. They ﬁrst
in the method we propose for generating good approaching sample antipodal grasps based on geometry-based heuristics
directioncandidatesanditerativelyimprovingthecandidates, [18], and rank them with the grasp quality metrics predicted
which leverages a geometry-based prior and an optimization by the convolutional neural networks. Since we cannot eval-
method. Speciﬁcally, the proposed method uses the cross- uate all possible grasps, sampling plausible grasp candidates
entropy method to optimize the approaching direction qual- iscrucialandofteniterativeoptimizationtechniques,suchas
ity, with initial seeds of surface normal directions. With the the cross entropy method [3], [19]–[22], are used. However,
found best approaching direction, we move the robot arm these techniques impose computational burden since the
to the viewpoint aligned with that direction and execute the networkmustbeiterativelyqueriedforabatchofpredictions.
best 4-DoF grasp found under that viewpoint. With the fully Meanwhile, incorporating the advances in the computer
convolutional 4-DoF grasp quality network combined with vision for pixel-wise prediction using fully convolutional
the proposed approaching direction selection method, we networks(FCNs)[23],recentapproacheshaveenableddense
efﬁciently search the space of 6-DoF grasps and enable the evaluation of the entire space of possible discretized grasp
6-DoF grasping without using any training data of 6-DoF actions given a depth image. Approaches used by [24], [25]
grasps. evaluate 3-DoF grasps and FC-GQ-CNN [4] evaluates 4-
We evaluate our method in a physics-based simulator and DoF grasps, both eliminating the need for sampling grasp
compare the performance with other approaching direction candidates. Especially, FC-GQ-CNN will play a key role in
selection methods. We show that the proposed method es- our proposed method since it can evaluate all 4-DoF grasps
pecially outperforms in densely cluttered scenes which are withinthedepthimageobservedataviewpointanditshows
challenging for 4-DoF grasping methods. We also deploy the state-of-the-art performance in the 4-DoF grasping.
ouralgorithminreal-worldgraspingscenariosusingarobot,
B. Learning 6-DoF Grasp Detection
and validate that the proposed method takes advantage of 6-
DoF grasping. To the best of our knowledge, the proposed To give a robot the maximum degree of freedom to select
method is the ﬁrst approach solving the general 6-DoF the grasp, we target the problem of 6-DoF grasp detection
grasp detection problem by replacing it with an approaching which is more challenging since the entire object geometry
direction selection problem, which is more affordable and including the occluded parts affects the grasp quality. Anal-
efﬁcient. ogous to the cascaded approach of 4-DoF grasp detection
methods, most of the 6-DoF approaches have two parts of
II. RELATEDWORK grasp candidates sampling and grasp quality evaluation. Yan
et al. [21] solves the partial observability by training the
A. Learning Grasp Detection
auxiliarytaskofreconstructingtheobjectgeometry.whereas
The goal of grasp detection is to ﬁnd a gripper con- ourmethoddealswiththepartialobservabilitybyleveraging
ﬁguration that maximizes the grasp quality metrics. The the second sight at the viewpoint with the selected best
grasp quality metrics, such as grasp wrench space (GWS) approaching direction.
analysis [12] and force-closure [13], physically analyze the Recent methods, GPD [9] and PointNetGPD [10], sample
geometry of the gripper and the object. Approaches for the grasp candidates using a geometry-based method in [26].
graspdetectionareeithermodel-basedormodel-free.Model- They sample a point in the observed point cloud , construct
based approaches [14]–[16] utilizes a data set of 3d object aDarbouxframewhichisabasisalignedwiththeestimated
models labeled with feasible grasps, by matching the sensor surfacenormalandlocalprincipalcurvature,andﬁndnearby
inputs with the templates during execution. However, these feasible grasps with local search. The authors of GPD
approaches suffer from poor generalization on novel objects handcrafted several projection features on point clouds for
and thus we focus on model-free approaches in this paper. thequalitynetworkinput,whiletheauthorsofPointNetGPD
Model-free methods rather solves the problem in data- exploited the PointNet architecture [27] to predict grasp
driven approaches. Recent methods exploit convolutional qualities from the point cloud within the gripper closing
neural network architectures to process raw RGBD inputs, area. 6-DoF grasp data collection for training the quality
and train a neural network with massive data sets of grasps, network of the GPD and PointNetGPD involves processing
images, and grasp quality metric labels [3], [5]–[8]. The whole meshes of objects, which are signiﬁcantly costly
1554
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 14:19:05 UTC from IEEE Xplore.  Restrictions apply. by applying a partition algorithm to the ﬂattened Q(X ).
v
We consider top-k quality since a high-quality approaching
directionmustcontainalargenumberofhigh-qualitygrasps.
Themostpromising4-DoFgraspatanapproachingdirection
v is argmax Q(X ), and the quality of a grasp g
x,y,z,ψ v |
predictedbyQgivenadepthmapX isdenotedbyQ(g X).
Given a depth map X, our original problem of 6-DoF
∗ ∈
grasp detection is to ﬁnd g SE(3) which maximizes
|
the grasp quality Q (g X), where Q is a nominal 6-
6d 6d
DoF grasp quality evaluation model. For any possible grasp
Fig. 2: An illustration of selecting an approaching direction v = (θ,φ),
whichisdeﬁnedwithapointonahemisphereabovethetable.Theproposed g = (θ,φ,ψ,x,y,z), g can be viewed as a 4-DoF grasp
method aligns the robot arm to the best approaching direction v∗ and from an approaching direction v =(θ,φ). After aligning on
performs4-DoFgraspingunderthenewviewpoint. thehemispherepointcorrespondingtov,ifweletdx,dy,dz
be a displacement from the corresponding hemisphere point
compared to the 4-DoF grasp data collection [3]. Recent of v to (x,y,z) in a rotated coordinate as shown in Figure
work [11] also uses the PointNet architecture for grasp 2, then executing a 4-DoF grasp g(cid:48) = (dx,dy,dz,0,0,ψ)
evaluation , and samples diverse grasps from a generative in the rotated coordinate is equivalent with executing g.
|
neural network. However, this work is targeted for grasping Therefore,thegraspqualityQ (g X)canalsobeevaluated
singulated objects. The abovementioned approaches which by Q(g(cid:48)|X ). In this paper, w6de cast the original problem
learn from 3D point cloud data can suffer from overﬁtting of ﬁnding gv∗ = (θ∗,φ∗,ψ∗,x∗,y∗,z∗) into the problem of
and performance degradation when the input point cloud selecting v∗ = (θ∗,φ∗) that maximizes the Q↓(X ) and
is sparse, while our method can efﬁciently sample grasp ﬁnding a 4-DoF grasp argmax (cid:48)Q(g(cid:48)|X ∗) at thke sevlected
g v
candidates in such cases. direction, where k = 1 makes two problems identical, i.e.,
Existing study on approaching direction selection for argmax Q (g|X) and argmax (cid:48)Q(g(cid:48)|X ∗) where v∗ =
g 6↓d g v
grasping [28] requires a predeﬁned model, and focuses on a argmax Q (X ) are same in the world coordinate.
viewpoint selection problem near a target grasp to improve v 1 v
thegraspquality.Hence,itcannotbeappliedtogeneralcases IV. GRASPAPPROACHINGDIRECTIONSELECTION
unlike the proposed method. METHOD
∗
We focus on ﬁnding the best approaching direction v =
III. PROBLEMFORMULATION
(θ,φ), assuming that a 4-DoF fully convolutional qual-
We assume a camera is attached to a robot arm, and ity network model Q is given. Accordingly, we propose
initially the camera captures a depth map X from the top an efﬁcient approaching direction selection method which
view. The depth map X is converted into a point cloud P. generates candidates of good approaching directions with
↓
Given a set of object O, the object point cloud is denoted high approaching direction qualities Q (X ). The proposed
k v
as P which is extracted from P by eliminating points methodconsistsoftwomainparts,whichareevaluationofan
O
associated with the table. We denote the 6-DoF grasp g by approaching direction quality and generation of approaching
∈ ∈ ∈
(R,T) SE(3) where R SO(3) and T = (x,y,z) directioncandidates.Weiterativelygeneratebetterapproach-
R
3 are the rotation and translation of the gripper in the ingdirectioncandidatesbasedontheevaluationofgenerated
initial gripper coordinate, respectively. We represent R with approaching direction candidates.
∈ R
Euler angles (θ,φ,ψ) 3, where θ,φ,ψ implies azimuth,
A. Evaluating Qualities of Grasp Approaching Directions
inclination, and rotation about gripper axis, respectively. We
only consider parallel jaw grippers in this paper. Inordertoevaluatethequalityofanapproachingdirection
↓
We assume a virtual hemisphere of a radius r, ﬂoating by v, i.e., Q (X ), the camera on the robot arm should be
k v
d above the table and deﬁne a grasp approaching direction moved to the point on the hemisphere corresponding to v.
as a direction from a point on the hemisphere to the center Since moving the robot arm to every sampled approaching
of the hemisphere as seen in Figure 2. Thus, an approaching direction is exhausting, we rather approximate depth images
directionisuniquelydeterminedbytwoangles,azimuthand seen at sampled approaching directions. Speciﬁcally, we
inclination, which respectively correspond to θ and φ of R. obtain the point cloud P from the depth map X from the
∈R
Wedenoteagraspapproachingdirectionbyv =(θ,φ) 2 top view, transform P according to a camera m(cid:98)ovement
and a depth map captured from v is denoted by X . Q for alignment with v, project the transformed point cloud,
v
denotes a fully convolutional grasp quality network model and interpolate it to fabricate a new depth image X . Algo-
∈ R × × × v
which outputs the quality map Q(X) W H K L for a rithm 1 shows the overall process to estimate the qualities
given depth map X, where W, H, K, and L represent the of approaching direction candidates. To evaluate multiple
number of bins discretizing the 3D grasp position and rota- approaching directions at once, multiprocessing can be used
tion angle, respectively. Then the quality of an approaching when implementing Algorithm 1.
direction v is deﬁned as the average of k-largest elements in Using k >1 makes it possible to leverage multi-modality
↓
Q(X ), which is denoted by Q (X ), and can be computed of successful grasps in the process of the approaching direc-
v k v
1555
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 14:19:05 UTC from IEEE Xplore.  Restrictions apply. tionselection.Sinceproposedgraspsmayviolatekinematics
and collision constraints, it is important to consider the
diversity of high quality grasps and generate approaching
direction candidates which lead to diverse choices of high
quality grasps.
Algorithm 1 ApproachingDirectionQuality
Require: k for considering the k-largest grasp qualities, camera (a) (b)
matrix C, 4-DoF fully convolutional grasp quality network Q
Fig. 3: Visualization of orthogonal bases at surface points sampled from
input: depth map X from the top view, approach direction v
the point clouds of cluttered objects. The red axes, blue axes, green axes
Convert X into the point cloud P indicatesurfacenormaldirections,majorprincipalcurvaturedirections,and
Compute the camera movement (R,T) from the top of the minor principal curvature directions, respectively. Black lines indicate the
hemisphere to the corresponding point of v surface normal directions computed by the standard principal component
Transform P with the inverse transform of (R,T) analysis.(a)AcaseofpointcloudcapturedintheMuJoCosimulator[30].
Project the transformed point cloud into the image plane w(cid:98)ith (b) A case of point cloud captured by the RealSense depth sensor in the
camer(cid:98)a matrix C realworld.
×
Interpolate the regular grid to make a 256 256 depth map X(cid:98)v Algorithm 2 SampleSmoothedNormals
ouPCtpaousmst:pXuQtve↓it(nhXt(cid:98)eoaQ)verageoftop-kpredictedgraspqualitiesQ↓k(Xv) Rinepquuti:red:epntuhmmbaepr oXf sfarmomplethsemto,pneviigehwborhood radius , φmax
k v Convert X into the point cloud P and extract the object point
cloud P
O ∈
nˆ(p ) = SurfaceNormalEstimation(p ) for all p P
B. Generation of Approaching Direction Candidates forij =0 to(cid:80)m∈−1 do i i O
The core of our algorithm is iterative improvement of Sample p P randomly uniformly
j O { ∈ || − ||≤ }
approaching direction candidates based on the result of the Compute-ballaboutpj:B(pj)= q PO : pj q 
qualities evaluated with Algorithm 1. For the improvement MCo(npvje)rt=the pe∈igBe(npvje)cnˆto(rp)cnˆo(rpre)sTponding to the largest eigen-
ofapproachingdirectioncandidates,weexploitthecrossen-
value of M(p ) into the approaching direction v =(θ ,φ )
j j j j
tropymethod(CEM),whichisaderivative-freeoptimization if φ >φ then Reject; resample v
j max j
methodwithasymptoticconvergenceproperties[19].Forour end for
{ ··· }
implementation, we iteratively ﬁt a mixture of Gaussians output: V = v ,v , ,v −
0 1 m 1
to the elite set of sampled approaching directions and re-
sample from the ﬁtted mixture of Gaussians in order to
ﬁnd maxima of the approaching direction quality. With this neighborhood as in [9], [10], [29]. Speciﬁcally, we perform
simple adaptive procedure, approaching directions of high PCA on the neighborhood surface normals without mean
qualities are found. However, the biggest drawback of the subtraction and extract the ﬁrst principal component as a
CEM is that it is serial in nature. It repeatedly queries the smoothed estimate of the surface normal. The entire process
quality network at every CEM iterations which puts a strain of sampling smoothed surface normals for initial candidates
on time constraints for approaching direction selection. of the grasp approaching direction is speciﬁed in Algorithm
In order to mitigate the computation time issue, we need 2.Inourimplementation,multiprocessingisusedtogenerate
a better starting point for our CEM iterations so that we samples in parallel when implementing Algorithm 2.
can reduce the number of CEM iterations and the number After sampling m smoothed surface normal directions for
of direction samples which are evaluated. Thus, we leverage initial seeds, we iteratively improve our grasp approaching
thegeometry-basedpriorforinitialseeds,ratherthanstarting direction candidates by running the CEM. By starting from
fromrandomsamplesofapproachingdirections.Speciﬁcally, highly informative samples, the mixture of Gaussians in our
we focus on the fact that surface normal vectors usually CEM iterations reaches modes of an optimal region for the
indicate good approaching directions for grasping, as many grasp approaching direction efﬁciently with a fewer number
point cloud-based work [9], [10] sample grasp candidates of iterations and samples. We name this CEM and surface
using surface normal vectors. Empirically, grasps from the normal-based approaching direction selection method as the
surface normal direction are expected to produce less col- GADS(GraspApproachingDirectionSelection)methodand
lisions and insert the gripper deeper which is important for the entire process of GADS is detailed in Algorithm 3.
stablygraspingroundobjectsasseeninFigure3-(a).Another After selecting the best approaching direction by the
important point is that the point cloud data from the real GADS method, the robot arm moves to capture the second
world are noisy that estimating the exact surface normal depth map at the chosen approaching direction, and plan the
from the standard principal component analysis (PCA) over best 4-DoF grasp with a 4-DoF grasp quality network. Note
a small neighborhood is impossible, as shown in Figure that our method extensively examine 6-DoF grasps by sam-
3-b. Therefore, we do not use the estimate of a surface plingviewpointcandidates,ratherthangraspcandidatesasin
normal using PCA. Rather, we use a smoothed estimate of other 6-DoF approaches [9]–[11], and it can be generalized
the surface normal by considering surface normals of the with any 4-DoF grasp quality model Q.
1556
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 14:19:05 UTC from IEEE Xplore.  Restrictions apply. Algorithm3GADS:GraspApproachingDirectionSelection The protocol of simulation is as follows. After spawning
Require: k for considering k-largest grasp qualities, number of objects of each scene on a simulator, we move a robot arm
CEMiterationr,numberofGaussianmixturesn,eliteproportion to an initial position to capture a depth map from the top
p, number of initial samples m, number of CEM samples c view, pick an object on the table with the detected grasp
input: depth map X from the top view
{ } and place it on another table. Then we move the arm to the
Sample(cid:98)m initial seeds v = SampleSmoothedNormals(X)
− i initialpositionandrepeattheprocess.Wetryeightgraspsfor
for j =0 to r 1 do (cid:98)
↓ densely cluttered scenes, and six grasps for lightly cluttered
Q (X )= ApproachingDirectionQuality(X,v ) for all i
Sokrt thveiapproaching directions qualities Q↓(Xi ) for all i scenes. If the robot picks an object up to 10cm above the
Fit the mixture of n Gaussians to the pk-elivtei set of the table, then the grasp is declared as a successful grasp.
approaching directions We compare six different methods, which includes 6-DoF
{ }
Replace vi withcapproachingdirectionssampledfromthe grasping methods with ﬁve approaching direction selection
ﬁtt(cid:98)ed mixture of Gaussians
methodsandthestate-of-the-art4-DoFbaselinemethod.The
end for
Q↓(X )= ApproachingDirectionQuality(X,v ) for all i ﬁve selection methods are as follows.
Seklectvtihe best approaching direction v∗ amongiall i 1) GADS: The proposed method.
∗ ∗ ∗
output: v =(θ ,φ ) 2) Random Select∈ion: Approa∈ching direction is sampled uni-
formly from θ [0,2π],φ [0,φ ].
max
3) CEM with Random Initial Seeds: GADS modiﬁed to start
∈
CEM with m uniformly sampled directions from θ
∈
[0,2π],φ [0,φ ].
max
4) Sampling Smoothed Normals: The best approaching direc-
tion is chosen from m samples in Algorithm 2.
5) SurfaceNormalHistogram:Asimplerandfasterworkaround
for sampling smoothed surface normals. This method con-
{ }
verts entire nˆ(p ) in Algorithm 2 into the approaching
{ i} { }
directions (θ ,φ ) ,andmakea2Dhistogramof (θ ,φ ) .
i i i i
Inthisway,wecanﬁndmdominatingsurfacenormaldirec-
tions of the entire point cloud of objects without sampling
and post-processing.
Fig. 4: Examples of cluttered objects scenes in simulation. (Left) Light
clutterwith10objects.(Right)Denseclutterwith20objects. For a 4-DoF grasp quality model for all approaching di-
rection selection methods, we use a pretrained FC-GQ-CNN
V. EXPERIMENTS model[4],whichisalsousedforthe4-DoFbaselinemethod.
We perform nearest-neighbor interpolation in Algorithm 1
We evaluate the proposed approaching direction selection
and compute the PCA surface normal estimate in Algorithm
methods(GADS)bothinsimulationsandrobotexperiments,
2 with a point cloud library [33].
for scenes with cluttered objects. We check the validity
For the common hyperparameters of the selection meth-
of GADS by comparing with other selection methods in
ods, we use k = 10 to consider top-10 grasp qualities, and
simulationandgetcompellingresultsinthecomparisonwith
put a limit on φ by φ =π/7, considering the kinematic
thestate-of-the-artgraspingalgorithmforthetaskofpicking max
feasibility of a Baxter robot. For CEM with Random Initial
cluttered objects in real-world experiments. All experiments
Seeds, we use m=24, c=12, r =2, n=3, and p=0.25.
are using a Baxter robot and the grasp detection algorithms
For Sampling Smoothed Normals, we use m=12, and =
ranonanotebookrunningUbuntu16.04witha2.6GHzIntel
1.5cm. For Surface Normal Histogram, we use m=12, and
Core i7-9750H CPU and an NVIDIA GeForce RTX 2060. ∈ ∈
bin size of (2/16π,0.02π) for (θ [0,2π],φ [0,φ ]).
max
For GADS, we choose m = 12, c = 8, r = 2, n = 2, and
A. Simulation for Comparison of Approaching Direction
p=0.5, to see if GADSperforms well in spite of using less
Selection Methods
samples than CEM with Random Initial Seeds.
For the simulation environment, we modiﬁed the simula- Simulation results including success rates and average
tionenvironmentinSurrealsuite[31],whichisbasedonthe selection times are shown in Table I. Selection time denotes
MuJoCophysicsengine[30].Toevaluateapproachdirection the sum of the time for viewpoint candidate generation and
selectionmethods,wemade40scenesforreproducingobject thetimeforviewpointqualityevaluation.Wealsotestedtwo
typesandpositions,whicharecomprisedof20sceneswhere variations of the selection methods, differing with k and the
10 objects are lightly cluttered on a table and 20 scenes use of a second camera capture at the chosen v to detect a
where 20 objects are densely cluttered. We aim to see if the exact 4-DoF grasp. Without the second sight, the robot can
proposed method takes advantage of 6-DoF grasping when execute the grasp detected in the course of evaluating the
objects are placed more densely. Objects are randomly cho- quality of the best viewpoint with Algorithm 1 though this
senfromanobjectsetwhichconsistsof15householdobjects grasp can be inaccurate since Algorithm 1 uses estimated
including a box, cup, and cylindrical shape. The objects are depth maps. We ﬁnd that choosing k = 10 than k = 1 and
imported from meshes used in the DexNet dataset [3], and using the second sight improves performance for almost all
decomposedintoconvexpartsusingV-HACDalgorithm[32] cases, which validates that considering the multi-modality
to simulate on MuJoCo. of good grasps for approaching direction selection is crucial
1557
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 14:19:05 UTC from IEEE Xplore.  Restrictions apply. k=10 k=1 k=10,nosecondsight Average
lightclutter denseclutter lightclutter denseclutter lightclutter denseclutter selection
(10objects) (20Objects) (10objects) (20Objects) (10objects) (20Objects) time
CEMwithRandomInitialSeeds 85.00% 82.50% 85.00% 75.63% 85.83% 81.25% 3.50s
SurfaceNormalHistogram 81.67% 78.75% 80.00% 78.13% 71.67% 65.00% 0.97s
SamplingSmoothedNormals 86.67% 82.50% 85.83% 75.63% 82.50% 81.25% 1.15s
GADS 90.83% 86.25% 88.30% 83.13% 81.67% 79.38% 2.48s
lightclutter(10objects) denseclutter(20objects)
FC-GQ-CNN[4] 87.50% 76.25% -
RandomSelection 77.50% 77.50% -
TABLEI:Averagesuccessratesofvariousgraspapproachingdirectionselectionmethodsfor20lightlyclutteredscenesand20denselyclutteredscenes
intheMuJoCosimulation.Averagecomputationtimesforeachapproachingdirectionselectionmethodsarealsodetailedintherightmostcolumn.
Set1 Set2 Set1 Set2
Average
isolated isolated clutter clutter
FC-GQ-CNN 70.00% 50.00% 56.67% 52.22% 54.44%
GADS 76.67% 53.33% 67.78% 62.22% 65.00%
TABLEII:AveragesuccessratesofFC-GQ-CNNandGADSfortwosets
ofobjectsatisolatedandclutteredconditionsinrobotexperiments.
clutter of 10 objects, and evaluate different grasp detection
methodsover15scenesforeachset,wheretheconﬁguration
of each scene is artiﬁcially reproduced for fair comparison.
Fig. 5: Two sets of 10 objects each, used in the robot experiment. (Left)
Successratesattheisolatedconditionaremeasuredbythree
Set1.(Right)Set2.
grasp attempts per each object in a stable pose.
Theselectedobjectsarechallengingtopickupeveninthe
and the second sight from the chosen direction is needed for
singulatedconditionasshowninTableII,andespeciallySet
more accurate grasping, respectively.
2containsﬁveobjectswhicheitheroftwograspingmethods
We can also see that the 4-DoF grasping performs as
failstograspinthreeattemptswhileSet1containsonesuch
well as the presented 6-DoF grasping methods in the lightly
object. Performance degradation in real-world experiment
clutteredscenarioswheremostobjectsareisolated,butinthe
mainly results from the poor quality of depth images in real
densely cluttered scenarios, 6-DoF grasping methods with
world, where thin objects in isolation are even undetectable
the approaching direction selection methods outperformed
in depth images often. Nevertheless, the GADS method
the 4-DoF grasping, which implies the importance of utiliz-
for 6-DoF grasping shows its superiority over the 4-DoF
ing full 6-DoF for picking up the densely cluttered objects.
FC-GQ-CNN method for cluttered scenes, even improving
Although the Surface Normal Histogram method is the most
success rates over the isolated condition by a margin of
time-saving, its performance is not signiﬁcantly better than
8.89% in Set 2. The proposed method has shown tendency
theRandomSelectionmethodsincediscretizationforthe2D
to seek the appropriate approaching direction according to
histogramleadstolimiteddiversityofapproachingdirections
the orientations of objects.
and degrades the approaching direction quality. Meanwhile,
theCEMwithRandomInitialSeedsmethodshowsimproved VI. CONCLUSION
performance while taking a longer time of 3.50s due to the
In this paper, we have proposed a novel hierarchical
serial nature of CEM. Overall, GADS performed the best,
approach for 6-DoF grasping which selects the best ap-
well-balancing between the performance and time trade-
proaching direction for 4-DoF grasping. The proposed grasp
off by exploiting informative geometry prior of smoothed
approaching direction selection method, GADS, enables 6-
surfacenormalsandreducingthenumberofsamplesneeded.
DoF grasping with a 4-DoF grasp quality model trained
with 4-DoF grasp data, eliminating the need for collecting
B. Evaluation on the Real Robot
time-consumingandexpensive6-DoFgraspdata.Byexploit-
We test the GADS method, which is the best approaching ing the fully convolutional grasp quality model which can
direction selection method for 6-DoF grasping veriﬁed in evaluate entire 4D action space in parallel, GADS gains its
simulation, with a 7-DoF arm of a Baxter robot and a efﬁciency, searching only 2D action space through the cross
RealSense D435 depth camera attached to the arm as shown entropy method with initial seeds of surface normals. 6-DoF
in Figure 1. We compare the performance of GADS with grasping with GADS has been shown to be advantageous
the state-of-the-art 4-DoF grasp detection method, FC-GQ- especially in densely cluttered scenes, outperforming the
CNN [4], to validate that the hierarchical grasping with state-of-the-art 4-DoF grasping method. But GADS has a
GADS exploits the beneﬁt of 6-DoF grasping without using room for improvement. For example, ﬁne-tuning the 4-DoF
6-DoF grasp data. We prepare 20 objects in total including qualitymodelwithmore4-DoFgraspdatafromtheinclined
householdobjects,ofﬁcesupplies,andfruits,anddividethem camera view and utilizing multi-camera view to enhance the
into two sets of 10 objects each as shown in Figure 5. We quality of point clouds and synthesized depth maps will be
follow the same protocol as the simulation experiments on topics for our future work.
1558
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 14:19:05 UTC from IEEE Xplore.  Restrictions apply. REFERENCES [17] Y. Jiang, S. Moseson, and A. Saxena, “Efﬁcient grasping from rgbd
images:Learningusinganewrectanglerepresentation,”inProc.ofthe
[1] H.Ahn,S.Choi,N.Kim,G.Cha,andS.Oh,“Interactivetext2pickup IEEEInternationalConferenceonRoboticsandAutomation(ICRA),
networksfornaturallanguage-basedhumanrobotcollaboration,”IEEE May2011.
RoboticsandAutomationLetters,vol.3,no.4,pp.3308–3315,2018. [18] I.-M. Chen and J. W. Burdick, “Finding antipodal point grasps
[2] J.S.Park,C.Park,andD.Manocha,“Intention-awaremotionplanning on irregularly shaped objects,” IEEE transactions on Robotics and
using learning based human motion prediction,” in Proc. of the Automation,vol.9,no.4,pp.507–512,1993.
Robotics:ScienceandSystems(RSS),Jul.2017. [19] R. Y. Rubinstein and D. P. Kroese, The Cross-Entropy Method.
[3] J.Mahler,J.Liang,S.Niyaz,M.Laskey,R.Doan,X.Liu,J.A.Ojea, Springer-Verlag,2004.
and K. Goldberg, “Dex-net 2.0: Deep learning to plan robust grasps [20] D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang,
with synthetic point clouds and analytic grasp metrics,” in Proc. of D.Quillen,E.Holly,M.Kalakrishnan,V.Vanhoucke,andS.Levine,
theRobotics:ScienceandSystems(RSS),Jul.2017. “Scalabledeepreinforcementlearningforvision-basedroboticmanip-
[4] V. Satish, J. Mahler, and K. Goldberg, “On-policy dataset synthesis ulation,”inProc.oftheConferenceonRobotLearning,Oct.2018.
for learning robot grasping policies using fully convolutional deep [21] X.Yan,J.Hsu,M.Khansari,Y.Bai,A.Pathak,A.Gupta,J.Davidson,
networks,” IEEE Robotics and Automation Letters, vol. 4, no. 2, pp. andH.Lee,“Learning6-dofgraspinginteractionviadeepgeometry-
1357–1364,2019. aware3drepresentations,”inProc.oftheIEEEInternationalConfer-
[5] I.Lenz,H.Lee,andA.Saxena,“Deeplearningfordetectingrobotic enceonRoboticsandAutomation(ICRA),May2018.
grasps,”TheInternationalJournalofRoboticsResearch,vol.34,no. [22] J. Suh, J. Gong, and S. Oh, “Fast sampling-based cost-aware path
4-5,pp.705–724,2015. planning with nonmyopic extensions using cross entropy,” IEEE
[6] S.Levine,P.Pastor,A.Krizhevsky,J.Ibarz,andD.Quillen,“Learning TransactionsonRobotics,vol.33,no.6,pp.1313–1326,2017.
hand-eye coordination for robotic grasping with deep learning and [23] J.Long,E.Shelhamer,andT.Darrell,“Fullyconvolutionalnetworks
large-scale data collection,” The International Journal of Robotics for semantic segmentation,” in Proc. of the IEEE Conference on
Research,vol.37,no.4-5,pp.421–436,2018. ComputerVisionandPatternRecognition,Jun.2015.
[7] L. Pinto and A. Gupta, “Supersizing self-supervision: Learning to [24] D.Morrison,P.Corke,andJ.Leitner,“ClosingtheLoopforRobotic
grasp from 50k tries and 700 robot hours,” in Proc. of the IEEE Grasping: A Real-time, Generative Grasp Synthesis Approach,” in
international Conference on Robotics and Automation (ICRA), May Proc.ofRobotics:ScienceandSystems(RSS),Jun.2018.
2016. [25] A. Zeng, S. Song, K.-T. Yu, E. Donlon, F. R. Hogan, M. Bauza,
[8] J.RedmonandA.Angelova,“Real-timegraspdetectionusingconvo- D.Ma,O.Taylor,M.Liu,E.Romo,N.Fazeli,F.Alet,N.C.Daﬂe,
lutionalneuralnetworks,”inProc.oftheIEEEInternationalConfer- R. Holladay, I. Morona, P. Q. Nair, D. Green, I. Taylor, W. Liu,
enceonRoboticsandAutomation(ICRA),May2015. T. Funkhouser, and A. Rodriguez, “Robotic pick-and-place of novel
[9] A. ten Pas, M. Gualtieri, K. Saenko, and R. Platt, “Grasp pose objects in clutter with multi-affordance grasping and cross-domain
detection in point clouds,” The International Journal of Robotics image matching,” in Proc. of the IEEE International Conference on
Research,vol.36,no.13-14,pp.1455–1473,2017. RoboticsandAutomation,May2018.
[10] H. Liang, X. Ma, S. Li, M. Go¨rner, S. Tang, B. Fang, F. Sun, and [26] A.tenPasandR.Platt,“Usinggeometrytodetectgraspposesin3d
J. Zhang, “PointNetGPD: Detecting grasp conﬁgurations from point pointclouds,”inRoboticsResearch. Springer,2018,pp.307–324.
sets,”inProc.oftheIEEEInternationalConferenceonRoboticsand [27] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “Pointnet: Deep learning
Automation(ICRA),May2019. on point sets for 3d classiﬁcation and segmentation,” in Proc. of the
[11] A. Mousavian, C. Eppner, and D. Fox, “6-dof graspnet: Varia- IEEE Conference on Computer Vision and Pattern Recognition, Jul.
tional grasp generation for object manipulation,” arXiv preprint 2017.
arXiv:1905.10520,2019. [28] M.GualtieriandR.Platt,“Viewpointselectionforgraspdetection,”in
[12] F. T. Pokorny and D. Kragic, “Classical grasp quality evaluation: Proc.oftheIEEE/RSJInternationalConferenceonIntelligentRobots
New algorithms and theory,” in Proc. of the IEEE/RSJ International andSystems(IROS),Sep.2017.
ConferenceonIntelligentRobotsandSystems(IROS),Nov.2013. [29] A. Ten Pas and R. Platt, “Localizing handle-like grasp affordances
[13] V.-D.Nguyen,“Constructingforce-closuregrasps,”TheInternational in 3d point clouds,” in Experimental Robotics. Springer, 2016, pp.
JournalofRoboticsResearch,vol.7,no.3,pp.3–16,1988. 623–638.
[14] A. Zeng, K.-T. Yu, S. Song, D. Suo, E. Walker, A. Rodriguez, [30] E. Todorov, T. Erez, and Y. Tassa, “Mujoco: A physics engine
and J. Xiao, “Multi-view self-supervised deep learning for 6d pose for model-based control,” in Proc. of the IEEE/RSJ International
estimation in the amazon picking challenge,” in Proc. of the IEEE ConferenceonIntelligentRobotsandSystems(IROS),Oct.2012.
International Conference on Robotics and Automation (ICRA), May [31] L. Fan, Y. Zhu, J. Zhu, Z. Liu, O. Zeng, A. Gupta, J. Creus-Costa,
2017. S. Savarese, and L. Fei-Fei, “Surreal: Open-source reinforcement
[15] J.Mahler,F.T.Pokorny,B.Hou,M.Roderick,M.Laskey,M.Aubry, learning framework and robot manipulation benchmark,” in Proc. of
K. Kohlhoff, T. Kro¨ger, J. Kuffner, and K. Goldberg, “Dex-net 1.0: theConferenceonRobotLearning,Oct.2018.
Acloud-basednetworkof3dobjectsforrobustgraspplanningusing [32] K. Mamou and F. Ghorbel, “A simple and efﬁcient approach for
amulti-armedbanditmodelwithcorrelatedrewards,”inProc.ofthe 3d mesh approximate convex decomposition,” in Proc. of the IEEE
IEEEInternationalConferenceonRoboticsandAutomation(ICRA), internationalConferenceonImageProcessing(ICIP),Nov.2009.
May2016. [33] R. B. Rusu and S. Cousins, “3d is here: Point cloud library (pcl),”
[16] A. Herzog, P. Pastor, M. Kalakrishnan, L. Righetti, T. Asfour, and in Proc. of the IEEE International Conference on Robotics and
S.Schaal,“Template-basedlearningofgraspselection,”inProc.ofthe Automation(ICRA),May2011.
IEEEInternationalConferenceonRoboticsandAutomation(ICRA),
May2012.
1559
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 14:19:05 UTC from IEEE Xplore.  Restrictions apply. 