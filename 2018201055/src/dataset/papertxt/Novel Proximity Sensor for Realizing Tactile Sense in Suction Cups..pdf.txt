2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
3D Orientation Estimation and Vanishing Point Extraction from Single
Panoramas Using Convolutional Neural Network
Yongjie Shi, Xin Tong, Jingsi Wen, He Zhao, Xianghua Ying*, Hongbin Zha
Abstract—3D orientation estimation is a key component of
many important computer vision tasks such as autonomous
navigation and 3D scene understanding. This paper presents
a new CNN architecture to estimate the 3D orientation of an
omnidirectional camera with respect to the world coordinate
system from a single spherical panorama. To train the pro-
(a)
posedarchitecture,weleveragea datasetofpanoramasnamed
VOP60KfromGoogleStreetViewwithlabeled3Dorientation,
including50thousandpanoramasfortrainingand10thousand
panoramasfortesting.Previousapproachesusuallyestimate3D
orientation under pinhole cameras. However, for a panorama,
due to its larger ﬁeld of view, previous approaches cannot be
suitable. In this paper, we propose an edge extractor layer to
utilize the low-level and geometric information of panorama,
an attention module to fuse different features generated by
(b) (c)
previouslayers.Aregressionlossfortwocolumnvectorsofthe
rotationmatrixandclassiﬁcationlossforthepositionofvanish- Fig. 1. Vanishing points in the perspective images and panorama. (a)
ing points are added to optimize our network simultaneously. Someperspectiveimagesinastreetview,andtheircorrespondingpartsare
The proposed algorithm is validated on our benchmark, and markedoutinthepanorama(b).Duetothenarrowﬁeldofview,someofthe
experimental results clearly demonstrate that it outperforms threeorthogonalvanishingpointsmaybeabsentorundetectablefromthese
previous methods. images.(b)Thepanoramacontainsmostoftheinformationinthescenes.
Six vanishing points corresponding to three orthogonal directions are all
insidethepanorama,asmarkedwithdifferentcolors.(c)Theviewingsphere
I. INTRODUCTION
related to the panorama. The coordinate axes of the camera coordinate
system are drawn with different colors, which are corresponding to the
Determining 3D pose of a camera or an object from
threeorthogonalvanishingpoints.
images is a fundamental problem in computer vision. It has
a wide range of potential applications in our life, such as
unmanned aerial vehicles (UAVs), self-driving automobiles, edges based approaches. Bazin et al. [12] propose a curve
telepresence robots, virtual reality (VR) and etc. So many basedmethod.Theyﬁrstprojectthepanoramaintoasphere,
approaches are proposed over the past years [1], [2], [3], thenﬁndcirclesandtheintersectionsofthem.However,this
[4], [5], [6], [7], [8], [9], [10], [11]. But all of them are kind of method are highly dependent on the accuracy of
based on the pinhole camera. circle detection and cannot be applied to the more complex
Given a pinhole camera, due to its narrow ﬁeld of view, scene. The CNNs techniques have high capacity to extract
some of the three orthogonal vanishing points may be features, and it may be a very good choice to detect the
absent or undetectable from images taken in some view vanishing points in panoramas. Unlike previous 3D rotation
directions (see Fig. 1a). Thereby, it is difﬁcult to identify estimation methods using Euler angles or the quaternion as
all the vanishing points in the images. For the panorama annotation[1][13],inthispaper,wedirectlyemploythetwo
with 360 degrees ﬁeld of view, all of the vanishing points column vectors of the 3D rotation matrix as the annotation.
corresponding to three orthogonal directions always appear The column vectors of the rotation matrix correspond to
in a single panorama (see Fig. 1b). Note that, the panorama the three orthogonal vanishing points (see Fig. 1c), which
and its relative viewing sphere contains totally six vanishing have strong semantic and geometric meaning. In this paper,
points(twoforeachofthreeorthogonaldirections),whereas we propose a new method to estimate the orientation of
a perspective image contains at most three vanishing points. panorama by introducing edge information and utilizing the
However, the lines of images in 3D space are often pro- fusion of multiple features to improve the performance.
jected into curves in panoramas, as shown in Fig. 1b, which Deep learning based methods need a lot of data to ﬁt.
makes detecting vanishing points not easy by using lines or However,thereexistsnopubliclarge-scaleoutdoorpanorama
database containing the labeled orientation with respect to
*Correspondingauthor
the world coordinates. To train our proposed network, we
The Authors are with Key Laboratory of Machine Perception, Peking
University, Beijing, P.R.China, 100871. E-mail: shiyongjie@pku.edu.cn, leverage a dataset of panoramas from Google Street View
xintong@pku.edu.cn,wenjingsi@pku.edu.cn,zhaohe97@pku.edu.cn,xhy- with labeled orientation including 50 thousand panoramas
ing@cis.pku.edu.cn,zha@cis.pku.edu.cn
fortrainingand10thousandpanoramasfortesting.Wename
This work was supported in part by State Key Development Program
GrandNo.2016YFB1001001,andNNSFCGrantNo.61971008. our dataset as VOP60K, which ”P” is the abbreviation of
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 596
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 15:46:40 UTC from IEEE Xplore.  Restrictions apply. the panorama contains little information about the vanishing
points vertical to the ground. If the camera faces to the ﬂy,
it contains little that about the horizontal vanishing points.
Therefore, the pitch is sampled in the uniform distribution
from-70degreesto60degrees.Weusuallymakethecamera
vertical to the ground approximately when taking photos
in life, so the roll is sampled in the uniform distribution
(a) (b) (c)
from -30 degrees to 30 degrees. Because of sampling many
Fig.2. AnnotationprocessofPOV60K.Givenapanorama,weﬁrstproject different kinds of locations randomly, our dataset contains
it to rectilinear image. Then we label the three vanishing points in the
differentscenes,includingcommercialarea,industrialbuild-
rectilinearimage.Thelaststepistoreprojectthelabeledvanishingpoints
totheinitialpanorama. ings, residential area, parking structures, and so on.
After collecting the panoramas from Google Street View,
the next step is to annotate the orientation of panoramas.
panorama, ”V” is the abbreviation of vanishing points and In this paper, we use rotation matrix R to represent the
”60K”denotesthetotalnumberofourdatasetis60thousand. orientation of omnidirectional camera relative to world co-
In summary, the main contributions of this study include: ordinate. R = [r ,r ,r ], where r , r and r denote
1 2 3 1 2 3
• We propose a new CNN structure to estimate the ori- three column vectors of the rotation matrix, which are also
entation of the omnidirectional camera by introducing three orthogonal vanishing points on the viewing sphere (a
edges information, attention mechanism, and multi-loss unit sphere). So the vanishing points corresponding to these
constraints. directions are required to be annotated in the panorama. We
• We construct VOP60K dataset to estimate the rotation ﬁrstly transform a panorama image into rectilinear images.
oftheomnidirectionalcamerawithrespecttotheworld Thenweannotatevanishingpointsintheserectilinearimages
coordinate system. manually. At last, we reproject the detected vanishing points
• Experiments demonstrate that our network architecture to the original panorama. The annotation process is shown
achieves better performance than previous methods. in Fig. 2.
WedescribethedetailsofconstructingVOP60KinSec.II. OurVOP60Kcontains60thousandpanoramaslabeled3D
InSec.III,wedesignournetworkarchitecture.Theproposed orientation and vanishing points, which has the largest total
network is evaluated on VOP60K in Sec. IV. Finally, some amountwithcompleteannotatedinformation,ascomparedto
concluding comments are provided in Sec. V. someexistingdatasets.SomepanoramasareshowninFig.3.
II. CONSTRUCTIONOFVOP60KDATABASEWITH
KNOWN3DROTATION
Panoramascontainraysof360degreesviewingdirections
ofscenes,whichmaybemorefeasibletoestimatetheorien-
tationofthecameracomparedwithimagestakenbypinhole
camera. There are several existing panorama datasets, but
theycannotsatisfyourdemand.Carusoetal.[14]construct Fig. 3. Our dataset contains different scenes downloaded from Google
aslamdatasetfortheomnidirectionalcamera,buttheimages StreetView.
are visible only in 180 degrees, not in 360 degrees, which
There should be some metrics to measure the annotation
miss rays in a half sphere of the scene. Xiao et al. [15]
errorofVOP60K.Althoughthereexistsnorealgroundtruth
also collect the dataset used to estimate the camera view.
ofcamerarotation,weknowthefactthatthevanishingpoints
However,thereisonly1Drotationangleinformationontheir
dataset. areorthogonalwithe(cid:88)ach(cid:13)otherinstructureds(cid:13)cene.Basedon
(cid:13) (cid:13)
Google Street View has been considered as an online it, the annotation error ca(cid:13)n be deﬁned as foll(cid:13)ows:
browsabledatasetwhichconsistsofbillionsofvariousstreet- 1 − π
δ = arccos(rTr ) (1)
level panoramas around the world [16]. Since its diversity 3 i j 2
andhugenumbers,wecollectdatafromGoogleStreetView i,j
(cid:54)
conveniently. First, we select ten richest cities in the USA, where i,j = 1,2,3,i = j. The rotation annotation error of
and then we select different areas in these cities randomly. omnidirectional camera with respect to the world coordinate
Afterthat,wesamplethelocationsrandomlyintheseareasto system is shown in Fig. 4. As can be seen from Fig. 4, 90%
◦
download panoramas. Since Google collected these panora- of images in VOP60K are under 2 annotation error, which
masevery10metersintherealworld,ourminimalsampling shows that VOP60K has high annotation accuracy.
rate is 10 meters. Totally, we choose 15023 locations as
our candidate to sample panoramas. Considering that our III. METHODS
model is to infer the orientation of the camera, we sample An overview of our network architecture is illustrated in
the horizontal rotation angle in a uniform distribution from Fig. 5. As the third column r of rotation matrix can be
× 3
0 degrees to 360 degrees. If the camera faces to the ground, obtained by r r , we only predict the ﬁrst two columns
1 2
597
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 15:46:40 UTC from IEEE Xplore.  Restrictions apply. Especiallyforcurvesofbuildingsandroads,asthevanishing
points are intersections of them. In this paper, we provide
1.0
our network with edge information in panorama as prior
0.9
knowledge to make it perform better. Our edge extractor
0.8
contains several operations. Firstly, we use two Sobel oper-
0.7
ators w and w along x and y axis to extract the gradients
y x y
ac0.6 of corresponding direction separately. wx and wycan be
ur0.5 described as
c    
Ac0.4 −1 0 1 1 2 1
−
0.3 w = 2 0 2 ,w = 0 0 0 . (2)
x − y − − −
0.2 1 0 1 1 2 1
0.1
Extracted gradient can be described as
0.00.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 ∗
Threshold (°) gxi =fliow ∗wx,i=1,2,...,C (3)
gi =fi w ,i=1,2,...,C
Fig.4. Annotationerrorofvanishingpoints.90%ofimagesinVOP60Kare y low y
◦
under2 annotationerror,whichshowsthatVOP60Khasahighannotation where fi represents input feature map of the ith channel,
accuracy. low ∗
C denotesthenumberofchannels. denotesconvolution,gi
x
TABLEI denotesthegradie(cid:113)ntalongxdirectionfortheithchannel,gyi
ARCHITECTUREDETAILSOFOURNETWORK.HOURGLASSMODULEIS denotes that along y direction. Then we can get the gradient
THESAMEAS[17].THEFIRSTTERMINSQUAREBRACKETSDENOTES g by
THEINPUTCHANNELANDTHEOUTPUTCHANNEL.THESECONDTERM gi = (gi)2+(gi)2,i=1,2,...,C. (4)
x (cid:26) y
DENOTESTHEKERNELSIZE.S1/2DENOTESTHECONVOLUTIONSTRIDE.
The ﬁnal edge map can be represented as
Name Layerproperties Outputsize ≥
Convs × × fi = gi, gi α (5)
Conv+BN+Relu [3*64,7*7,S2] 64 H/2 W/2 edge 0, gi <α
× ×
Conv+BN [64*256,7*7,S2] 256 H/4 W/4
Score where α denotes the threshold parameter of the ith channel,
× ×
Conv+BN+Softmax [256*2,1*1,S1] 2 H/4 W/4 fi denotes the edge feature map of the ith channel. The
edge
Score edge extractor proposed by ourselves can be implemented
× ×
Conv+BN [2*256,1*1,S1] 256 H/4 W/4 efﬁciently. In all of our experiments, we choose α=0.3.
B. Attention Module
of rotation matrix which correspond to two vanishing points
For the ﬁrst stacked layer, we obtain four output feature
in the panorama. The details of our network can be seen
maps,includingtheinputofhourglassmodule,theoutputof
in Table I. The input of our network is a panorama whose
× × thehourglassmodule,edgefeaturesandtheoutputbyscore
shape is 256 512 3. The outputs of our network are layer.Differentfeaturemapsrepresentdifferentinformation.
two probability maps for two vanishing points, whose shape
× For example, the feature map output by hourglass module
is 128 256. Each point in probability map denotes the contains more global information, the feature map input to
possibility of it being a vanishing point. Given a panorama,
the ﬁrst hourglass module contains more source information
we ﬁrst use several convolution layers to extract the low-
while the extracted edge map contains more low level
levelinformation.Consideringthefactthat,vanishingpoints
and geometry information. In previous methods, researchers
appeared on different locations are interdependent. So we
mainly add all the information directly and propagate it
adopt a two blocks based hourglass module proposed by
to next layer [17]. However, for different kinds of feature
Newell et al. [17] to get the global information. The output
maps, they have different effects on the rotation estimation.
of hourglass module is processed by the score module in
The network needs to utilize these feature maps differently.
order to add intermediate or ﬁnal supervision. The score
Unlike previous methods [18], [19] who apply channel or
module and score module are used to keep the same shape
spatialattentionseparately,weﬁrstfusethefourinputfeature
as the output by hourglass module. Besides, we propose an
maps and then choose the important parts along the channel
edge extractor, attention module, and impose a classiﬁcation
direction. At last, we apply different spatial attention for
loss on our network. We will describe the details of them in
different input feature maps. The details of our attention
the following sections.
mechanism can be seen in Fig. 6. We ﬁrstly concatenate
fourdifferentinputfeaturemapsalongthechanneldirection,
A. Edge Extractor
∈ R × × ×
denoted by F (4 256) 64 128. Our method for
Previousmethodstoestimatetherotationofcamerausing concat
obtaining attention is ﬁrstly to generate the summarized
CNNs mainly train an end to end network [1] which cannot
feature map as following:
explicitly use the geometry clues. For pose estimation in
∗
panorama,itisimportanttoutilizethelowlevelinformation. F =W F +b (6)
conv concat
598
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 15:46:40 UTC from IEEE Xplore.  Restrictions apply. Fig. 5. Our network structure. Given a panorama as input, our network outputs two probability maps for two vanishing points. We take the hourglass
moduleasourbasecomponentasitcangetglobalinformation.Besides,wedevelopanedgeextractortoutilizetheedgeinformationandattentionmodule
tofusedifferentinformation.Atlast,weusetheregressionlossforr1andr2andclassiﬁcationlossforvanishingpointspositiontooptimizeournetwork.
simultaneouslytofuseoutputfeaturesgeneratedbyprevious
layers.
C. Multi-Loss Constraints
Our network generates two probability maps for two
vanishingpoints,seeFig.5.Consideringthecomputingcost,
×
we set the shape of probability maps are 128 256. Every
pointintheprobabilitymapdenotesthepossibilityofitbeing
a vanishing point. Considering the fact that, we can warp
a panorama into a unit sphere. Every point in panorama
corresponds to a unit vector in 3D space. For a point in
Fig.6. Attentionmodule.Weﬁrstlyconcatenatefourinputfeaturemaps
along channel direction to get Fconcat. Then convolution and softmax probabilitymap,thehigherprobabilityofitbeingavanishing
layersareaddedtogetattentionmapFatt.WemultiplyFattandFconcat pointinpanorama,thehigherpossibilityofitbeingacolumn
tdoirgecettiothneswtoeiggehttetdhefeoaututpruetmFaopuFtpwuetg..Finally,weaddFwegalongchannel vector(r1orr2)intheunitsphere.Anaiveandintuitiveway
is that using the output probability map, we can weight all
unitvectorscorrespondingtoallpixelsinpanorama,andadd
∗
where denotes convolution operation., W denotes the them together to represent the column vector of the rotation
convolution ﬁlters, and b is the bias. F ∈ R4×64×128 matrix. The column ve(cid:88)ctor(cid:88)can be obtained by
conv
summarizes information of all channels in F . Then we
concat 128 64 ∗
apply softmax along the channel dimension for Fconv to get rˆi = pˆi,x,y qx,y (9)
theattentionmapF .Supposingfc,x,y denotesthevaluein
att conv x=1y=1
cth channel at position (x,y) in the feature map F . The
conv where i=1,2, rˆ represents the ith vanishing point in unit
Softmax operation is applied to f along dim channel as i
follows: (cid:80) conv spherepredictedbyCNNs,anditalsocorrespondstotheith
fc,x,y = efcco,xn,vy (7) coofltuhmenitvhecchtoarnnoefl.thqe rotraetpiorensemnatstritxh.epˆsiphreeprerecseonotrsdihneaatetmsaopf
att 4c=1efcco,xn,vy position(x,y)inthexo,yutputprobabilitymap.Thenwedeﬁne
where fact,xt,y denotes the value of attention map Fatt in tthheeLou2tpnuotrmanadstlhoessgfruonucntido(cid:88)ntrutothm. easurethedistancebetween
the cth channel at position (x,y), x = 1,2,...,128, y =
1,2,...,64. Then the attention map Fatt is applied to the L 2 (cid:107) − (cid:107)
feature map Fconcat by broadcasting and dot product which reg = rˆi ri 2 (10)
can be described as i=1
(cid:12) Equation9hasmanytrivialsolutions,anditcannotpredict
Fweg =Fconcat Fatt (8) the precision position of vanishing points in the panorama.
In order to solve it, the network is added with another
whereF isnewfeatureweightedbyattentionmapF ,
(cid:12) weg atten classiﬁcation loss to classify if the current position is the
denotes broadcasting φ to the same size of F and
concat vanishing point or not in panorama. In this paper, we use
element-wise product. Through the above operation, the
(cid:88)(cid:88)(cid:88)
cross-entropy loss which can be described as
information which is important can be excited, while that is
trivial will be suppressed. At last, we add the weighted four L − 2 128 64
feature map together to get Foutput. Compared to traditional cls = (pi,x,ylogpˆi,x,y (11)
attention mechanism, we make different kinds of features i=1x=1y=1
− −
have different weights in spatial and channel dimension +(1 p )log(1 pˆ ))
i,x,y i,x,y
599
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 15:46:40 UTC from IEEE Xplore.  Restrictions apply. where pˆ represents the probability of the ith vanishing
i,x,y
pointexistinginthepositionof(x,y)predictedbynetworks,
andp denotesthegroundtruth.Thetwolossisdescribed
i,x,y
as follows:
L L ∗L
= +λ (12)
reg cls
where λ is the coefﬁcient to balance the regression and
classiﬁcation loss. In all of our experiments, we choose
λ=0.4.
IV. EXPERIMENTS
A. Evaluation
There are two kinds of common error metrics to evaluate
the rotation error [20] as fo(cid:88)llows: (a) PCKcurveswithcross-entropyloss.
2 (cid:107) − (cid:107)
Err1= (cid:13) rˆ r(cid:13), (13)
(cid:13) i (cid:13)i
(cid:13) (cid:13)
i=1
Err2= logRRˆT (14)
where R denotes the ground truth of the rotation, Rˆ denotes
the predicted rotation by network. The two evaluations
mainlymeasuretheerroroftheestimatedrotation.However,
we also want to know the distribution of our estimation. So
weadoptthePercentageCorrectKeypoints(PCK)[17]ofthe
two kinds of errors. In detail, we select different thresholds
for this two errors, and then for different thresholds, we
count the percentage of the estimation rotation error less (b) PCKcurveswithoutcross-entropyloss.
than the corresponding threshold, which is called accuracy
Fig.7. ThePCKcurvesinVOP60Kofourmethod.(a)Thetestaccuracy
corresponding to this threshold, as shown in Fig. 7. ofdifferentmodelswithaddingcross-entropyloss.(b)Thetestaccuracyof
differentmodelswithoutaddingcross-entropyloss.
B. Results and Analysis
TABLEII improvement brought by introducing the classiﬁcation and
INFLUENCEOFDIFFERENTPARTS regression loss. Comparing the model with regression and
classiﬁcation losses and that with only regression loss, we
Methods Err1 Err2 ﬁnd the former model can make much higher performance
Reg 2.399 2.163 than the latter. After adding the classiﬁcation loss, the
Edge+Reg 1.841 1.896
accuracy of the model improve much. Fig. 8 also visualizes
Attention+Reg 1.825 1.707
Reg+Cla 0.093 0.064 the rotation predicted by our proposed model and baseline.
Edge+Reg+Cla 0.088 0.058 Our method is robust to extremely difﬁcult case, e.g., the
Attention+Edge+Reg+Cla 0.079 0.053 texture is rare.
To gain insights in how classiﬁcation model works, we
We denote the experiment which is only added with compare the baseline model with the proposed model by
regression loss as baseline. Ablation study can be seen in visualizing the feature map in last layer, the score maps as
Table II. The best model is adding the attention module, demonstrated in Fig. 9. We observe the baseline model may
edge extractor and cross-entropy loss simultaneously which have difﬁculty in localizing the vanishing points accurately.
is the last row in the table. To investigate the performance Buttheclassiﬁcationmodelcanlocalizeitmoreﬁnely,which
of the proposed Edge extraction model, attention model of is beneﬁt for predicting the rotation.
multiple kinds of features and multiple losses conveniently,
C. Compare with Previous Methods
PCK curves are visualized in Fig. 7.
Weﬁrstevaluatetheedgeextractor.Byaddingtheedgeex- Bazin et al. [12] propose a rotation estimation method
tractor, we get 60% PCK score, which is a 2% improvement which is the most related to our work. We compare our
comparedtothebaselinemodelforthreshold0.09,asshown methodwithitinourtestdatasetinthissection.Alexnet[21]
inFig.7a.Bygeneratingattentionmapsfrommultiplekinds and VGG [22] are proposed to solve the problem of image
of features, our method obtains a further 9% improvement classiﬁcation. We modiﬁed the last fully connected layer to
for threshold 0.09, as shown in Fig. 7a. We also show the output r and r to compare with our method. In addition
1 2
600
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 15:46:40 UTC from IEEE Xplore.  Restrictions apply. Fig.8. Thevisualizationoftheestimatedresultsofthebaselineandourproposednetwork.Theﬁrstcolumnrepresentstheoriginalinputimages.The
second column represents the visualization of the estimated results based on the original hourglass net only with adding regression loss. The third one
representsthosebasedonhourglassnetwithonlyaddingclassiﬁcationloss.Theforthonerepresentsthosebasedonhourglassnetwithaddingclassiﬁcation
lossandedgeextractor.Theﬁfthonerepresentsthosebasedonournetwithaddingclassiﬁcationloss,edgeextractorandattentionmodule.Theboldred,
greenandblueaxesrepresentthegroundtruthoftheorientationofthecamera.Thedottedred,greenandblueaxesrepresentthepredictedresults.
TABLEIII
COMPARISONWITHOTHERMETHODS
Method Err2
Alexnet[21]+Rotationmatrix 0.137
Alexnet[21]+Eulerangles 2.0126
Alexnet[21]+Quaternion 0.296
Vgg16[22]+Rotationmatrix 0.120
Vgg16[22]+Eulerangles 1.732
Vgg16[22]+Quaternion 0.183
Bazinetal.[12] 0.091
Ourmethod 0.053
matrix, quaternion is lack of geometric meaning as the ﬁrst
two column vectors of rotation matrix correspond to the two
Fig.9. Featuremapvisualizationofmodelwithcross-entropyloss(theﬁrst
column)andthatofmodelwithoutone(thesecondcolumn).Thelocations vanishing points. Even using two column vector as label for
◦
ofvanishingpointswithgroundtrutharemarkedwith“ ”,andthelocations Alexnet and Vgg16, Our method still outperforms both of
×
ofpredictedvanishingpointsaremarkedwith“ ”.
them.
V. CONCLUSIONS
to rotation matrix, there are several other formats used to
representtheorientationofrigidbodywithrespecttoaﬁxed In this paper, we propose a new CNN architecture to
system, including Euler angles, quaternion.To compare the estimate the omnidirectional camera orientation with respect
performance of different representation on Alexnet [21] and to the world from single panorama. To train our network,
VGG [22], we modiﬁed the last layer of Alexnet [21] and we leverage a dataset named VOP60K with labeled 3D
VGG [22] to output Euler angles and quaternion. L loss orientation including 50 thousand panoramas for training
2
is used to optimize these networks. Results can be seen in and 10 thousand panoramas for testing. Our key idea is to
Table III. As can be seen in Table III, our method achieves train an attention-based CNN that regresses the ﬁrst column
the state-of-the-art result. Traditional method proposed by vectorandthesecondvectoroftherotationmatrix,classiﬁes
Bazin et al. [12] relying on curves detection performs not thevanishingpointssimultaneouslybyintroducinglow-level
so good compared with ours. It might be related to the and geometric information. Ablation study is done to show
fact that our dataset has more complex scenes. For Alexnet the effectiveness of each parts of our proposed network
andVgg16,resultsofusingrotationmatrixasrepresentation architecture. Further experiments show that our network
perform better than that of using euler angles or quaternion. outperformspreviouslearning-basedandtraditionalmethods.
When employing Euler angles as the representation, the pe- Although what we discuss is to estimate camera orientation
riodic issues of trigonometric functions may be very serious from one panorama, our approach can also be extended to
in case of wide range of rotation. Compared to the rotation multiple panoramas.
601
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 15:46:40 UTC from IEEE Xplore.  Restrictions apply. REFERENCES urbanenvironment,”TheInternationalJournalofRoboticsResearch,
vol.31,no.1,pp.63–81,2012.
[1] S. Mahendran, H. Ali, and R. Vidal, “3d pose regression using con- [13] A.Kendall,M.Grimes,andR.Cipolla,“Posenet:Aconvolutionalnet-
volutionalneuralnetworks,”inProceedingsoftheIEEEInternational workforreal-time6-dofcamerarelocalization,”inIEEEInternational
ConferenceonComputerVision,2017,pp.2174–2182. ConferenceonComputerVision,2015,pp.2938–2946.
[2] M.GaronandJ.-F.Lalonde,“Deep6-doftracking,”IEEEtransactions [14] D. Caruso, J. Engel, and D. Cremers, “Large-scale direct slam for
on visualization and computer graphics, vol. 23, no. 11, pp. 2410– omnidirectional cameras,” in IEEE/RSJ International Conference on
2418,2017. IntelligentRobotsandSystems,2015,pp.141–148.
[3] W. Kehl, F. Manhardt, F. Tombari, S. Ilic, and N. Navab, “Ssd-6d: [15] J.Xiao,K.A.Ehinger,A.Oliva,andA.Torralba,“Recognizingscene
Making rgb-based 3d detection and 6d pose estimation great again,” viewpointusingpanoramicplacerepresentation,”inIEEEConference
in IEEE Conference on Computer Vision and Pattern Recognition, onComputerVisionandPatternRecognition,2012,pp.2695–2702.
2017,pp.1521–1529. [16] M.Cavallo,“3dcityreconstructionfromgooglestreetview,”Comput.
[4] D.Jia,Y.Su,andC.Li,“Deepconvolutionalneuralnetworkfor6-dof Graph.J,2015.
imagelocalization,”arXivpreprintarXiv:1611.02776,2016. [17] A. Newell, K. Yang, and J. Deng, “Stacked hourglass networks for
humanposeestimation,”inEuropeanConferenceonComputerVision.
[5] E.Brachmann,F.Michel,A.Krull,M.YingYang,S.Gumhold,etal.,
Springer,2016,pp.483–499.
“Uncertainty-driven 6d pose estimation of objects and scenes from
[18] J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” in
a single rgb image,” in IEEE Conference on Computer Vision and
Proceedings of the IEEE conference on computer vision and pattern
PatternRecognition,2016,pp.3364–3372.
recognition,2018,pp.7132–7141.
[6] J.Kosˇecka´ andW.Zhang,“Videocompass,”inEuropeanconference
[19] S. Woo, J. Park, J.-Y. Lee, and I. So Kweon, “Cbam: Convolutional
oncomputervision. Springer,2002,pp.476–490.
blockattentionmodule,”inProceedingsoftheEuropeanConference
[7] J.M.CoughlanandA.L.Yuille,“Manhattanworld:Compassdirec-
onComputerVision(ECCV),2018,pp.3–19.
tionfromasingleimagebybayesianinference,”inIEEEInternational
[20] D. Q. Huynh, “Metrics for 3d rotations: Comparison and analysis,”
ConferenceonComputerVision,vol.2,1999,pp.941–947.
JournalofMathematicalImagingandVision,vol.35,no.2,pp.155–
[8] E.Rublee,V.Rabaud,K.Konolige,andG.Bradski,“Orb:Anefﬁcient
164,2009.
alternative to sift or surf,” in IEEE International Conference on
[21] A.Krizhevsky,I.Sutskever,andG.E.Hinton,“Imagenetclassiﬁcation
ComputerVision,2011,pp.2564–2571.
with deep convolutional neural networks,” in Advances in neural
[9] H.Bay,A.Ess,T.Tuytelaars,andL.VanGool,“Speeded-uprobust
informationprocessingsystems,2012,pp.1097–1105.
features(surf),”Computervisionandimageunderstanding,vol.110,
[22] K. Simonyan and A. Zisserman, “Very deep convolutional networks
no.3,pp.346–359,2008.
for large-scale image recognition,” arXiv preprint arXiv:1409.1556,
[10] D. G. Lowe, “Distinctive image features from scale-invariant key-
2014.
points,” International journal of computer vision, vol. 60, no. 2, pp.
91–110,2004.
[11] M. E. Antone and S. Teller, “Automatic recovery of relative camera
rotationsforurbanscenes,”inIEEEConferenceonComputerVision
andPatternRecognition,vol.2,2000,pp.282–289.
[12] J.-C. Bazin, C. Demonceaux, P. Vasseur, and I. Kweon, “Rotation
estimationandvanishingpointextractionbyomnidirectionalvisionin
602
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 15:46:40 UTC from IEEE Xplore.  Restrictions apply. 