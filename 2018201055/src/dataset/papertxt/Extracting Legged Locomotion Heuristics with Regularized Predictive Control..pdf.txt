2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
Training Adversarial Agents to Exploit Weaknesses in Deep Control
Policies
Sampo Kuutti, Saber Fallah, Richard Bowden
Abstract—Deep learning has become an increasingly com- edge cases where collisions are more likely to occur, would
mon technique for various control problems, such as robotic be seen rarely [21]. Therefore, by using an adversarial agent
arm manipulation, robot navigation, and autonomous vehicles.
whoseaimistodeliberatelycreatetheseedgecasescenarios,
However, the downside of using deep neural networks to learn
betterinsightsintopossiblefailurecasescanbeobtainedwith
control policies is their opaque nature and the difﬁculties of
validating their safety. As the networks used to obtain state- reduced training times.
of-the-art results become increasingly deep and complex, the The concept of utilising an adversarial agent to disturb
rules they have learned and how they operate become more a machine learning agent has been suggested previously,
challenging to understand. This presents an issue, since in
for example, by Morimoto & Doya [22], who used an
safety-criticalapplicationsthesafetyofthecontrolpolicymust
actor-disturbor-critic method, where the disturbor aimed to
beensuredtoahighconﬁdencelevel.Inthispaper,wepropose
anautomatedblackboxtestingframeworkbasedonadversarial ﬁnd the worst disturbance to reduce the performance of a
reinforcement learning. The technique uses an adversarial controller. This was used in the training loop of a rein-
agent, whose goal is to degrade the performance of the target forcement learning agent to improve the robustness of the
model under test. We test the approach on an autonomous
control policy to disturbances, and was demonstrated in an
vehicle problem, by training an adversarial reinforcement
invertedpendulumtask.Theframeworkwasextendedtouse
learning agent, which aims to cause a deep neural network-
driven autonomous vehicle to collide. Two neural networks DNNs for estimating the control policy and disturbances
trained for autonomous driving are compared, and the results in a deep reinforcement learning framework by Pinto et
from the testing are used to compare the robustness of their al. [23], and was demonstrated successfully in a robotic
learnedcontrolpolicies.Weshowthattheproposedframework
manipulation task. For autonomous vehicles, the idea of
is able to ﬁnd weaknesses in both control policies that were
learning to automatically ﬁnd failure cases was suggested
not evident during online testing and therefore, demonstrate a
signiﬁcant beneﬁt over manual testing methods. as early as 1992, by Schultz et al. [24], who used genetic
algorithms to ﬁnd test cases that exposed weaknesses in
I. INTRODUCTION autonomous aerial vehicle controllers. The results suggested
this could be an effective alternative to manual testing of
The rise of deep learning has resulted in rapid progress
complexsoftwarecontrollers.Inmorerecentwork,Behzadan
in many ﬁelds, with state-of-the-art results obtained in ﬁelds
& Munir [25] demonstrated that a reinforcement learning
suchasimageclassiﬁcation,soundrecognition,andlanguage
agent could be trained to create collisions with other road
processing [1]–[3]. The strong capability of Deep Neural
vehicles, by training an agent to collide against two agents,
Networks(DNNs)formodellinghighlynon-linearandcom-
a DNN and a rule-based system. The number of episodes to
plexfunctionshasresultedintheadoptionofDNNsinmany
convergence and minimum time-to-collision were then used
control problems. Important results in control applications
to argue the DNN was the safer control policy. However, by
such as robotic arm manipulation, robot navigation, and
having no constraints on the adversarial agent it is likely to
autonomousvehiclecontrolhavebeenachievedthroughdeep
learnabehaviourunlikeanyhumandriver,whichcouldlimit
learning [4]–[10]. However, in safety-critical applications,
insightsintoplausiblecollisioncasesthatmighthappenifthe
the safety of the control policy must be fully guaranteed be-
DNN control policies were deployed in the real world. For
foreitiscommerciallydeployable.Thispresentsasigniﬁcant
instance,intheexamplesshownbyBehzadan&Munir[25],
obstacle to the deployment of DNN-based control policies
the adversarial agent approached the target vehicle from the
in safety-critical applications such as autonomous driving
rear at high velocity, making collision avoidance extremely
[11], [12]. As the operational environment of the system
difﬁcult.Moreover,thistypeofcollisiondoesnotnecessarily
becomes increasingly complex, it becomes infeasible to test
represent a vulnerability in the control policy under test, as
the control policy in all possible scenarios it may encounter
the adversarial agent would be considered at fault in a real
[13]–[16]. Therefore, methods for testing and understanding
worldcollision[26].Perhapstheclosestworktoourresearch
the safety of these opaque systems are necessary [17]–[20].
is Adaptive Stress Testing (AST) by Koren et al. [27]. AST
Moreover, in tasks such as autonomous driving, testing the
aimstoﬁndthemostlikelycollisioncasesforanautonomous
systeminanaturalisticdrivingenvironmentwouldmeanthat
vehicle by manipulating the actions of pedestrians in the
SampoKuuttiandSaberFallaharewiththeConnectedandAutonomous simulation environment and the noise in the observations
Vehicles Lab, University of Surrey, Guildford, GU2 7XH, UK. Email: of the control policy under testing. However, this approach
{ }
s.j.kuutti,s.fallah @surrey.ac.uk
has several weaknesses which limit the insight it can offer
RichardBowdeniswiththeCentreforVision,SpeechandSignalProcess-
ing,UniversityofSurrey,GU27XH,UK.Email:r.bowden@surrey.ac.uk into the vulnerabilities in the autonomous system under
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 108
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 10:21:11 UTC from IEEE Xplore.  Restrictions apply. testing. For example, in majority of the collisions found, are then updated, such that the expected future rewards are
the blame for the collision would fall on the pedestrians maximised. As the agent interacts with the environment it
controlled by AST. Furthermore, the AST framework was learns through trial-and-error a state-action mapping for an
∗
only evaluated on a simple rule-based vehicle following optimal policy π (s ), which maximises the discounted sum
t
system. Instead, in our approach there are constraints on ofrewardsovertimegivenbythereturnsR .Therefore,this
t
the behaviour of the adversarial agent to maintain plausible exploration of the operational environment can be leveraged
(cid:88)
driving trajectories and the focus is to ﬁnd vulnerabilities to explore potential weaknesses in black box systems.
whichleadtocollisionswheretheautonomousvehiclebeing ∞
tested is at fault. Moreover, the observations of the system R = γkr (1)
t t+k
under testing are not manipulated in any way, therefore all
k=0
collision cases found by the proposed framework demon- ∈
where γ [0,1] is the discount factor used to prioritise
strate a vulnerability in the learned deep control policy.
immediate rewards over future rewards.
In this paper, we propose a technique for targeted black
box testing, using a reinforcement learning algorithm to B. Reinforcement Learning
ﬁnd the test scenarios which are most likely to cause the
Inourframework,thealgorithmusedtotraintheadversar-
black box control policy to fail. The proposed system has
ial agent is Advantage Actor Critic (A2C) [34], which uses
no knowledge of the internal mechanisms of the control
an actor-critic network architecture, as shown in Fig. 1. The
policy under testing, but instead learns a behaviour which ∗
actor network estimates the optimal policy function π (s ),
ﬁnds failure cases for the control policy. In this way, the t
which aims to maximise the expected rewards. Meanwhile,
powerful function approximation capabilities of DNNs are
the critic network estimates the value of being in a given
used to ﬁnd the weaknesses in other DNNs, and the testing
state, with the Value function V(s). The weights of both
procedure can therefore be fully automated. The proposed
networks are then updated based on the Advantage function
framework is tested in an autonomous driving problem,
A(s ,a ):
where the Adversarial Reinforcement Learning (ARL) agent t t E |
V(s)= [R s =s] (2)
is attempting to cause a vehicle following model to crash. t t
Note that our approach is distinct to work on adversarial Q(s,a)=E[R |s =s,a] (3)
t t
attacks [28]–[30], as we are not manipulating the inputs
to the target DNN, instead we place another agent in the (cid:88)−
A(s ,a )=Q(s ,a ) V(s )
sameenvironmentwhichaimstodeliberatelycausethetarget t t t t t
−
control policy to fail. Similarly, our approach is distinct to ≈n 1 −
γkr +γnV(s ) V(s ) (4)
research into adversarial robustness [31]–[33], as we do not t+k t+n t
aim to train the model to be robust to adversarial examples, k
E
instead we aim to leverage the adversarial agent to ﬁnd Where denotes expectation, V(s ) is the value function,
t
failure cases in the target models more reliably than manual and Q(s ,a ) is the quality function estimating the value of
t t
testing methods can, and understand the weaknesses present each action for a given state [35], [36].
in the deep control policies.
The remainder of this paper is structured as follows.
Section II presents the necessary background, methodology,
andgeneralframeworkbehindARL.Thesimulationsresults
of the vehicle following use case are presented in Section
III. Finally, concluding remarks are given in Section IV.
II. METHODOLOGY
A. Markov Decision Processes Fig.1:Anactor-criticnetworkarchitecture.Thedashedlines
Reinforcement learning allows an agent to learn through represent network updates [37].
interactionwithitsenvironment.Reinforcementlearningcan
beformallydescribedbyaMarkovDecisionProcess(MDP), The network architectures for both networks are as fol-
{S A P R} S
denoted by a tuple , , , , where represents the lows. The actor network has 3 fully-connected layers, fol-
A P
statespace, representstheactionspace, denotesthestate lowed by a Long Short-Term Memory (LSTM) [38] layer,
R
transition probability model, and is the reward function. which is fully connected to the output layer. The actor
∈S
Ateachtimestept,theagentobservesstates andtakes network estimates the stochastic control policy with two
∈A t
an action a , according to its policy π(s ), causing the outputs, mean value µ and estimated variance σ2, which are
t t N
environment to transition to the next state s according to used to generate a Gaussian distribution from which the
| t+1 ∼ N
thetransitiondynamics p(s s ,a ) asgivenbythetransi- action is sampled, such that a (µ,σ2). Meanwhile, the
P t+1 t t
tion probability model . The agent then receives a reward critic network uses only 2 fully-connected layers followed
R
r , according to the reward function , and observes the by the output layer to estimate the value function V(s). All
t
new state of the environment s . The network parameters hidden neurons use a ReLU-6 activation [39], whilst the µ
t+1
109
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 10:21:11 UTC from IEEE Xplore.  Restrictions apply. TABLE I: Final network hyperparameters.
thelongitudinalactionsofthevehicle,usingtheobservations
Parameter Value sItL =[vf,vrel,th]. Both models aim to maintain a 2 s time
No.hiddenlayers(actor) 3 headway t from the lead vehicle. The time headway is a
h
No.neuronsperhiddenlayer(actor) 50 measure of intervehicular distance in time, given as follows:
No.ofLSTMunits(actor) 16
No.hiddenlayers(critic) 2 t = xrel (8)
No.neuronsperhiddenlayer(critic) 50 h v
Learningrate(actor),ηactor 1x10-4 f
Learningrate(critic),ηcritic 1x10-2 where xrel is the relative distance between the two vehicles
Discountfactor,γ 0.99 in m, and v is the velocity of the following vehicle in m/s.
Entropycoefﬁcient,β 1x10-4 f
The training was broken down into 5-minute episodes,
RMSProp 1x10-10
RMSPropdecayα 0.9 where the episode ends after the 5 minutes have passed or a
RMSPropmomentum 0.0 collision occurs. At the start of each episode, a road friction
∈ { }
coefﬁcient CoF 0.4, 0.425, ... , 1.0 was randomly
chosen. It should be noted that a collision may be easier to
uses a tanh activation, the σ2 uses a softplus activation, and causeinlowfrictionconditionsasthereactiontimerequired
the value estimate has a linear activation. for the follower vehicle reduces [43], however none of the
A2C training is formulated as in [37], by updating the agents can observe the road friction coefﬁcients and should
actor and critic networks in separate update steps, using a therefore learn a policy which generalises to different road
L L R
policy loss and value loss functions, respectively, as conditions. The reward func(cid:18)tion f(cid:19)or training the ARL
π v
given by: agent was given based on the time headway:
L
=(A(s ,a ))2 (5)
v t t R 1
L − | − =min ,100 (9)
π = logπ(at st)A(st,at) βH(π(st)) (6) th
Thus, the reward function rewards low time headways,
where β is the entropy coefﬁcient and H(π(s )) is the en-
t encouragingcollisionstooccur.Therewardiscappedat100,
tropyaddedtoencourageexplorationinthepolicy,calculated
asotherwisetherewardfunctionwouldtendtowardsinﬁnity
as
1 as the time headway reaches zero.
H(π(s ))= (log(2πσ2)+1) (7)
t 2 The velocity and the acceleration of the lead vehicle were
limitedtoensurethatthevehiclebehaviourremainsplausible
BothnetworksareupdatedusingRMSPropoptimiser[40]
and the velocity is in the highway driving range, as well as
during training, using their respective loss functions. The
to obtain insights into the effect of the driving speeds on the
ﬁnal hyperparameters of the network architecture are shown
robustnessofthevehiclefollowingmodels.Theacceleration
in Table I. ∈
was always limited to a [-6, 2] m/s2, whilst four
lead ∈
C. Training Environment velocity ranges were tested as v [17, 30], [12, 35],
lead
[12, 30], [17, 35] m/s. For each velocity constraint and
Theautonomousdrivingsimulationwasdeﬁnedasavehi-
vehiclefollowermodelcombination,5trainingrunsof2,500
cle following scenario in highway driving. Two vehicles are
episodes were completed.
drivingathighwayspeedsonastraightroad.Thefolloweris
aDNNtrainedtofollowaleadingvehicleatasafedistance,
III. SIMULATIONRESULTS
whilst the lead vehicle is the adversarial agent whose aim
A. Results
is to ﬁnd weaknesses in the follower’s control policy. In
order to do this, the adversarial agent must create collisions, The average number of collisions and episodes until ﬁrst
thus proving the follower’s control policy is unsafe. For this collision for each velocity range and vehicle follower model
scenario, the input to the ARL network are the follower canbeseeninTablesIIandIII,respectively.Ininitialtesting,
∈
vehiclevelocityv ,followervehicleaccelerationa ,relative theleadvehiclewaslimitedtov [17,30]m/s.Sincethe
f f lead
velocity to the follower v , and time headway between the vehiclefollowingmodelsweretrainedinthisvelocityrange,
rel
two vehicles t , such that sARL = [v ,a ,v ,t ]. The it tests their robustness in their training domain. The ARL
h t f f rel h
output of the network is the lead vehicle acceleration for agentwasthentrainedfor2,500episodesagainstbothagents,
the next time step. The simulation time steps are ﬁxed at 40 for which the results can be seen in Fig. 2(a). The results
ms. demonstrate the IL model is susceptible to an adversarial
We demonstrate this framework by attacking two previ- agent, and thus the ARL agent can cause collisions to occur.
ously published DNN models trained for vehicle following On the other hand, the RL model has zero collisions with
usingtheIPGCarMakersimulator[41],(1)aReinforcement the ARL agent, and as can be seen from Fig. 2(a) the
Learning(RL)model[37]and(2)anImitationLearning(IL) minimum time headway in the episodes remains near the
model [42]. The RL model uses a feedforward network with target headway of 2 s. This shows a signiﬁcant beneﬁt of
an LSTM layer to control the longitudinal actions of the the RL model over the IL one, in terms of robustness to an
vehicle using the observations sRL = [v ,a ,v ,t ]. The adversarial agent. The second set of experiments, shown in
t f f rel h
IL model uses a simple feedforward network to also control Fig.2(b),relaxedthevelocityconstraintsontheleadvehicle,
110
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 10:21:11 UTC from IEEE Xplore.  Restrictions apply. TABLE II: Average number of collisions for different lead
which it continuously accelerates and decelerates between
vehicle velocity constraints. Averaged over 5 training runs
high and low velocities, until the follower vehicle comes
of 2,500 episodes each.
close to it with a high acceleration rate, at which point
the lead vehicle then decelerates at maximum deceleration.
v ImitationLearning ReinforcementLearning
lead Meanwhile, in the plots (c) and (d), the ARL agent has
[17,30]m/s 486.6 0.0
adopted a strategy in which it ﬁrst decelerates to a low
[12,35]m/s 644.0 2.0
velocity, and once both vehicles are at low velocities it
[12,30]m/s 799.6 0.0
starts to accelerate back to the maximum velocity, followed
[17,35]m/s 315.2 1.2
by waiting until the following vehicle is approaching it at
high acceleration, when it ﬁnally decelerates and creates a
TABLE III: Average number of episodes until ﬁrst collision collision. These results reveal a ﬂaw in the IL model, where
found for different lead vehicle velocity constraints. Aver- it continues to accelerate when the t > 2 s, trying to reach
h
aged over 5 training runs of 2,500 episodes each. the target t of 2 s, even if the lead vehicle is decelerating
h
and there is a large relative velocity difference between the
v ImitationLearning ReinforcementLearning
lead vehicles.Findingdifferentcollisionmodesisbeneﬁcial,asit
[17,30]m/s 563.2 0.0
offersfurtherinsightintothedifferentvulnerabilitiespresent
[12,35]m/s 579.2 922.3
in the control policy. Therefore, by exploiting information
[12,30]m/s 245.3 0.0
frommultipletrainingrunswheretheARLisusingdifferent
[17,35]m/s 1030.8 2451.0
collision modes, valuable insight into the weaknesses of the
DNN under testing can be obtained.
∈
to v [12, 35] m/s increasing the maximum velocity
lead B. Discussion
and decreasing the minimum velocity. These velocity ranges
are outside the distribution the vehicle following models The overall testing completed accounts for a total of
experienced during training, and therefore also test model 100,000 episodes, or over 8000 simulated hours of testing.
generalisation capability. From the results, it can be seen This resulted in a total of 11243 collision cases found,
that both models are more susceptible to an attack in this which includes 11227 and 16 for the IL and RL mod-
domain, but nevertheless the RL model still demonstrates els, respectively. This clearly demonstrates the signiﬁcantly
signiﬁcant safety beneﬁts over the IL model. The two last higher robustness of the RL model to the presence of an
∈
velocityrangestestedwerev [12,35]and[17,30]m/s, adversarial agent. Moreover, these results demonstrate that
lead
relaxing the minimum and maximum lead vehicle velocity the proposed ARL framework is able to ﬁnd failure cases
constraints, respectively. The results can be seen in Fig. forbothcontrolpoliciesundertesting.Comparedtothetype
2(c) and (d). Comparing the two sets of experiments, it can of manual test case deﬁnition often used for vehicle safety
be seen that relaxing the minimum velocity and allowing testing, this can be highly beneﬁcial for testing complex
the lead vehicle to drive at lower speeds enables it to ﬁnd blackboxcontrolsystems.Forinstance,bothcontrolpolicies
collision cases more easily. In both cases, collision cases tested here, were tested for 10 hours of simulated vehicle
against the IL model are found. However, the results from followingintheiroriginalworks,wheretheleadvehiclealso
Tables II and III show that the ARL is able to exploit the drove at highway speeds. In this manual testing, the types
IL model signiﬁcantly more often and earlier in its training. of trajectories executed by the lead vehicle were manually
On the other hand, the RL model only collides in the higher deﬁned (including both naturalistic driving and emergency
velocity experiments, although this occurs relatively rarely manoeuvrers),wheretheparameters(e.g.maximumvelocity,
and only at the very end of the ARL agent’s training phase. acceleration, time to execute manoeuvrer etc.) were ran-
Further investigation into the type of behaviour the ARL domised during testing. The constraints on the lead vehicle
∈
was adopting during training revealed that, for a single used in the manual test case deﬁnition were v [17,40]
∈ − lead
training run, the ARL tends to converge to a singular type m/s and a [ 6,2] m/s2, and road friction coefﬁcient
lead ∈
of behaviour that leads to collisions and these behaviours was uniformly sampled from CoF [0.4,1.0], representing
can vary signiﬁcantly between different training runs. While similar driving conditions to those in the adversarial testing
somedifferencesintheconvergedbehaviouroftheagentcan framework presented here. The results for these driving
be expected due to the variance in reinforcement learning tests are shown in Table IV and show that during normal
[44]–[47], these results show signiﬁcant differences between testing not a single collision was found. This demonstrates
different trained agents. For instance, example collision sce- how effective our ARL is at ﬁnding weaknesses in DNN-
nariosareshowninFig3,where2collisionsfrom1training based control policies. Indeed, the results from the manual
run are shown in the top subﬁgures, whilst 2 collisions from testing would suggest the IL model to be the safer control
another training run are shown in the bottom subﬁgures. For policy. However, our testing framework exposes signiﬁcant
consistency, both training runs are attacking the IL model, vulnerabilities in the IL model, demonstrating that the RL
with the same velocity constraints. As can be seen in the control policy is signiﬁcantly more robust to the presence of
ﬁrst two plots, the ARL agent has adopted a strategy in an adversarial agent.
111
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 10:21:11 UTC from IEEE Xplore.  Restrictions apply. ∈ ∈
(a) v [17, 30] m/s. (b) v [12, 35] m/s.
lead lead
∈ ∈
(c) v [12, 30] m/s. (d) v [17, 35] m/s.
lead lead
Fig. 2: Comparison of the two vehicle following agents’ minimum t per episode over training runs. Averaged over 5 runs,
h
with standard deviation shown in shaded colour.
TABLE IV: 10-hour driving test with manually deﬁned lead
which degrade the performance of the target system. This
vehicle trajectories.
general concept could be used to analyse vulnerabilities in
control policies used in multi-agent environments, such as
Parameter ImitationLearning ReinforcementLearning
robotic manipulation or unmanned aerial vehicles. In our
min.x 23.844m 7.780m
rel work, the ARL approach was tested in an autonomous vehi-
meanx 57.37m 58.01m
rel cle use case, where the aim of the ARL agent was to cause
max.v 8.878m/s 7.891m/s
rel the vehicle behind it to collide into it. Two neural network
meanv 0.0197m/s 0.0289m/s
rel models trained for vehicle following were tested, one which
min.t 1.738s 1.114s
h uses imitation learning and the other using reinforcement
meant 1.990s 2.007s
h
learning. Both models had no collisions when manually
collisions 0 0
tested in their original works. The ARL agent was shown
tobeabletolearnadrivingbehaviourwhichcancauseboth
target models to collide into the lead vehicle. This in itself
IV. CONCLUDINGREMARKS demonstrates the signiﬁcant beneﬁt of this type of targeted
In this paper, an automated testing framework for deep adversarial black box testing. Also, the results showed that
neural networks was presented. The proposed framework thereinforcementlearningmodelissigniﬁcantlymorerobust
is based on adversarial reinforcement learning, where an to this kind of adversarial behaviour, demonstrating the
adversarial agent is placed in the same environment with safety beneﬁt of the reinforcement learning model over the
the system under testing. By training the adversarial agent imitation learning model. This type of adversarial testing
through reinforcement learning, the agent learns behaviours frameworkprovidesanimportanttechniquefortestingblack
112
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 10:21:11 UTC from IEEE Xplore.  Restrictions apply. (a) (b)
(c) (d)
Fig. 3: Comparison of collision scenarios between training runs, (a) and (b) are from training run 1, whilst (c) and (d) are
∈
from training run 2. Both training runs use velocity constraints of v [17, 30] m/s and the IL model as the vehicle
lead
follower.
box control policies, and can be used to benchmark and [3] I.Sutskever,O.Vinyals,andQ.V.Le,“Sequencetosequencelearning
compare deep control policies as well as to gain additional withneuralnetworks,”inAdvancesinNeuralInformationProcessing
Systems(NIPS),2014,pp.3104–3112.
insights into the types of edge cases the policies are likely
[4] S. Levine, C. Finn, T. Darrell, and P. Abbeel, “End-to-end training
to fail in. of deep visuomotor policies,” The Journal of Machine Learning
Research,vol.17,no.1,pp.1334–1373,2016.
ACKNOWLEDGMENT [5] S.Gu,E.Holly,T.Lillicrap,andS.Levine,“Deepreinforcementlearn-
ing for robotic manipulation with asynchronous off-policy updates,”
This work was funded by the EPSRC under grant agree- in2017IEEEInternationalConferenceonRoboticsandAutomation
(ICRA). IEEE,2017,pp.3389–3396.
ments (EP/R512217/1) and Innovate UK Autonomous Valet
[6] M. A. Lee, Y. Zhu, K. Srinivasan, P. Shah, S. Savarese, L. Fei-Fei,
Parking Project (Grant No 104273). We would also like to A. Garg, and J. Bohg, “Making sense of vision and touch: Self-
thank NVIDIA Corporation for their GPU grant. supervised learning of multimodal representations for contact-rich
tasks,”in2019InternationalConferenceonRoboticsandAutomation
(ICRA). IEEE,2019,pp.8943–8950.
REFERENCES [7] Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei, and
A. Farhadi, “Target-driven visual navigation in indoor scenes using
[1] A.Krizhevsky,I.Sutskever,andG.E.Hinton,“Imagenetclassiﬁcation deepreinforcementlearning,”in2017IEEEInternationalConference
with deep convolutional neural networks,” in Advances in Neural onRoboticsandAutomation(ICRA). IEEE,2017,pp.3357–3364.
InformationProcessingSystems(NIPS),2012,pp.1097–1105. [8] S. Kuutti, R. Bowden, Y. Jin, P. Barber, and S. Fallah, “A survey
[2] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, of deep learning applications to autonomous vehicle control,” IEEE
A.Senior,V.Vanhoucke,P.Nguyen,T.N.Sainathetal.,“Deepneural TransactionsonIntelligentTransportationSystems,2020.
networks for acoustic modeling in speech recognition: The shared [9] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp,
views of four research groups,” IEEE Signal Processing Magazine, P.Goyal,L.D.Jackel,M.Monfort,U.Muller,J.Zhangetal.,“Endto
vol.29,no.6,pp.82–97,2012. end learning for self-driving cars,” arXiv preprint arXiv:1604.07316,
113
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 10:21:11 UTC from IEEE Xplore.  Restrictions apply. 2016. low,andR.Fergus,“Intriguingpropertiesofneuralnetworks,”arXiv
[10] F. Codevilla, M. Miiller, A. Lo´pez, V. Koltun, and A. Dosovitskiy, preprintarXiv:1312.6199,2013.
“End-to-enddrivingviaconditionalimitationlearning,”in2018IEEE [29] I.J.Goodfellow,J.Shlens,andC.Szegedy,“Explainingandharness-
InternationalConferenceonRoboticsandAutomation(ICRA). IEEE, ingadversarialexamples,”arXivpreprintarXiv:1412.6572,2014.
2018,pp.1–9. [30] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and
[11] K.R.VarshneyandH.Alemzadeh,“Onthesafetyofmachinelearning: A.Swami,“Thelimitationsofdeeplearninginadversarialsettings,”in
Cyber-physical systems, decision sciences, and data products,” Big 2016IEEEEuropeanSymposiumonSecurityandPrivacy(EuroS&P).
data,vol.5,no.3,pp.246–255,2017. IEEE,2016,pp.372–387.
[12] M.Borg,C.Englund,K.Wnuk,B.Duran,C.Levandowski,S.Gao, [31] K.Y.Xiao,V.Tjeng,N.M.Shaﬁullah,andA.Madry,“Trainingfor
Y. Tan, H. Kaijser, H. Lo¨nn, and J. To¨rnqvist, “Safely entering the faster adversarial robustness veriﬁcation via inducing relu stability,”
deep: A review of veriﬁcation and validation for machine learning arXivpreprintarXiv:1809.03008,2018.
andachallengeelicitationintheautomotiveindustry,”arXivpreprint [32] D. Tsipras, S. Santurkar, L. Engstrom, A. Turner, and A. Madry,
arXiv:1812.05389,2018. “Robustness may be at odds with accuracy,” arXiv preprint
[13] S. Burton, L. Gauerhof, and C. Heinzemann, “Making the case for arXiv:1805.12152,2018.
safety of machine learning in highly automated driving,” in Inter- [33] L. Schmidt, S. Santurkar, D. Tsipras, K. Talwar, and A. Madry,
national Conference on Computer Safety, Reliability, and Security. “Adversariallyrobustgeneralizationrequiresmoredata,”inAdvances
Springer,2017,pp.5–16. in Neural Information Processing Systems (NIPS), 2018, pp. 5014–
5026.
[14] N.KalraandS.M.Paddock,“Drivingtosafety:Howmanymilesof
[34] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,
drivingwouldittaketodemonstrateautonomousvehiclereliability?”
D. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep
TransportationResearchPartA:PolicyandPractice,vol.94,pp.182–
reinforcement learning,” in International Conference on Machine
193,2016.
Learning(ICML),2016,pp.1928–1937.
[15] W. Wachenfeld and H. Winner, “The new role of road testing for
[35] S.Bhatnagar,M.Ghavamzadeh,M.Lee,andR.S.Sutton,“Incremen-
the safety validation of automated vehicles,” in Automated Driving.
talnaturalactor-criticalgorithms,”inAdvancesinNeuralInformation
Springer,2017,pp.419–435.
ProcessingSystems(NIPS),2008,pp.105–112.
[16] E.Coelingh,J.Nilsson,andJ.Buffum,“Drivingtestsforself-driving
[36] R.S.SuttonandA.G.Barto,ReinforcementLearning:AnIntroduc-
cars,”IEEESpectrum,vol.55,no.3,pp.40–45,2018.
tion. MITpressCambridge,1998,vol.135.
[17] P. Koopman and M. Wagner, “Challenges in autonomous vehicle
[37] S.Kuutti,R.Bowden,H.Joshi,R.deTemple,andS.Fallah,“End-to-
testing and validation,” SAE International Journal of Transportation
endreinforcementlearningforautonomouslongitudinalcontrolusing
Safety,vol.4,no.1,pp.15–24,2016.
advantage actor critic with temporal context,” in 2019 IEEE 22nd
[18] P. Van Wesel and A. E. Goodloe, “Challenges in the veriﬁcation of IntelligentTransportationSystemsConference(ITSC). IEEE,2019,
reinforcement learning algorithms,” Technical report, NASA, Tech.
pp.2456–2462.
Rep.,2017. [38] S.HochreiterandJ.Schmidhuber,“Longshort-termmemory,”Neural
[19] D. Castelvecchi, “Can we open the black box of ai?” Nature News, computation,vol.9,no.8,pp.1735–1780,1997.
vol.538,no.7623,p.20,2016. [39] A. Krizhevsky and G. Hinton, “Convolutional deep belief networks
[20] A. Adadi and M. Berrada, “Peeking inside the black-box: A survey oncifar-10,”Unpublishedmanuscript,vol.40,no.7,2010.
on explainable artiﬁcial intelligence (xai),” IEEE Access, vol. 6, pp. [40] T.TielemanandG.Hinton,“Lecture6.5-rmsprop:Dividethegradient
52138–52160,2018. by a running average of its recent magnitude,” COURSERA: Neural
[21] F. Codevilla, E. Santana, A. M. Lo´pez, and A. Gaidon, “Exploring networksformachinelearning,vol.4,no.2,pp.26–31,2012.
the limitations of behavior cloning for autonomous driving,” in Pro- [41] IPG Automotive GmbH, “Carmaker: Virtual testing of automobiles
ceedings of the IEEE International Conference on Computer Vision and light-duty vehicles,” 2017. [Online]. Available: https://ipg-
(ICCV),2019,pp.9329–9338. automotive.com/products-services/simulation-software/carmaker/
[22] J. Morimoto and K. Doya, “Robust reinforcement learning,” Neural [42] S. Kuutti, R. Bowden, H. Joshi, R. de Temple, and S. Fallah, “Safe
computation,vol.17,no.2,pp.335–359,2005. deepneuralnetwork-drivenautonomousvehiclesusingsoftwaresafety
[23] L.Pinto,J.Davidson,R.Sukthankar,andA.Gupta,“Robustadversar- cages,” in 2019 20th International Conference on Intelligent Data
ial reinforcement learning,” in Proceedings of the 34th International EngineeringandAutomatedLearning(IDEAL). Springer,2019,pp.
Conference on Machine Learning (ICML). JMLR. org, 2017, pp. 150–160.
2817–2826. [43] K.Reif,“Brakes,brakecontrolanddriverassistancesystems,”Weis-
[24] A.C.Schultz,J.J.Grefenstette,andK.A.DeJong,“Adaptivetesting baden,Germany,SpringerVieweg,2014.
of controllers for autonomous vehicles,” in Proceedings of the 1992 [44] P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and
Symposium on autonomous underwater vehicle technology. IEEE, D. Meger, “Deep reinforcement learning that matters,” in Thirty-
1992,pp.158–164. SecondAAAIConferenceonArtiﬁcialIntelligence,2018.
[25] V. Behzadan and A. Munir, “Adversarial reinforcement learning [45] J.Schulman,P.Moritz,S.Levine,M.Jordan,andP.Abbeel,“High-
framework for benchmarking collision avoidance mechanisms in au- dimensional continuous control using generalized advantage estima-
tonomousvehicles,”arXivpreprintarXiv:1806.01368,2018. tion,”arXivpreprintarXiv:1506.02438,2015.
[26] S. Shalev-Shwartz, S. Shammah, and A. Shashua, “On a for- [46] J.Romoff,P.Henderson,A.Piche´,V.Francois-Lavet,andJ.Pineau,
mal model of safe and scalable self-driving cars,” arXiv preprint “Reward estimation for variance reduction in deep reinforcement
arXiv:1708.06374,2017. learning,”arXivpreprintarXiv:1805.03359,2018.
[27] M.Koren,S.Alsaif,R.Lee,andM.J.Kochenderfer,“Adaptivestress [47] L.WeaverandN.Tao,“Theoptimalrewardbaselineforgradient-based
testing for autonomous vehicles,” in 2018 IEEE Intelligent Vehicles reinforcementlearning,”inProceedingsoftheSeventeenthconference
Symposium(IV). IEEE,2018,pp.1–7. onUncertaintyinartiﬁcialintelligence. MorganKaufmannPublishers
Inc.,2001,pp.538–545.
[28] C.Szegedy,W.Zaremba,I.Sutskever,J.Bruna,D.Erhan,I.Goodfel-
114
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 10:21:11 UTC from IEEE Xplore.  Restrictions apply. 