2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
Constrained Filtering-based Fusion of Images, Events,
and Inertial Measurements for Pose Estimation
Jae Hyung Jung and Chan Gook Park
Abstract—In this paper, we propose a novel ﬁltering-based
method that fuses events from a dynamic vision sensor (DVS),
images,andinertialmeasurementstoestimatecameraposes.A
DVS is a bio-inspired sensor that generates events triggered
by brightness changes. It can cover the drawbacks of a
conventional camera by virtual of its independent pixels and
high dynamic range. Speciﬁcally, we focus on optical ﬂow
obtained from both a stream of events and intensity images in
which the former is much like a differential quantity, whereas
the latter is a pixel difference in a much longer time interval
than events. This nature characteristic motivates us to model
optical ﬂow estimated from events directly, but feature tracks Fig. 1. A sample image (dataset from [25]) during signiﬁcant angular
for images in the ﬁlter design. An inequality constraint is motion:(a)Anintensityimagewithtrackedfeatures(red)andfailedtracks
considered in our method since the inverse scene-depth is (green) during 47ms. Many of them are failed to track due to the motion
larger than zero by its deﬁnition. Furthermore, we evaluate blur.(b)Asynthesizedeventframewithopticalﬂowmeasurements(green
our proposed method in the benchmark DVS dataset and a arrows)withatemporalwindow3ms
dataset collected by the authors. The results reveal that the
presented algorithm has reduced the position error by 49.9%
on average and comparable accuracy only using events when and then a navigation solution might diverge eventually. A
compared to the state-of-the-art ﬁltering-based estimator. device that outputs intensity for a pixel along with an event
from the same photodiode was introduced in [2]. This work
I. INTRODUCTION
foresaw the combination of the standard frame and events
Aneventcameraordynamicvisionsensor(DVS)isabio- would serve as an attractive sensor: standard image provides
inspired sensor that outputs asynchronous events generated intensity information even in static, while events are well-
by brightness changes rather than absolute intensities of triggered in a rapid motion.
each pixel. It addresses limitations of a conventional camera In our approach, to reliably recover pose even in high
featuring a very high dynamic range (130 dB), and more dynamics,wefocusonopticalﬂowmeasurementsthatcanbe
importantly independent pixels that avoid the motion blur estimated from both an intensity image and an event stream.
due to a rapid ego-motion [1], [2]. A DVS opens a new way Optical ﬂow from an image is much like a pixel difference
to access visual information previously only perceived as an among a two-view geometry divided by the time interval,
array ﬁlled with the absolute amount of light in most of whereasopticalﬂowobtainedfromaneventstreamisviewed
computer vision algorithms. By virtue of these advantages, asadifferentialquantitymadefromaspatiotemporalwindow
events could be an attractive solution in target tracking usually much shorter than the sampling frequency of an
or robotics in which a high dynamic motion frequently intensity image in a rapid motion scenario. This motivates
arises.However,eventsfromamonocularsystemstillcannot us to utilize the optical ﬂow measurements from an event
resolve the scale ambiguity. stream directly, but feature tracks for images.
Visual-inertial fusion is a popular solution because of An earlier work of [13] recognized the complementary
its well-known complementary characteristics: an image property of events and images and proposed a method
provides rich information for localization, while inertial to fuse a visual-inertial system with a stream of events.
measurements from an inertial measurement unit (IMU) However, our approach is basically different to the previous
ﬁll the gap between images that has typically much lower work other than the back-end implementation. Speciﬁcally,
sampling rates. Rapid motion and high dynamic range sce- we directly utilize optical ﬂow obtained from a stream of
narios are still challenging that obscure visual measurement. events, whereas the authors synthesized a motion-corrected
Consequently, biases of an IMU are not properly estimated, event frame to obtain feature tracks extracted from sharp
frames. Therefore, our method does not require any time
This work was supported by Samsung Electronics Co.,Ltd. through the synchronization between event and intensity frames: optical
SamsungSmartCampusResearchCenter(0115-20190032)andtheMinistry
ﬂowfromeventsupdatestheestimatorasynchronouslyasits
of Science and ICT of the Republic of Korea through the Space Core
TechnologyDevelopmentProgram(NRF-2018M1A3A3A02065722) inherent characteristic.
AllauthorsarewiththeDepartmentofMechanicalandAerospaceEngi- Fig. 1 shows an example of estimated optical ﬂow mea-
neering,AutomationandSystemsResearchInstitute,SeoulNationalUniver-
surements from images and a stream of events in which
sity,Seoul08826,RepublicofKorea.lastflowers@snu.ac.kr;
chanpark@snu.ac.kr events provide body velocity information from the optical
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 644
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 15:55:45 UTC from IEEE Xplore.  Restrictions apply. ﬂow even in the challenging condition. Moreover, optical On the other hand, a ﬁltering-based estimator was pro-
ﬂow is much less sensitive to outliers than the multi-view posed in [21] using their previous work on estimating op-
track since it only depends on a two-view measurement. At tical ﬂow [18], and the multi-state constraint Kalman ﬁlter
thebestoftheauthors’knowledge,itistheﬁrsttimetofuse (MSCKF) [22]. However, they reported that their method
optical ﬂow from events, feature tracks from images, and could not run in real-time due to the heavy computation of
inertial measurements in a framework of a ﬁltering-based the two EM algorithms. Since our method only considers
estimator. The main contribution of our novel method is a two-view geometry of a feature from events, we only
threefold: utilize the ﬁrst part of the EM algorithm of [18] to reduce
• Filtering-based hybrid estimator using IMU and optical computation while extracting meaningful information.
ﬂow computed from events and images, respectively. In a speciﬁc application, a state can only be laid in the
• Inequality constraint on the inverse scene-depth is con- feasible region due to its deﬁnition or a physical constraint.
sidered in the ﬁltering to more precisely model optical Especially in chemical engineering in which concentration
ﬂow. should be larger than or equal to zero, linear and nonlinear
• The proposed method is evaluated on the event camera constrained ﬁltering has been actively researched [15], [16],
dataset[25]quantitativelyandadatasetcollectedbythe [17].Sinceoneofourstatevariables,theinversescene-depth
authors qualitatively. should be larger or equal to zero, we formulated our EKF-
based estimator in a framework of the projection approach
II. RELATEDWORKS [24] in the constrained ﬁltering.
Since a DVS conveys different types of output in nature,
conventionalcomputervisiontechniquesshouldbemodiﬁed, III. NOTATIONS
{ }
or a new algorithm has to be designed. In this paper, we denote the global frame as G that
Benosman et al. [6] proposed a method that estimates is a local tangent frame aligned with the gravity direction.
{ }
opticalﬂowofeacheventbyﬁttingaplanetothesurfaceof The body frame B is coincident with axes of an IMU,
(cid:55)→
activeeventsthatis(x,y) twherex,yisaneventlocation while the intensity camera and the event camera frames
{ } { }
and t is a timestamp. The optical ﬂow is expressed as the are designated as C and E , respectively. Their origins
normalvectoroftheﬁttedplane.Byleveraging[6],Muggler are at the optical centers of each camera, and the XYZ
et al. [7] devised the lifetime (time taken to move one pixel) axesfollowtheright-down-forwardconvention.Avectorand
of an event through which one can take a sharp snapshot matrix are designated by a small and capital bold letter such
withoutaccumulatinganartiﬁcialtemporalwindow.Gallego as a and A. A vector is described by a reference, resolved,
et al. [8] presented a general framework called contrast and object frame. xG expresses a physical quantity, x in
B { }
maximizationtorecoverego-motion,depthand,opticalﬂow which the reference/resolved frames are G , and the object
{ }
that are one of the most crucial model parameters in the frame is B . When a reference and resolved frames are
multipleviewgeometry.Thisapproachwasfurtheranalyzed different, we will explicitly mention them in the text. A
∈ SO
by [9], [10] that presented objective functions in estimating matrix, RG (3) stands for the direction cosine matrix
B
optical ﬂow in which the former more focused on the thattransformsaresolvedframefromB toG.Wedenotethe
aperture problem. Zhu et al. [18] presented an expectation- corresponding unit quaternion as q . For a random vector
GB −
maximization (EM) based algorithm that solves a soft data- x, we deﬁne the error vector as x˜ = x xˆ where the hat
associationproblemtoestimateopticalﬂowinspiredbyEM- means estimated value.
ICP [20]. Gehrig et al. [11] posed a maximum likelihood
approach for aligning intensity images with events. This IV. THEPROPOSEDMETHOD
corresponds to an image registration problem in which the Our method fuses measurements from a conventional,
image acts as a template as similar to the KLT tracker [19]. event camera, and an IMU to track a pose of the body
{ } { }
The back-end of a visual-inertial navigation system can frame B withrespecttotheglobalframe G .Theoverall
be implemented either based on optimization [3], [4] or ﬂowchart is shown in Fig. 2. Feature points from images
ﬁltering method [5], [22]. For an event-based visual-inertial are tracked by Kanade-Lucas-Tomasi (KLT) tracker [19]
system, [12], [13], [14] presented an optimization-based to give a relative constraint to the estimator. Events are
estimator using a DVS. Rebecq et al. [12] synthesized a tracked by solving a soft data-association problem that was
motion-correctedeventframeandusedaconventionalvision originally proposed in [18]. Since we are only interested
techniquetotrackasetoffeatures,andVidaletal.[13]also in instance optical ﬂow of given features, and estimator
fused images along with events and inertial measurements does not suffer from a tracking drift, the ﬁrst part of the
based on [12]. However, the work of [13] set the ﬁxed EM algorithm of [18] while excluding the afﬁne alignment
numberofeventwindowsthatshouldbeappropriatelytuned is employed to estimate the ﬂow. Then, the constrained
according to the working environment to synchronize event EKF fuses the likelihood and prior in which the inverse
frames to standard frames forcibly. In contrast, our method scene-depth (average depth of a scene) is projected into the
does not require any synchronizations of events and images constrained surface when the constraints are violated.
sincewedirectlyextractthevelocitycomponentfromoptical The below subsections will review the event tracker, and
ﬂow measurements of events. describe the proposed ﬁler in detail.
645
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 15:55:45 UTC from IEEE Xplore.  Restrictions apply. Fig. 2. A ﬂowchart of the proposed algorithm. Intensity images are processed in Vision front-end using a conventional image processing technique to
yieldfeaturetracks,whileopticalﬂowmeasurementsofeventsaresolvedthroughtheexpectation-maximization(EM)algorithminEventfront-end.The
estimatorconstraintsarangeofthescene-depthfrompriorknowledgeofanenvironment.
A. The event tracker l as a Gaussian distribution where its mean is l with a
j j
standard deviation as 2 pixels, and set the template as time-
Events are generated from an edge with an intensity
shifted events from a previous step as in [18]. In particular,
change. It is dissimilar to the conventional images in nature, { }
therefore to extract information for localization, a different for the ﬁrstly tracked feature the template lj 0 (subscript
means the time step) is taken as events back-propagated by
method should be employed. An i-th event is a 4-length { } { − }
vector that is, the curren∈tly estimated optical ﬂow lj 0 = ξi ∆tiu
where ξ e . Then for the next epoch, the template is set
{ } i
e = t ,ξ ,p (1) as the forward-propagated events given the optical ﬂow, that
i i i i { } { − } −
is l = ξ +(∆t ∆t )uˆ where ∆t =t t .
where t is a timestamp, ξ is a pixel coordinate in a 2D j 1 i e i 0 e 1 0
i i
image plane, and p stands for the polarity of an event (-1
i B. The Filter design
for negative, +1 for positive).
By assuming that optical ﬂow u within a spatiotemporal We see that optical ﬂow from a conventional image is
window is constant, we can estimate it by solving the below much like difference of pixel positions in a ﬁnite camera
(cid:88) (cid:88)
minimization problem [18], [21]. sampling time resulted from an image alignment. On the
other hand, optical ﬂow obtained from events represents a
(cid:107) − − (cid:107)
uˆ =argmin w (ξ ∆t u) l 2 (2) differential element that is estimated in a much shorter time
ij i i j 2
u ∈ ∈ than the former. This motivated us to fuse measurements
ei Wlj T
from two complementary sources using previous works of
In this expression, ∆t is a time interval between reference
i [22], [23].
time of W and i-th event time. w is a probability that i-th
ij The error state vector consists of 15 states of an inertial
event was generated from the j-th template l , hence a soft
j navigation system, an inverse scene-depth, and sliding win-
data-assoc(cid:110)iation. Also, W is a spatiotemporal windo(cid:111)w, that (cid:2) (cid:3)
dows, that is
is deﬁned as
(cid:104) (cid:105)
| (cid:107) − − (cid:107) ≤ ≤ T
W = e (ξ ∆t u) f 2 , t t<t (3) x˜ = x˜T λ˜ x˜T
i i i 2 0 1 (cid:104)I S (cid:105)
T
whereandf areuser-deﬁnedspatialwindowsizeandpixel x˜I = θ˜GTB p˜GB,T v˜BT b˜Ta b˜Tg
coordinate of a tracked feature, respectively. Also, t and t
aretimewindow.Geometrically,itisasetofeventst0hatfall1s x˜Si = θ˜GTBi p˜GBi,T (5)
within a circle of radi(cid:110)us  given optical ﬂ(cid:111)ow. Similarly, the
States in x˜ are the attitude, position, velocity, and biases
template window T is deﬁned as I
of an accelerometer and a gyroscope in turn. We deﬁne
T = lj| (cid:107)lj −f(cid:107)22 ≤ (4) tRheGa≈ttit{uIde+er[θr˜or t]h×a}tRˆisGpwerhteurrebe[d·]×friosma tshkeewle-sfytmsimdeetraics
B 3 GB B
that is a set of template features that lie within the circle. matrix operator for a given vector. Also, the body velocity
Unfortunately, w in (2) depends on the parameter u, denoted by v is equal to RBvG. While sliding windows
ij B G B
and can be evaluated up to the expectation. The expectation x˜ are directly related to the feature tracks from the vision
S
step estimates w , and the maximization step estimates u front-end, the inverse scene-depth λ that is an inverse of the
ij
by maximizing the likelihood. We model ξ generated from average feature depth models the optical ﬂow from events.
i
646
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 15:55:45 UTC from IEEE Xplore.  Restrictions apply. Thenominalstatesarenumericallyintegratedthatprovide In this expression, we formulate the i-th inverse-depth as
∼
linearization points for the estimator, whereas error states of λ = λˆ + λ˜ + n where n N(0,σ2), i.e. this
theattitude,position,andvelocityaregovernedbyfollowing reiquires only 1 dimλeinsional statλei λ˜. Also, n i stands for
z˙i
1st Markov processes, ameasurementnoiseofopticalﬂowobtainedfromtheevent
θ˜˙GB =−RˆGBθ˜GB −RˆGBng fisrocnat-leibnrda.teNdotientahdavtawnceea.sAsucmcoerdthinagt thtoe (e1x1tr)i,nsthicepparroajmecetteedr
−
p˜˙G = [RˆGvˆ ]×θ˜ +RˆGv˜ error equation reveals that the body velocity, gyro bias and
B B B GB B B
− − scene-depth are directly related to the measurement.
v˜˙ =RˆB[gG]×θ˜ [w bˆ ]×v˜
B G GB m g B
− −
(b˜ +n ) [vˆ ]×(b˜ +n ) (6) C. Constrained Kalman Filter
a a B g g
where n and n are white zero-mean Gaussian processes Itisobviouslyseenthattheinversescene-depthλislarger
a g than 0 by its deﬁnition. Furthermore, we can reasonably
for an accelerometer and a gyroscope, respectively. gG is
{ } assign the minimum and maximum range of the inverse
the gravity vector expressed in G , and w is an angular
velocity measured from a gyroscope. In amddition to (6), scene-depth m and M depending on prior knowledge of
an environment. Therefore, this is resolved to the quadratic
biases and the inverse scene-depth are modeled as random
programming (QP) with an inequality constraint.
walks.
The measurement model for a j-th feature track is given ∆x∗ =argmin(∆x−∆xˆ)TP−1(∆x−∆xˆ),
by a pinhole projection Π with an additive white zero-mean ∆x
− ≤ ≤ −
Gaussian noise n that is uncorrelated to the system model subjecttom λˆ dT∆x M λˆ (12)
v
noise. Assuming that intrinsic parameters are calibrated, the
pinhole model is Here,∆xˆ istheestimatederrorsuchthat∆xˆ =Krinwhich
K and r are Kalman gain and the innovation of the ﬁlter.
zj =Π(pCfj)+nv P is the covariance matrix of the EKF. In our setting, d is
=λ pC +n (7) given as a column vector(cid:2)in which its(cid:3)elements are all zero
j fj v except for the inverse scene-depth,
where λj is the j-th inverse depth. The error equation of (7) ··· ··· T
isprojectedintothenullspaceofthefeature-relatedJacobian d= 0 1 0 (13)
matrix to eliminate feature states [22]. We solve for the
The solution of (12) is the minimum variance, that is
feature depth by using inverse parameterization with the − ∗ ≤ −
cov(x˜ ∆x ) cov(x˜ ∆x) [24]. The QP with the scalar
Levenberg-Marquardt algorithm.
constraint of (12) can be easily solved. If the unconstrained
Taking a time derivative for both sides of (7), an i-th
solutionsatisﬁestheinequality,thenﬁnished.Otherwise,the
optical ﬂow model estimated from the event front-end is
QP with the equality constraint gives the solution. This is
z˙ =λ˙ pE +λ p˙E equivalent to project the ﬁlter state onto the constrained
i i fi i fi surface whenever the condition is violated.
λ˙ −
= i(λ pE)+[wE]×(λ pE) λ vE (8)
λ i fi G i fi i GE V. EXPERIMENTS
i
Wedenotethenormalizedfeaturepointasp¯E =λ pE inthe Inthissection,wevalidateourproposedposeestimatorin
following.Toeliminatethedependenceonthfei timeidefriivative two datasets: the event-camera dataset [25] and the author-
of the inverse d(cid:0)epth, (8) is projected to the(cid:1)left null space of collected dataset. The goal of this section is to analyze the
p¯E [23]. accuracy of the estimator, either using events, images, and
fi both of them in the event-camera dataset. Moreover, we
Ni z˙i+[wGEE]×p¯Efi +λivGEE =0 compare our method to EVIO [21] that is a ﬁltering-based
N p¯E =0 × , N NT =I (9) estimatorasoursquantitatively.Also,weshowthequalitative
i fi 2 3 i i 2 result at the author-collected dataset that exhibits a rapid
Note that N is the left nullspace of p¯E that has orthnormal
bases. We(cid:8)eixpress (9) with respect tofi{B} and deﬁne th(cid:9)is motion.
measurement as y, that is A. The event-camera dataset
y =N z˙ +[REw ]×p¯E +λ RE(v +[w ]×)pB Theevent-cameradataset[25]provides6-DOFIMUread-
i i i B B fi i B B B E(10) ings, intensity images, and events captured by DAVIS [2]
along with the ground-truth from a motion-capture system.
{ }
where w is an angular rate of B , and RE and pB are Speciﬁcally, the spatial resolution of the image sensor is
extrinsicBparameters between {B} and {E}. IBdeally, (1E0) is 240×180 that outputs events up to 12 Meps, while the IMU
equaltozerowithoutanynoises.Perturbing(10)bytheerror readingsandimagesaregivenat1000Hzandaround24fps,
state up to 1st order yields respectively. It offers trajectories of rotating, translating, and
{ bothmaneuversinindoorandoutdoorenvironments.Among
y˜ =N λˆREv˜ +([p¯E]×RE +λˆRE[pB]×)(b˜ +n )
i i B B − fi B B E }g g severalsequences,wechoosetranslationand6dof sets.This
+REB(vˆB +[wm bˆg]×pBE)(λ˜+nλi)+nz˙i (11) is because a feature parallax generated from a pure rotation
647
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 15:55:45 UTC from IEEE Xplore.  Restrictions apply. TABLEI
MEANPOSITIONANDATTITUDEERRORINTHEEVENT-CAMERADATASET
Image+Event Event Image EVIO[21]
Position Attitude Position Attitude Position Attitude Position Attitude
[%] [deg/m] [%] [deg/m] [%] [deg/m] [%] [deg/m]
boxes6dof 0.98 0.024 2.88 0.063 1.07 0.031 3.61 0.34
boxestranslation 1.24 0.077 1.50 0.059 6.17 0.054 2.69 0.09
hdrboxes 1.15 0.081 2.45 0.083 0.90 0.082 1.23 0.05
hdrposter 0.57 0.160 2.38 0.104 0.84 0.046 2.63 0.11
poster6dof 0.91 0.183 2.53 0.099 1.03 0.272 3.56 0.56
postertranslation 1.83 0.314 3.43 0.122 0.67 0.462 0.94 0.02
shapes6dof 0.59 0.295 4.91 0.267 2.39 0.192 2.69 0.40
shapestranslation 0.84 0.258 5.25 0.567 1.02 0.765 2.42 0.52
dynamic6dof 0.98 0.162 6.23 0.234 0.79 0.155 4.07 0.56
dynamictranslation 0.89 0.149 4.92 0.170 0.48 0.160 1.90 0.02
Fig. 3. Representative results in boxes6dof. (a) True 3D position obtained from the motion-capture system, (b) Linear and angular velocity proﬁle
computedfromnumericaldifferentiationofthepositionandthegyroscoperespectively,(c)Poseerrorsfor3caseswhereverticalbarsindicatetherapid
rotationaround42sec. (cid:80)
−
synthesized event frame, namely I(ξ) = δ(ξ ξ ).
i i
Throughout all experiments, we set the maximum number
of event feature as 30, the upper limit number of the spa-
tiotemporal window as 30,000 events and the next temporal
size as 3 times 65% percentile of the lifetime. Chi-squared
testrejectsopticalﬂowoutlierswith95%belief.Forintensity
featuretracker,50ofShi-TomasifeaturesaretrackedbyKLT
tracker with 8-pt RANSAC. For the inequality constraint in
− −
(12), we let m=0.25m 1 and M =10m 1.
We summarize the estimation accuracy in Table I. Event
means that the ﬁlter is updated by the optical ﬂow from
events in (11), and Image takes only feature tracks from
images in (7) that has 5 sliding windows. Image + Event
representsahybridestimatorthatobtainsmeasurementsfrom
the both front-end. Note that Event is implemented by using
Fig.4. Positionerrorandestimatedaveragescene-depth1/λˆintheﬁrst8 theconstrainedﬁlteringinSectionIV-C.Fortheerrormetric,
secondsofboxestranslationusingonlyeventswiththeconstrained(Event) (cid:107) − (cid:107)
andunconstrained(uEvent)ﬁltering. w||e report the posi|t|ion error p pˆ and the attitude error
dcm2rvec(RRˆT) as mean Euclidean distance divided by
the total traveled distance. In Table I, it is important to note
does not convey any information for the reconstruction in a that not only Event alone exhibits comparable accuracy to
monocular vision setting. the ﬁltering-based state-of-the-art EVIO, but also Image +
We implement the proposed algorithm in MATLAB. For Event shows the best accuracy all cases in overall.
an event tracker, we process events through the open-source The complementary characteristic of events and intensity
MATLAB script of [21] using only 1st part of the EM images is highlighted in Fig. 3. At 42 seconds, the position
algorithm. In particular, Harris corners are detected in a error of Image in Fig. 3c is suddenly increased due to the
648
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 15:55:45 UTC from IEEE Xplore.  Restrictions apply. Fig.5. Estimatedpositionsof(a)desk translationand(b)desk inf.
largeangularmotionthatisobservedinFig.3b.However,the
position error of Image + Event slowly increases regardless
of the rapid motion. A sample image is shown during that
maneuverinFig.1inwhichmostoftheintensityfeaturesare
failed,whereasthereareplentyofopticalﬂowmeasurements
from events. Fig. 6. Synchronized standard and event frame synthesized just for the
Tovalidatetheeffectoftheinversescene-depthconstraint, visualizationinahighdynamicrange(a,b),andinahugeangularvelocity
(c,d)oftheauthor-collecteddataset.ThelegendisthesameasinFig.1.
wetestallsequencesinTableIusingopticalﬂowfromevents
with (Event) and without (uEvent) the projection (12). As a
result, 3 out of 10 sequences are failed meaning that the
fromastreamofeventscouldimprovethepositionaccuracy
ﬁlter diverges in uEvent. Fig. 4 shows the position error
in the author-collected dataset.
and estimated scene-depth in boxes translation. In Event,
Our future works include a real-time implementation and
the ﬁlter state is projected onto the constrained surface
evaluation of the algorithm. This would enable us to further
when 1/λˆ >4m. This effectively maintains the scene-depth
analyzethecomputationaltimequantitativelyoftheproposed
as physically feasible metric. In contrast, the scene-depth
method. We expect that the proposed fusion scheme can
in uEvent exceeds the upper limit and the position error
be implemented using an optimization back-end, and would
diverges.
yield comparable pose accuracy to the state-of-the-art opti-
B. The author-collected dataset mizationcounterpart,[13].Inadditiontothis,wehavefound
that a corner extraction from the naively reconstructed event
We recorded two sequences of dataset called
frame includes even edges that cause the aperture problem
desk translation and desk inf using DAVIS-240C (iniVation
duetotheblurredframe.Asfuturework,thiscanberesolved
AG) in a desk environment. The author excited the hand-
∞ byusingasharpeventframefromthelifetimeestimation[7],
held sensor making the rectilinear and shaped trajectory
or the event-based corner detector [26] can be employed.
shown in Fig. 5. They contains challenging sequences
exhibiting the maximum angular rate as 294.4 and 297.6
ACKNOWLEDGMENT
deg/s respectively. Limitations on the standard frames are
seen in Fig. 6(a,c): a low light condition and high dynamic Wethanktheauthorsin[21]forreleasingtheeventtracker
motion. However, events is able to capture a set of optical as an open-source.
ﬂowinastreamofeventsasshowninFig.6(b,d).Estimated
positions are drawn in Fig. 5 in which Image does not show REFERENCES
∞
clear rectilinear trajectory in Fig. 5(a) nor shape in Fig.
×
[1] P.Lichtsteiner,C.Posch,andT.Delbruck,”A128 128120dB15
5(b), but we see the clear shape of trajectories in Image +
us Latency Asynchronous Temporal Contrast Vision Sensor,” IEEE
event. JournalofSoild-StateCircuits,vol.43,no.2,pp.566-576,2008.
×
[2] C.Brandli,R.Berner,M.Yang,S.Liu,andT.Delbruck,”A240
VI. CONCLUSION 180130dB3sLatencyGlobalShutterSpatiotemporalVisionSensor,”
IEEE Journal of Soild-State Circuits, vol.49, no.10, pp.2333-2341,
In this paper, we have proposed a ﬁltering-based pose es-
2014.
timatorthatfusesevents,images,andinertialmeasurements. [3] S. Leutenegger, S. Lynen, M. Bosse, R. Siegwart, and P. Furgale,
We directly model optical ﬂow from events with the in- ”Keyframe-based visual–inertial odometry using nonlinear optimiza-
tion,” The International Journal of Robotics Research, vol.34, no.3,
equality constraint and feature tracks from intensity images.
pp.314-334,2015.
The complementary characteristic of events and intensity [4] T.Qin,P.Li,andS.Shen.”Vins-mono:Arobustandversatilemonoc-
images yields better performance than when fused alone ular visual-inertial state estimator.” IEEE Transactions on Robotics,
no.34,no.4,1004-1020,2018.
with an IMU in the low light and fast motion scenario. Our
[5] S.Heo,andC.G.Park,”ConsistentEKF-basedvisual-inertialodome-
experimental results reveal that our method has decreased tryonmatrixLiegroup,”IEEESensorsJournal,vol.18,no.9,pp.3780-
the position error by 49.9% at the benchmark dataset when 3788,2018.
[6] R. Benosman, C. Clercq, X. Lagorce, S. Ieng, and C. Bartolozzi,
compared to the state-of-the-art ﬁltering-based estimator.
”Event-basedvisualﬂow,”IEEEtransactionsonneuralnetworksand
Moreover, we see that body velocity information obtained learningsystems,vo.25,no.2,pp.407-417,2013.
649
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 15:55:45 UTC from IEEE Xplore.  Restrictions apply. [7] E.Mueggler,C.Forster,N.Baumli,G.Gallego,andD.Scaramuzza, [18] A. Z. Zhu, N. Atanasov, and K. Daniilidis., ”Event-based feature
”Lifetimeestimationofeventsfromdynamicvisionsensors,”IEEEin- trackingwithprobabilisticdataassociation,”2017IEEEInternational
ternationalconferenceonRoboticsandAutomation(ICRA),pp.4874- ConferenceonRoboticsandAutomation(ICRA),pp.4465-4470,2017.
4881,2015. [19] B.D.Lucas,andT.Kanade,”Aniterativeimageregistrationtechnique
[8] G. Gallego, H. Rebecq, D. Scaramuzza, ”A unifying contrast maxi- with an application to stereo vision,” International Joint Conference
mization framework for event cameras, with applications to motion, onArtiﬁcialIntelligence(IJCAI),pp.674-679,1981
depth, and optical ﬂow estimation,” IEEE Conference on Computer [20] S. Granger, and X. Pennec, ”Multi-scale EM-ICP: A fast and robust
VisionandPatternRecognition(CVPR),pp.3867-3876,2018. approachforsurfaceregistration,”EuropeanConferenceonComputer
[9] T.Stoffregen,L.Kleeman,”Eventcameras,contrastmaximizationand Vision,pp.418-432,2002.
rewardfunctions:ananalysis,”IEEEConferenceonComputerVision [21] A.Z.Zhu,N.Atanasov,andK.Daniilidis,”Event-basedvisualinertial
andPatternRecognition(CVPR),pp.12300-12308,2019. odometry,” 2017 IEEE Conference on Computer Vision and Pattern
[10] G.Gallego,M.Gehrig,andD.Scaramuzza.”FocusIsAllYouNeed: Recognition(CVPR),pp.5816-5824,2017.
Loss Functions For Event-based Vision,” 2019 IEEE Conference on [22] A.I.Mourikis,andS.I.Roumeliotis,”Amulti-stateconstraintKalman
ComputerVisionandPatternRecognition(CVPR),pp.12280-12289, ﬁlter for vision-aided inertial navigation,” 2007 IEEE International
2019. ConferenceonRoboticsandAutomation(ICRA),pp.3565-3572,2007.
[11] D.Gehrig,H.Rebecq,G.Gallego,andD.Scaramuzza,”EKLT:Asyn- [23] M. Bloesch, S. Omari, P. Fankhauser, H. Sommer, C. Gehring, J.
chronous Photometric Feature Tracking Using Events and Frames,” Hwangbo,M.A.Hoepﬂinger,M.Hutter,andR.Siegwart,”Fusionof
InternationalJournalofComputerVision,pp.1-18,2019. optical ﬂow and inertial measurements for robust egomotion estima-
tion,”2014IEEE/RSJInternationalConferenceonIntelligentRobots
[12] H. Rebecq, T. Horstschaefer, D. Scaramuzza, ”Real-time Visual-
andSystems,pp.3102-3107,2014.
InertialOdometryforEventCamerasusingKeyframe-basedNonlinear
[24] Simon, Dan, ”Optimal state estimation: Kalman, H inﬁnity, and
Optimization,”InProc.Brit.Mach.Vis.Conf.,2017.
nonlinearapproaches,”JohnWiley&Sons,2006.
[13] A.R.Vidal,H.Rebecq,T.Horstschaefer,D.Scaramuzza,”Ultimate
[25] E.Mueggler,H.Rebecq,G.Gallego,T.Delbruck,andD.Scaramuzza,
SLAM?Combiningevents,images,andIMUforrobustvisualSLAM
”The event-camera dataset and simulator: Event-based data for pose
in HDR and high-speed scenarios,” IEEE Robotics and Automation
estimation, visual odometry, and SLAM,” The International Journal
Letters,vol.15,no.2pp.994-1001,2018.
ofRoboticsResearch,vol.36,no.2,pp.142-149,2017.
[14] E. Mueggler, G. Gallego, H. Rebecq, and D. Scaramuzza,
[26] E. Mueggler, C. Bartolozzi, and D. Scaramuzza, ”Fast Event-based
”Continuous-time visual-inertial odometry for event cameras,” IEEE
CornerDetection,”In.Proc.Brit.Mach.Vis.Conf.,2017.
TransactionsonRobotics,vol.34,no.6,pp.1425-1440,2018.
[15] D.Simon,”Kalmanﬁlteringwithstateconstraints:asurveyoflinear
and nonlinear algorithms.” IET Control Theory Applications, vol.4,
no.8,pp.1303-1318,2010.
[16] P.Vachhani,S.Narasimhan,andR.Rengaswamy,”Robustandreliable
estimationviaunscentedrecursivenonlineardynamicdatareconcilia-
tion,”Journalofprocesscontrol,vol.16,no.10,pp.1075-1086,2006.
[17] B.M.Bell,J.V.Burke,andG.Pillonetto.”Aninequalityconstrained
−
nonlinearKalman Bucysmootherbyinteriorpointlikelihoodmaxi-
mization,”Automatica,vol.45,no.1,pp.25-33,2009.
650
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 15:55:45 UTC from IEEE Xplore.  Restrictions apply. 