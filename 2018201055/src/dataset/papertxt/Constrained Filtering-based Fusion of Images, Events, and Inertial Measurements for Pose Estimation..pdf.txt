2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
Learning to Generate 6-DoF Grasp Poses with Reachability Awareness
Xibai Lou1, Yang Yang2 and Changhyun Choi1
Abstract—Motivated by the stringent requirements of un-
structured real-world where a plethora of unknown objects
resideinarbitrarylocationsofthesurface,weproposeavoxel-
based deep 3D Convolutional Neural Network (3D CNN) that B
generatesfeasible6-DoFgraspposesinunrestrictedworkspace
with reachability awareness. Unlike the majority of works
that predict if a proposed grasp pose within the restricted
workspacewillbesuccessfulsolelybasedongraspposestability,
our approach further learns a reachability predictor that
evaluatesifthegraspposeisreachableornotfromrobot’sown
experience.Toavoidthelaboriousrealtrainingdatacollection,
we exploit the power of simulation to train our networks
on a large-scale synthetic dataset. This work is an early Fig. 1: Example of searching for a feasible grasp pose.
attempt that simultaneously learns grasping reachability while Two clusters of objects are randomly arranged on the table.
proposing feasible grasp poses with 3D CNN. Experimental The green masks represent reachable approaching directions
results in both simulation and real-world demonstrate that
whereas the red mask is for unreachable ones. Note that in
our approach outperforms several other methods and achieves
the left ﬁgure the robot has reached its limit, wherein the
82.5% grasping success rate on unknown objects.
chance of ﬁnding a stable and reachable grasp in clutter A
Index Terms—Grasping, Deep Learning in Robotics and
is much greater than that in clutter B.
Automation, Perception for Grasping and Manipulation
I. INTRODUCTION
which is effective in learning stable 6-DoF grasp poses and
Real-world applications demand robotic manipulation al- generalizing to novel objects [4]. Furthermore, the relatively
gorithms that are efﬁcient in arbitrary workspace where small sim-to-real gap brought by depth is promising for
objects may not be reachable. Fig. 1 illustrates a scenario direct real-world application. RP is effective in estimating
where such an algorithm needs to 1) decide which of the the reachability of the sampled grasp poses without going
sampled grasp pose candidates are more reachable and 2) through the computationally expensive motion planning al-
grasp as many objects as possible from the dense clutter gorithms. This attribute is jointly determined by the grasp
with minimal efforts. pose and the kinematic constraints of a robot arm. Our
The predominant top-down grasping is often restricted in approachdiscoversthisintervenedrelationshipandimproves
narrowly prepared workspace [1], whereas practical prob- grasping efﬁciency by learning to approximate the grasping
lems are often in extended and obstacle-rich environments reachability from self-exploring experience. The immediate
that require ﬂexible 6-DoF grasp poses to reach objects. Al- challenge here is how to train these models, which typically
beit extensive researches have been conducted on this topic, require hundreds of thousands of labeled data in order to
the grasping reachability problem remains challenging. Cur- generalize. Many learning-based grasping approaches suf-
rent6-DoFapproachesgraspwithinrestrictedworkspaceand fered from insufﬁcient training data since real robot data
only predict successful grasps by analyzing grasp poses and are notoriously expensive to collect. We exploit the power
object shapes [2]. When applied to unrestricted workspace, of a robot simulator and solve this problem with large-scale
however, these approaches experience excessive planning self-supervision.
failures that jeopardize grasping efﬁciency. We present a
Our work is inspired by human behavior; we naturally
reachability aware 3D deep Convolutional Neural Network
prefer to grasp closer objects with appropriate hand and
(3D CNN) that addresses these concerns by proposing fea-
arm poses, whereas for distant objects we either adjust
sible 6-DoF grasp poses that are both stable and reachable.
our hand pose or abort grasping. Likewise, we propose an
Our approach consists of a 3D CNN and a Reachabil-
approach to mimic such a highly efﬁcient grasping strategy.
ity Predictor (RP). 3D CNN learns spatial information [3]
We conducted several experiments and ablation studies in
simulation as well as real-world where our approach outper-
*This work was in part supported by the MnDRIVE Initiative on
forms several comparable approaches in densely cluttered
Robotics,Sensors,andAdvancedManufacturing.
1X. Lou and C. Choi are with the Department of Elec{trical and Com- settings and generalizes to novel objects. To the best of
puterEn}gineering,Univ.ofMinnesota,Minneapolis,USA lou00015, our knowledge, our grasping strategy is the ﬁrst attempt
cchoi @umn.edu
to generate reachability aware 6-DoF poses in dense clutter
2Y.YangiswiththeDepartmentofComputerScienceandEngineering,
Univ.ofMinnesota,Minneapolis,USAyang5276@umn.edu usingavoxel-based3DCNN.Themaincontributionsofour
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 1532
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 01:16:49 UTC from IEEE Xplore.  Restrictions apply. work are bi-folded: Originated from computer vision tasks such as object
• 3DCNN-basedgraspposegenerationin6-DoFtakes recognition [3], 3D data segmentation [21], and scene com-
pletion [22], voxel-based 3D CNN have also been applied
advantage of our grasp pose sampling algorithm that
to robotic grasping [4]. Choi et al. showed 3D CNN trained
uniformly samples over the entire 3-dimensional space.
with real-robot data is effective in classifying discrete grasp
In order to predict feasible 6-DoF grasp pose and gen-
poses of a soft hand for a single object. Our works differ
eralizetonovelobjects,weexploitlarge-scalesynthetic
in two ways. First, our problem is much more difﬁcult
data collection via self-supervision. Furthermore, the
in that we search in a 6-DoF space, which is necessary
domain-invariantnatureof3DCNNfacilitatesthedirect
for non-compliant grippers. Second, generalization of 6-
application to real robot.
DoF grasping requires exponentially more data that are only
• ReachabilityPredictorlearnstherobotcapabilityfrom
possible to collect in simulation, where we develop a data
extensive self-exploration and eliminates the need for
collectionframeworkandshowourapproachcanbedirectly
human-imposed constraints such as workspace restric-
applied to real robot.
tions and approaching direction ﬁltering. It is able to
predict the reachability of the sampled grasp pose can-
B. Grasp Pose Reachability
didates,thusincreasethegraspingandplanningsuccess
rate of 6-DoF grasp pose in unrestricted workspace. Robotic manipulation such as grasping needs to solve an
Since it is decoupled from grasp learning, RP is ap- inversekinematicproblemtoﬁndapathforthemanipulation
plicable to other manipulation learning. tool. The grasping range of a given robot arm is determined
byitsmaximummanipulabilityoftheclosedkinematicchain
II. RELATEDWORK [23]. This problem is nontrivial and an analytical solution is
often not easy to ﬁnd. Modern approaches employ fast mo-
A. Object Grasping tion planning algorithms such as RRT [24], and its variants
[25].Duetotherandomexploringnatureofthesealgorithms,
Thoughtherearedifferenttaxonomiesofroboticgrasping
a solution for valid grasp poses is not guaranteed. To avoid
[5],[6],theexistingworksofroboticgraspingarecommonly
excessive planning failures, the majority of approaches in
dividedintotwogroups:traditionalmodel-basedandmodern
robotic grasping restricts testing objects within a restricted
learning-based approaches.
workspace [2], [4], [1], trading operation capability with
Traditionalmodel-basedapproachesofteninvolvephysical
computationsimplicity.Somepreviousworkstriedtoaddress
modeling of objects and thus require full knowledge of the
this limitation by an using ofﬂine database to estimate the
objects such as shapes, weights, friction coefﬁcients [7], [8].
grasping reachability [26]. Akinola et al. proposed an online
Morerecentapproachesutilizegraspqualitymetricstoselect
graspplanningmethodthatqueriesalargedatabaseoffeasi-
the force closure grasps from pre-planned sets by analyzing
ble grasp poses [27]. Distinguished from those approaches,
the contact wrench space [9]. These approaches grasp efﬁ-
our work is one of the early explorations that learns this
cientlywithaccuratemeasurementsandmodels,however,the
reachability from synthetic dataset, which allows the robot
prerequisite efforts scale-up fast when implemented in real
to work at an increased capacity.
unstructured environment where novel objects are prevalent.
Recent learning-based approaches apply deep neural net-
III. PROBLEMFORMULATION
works to various robotic grasping problems [10], [11], [12],
[13], [14], [15]. The majority of these approaches employ
We aim to ﬁnd feasible 6-DoF grasp poses that are
convolutional neural networks (CNNs) that take monocular
simultaneously stable and reachable. The problem is carried
RGB images or 2.5D depth image as input and map the
out in two stages. At the ﬁrst stage, the robot ﬁnds a set
extracted features to a less complicated 3-DoF grasp pose
of stable grasp candidates, some of which may not have
thatincludesagraspingpointon2Dimageplaneandacorre-
valid motion plans due to invalid inverse-kinematic solution.
spondingwristorientation[10],[12],[13],[1],[16].[17]and
At the second stage, the robot evaluates each grasp pose
[2]extendbeyondthestandard3-DoFapproaches.tenPaset
and excludes the unreachable ones. The problem can be
al.trainedthenetworkswithRGB-Dimagestoevaluateaset
formalized as follows:
of sampled grasp candidates. Since their sampling algorithm
is based on geometric reasoning, they were able to ﬁnd 6- Deﬁnition 1. A grasp pose X∈SE(3) is stable if it is able
DoF grasp poses. Due to the data hungry nature of learning- to form a force closure grasp. This attribute is independent
based approaches, generating large-scale training dataset is of kinematic constraints.
necessary. One approach is to label robot trails manually
Deﬁnition 2. A grasp pose X ∈ SE(3) is reachable if the
[16],[4].Alesslaboriouswayistocollectdatainsimulation
givenrobotarmisabletoachievetheposewithoutviolating
[12], [11], [13]. Although fast and scalable, the sim-to-real
the physical limits of itself and the environment.
gap may render features learned in simulation inaccurate in
real-world. Some re-train the network with real-world data Deﬁnition 3. Given a point cloud P ⊂ R3, the goal is to
[1],otherssolvetheproblembyeitherﬁne-tuningordomain ﬁnd a feasible grasp pose X ∈ SE(3) that is both stable
f
adaptation [18], [19], [20]. and reachable.
1533
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 01:16:49 UTC from IEEE Xplore.  Restrictions apply. P
∗ argmaxpf(X,R)
X1 X
Voxelization 3D CNN
Planar 
Segmentation
∗
P′ X2
··· ··· ···
p
∗
XN
Grasp Pose  Reachability 
Sampling Predictor
(cid:48)
Fig. 2: Grasping Pipeline. Object point cloud P is obtained from planar segmentation of point cloud P. For each of the
(cid:48)
sampled grasp poses X∈X, the object point cloud P is voxelized to voxel grid V and transformed by the corresponding
grasp pose candidate X. The input voxel grid is then passed to 3D CNN while the grasp candidate X is fed to RP for
evaluation. The most probable grasp pose is chosen and executed by the robot manipulator.
The grasp pose X is deﬁned with respect to the robot metric, i.e., S =1 does not indicate S =1 and vice versa.
s r
coordinate frame. The point cloud P is obtained via a depth Let S denote a binary-valued grasping feasibility metric,
g
sensor with known extrinsic parameters, by which the cloud where S = 1 indicates the grasp pose is feasible, and
g
P is transformed from the sensor coordinate frame to the therefore stable and reachable according to Deﬁnition 3.
robot coordinate frame. An important assumption is:
IV. PROPOSEDAPPROACH
Assumption 1. The set of 6-DoF grasp candidates X is
A. Generating Feasible Grasp Poses
randomly generated over the points p in the point cloud
P, i.e., p∈P. The objective of our approach is to predict the most fea-
sible grasp pose from a set of randomly sampled candidates
Given a point cloud P and a grasp candidate X ∈ X, in multiple settings. According to Deﬁnition 3, the selected
where X denotes a set of N uniformly generated 6-DoF grasp pose should be both stable and reachable. Training
grasp candidates, let Ss(X)∈ {0,1} denote a binary-valued one generic model on this task leads to unsatisfactory per-
stability metric where Ss = 1 indicates that the grasp is formance, as the network may falsely ascribe the reason of
stable according to Deﬁnition 1. Our goal is to estimate
a failed grasp to unstable grasp poses, while the true cause
the grasping stability ps(X) = Pr(Ss = 1|X) by self- is poor reachability, and vice versa. As mentioned in the
supervised learning. To train the network more efﬁciently,
previous section, these two prerequisites of a feasible grasp
we constrain our grasp sampling algorithm as follows:
poseareentirelyindependentofeachother.Thisallowsusto
Constraint 1. The grasp pose X ∈ SE(3) is constrained solve the credit assignment problem by decoupling the task
in that the sampled grasp candidates are limited so as to into two independent sub-problems. The grasping success
approach the target object from the top hemisphere. probability pg(X,R) can be decomposed as follow:
For an arbitrary grasp candidate X, the wrist orientation pg =Pr(Sg =1|X,R)
has no inﬂuence over its reachability since the joint limit =Pr(S =1,S =1|X,R)
r s (1)
is (−π,π). Therefore, a valid robot grasp X is determined
by the combination of grasp location (x,y,z) ∈ R3 and =Pr(Sr =1|Ss =1,X,R)×Pr(Ss =1|X)
R =p | (X,R)×p (X)
approaching direction (a ,a ,a ) ∈ 3. We construct the rs s
x y z
reachability determinant a = (x,y,z,a ,a ,a ). To allow We trained a 3D CNN grasp pose predictor and a reacha-
x y z
the robot fully explore the workspace, we assume that: bility predictor to estimate the resulting grasping stability p
s
and grasping reachability p | respectively.
Assumption 2. Therobotworkspaceisunrestrictedandcan rs
The grasping pipeline is described in Fig. 2. The system
be anywhere within the camera observation space. (cid:48)
ﬁrst obtains the object point cloud P from planar segmen-
Let R denote the robot and S (X,R) ∈ {0,1} a binary- tation of point cloud P and samples N grasp candidates
r (cid:48)
valued reachability metric, where S = 1 indicates that over P via the sampling algorithm described in Section IV-
r
the grasp is reachable. We wish to learn to predict the B. For each sampled grasp candidate, objects point cloud
Z × ×
grasping reachability p (X,R) = Pr(S = 1|X,R) from is voxelized to a 3D voxel grid V ∈ 32 32 32, where
r r
self-supervised exploration. It is important to notice that the each voxel in the grid is either 0 (not occupied) or 1
stabilitymetricofaposeXisindependentofitsreachability (occupied). The total physical edge length of the voxel
1534
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 01:16:49 UTC from IEEE Xplore.  Restrictions apply. Algorithm 1 Reachability Aware 3D CNN Grasping
%)
re12a::chPXOIan(cid:48)bu←p←itlpuituGtyP:trl:paparfesoendaipanisPctirbtooSlcsereloeNgguSrmdraaesmpPnptp,laoi3stDenigXo(CnfPN(∈(cid:48)NP)S)EGr(a3s)p model Ns, PredictionAccuracy(0000....78895050 1 5 10 15 20 25 30 35 40 45 50 55 60
NumberofTrainingSamples(1e3)
3: for X∈X do Fig. 4: Prediction accuracy w.r.t. the size of dataset. 3D
4: a←Extract(X) CNN demands large-scale labeled data, tailored to the task,
(cid:48)
5: V ←Voxelization(P ,X) and such requirement is only attainable in simulation.
6: p | ←N .Feedforward(a)
rs r
7: ps ←Ns.Evaluate(V) target objects within two threshold values. Given a desired
8: pg(X)←pr|s×ps number of samples N and approaching vector thresholds
◦ ◦
9: X ←argmax p (X) γ ,γ ∈ [0 ,90 ], where γ < γ , the algorithm uniformly
f X g 1 2 1 2
10: Grasp(X ) selects N grasp points from the input point cloud P. For
f
each grasp point, a random pose is associated. Fig. 3 shows
three examples of the sampled grasp pose candidates. The
sampling algorithm gives little constraints to the generated
grasp candidates, so the network is able to learn from the
failed grasps, such as not to grasp a corner or collide with
the object.
Fig. 3: Examples of randomly sampled grasp candidates. C. Network Architecture
The grasp candidates are sampled over a triangle shape. For We borrow the network structure from [4]; the ﬁrst layer
each candidate, the wrist orientation is sampled in [0,2π] has32ﬁltersofsize5×5×5,thesecondlayerhas32ﬁlters
while approaching direction bounded by [γ ,γ ]. Different of 3×3×3. The features are condensed by a Max Pooling
1 2
thresholds of the approaching direction are visualized here. layer of 2×2×2, followed by two dense layers of 128 and
◦
Left ﬁgure is searching in full 6-DoF by setting γ = 0 and 1. A given grasp pose can either be 1 (stable grasp) or 0
◦ 1
γ = 90 , the middle one shows a search constrained by γ (unstable grasp), so we use binary cross-entropy as the loss
2 ◦ ◦ 1
= 30 and γ = 60 and on the right is a top grasp pose by function. We use the sigmoid activation function in the ﬁnal
2
setting both γ and γ to 0. layer to predict the grasping stability for the voxel grid V.
1 2
Reachability predictor consists of an input layer, one
hidden layer of size 16, one hidden layer of size 8
grid is 0.1m, equivalent to crop the object point cloud by
and a ﬁnal output layer. The input to the neural net-
a 0.001m3 cubic box that centered at grasping point p.
work is a 6-dimensional reachability determinant a =
We choose cropping rather than segmenting an object from
(x,y,z,a ,a ,a ), and the output is a binary classiﬁcation
the point cloud as it preserves surrounding geometry that x y z
result, where 1 denotes the grasp pose is reachable.
helps avoid collisions in dense clutter. The object voxel grid
To integrate these two networks, we embed the grasping
V may partially contain voxels of adjacent objects, which
reachability into the grasping success probability by mul-
contributestolowergraspingstabilitypredictions.Therefore,
tiplying the two results. The ﬁnal output p indicates the
less surrounded objects, i.e., objects on the peripherals, are g
grasping success probability of X.
prioritized, resulting in an onion-peeling grasping pattern.
Our 3D CNN architecture is similar to those in [3] and D. Data Collection and Training
[4].Weembedthegraspposewithinthevoxelgridbytrans-
Training 3D CNN with multiple objects is ineffective
formingitwithgraspcandidateX.The3DCNNwillpredict because their shape varies for every grasp pose, preventing
the grasping stability ps(X) based on the transformed voxel 3D CNN from generalizing object geometries. We used 8
grid.Thereachabilitypredictorthenextractsthereachability
primitive shapes from [1] as our training objects. The self-
R
determinant a ∈ 6 from each randomly generated pose supervised robot interacts with a single object and collects
X and estimates the grasping reachability pr|s(X,R). The 60,000 labeled voxel grids to train the 3D CNN and 10,000
grasping success probability pg is calculated by multiplying data to train the RP. A testing dataset of 1000 data is
thetwoterms,Algorithm1explainsthispredictionprocedure
reserved to evaluate the prediction accuracy of 3D CNN.
in detail.
Fig. 4 compiles this result with respect to the training data
size from 1,000 to 60,000. It is clear that the prediction
B. Grasp Pose Sampling Algorithm
accuracy beneﬁts from large-scale training data. We also
Unlike the geometric reasoning-based sampling algorithm show twoprediction resultsofthe RPin Fig. 5,as expected,
proposed in [2], to ensure that the robot comprehensively an approaching direction toward the robot itself results in
explores the 6-DoF action space, we present a ﬂexible narrower reachable space. Our approach learns to select
algorithm that uniformly samples grasp candidates over the candidates with more vertical approaching direction as they
1535
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 01:16:49 UTC from IEEE Xplore.  Restrictions apply. %)100 RAND
Rate( 6800 GVPPGD1c
GPD
ng 40 VPG-G
aspi 20 OURS
Gr 0
challenging standard topgrasping
%)100
Fig.5:Visualization of Reachability Predictor.Wevisual- ate( 80 RGAPDN1Dc
izethepredictionsfortwoposesoveranarbitraryworkspace, R 60 VPG
n GPD
colored in black. Dark blue indicates poor reachability. pletio 2400 VOPUGR-SG
m
Co 0
challenging standard topgrasping
Fig. 7: Performance in simulation. The grasping success
rate (top) and completion rate (bottom) of each approach in
three different settings. The plots clearly show the effective-
nessofourapproach,whichachieves71.5%graspingsuccess
rate in challenging scenario and 100% completion rate in all
experiments.
(a) Standard (b) Challenging (c) Ablation
TABLE I: Ablation Study of Reachability Predictor
Fig. 6: Simulation experiments. Our approach is evaluated
in (a) the standard scenario with randomly arranged 10 ob-
RAND 3DCNN OURS
jects,(b)thechallengingscenariowith30randomlydropped GraspingSuccessRate 13.33 32.67 82.67
objects and (c) ablation study. PlanningSuccessRate 26.67 37.33 96.00
are generally more stable and reachable in our training and
scenario. The goal is to grasp 10 objects that are randomly
testing environments.
dropped to the center of the ground. We notice that the RP
effectivelypenalizesapproachingdirectionstowardtherobot
V. EXPERIMENTS
whengraspingpointexceedsitslearnedthresholds,asshown
Our experiments aim to answer the following four ques-
in Fig. 5. To fairly compare our approach and GPD with
tions: i) How much does our approach outperform other
VPG, we also report the top grasping success rate for each
approaches? ii) Can our approach generalize to cluttered
method.Aninterestingobservationisthattheperformanceof
objects and novel objects? iii) Is the reachability predictor
GPD improves signiﬁcantly as top-down grasping poses are
effectively targeting reachable objects? iv) Do our networks
mostly stable and reachable in this setting. This indirectly
trained in simulation work well in real robot?
corroborates the importance of reachability awareness when
To answer these questions, we conducted experiments in
proposing 6-DoF poses. The challenging scenario compares
both simulated and real settings. We compare our approach
our approach to the others in a densely cluttered setting
with 3 other approaches: 1) RAND, a baseline that randomly
where 30 objects are randomly dropped to the center. This
generates a grasp pose without any learning, 2) GPD, Grasp
triples the workspace density thus demonstrates our ability
Pose Detection proposes 6-DoF grasp candidates based-on
to generalize to more cluttered scenarios.
geometricreasoningandevaluatesthemwithRGB-Dimages
Fig. 7 presents the average grasping success rates and
trained CNN [2] and 3) VPG, Visual Pushing for Grasping
completion rates over 30 runs for each method, where
[1] learns the synergy between pushing and grasping with a
graspingsuccessrate= numberofsuccessfulgrasps andcompletion
reinforcement learning to clean cluttered objects. numberofproposedgrasps
rate = numberofobjectsgrasped. With only one camera and no
totalnumberofobjects
A. Simulation Experiments help of pushing to declutter the challenging scenario, our
The simulation environment is built in V-REP [28] with approach achieved the highest grasping success rate and
Bullet[29]Physicsengine2.83.ItincludesaUR5robotarm completion rate. Our approach suggests feasible grasp poses
equipped with an RG2 gripper. We noticed GPD under this as long as the object is within the view, while VPG may
one camera setup (GPD1c) can not perform well as their accidentally force objects out of its workspace.
geometric-based grasp sampling suffers from partial point We report an ablation study of the reachability predictor
cloud.AnothercamerawasaddedforGPDtoreproducetheir by grasping from two clusters of objects, one of which is
completesetup.TheexperimentsareillustratedinFig.6.For partially unreachable. We tested our approach with (OURS)
all the experiments, a grasp is successful only if the object and without (3DCNN) the RP to grasp the objects. Given
is lifted by 15 cm. If the robot failed 10 times (including only 5 chances for each run, the robot has to choose the
planning failures) consecutively or all the objects have been most stable and reachable pose to succeed. We report the
removed, this run is completed. averagegraspingsuccessrateandplanningrateover30runs
We adopt the same testing setup as reported in [1] for for RAND, 3DCNN and OURS in Table I, for a total of 150
a fair comparison. In a standard multiple objects grasping grasps. Planning efﬁciency = numberofsuccessfulplanning . Our
totalnumberofgraspingplanned
1536
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 01:16:49 UTC from IEEE Xplore.  Restrictions apply. Banana Power Bank Advil Tape Perrier bottle
Fig. 8: Real-world novel objects. We use ﬁve novel objects
to show the generalization capability of our approach. The
shape, texture and size of the testing objects are different
from the training objects in simulation as well.
%)100 RAND
Rate( 6800 GVPPGD1c
OURS
ng 40
pi
Gras 200 Fig. 10: Real-world experiment pictures. The voxel-based
advil banana perrierbottle tape allobjects 3D CNN approach minimizes the gap between simulation
Fig.9:Graspinggeneralizability.Thegraspingsuccessrate
and real-world. Our simulation trained network is able to
offourapproachesonthe5testobjects.Theaveragegrasping
clean dense clutters (top), differentiate reachable grasps
success rate of our approach is 82.5%, surpassing all others
(middle) and grasp an object in a constrained environment
on novel objects.
which is not feasible by the top grasping approaches, such
as VPG (bottom).
reachabilityaware3DCNNisabletoachieve96%planning
rate,animprovementof58.67%comparedto3DCNNonly. TABLE III: Reachability Predictor in Real-world
B. Real Robot Experiments
RAND 3DCNN OURS
We directly evaluate our approach on a Franka Emika GraspingSuccessRate 3.33 23.33 66.67
PlanningEfﬁciency 26.67 34.67 88.67
Panda robot arm without any ﬁne-tuning. Fig. 8 and Fig. 10
show the real robot experiments, which include four scenar-
ios: 1) challenging scenario, 2) ablation study, 3) random
when grasping considerably larger objects such as the water
household objects and 4) obstacle rich environment.
bottle,ourapproachpreferssmallershapessuchasthebottle
We compare our approach with RAND, GPD1c (GPD with
neck and cap. This is due to the limited physical voxel grid
one camera, due to hardware limitation), and VPG in the
size that can only ﬁt in shapes that are similar in size to our
challenging scenario, reproduced from simulation by 30 toy
training objects. We also give an example of our system’s
blocks. Table II compiles the performance of each method
grasping ﬂexibility by positioning an object under a chair to
from averaging the results of 10 runs. Our approach is
reﬂect real-world challenges. As shown in Fig. 10, the robot
able to perform consistently in real-world and achieves the
is able to complete the task by selecting a feasible grasp
highest grasping success rate and completion rate. Despite
which is not possible with the top grasping approaches.
the robustness of 3D CNN to sensor noise [4], our grasp
samplingalgorithmmayproposeavacantgraspingpointthat
VI. CONCLUSIONS
contributes to a misaligned pose. VPG occasionally forces
objectsoutofitsworkspaceandignoresthem.GPD1csuffers In this work, we presented a deep learning approach to
from predicting unreachable poses and partial point cloud. generate 6-DoF grasp poses with reachability awareness. A
Ablation scenario is reconstructed with toy blocks in the 3D CNN model that estimates grasping stability was trained
samefashionasinsimulation,ourapproachdemonstratesan with a large-scale dataset obtained from simulated self-
efﬁciency improvement as seen previously, but the gap be- supervision. A reachability predictor that improves reach-
tweenthesimulatedUR5andtherealPandaarmdeteriorates ability awareness of 3D CNN was trained similarly. Our
the performance. Quantitative results are shown in Table III. approachoutperformedseveralcomparabledeeplearningap-
To test our real-world generalizability, we further experi- proachesinbothsimulationandreal-world.Furthermore,our
mentwithnovelobjects,showninFig.8.Weran10testson method achieved 82.5% grasping success rate on unknown
eachobject,theresultissummarizedinFig.9.Ourapproach objects.AblationstudyshowedtheRPsigniﬁcantlyincreased
is able to extract 3D geometric features from novel shapes the planning efﬁciency of 3D CNN by 54% in real-world
andselectgraspposecandidatesaccordingly.Wenoticedthat experiments.
The limitations of our work suggest two directions for
futurework.First,ourpointcloud-basedsamplingalgorithm
TABLE II: Challenging Scenario in Real-world
is susceptible to sensor noise. It is of our interest to in-
vestigate whether a voxel-based method is able to enhance
RAND GPD1c VPG OURS
GraspingSuccessRate 13.10 31.23 64.76 75.20 its robustness. Second, we only applied RP to grasping and
CompletionRate 14.58 12.75 94.33 100.0 wouldliketoexploreifRPcanhelpothermotionprimitives.
1537
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 01:16:49 UTC from IEEE Xplore.  Restrictions apply. REFERENCES [22] S.Song,F.Yu,A.Zeng,A.X.Chang,M.Savva,andT.Funkhouser,
“Semantic scene completion from a single depth image,” in Pro-
ceedings of the IEEE Conference on Computer Vision and Pattern
[1] A.Zeng,S.Song,S.Welker,J.Lee,A.Rodriguez,andT.Funkhouser,
Recognition,2017,pp.1746–1754.
“Learning synergies between pushing and grasping with self-
[23] F. Parkand J. W.Kim, “Manipulability ofclosed kinematic chains,”
supervised deep reinforcement learning,” in IEEE/RSJ International
Journalofmechanicaldesign,vol.120,no.4,pp.542–548,1998.
ConferenceonIntelligentRobotsandSystems(IROS),2018.
[24] S.M.LaValle,J.J.Kuffner,andJr.,“Rapidly-exploringrandomtrees:
[2] A. ten Pas, M. Gualtieri, K. Saenko, and R. Platt, “Grasp pose
Progressandprospects,”2000.
detection in point clouds,” The International Journal of Robotics
[25] J.J.KuffnerJrandS.M.LaValle,“Rrt-connect:Anefﬁcientapproach
Research,vol.36,no.13-14,pp.1455–1473,2017.
tosingle-querypathplanning,”inICRA,vol.2,2000.
[3] D.MaturanaandS.Scherer,“Voxnet:A3dconvolutionalneuralnet-
[26] O. Porges, T. Stouraitis, C. Borst, and M. A. Roa, “Reachability
workforreal-timeobjectrecognition,”in2015IEEE/RSJInternational
and capability analysis for manipulation tasks,” in ROBOT2013:
ConferenceonIntelligentRobotsandSystems(IROS). IEEE,2015,
First Iberian Robotics Conference, M. A. Armada, A. Sanfeliu, and
pp.922–928.
M. Ferre, Eds. Cham: Springer International Publishing, 2014, pp.
[4] C. Choi, W. Schwarting, J. DelPreto, and D. Rus, “Learning object
703–718.
graspingforsoftrobothands,”IEEERoboticsandAutomationLetters,
[27] I. Akinola, J. Varley, B. Chen, and P. K. Allen, “Workspace aware
vol.3,no.3,pp.2370–2377,2018.
onlinegraspplanning,”2018.
[5] A. Sahbani, S. El-Khoury, and P. Bidaud, “An overview of 3d
[28] E. Rohmer, S. P. Singh, and M. Freese, “V-rep: A versatile and
objectgraspsynthesisalgorithms,”RoboticsandAutonomousSystems,
scalablerobotsimulationframework,”in2013IEEE/RSJInternational
vol.60,no.3,pp.326–336,2012.
ConferenceonIntelligentRobotsandSystems. IEEE,2013,pp.1321–
[6] J. Bohg, A. Morales, T. Asfour, and D. Kragic, “Data-driven grasp 1326.
synthesisasurvey,”IEEETransactionsonRobotics,vol.30,no.2,pp. [29] E. Coumans, “Bullet physics simulation,” in ACM SIGGRAPH 2015
289–309,2013. Courses, ser. SIGGRAPH ’15. New York, NY, USA: ACM, 2015.
[7] A. Miller and P. Allen, “Graspit! a versatile simulator for robotic [Online].Available:http://doi.acm.org/10.1145/2776880.2792704
grasping,” IEEE Robotics & Automation Magazine, vol. 11, no. 4,
pp.110–122,2004.
[8] C.FerrariandJ.F.Canny,“Planningoptimalgrasps.”inICRA,vol.3,
1992,pp.2290–2295.
[9] J. Weisz and P. K. Allen, “Pose error robust grasping from contact
wrench space metrics,” in 2012 IEEE international conference on
roboticsandautomation. IEEE,2012,pp.557–562.
[10] I.Lenz,H.Lee,andA.Saxena,“Deeplearningfordetectingrobotic
grasps,”TheInternationalJournalofRoboticsResearch,vol.34,no.
4-5,pp.705–724,2015.
[11] D. Kappler, J. Bohg, and S. Schaal, “Leveraging big data for grasp
planning,” 2015 IEEE International Conference on Robotics and
Automation(ICRA),pp.4304–4311,2015.
[12] L. Pinto and A. Gupta, “Supersizing self-supervision: Learning to
graspfrom50ktriesand700robothours,”in2016IEEEinternational
conference on robotics and automation (ICRA). IEEE, 2016, pp.
3406–3413.
[13] J.Mahler,J.Liang,S.Niyaz,M.Laskey,R.Doan,X.Liu,J.A.Ojea,
and K. Goldberg, “Dex-net 2.0: Deep learning to plan robust grasps
withsyntheticpointcloudsandanalyticgraspmetrics,”arXivpreprint
arXiv:1703.09312,2017.
[14] H.Liang,X.Lou,andC.Choi,“Knowledgeinduceddeepq-network
foraslide-to-wallobjectgrasping,”arXivpreprintarXiv:1910.03781,
2019.
[15] Y.Yang,H.Liang,andC.Choi,“Adeeplearningapproachtograsping
theinvisible,”IEEERoboticsandAutomationLetters,2020.
[16] S.Levine,P.Pastor,A.Krizhevsky,J.Ibarz,andD.Quillen,“Learning
hand-eye coordination for robotic grasping with deep learning and
large-scale data collection,” The International Journal of Robotics
Research,vol.37,no.4-5,pp.421–436,2018.
[17] M. Gualtieri, A. Ten Pas, K. Saenko, and R. Platt, “High precision
graspposedetectionindenseclutter,”in2016IEEE/RSJInternational
ConferenceonIntelligentRobotsandSystems(IROS). IEEE,2016,
pp.598–605.
[18] K. Fang, Y. Zhu, A. Garg, A. Kurenkov, V. Mehta, L. Fei-Fei, and
S. Savarese, “Learning task-oriented grasping for tool manipulation
from simulated self-supervision,” arXiv preprint arXiv:1806.09266,
2018.
[19] Y. Chebotar, A. Handa, V. Makoviychuk, M. Macklin, J. Issac,
N. Ratliff, and D. Fox, “Closing the sim-to-real loop: Adapting
simulationrandomizationwithrealworldexperience,”arXivpreprint
arXiv:1810.05687,2018.
[20] K. Bousmalis, A. Irpan, P. Wohlhart, Y. Bai, M. Kelcey, M. Kalakr-
ishnan, L. Downs, J. Ibarz, P. Pastor, K. Konolige, et al., “Using
simulation and domain adaptation to improve efﬁciency of deep
roboticgrasping,”in2018IEEEInternationalConferenceonRobotics
andAutomation(ICRA). IEEE,2018,pp.4243–4250.
[21] H. Su, V. Jampani, D. Sun, S. Maji, E. Kalogerakis, M.-H. Yang,
and J. Kautz, “Splatnet: Sparse lattice networks for point cloud
processing,” in Proceedings of the IEEE Conference on Computer
VisionandPatternRecognition,2018,pp.2530–2539.
1538
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 01:16:49 UTC from IEEE Xplore.  Restrictions apply. 