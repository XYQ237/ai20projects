2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
Adversarial Feature Training
for Generalizable Robotic Visuomotor Control
Xi Chen1, Ali Ghadirzadeh1,2, Ma˚rten Bjo¨rkman1 and Patric Jensfelt1
Abstract—Deep reinforcement learning (RL) has enabled
training action-selection policies, end-to-end, by learning a
function which maps image pixels to action outputs. However,
it’s application to visuomotor robotic policy training has been
limited because of the challenge of large-scale data collection
when working with physical hardware. A suitable visuomotor
policyshouldperformwellnotjustforthetask-setupithasbeen
trainedfor,butalsoforallvarietiesofthetask,includingnovel
objects at different viewpoints surrounded by task-irrelevant
objects. However, it is impractical for a robotic setup to
sufﬁciently collect interactive samples in a RL framework to
generalize well to novel aspects of a task.
In this work, we demonstrate that by using adversarial
training for domain transfer, it is possible to train visuomotor
Fig.1:Apolicyisﬁrsttrainedinthesimplestpossiblesetup
policies based on RL frameworks, and then transfer the
basedonatemplateobjectwithoutclutter.Thetrainedpolicy
acquired policy to other novel task domains. We propose to
leverage the deep RL capabilities to learn complex visuomotor is then transferred to work with other task objects in a more
skills for uncomplicated task setups, and then exploit transfer realistic setup, i.e., in the presence of visual clutter.
learning to generalize to new task domains provided only still
imagesofthetaskinthetargetdomain.Weevaluateourmethod ceptionandcontrollayersofadeepnetworkinanend-to-end
on two real robotic tasks, picking and pouring, and compare
fashion.Theﬁrstapproachistotraintheperceptionlayersof
it to a number of prior works, demonstrating its superiority.
thenetworkseparatelybysyntheticvisualdatainsimulation
I. INTRODUCTION [3]–[7].Thediscrepanciesbetweenthereal-worlddatadistri-
bution and the simulated one is compensated by generating
Recently, end-to-end training of the perception system
a wide range of data in simulation by randomizing task-
jointlywiththemotorcontrolpartofadeepaction-selection
irrelevantelementsoftheenvironment,atechniqueknownas
policyhasshownremarkablesuccesstosolveawidevariety
domain-randomization. However, rendering a diverse set of
of robotic problems. Deep reinforcement learning (RL) has
syntheticimagesisoftentoolaborious.Thesecondapproach
become a popular framework to train visuomotor action-
is to introduce a number of auxiliary loss functions to pre-
selectionpoliciesthatdirectlymaprawimagepixelstomotor
traintheperceptionlayersfollowedbyend-to-endtrainingof
actions, eliminating the need for large-scale image labeling.
the entire network (see [8] for an overview). Auxiliary loss
However, lack of generality is still a common problem
functions are typically used to identify objects of interest
when training a visuomotor action-selection policy. As an
[2], to reconstruct them in an autoencoder architecture [9],
example,apolicytrainedtopourliquidintoamugwouldfail
[10], to localize them [11], or to predict the next visual
ifthemugisreplacedbyaglassthathasnotbeenseenduring
state of the objects based on the current one and the applied
the training. Similar failures might occur when changing
action [12]. However, compared to the end-to-end training
the scene, e.g., when adding novel visual clutter to the
approach, there is no direct way to extract visual features
background. Interactive RL training is generally very costly
that are relevant to the task. As an example, features which
and requires a considerable number of training samples to
aregoodatreconstructingtheoriginalimagesoftaskobjects
converge.Therefore,itisnotpracticaltotrainasinglerobotic
inanautoencoderarchitecturearenotnecessarilysuitablefor
policyfordifferentvariationsoftaskobjectsandcontexts[1].
object manipulation.
Anappealingsolutiontothisproblem,whichissuggestedby
Singh et al., [2], and which we pursue in the current work, In this work, we introduce an end-to-end training method
is to ﬁrst train a policy in a simple task setup and then to basedonadversarialtrainingtoextractvisualfeatureswhich
generalize to other variations of the task using still images generalize well to other instances of task objects, assuming
of task objects in different domains. thatweaklylabeledstillimagesofsuchobjectsareprovided.
Priorworkhasconsideredmainlytwodifferentapproaches Here, weak labels mean binary values specifying whether a
toaddressthelackofgeneralitywhenjointlytrainingtheper- given image contains a task object or not. The main idea,
showninFig.1,istotraintheperceptionsystemtogenerate
similar visual features in different contexts where the same
1RPL,KTHRoyalInstituteofTechnology,Sweden
sequence of motor actions successfully completes the task.
2IntelligentRoboticsresearchgroup,AaltoUniversity,Finland
[xi8][algh][celle][patric]@kth.se In our experiments, we show that we can train a policy for
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 1142
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 05:05:50 UTC from IEEE Xplore.  Restrictions apply. only a template object, an object with the simplest possible Singh et al., [2], proposed to include a binary classiﬁcation
geometryandtexture,withoutaddingvisualclutter,andthen taskwhich,similartoourwork,determinesiftheinputimage
selectively transfer the learned policy to other task objects. contains the task object or not. We extend the work in [2]
The advantages of our method are threefold: (1) improving by introducing an adversarial loss beside the classiﬁcation
the efﬁciency by training the most data demanding phase, loss. We empirically demonstrate that introducing this loss
i.e., the policy optimization, in an uncomplicated task setup, improves the performance considerably.
(2) circumventing the tedious and time-consuming labeling
B. Unsupervised domain adaptation
process, since unlike most prior work, our method does
not need bounding box labels to determine task objects, Unsupervised domain adaptation focuses on scenarios
and (3) gaining full control of what to include into the set where labeled data are not available from the target domain.
of task objects by weakly labeling images that contain the Earlier studies on domain adaptation try to learn domain-
desiredobjects.Ourempiricalanalysisdemonstratesthatour invariant features by minimizing differences between the
method outperforms prior work with a good margin, when domain distributions given a distance measure. The maxi-
performing the task for novel objects, objects for which mum mean discrepancy (MMD) loss is used by [18]–[20] to
only cluttered still images are provided. We demonstrate the align source and target features. The correlation alignment
appropriateness of our method by training and evaluating (CORAL) is applied in [21], [22] that tries to align the
feed-forward action-selection policies for two real robotic second-order statistics of the two domains. The contrastive
tasks,pickingandpouringusinganABBYuMirobot.Videos domain discrepancy (CDD) loss is proposed in [23], which
ofourexperimentscanbefoundathttps://youtu.be/ minimizestheintra-classdomaindiscrepancyandmaximizes
NADwzcm_FmQ. theinter-classdomaindiscrepancy.Othermethodschooseto
The organization of this paper is as follows: In the learn domain-agnostic representations by using adversarial
next section, we review related work. Sec. III introduces training [24], [25]. Tzeng et al., [26] proposed a general
our method. Experimental results are provided in Sec. IV. framework for adversarial deep domain adaptation (ADDA)
Finally, Sec. V will conclude our discussion and suggest which uses GAN [27] loss to align the two domains. Most
future work. of these methods are applied to computer vision classiﬁca-
tion tasks with relatively small domain shifts. Our method
II. RELATEDWORK combines the adversarial domain adaptation in [26] with the
In this section, we review prior work to extract a compact attention mechanism introduced in [2] to attend to the task
feature representation of visual inputs for learning visuomo- objects and align the feature representations of the target
tor control policies. Also, since our method is categorized domain and the source domain.
as transfer learning, we brieﬂy describe related studies to
III. METHOD
unsupervised domain adaptation.
In this section, we introduce our method based on adver-
A. Visual representation learning for visuomotor control sarialtrainingtoextractasetofvisualfeaturesastheoutput
of a perception model which generalizes well to other task
Levineetal.,[13]proposedend-to-endtrainingofpercep-
related objects.
tionandcontrolnetworksforanumberofroboticvisuomotor
tasks.However,trainedpolicieseasilyfailinnewtasksetups
because of the limited number of training samples which
can be realistically collected from a real robotic setup. Pre-
training the perception model using autoencoders [3], [9],
[10] has gained in popularity since such training does not
requirerealrobotsamples.However,thesemethodsmaynot
beabletoextracttask-speciﬁcfeaturesastheyarenottrained
Fig. 2: Our deep policy network maps an input sensory data
explicitly for a given task.
to a sequence of actions through three sub-networks: the
Training with auxiliary tasks which share the state repre-
perceptionnetwork,thelow-dimensionalpolicynetworkand
sentations with the primary task is another commonly used
the motor trajectory network.
approach to acquire task-speciﬁc features [12], [14]–[16].
In robotic manipulation applications, object detection and
A. Problem Formulation
localization can be considered as two important auxiliary
taskstohelptheextractionofsuitablevisualfeatures.Devin The problem we address is how to train a deep feed-
et al., [11] proposed a two-stage approach that ﬁrst trains forward policy network which maps an input image I to a
a task-independent attention module to attend to objects sequence of motor actions u − , where T is the length
t=0:T 1
in the scene. Then, they train a task-speciﬁc network to of the motor trajectory, and u is the M-dimensional motor
t
ﬁnd out the task objects provided a number of human action at time-step t. Similar to our earlier work [10], we
demonstrations. Jang et al., [17] proposed a method to learn split a deep policy network into three sub-networks, which
arepresentation tolocalizeobjects bycomparing thechange are (1) the perception, (2) the low-dimensional policy, and
in the visual features when adding or removing an object. (3) the motor trajectory networks, as shown in the Fig. 2.
1143
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 05:05:50 UTC from IEEE Xplore.  Restrictions apply. The perception network is responsible for ﬁnding a suit- representedbythelatentvalues.Thetaskdatasetincludesthe
able representation of the input image. The primarily goal input-output data of the pre-trained low-dimensional policy
of this work is to ﬁnd a method to train the perception and the perception network.
D
modelsuchthatconsistentfeaturesareextractedfordifferent Task object dataset (target domain data ): Since our
T
task objects. This would lead to a higher success rate when goal is to train a policy which generalizes well to other task
handling different objects, since the output of the perception objects, we collect still images of such objects surrounded
networkwouldbesimilardespitedifferencesintaskobjects. by visual clutter as they appear in real task setups. The task
Themotortrajectorynetworkisagenerativemodelwhich object dataset includes these images accompanied by weak
mapsalatentvariableinputintoatrajectoryofmotoractions. labelsthatspecifywhetheranimagecontainsataskobjector
This means that the sequential decision making problem not. However, we do not specify which object it is or where
is converted into a multi-armed bandit problem without the object is located in the image.
temporal complexities. In other words, instead of searching
for optimal motor actions for every time-step, a single low- C. Adversarial Feature Training
dimensionallatentvariable(typicallylessthan5dimensions)
In this section, we introduce our method for end-to-
that represents the complete motor trajectory is searched
end training of the perception and policy networks. These
for. Similarly to our earlier work [3], [8], [10], variational
networks are pre-trained jointly using the RL method as
autoencoders (VAEs) [28] are used to ﬁnd the trajectory
explained earlier. In this phase, both are further trained to
space, with the trajectory network pre-trained and kept ﬁxed
perform well for novel task objects. The problem we are
during policy training that follows.
addressingcanbeformulatedasatransferlearningproblem,
The low-dimensional policy is implemented as a small
where the primarily goal is to transfer the knowledge gained
network which maps visual features to the latent space of
from RL training with the template object (source domain
the generative model. The parameters of this network are D D
)totherestoftaskobjects(targetdomain ).Themain
commonly trained using reinforcement learning, e.g., the S T
idea is to extract a set of visual features as the output of the
proximalpolicyoptimization(PPO)method[29].Inpractice,
perception model which are suitable for the RL task while
due to the reduced dimensionalities of both features and
not being distinguishable whether the input image comes
trajectories,mostmodernpolicyoptimizationmethodcanbe
from the source domain or the target domain.
applied. During training, an input image is ﬁrst represented
We introduce two auxiliary networks, a discriminator and
by a set of features when it is passed through the perception
aclassiﬁer,whicharenotpartoftheﬁnalmodelbutwillonly
model.Thepolicynetworkmapsthefeaturestoavalueinthe
be used during training. The discriminator network receives
latentspace.Thelatentvalueisthenpassedtothegenerative
the visual features generated by the perception model and
model to produce a full trajectory of the motor actions. A
decides whether the input is an image from the source
terminal reward value is received at the completion of the
domain or from the target domain. Similarly, the classiﬁer
task, determining the success of the agent to accomplish
network receives the visual features as input and outputs a
the goal. The policy is then trained, end-to-end, with the
classiﬁcation of whether there is a task object in a target
perception network, to maximize the expected value of the
domain input image or not.
terminal reward.
Wetraintheperception,thepolicy,aswellastheauxiliary
Since the main focus of this paper is on training the
networks, the discriminator and the classiﬁer, jointly by op-
perception network, we refer to our earlier studies ( [3], [8],
timizingthethreelossfunctionsintroducedinthefollowing:
[10])fordetailsonthetrainingofpoliciesandthegenerative
L L D L D L D ∪D
motor trajectory models. = ( )+ ( )+ ( ), (1)
task S c T D S T
B. Training Datasets where L is the task loss function, L is the binary
task L c
To avoid the expensive process of training deep visuomo- classiﬁcation loss, and is the binary discriminator loss.
D
tor policies based on real robot data and at the same time Details on the losses are provided as follows:
L
allow policies to generalize to novel variations of the task, Task loss: is a mean squared error (MSE) loss
task
we propose to ﬁrst train the policy given interactive data applied to the source domain data to learn the mapping
collected by playing with a template object only, and then from the template object images to the corresponding latent
to transfer the gained manipulation knowledge to the rest of motor trajectory space as provided by the task dataset. The
the task objects assuming that still images of those objects perception and the policy networks are optimized jointly to
are provided. Therefore, we need to construct two sets of minimize the task loss function.
L
training data as described in the following: Classiﬁcation loss: is a binary cross-entropy classiﬁca-
D c
Task dataset (source domain data ): Using reinforce- tionloss,similarto[2],whichisusedtoclassifywhetherthe
S
ment learning we train the low-dimensional policy and the inputimagecontainsataskobjectornot.Theperceptionand
perceptionmodeljointlyusingvisualimagesofthetemplate classiﬁernetworksaretrainedjointlyusingthetargetdomain
object without background clutter. Once the policy training data.This lossfunction helpsthe perceptionmodelto attend
has converged, we record samples including input visual to task objects without requiring an extra effort to explicitly
imagesandoutputmotoractions.Inourcase,theactionsare locate the objects by manually drawing bounding-boxes.
1144
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 05:05:50 UTC from IEEE Xplore.  Restrictions apply. L
Discriminator loss: is a discriminator loss that forces
D
the extracted features to be distributed similarly irrespective
of their origin, whether the features come from the source
domaindataorthetargetdomaindata,i.ewhethertheimage Fig. 3: Task objects used in the pouring task. The object on
contains a single template object or a target object hidden in theleftisthetemplateobject.ThetargetobjectIDislabeled
visual clutter. on the right bottom of the images.
Thediscriminatorandperceptionmodelsaretrainedusing
theadversarialtrainingparadigm.Givensampleimagesfrom image.Wecollect70imagesforeachtaskobject.Therefore,
×
the source and the target domains, the perception model there are 3 70 = 210 images for the pouring task and
×
generates visual features corresponding to the input. The 15 70 = 1,050 images for the picking task. We collect
discriminator network is then trained to tell the source and 1,000imagesofvisualclutter,whicharesharedbetweenthe
the target domain data apart by observing the corresponding pouring and picking tasks. The locations and orientations of
feature vector. Next, the perception network is updated to all objects are randomly selected, as well as the number of
generate new visual features such that the current discrim- task-irrelevant objects representing the visual clutter.
inator cannot identify whether the features come from the
source domain or from the target domain. B. Baseline methods
To sum up, the task and discriminator losses are required We compare our method to two baseline methods closely
to extract a set of visual features suitable for accomplishing related to ours: ADDA [26] and GPLAC [2]. ADDA is
the task, not only for the template object, but for the rest of an unsupervised domain adaptation method which uses a
the task objects when these are surrounded by visual clutter. task loss and a discriminator loss to update the network.
The classiﬁcation loss speeds up the training by implicitly GPLAC is a method that learns generalizable robotic skills
guiding the network where to attend in the input image. fromweaklylabeleddatausingtaskandclassiﬁcationlosses,
while our method uses a combination of task, discriminator
IV. EXPERIMENT
and classiﬁcation losses. However, since neither baseline
In the experiment section, we demonstrate two real-world method is able to perform satisfactorily on the task dataset
robotic manipulation tasks and evaluate our method based thatwehave,weprepareanothertaskdatasetwherethetem-
on (1) if our method can handle different visual clutter in plate object is placed together with task-irrelevant objects.
the scene, and (2) if the learned policy can generalize well We thus reduce the complexity of the task by providing
to different task objects, including objects that were unseen extra information of the irrelevant objects, which reduces
during policy training. the discrepancy between the source and target domains.
We train two more models of ADDA and GPLAC using
A. Tasks setup and data collection
the task dataset with visual clutter and refer to these as
Our experiments are performed on an ABB Yumi robot. ADDA extraInfoandGPLAC extraInfo.Fig.5illustratesthe
The experiments include two tasks, a pouring task and a training setup for each method.
picking task. In the pouring task, the robot needs to pour There are some implementation differences between the
the content of a small cup into a desired cup placed on a baseline methods used in our work and that of the original
table, and in the picking task, the robot has to grasp and paper. For ADDA in [26], the perception network for the
lift a cuboid object from the table. The cups and the cuboid source and target domains are separate. In our work, we
objects are placed together with other task-irrelevant objects foundthatsharingtheweightsoftheperceptionnetworkgen-
that are regarded as visual clutter. For each task, we ﬁrst erates better performance, especially for ADDA extraInfo.
train a policy using a template object and then transfer the Thus, we train ADDA and ADDA extraInfo using a shared
learned policy to other task objects, as explained in Sec. III. perception network for both source and target domains. As
We select 3 cups for the pouring task and 20 cuboid objects describedin[2],GPLACappliesaspatialsoftmaxlayerafter
for the picking task. The template objects and task objects the convolution layer to extract the position of points of
used in the experiments are shown in Fig. 3 and 4. maximal activation in each channel. In order to extract the
As described in Section III-B, for each task, we ﬁrst train spatial feature, we increase the number of channels from 1
the low-dimensional policy and perception model jointly to16andextract16pointsasspatialfeatures,whicharethen
on the template object using reinforcement learning. Then feed to the rest of the network.
we record 500 input RGB images of the template object
C. Experiment results
in different locations and orientations, together with 500
corresponding latent values of the low-dimensional policy We perform 10 tests for each task object, which is placed
as the task dataset (source domain). in different random locations and orientations with different
Thetargetdomaindatasetcontainsstillimagesofthetask conﬁgurations of visual clutter, and assign a score to rep-
objects and visual clutter. To each image in this dataset, resent the completion of the task. For the pouring task, we
we assign a binary label. If the image contains only visual assign0whenthecontentfallsoutsidethedesiredcupand1
clutter, we labeled it as 1, but if it contains a task object, we when it falls inside. For the picking task, we assign 0 when
label it as 0. Maximum one task object can appear in each the robot fails to grasp or lift the task object, 0.5 when the
1145
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 05:05:50 UTC from IEEE Xplore.  Restrictions apply. Fig. 4: Task objects used in the picking task. The object on the left is the template object. The object ID is labeled on the
right bottom of the image. Objects from 16-20 are unseen objects which are not included in the weakly labeled dataset.
TABLE II: Success rate of picking task on target objects
ADDA GPLAC
objectID Ours ADDA
extraInfo extraInfo
1 1.00 0.70 0.85 0.85
2 0.90 0.25 0.60 0.05
3 0.95 0.60 0.50 0.50
4 0.90 0.40 0.55 0.45
5 0.90 0.30 0.80 0.10
Fig. 5: Two task datasets used in the experiments. We 6 1.00 0.25 0.70 0.95
7 0.80 0.50 0.80 0.40
train our method, ADDA and GPLAC using the task
8 0.85 0.60 0.65 0.30
dataset contains only the template object (top left), and we 9 0.80 0.25 0.40 1.00
train ADDA extraInfo and GPLAC extraInfo using the task 10 0.90 0.45 0.80 0.60
11 0.80 0.15 0.85 0.35
dataset contains both the template object and visual clutter
12 0.90 0.00 0.90 0.95
(top right). All methods should adapt to the same target 13 0.75 0.00 0.75 0.80
domain that contains task object and visual clutter (middle). 14 0.85 0.85 0.60 0.00
15 0.65 0.00 0.35 0.00
Average
0.86 0.35 0.67 0.49
TABLE I: Success rate of pouring task on target objects Successrate
ADDA GPLAC
objectID Ours
extraInfo extraInfo
1 1.0 0.9 1.0 resulting in the policy not showing a desired behavior on
2 1.0 0.0 0.0 the task object. In Fig. 7, we see that the spatial features
3 1.0 0.7 0.1
extracted by GPLAC are spread over the entire image.
Average
1.00 0.53 0.37
Successrate Whenweaddtask-irrelevantobjectstothesourcedomain
for training, both ADDA extraInfo and GPLAC extraInfo
result in improved performance, especially for cup 1 in the
robot successfully lifts it but with an unstable grasping pose pouring task (in Fig. 3) which is similar to the template
(e.g. grasp the edge of the object), and 1 when the robot cup. However, they still fail to generalize to objects with a
successfully lifts it with a grasping pose close to the center differentcolorthanthetemplateobject,suchascup2.From
of object. We measure the average score as the success rate Fig. 7 we can see that ADDA extraInfo generates stronger
andlisttheexperimentalresultofthepouringtaskinTableI features on cup 1 and cup 3, but could not detect cup 2.
and the picking task in Table II. Additionally, in Table III, GPLAC extraInfo also detects cup 1 successfully but fails
we report the picking performance on 5 objects that were on cup 2 and cup 3.
not seen during training. The results of ADDA and GPLAC Forbettervisualizationofthedistributionofeachdomain,
on the pouring task and the result of GPLAC on the picking we plot the extracted features in Fig. 6. The dimension of
task are not reported as we failed to learn a working policy. features is reduced to 2 using t-SNE [30]. Cases containing
1) Learningtoignorevisualclutter: Ourmethodachieves template object are plotted in red, those containing task ob-
100% success rate on the pouring task while ADDA and jectsareplottedingreen,datacontainingpuretask-irrelevant
GPLAC fail to complete the task. In the pouring task, the objects are plotted in blue. In the ideal case, the red points
model should recognize the task object while ignoring the should mix with the green points, meaning that the features
visual clutter including objects that look very similar to the extracted from the source domain (red points) are well-
templateobject.ThistaskisdifﬁcultforADDAasthemodel aligned with the features extracted from the target domain
easily misinterprets a cup in the visual clutter as the target (greenpoints).Thebluepointsshouldbewellseparatedfrom
cup. In Fig. 7, we can see that ADDA extracts stronger both red points and green points, meaning that we are not
features on the visual clutter cup than the task cup, which usingfeaturesfromthetask-irrelevanttolearnthepolicy.Itis
leads the policy to attend to the wrong cup. The task is also clearfromFig.6thatthebluepointsandthegreenpointsof
challenging for GPLAC. Even with the classiﬁcation loss ADDA are mixed, which indicates ADDA extracts features
on the weakly labeled dataset, which helps the network to fromthewrongobjects.Afteraddingextrainformationtothe
attend to features on the task object, the features are not source domain, the alignment/separation are improved. The
strong enough to eliminate the inﬂuence of visual clutter in result of GPLAC is not as clear as other methods, as it uses
the scene. The perception network is not able to generate 16 spatial point feature instead of image features. However,
features consistent between the source and target domains, we can still observe improvements in local areas.
1146
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 05:05:50 UTC from IEEE Xplore.  Restrictions apply. Fig. 6: The distribution of visual features extracted by different methods. The distribution of data used for pouring task is
shown in the ﬁrst row and the distribution for picking task is shown in the second row. The dimension of the feature is
reduced using t-SNE. The features of images contain template object are plotted in red, those contain task object are plotted
in green, and data of visual clutter are plotted in blue.
considerablyimprovestheperformance.However,compared
toourmethod,theabilityofgeneralizeisstilllimitedforthe
baseline methods. As reported in Table II and Table III, we
achieveasuccessrateofover80%onmosttaskobjectswhile
the baseline methods, even with extra training information,
only perform better on a few objects. The visualization of
thefeaturedistributionisshowninFig.6,whereourmethod
demonstratesabetteralignment/separationperformancethan
the baseline methods.
V. CONCLUSIONS
In this paper, we introduced a method to train visuomotor
robotic control policies which generalize well to unseen
task domains. We formulated the problem as a transfer
learning problem in which the labels (correct motor actions)
Fig. 7: Examples of visual feature extracted by different are missing for the target domain data, i.e., images of the
methods. The image in the ﬁrst column is the input RGB task objects. We demonstrated that we can train a policy
dataandtherestoftheimagesaretheextractedfeaturefrom for an uncomplicated task setup using a template object,
theinputimage.ForGPLACandGPLAC extraInfo,weplot and then generalize to novel task domains using the ad-
the extracted points on the input images. versarial learning approach and the auxiliary classiﬁcation
task introduced in this work. In this way, there is no need
TABLE III: Success rate of picking task on unseen objects for RL interactive training by collecting a huge number
of real robot data. Furthermore, tedious image labeling is
ADDA GPLAC
objectID Ours ADDA also avoided since, similar to the prior work, we only need
extraInfo extraInfo
16 0.75 0.10 1.00 1.00 weakly labeled images. We evaluated our method on two
17 0.70 0.20 0.55 0.00 real robotic tasks, pouring and picking, and compared it to
18 0.85 0.55 0.45 0.65
twobaselinemethods.Theexperimentalresultsdemonstrated
19 0.90 0.65 0.80 0.60
20 0.90 0.00 0.00 0.00 that our method achieves considerably better performance to
Average 0.82 0.30 0.56 0.45 successfully accomplish the tasks with novel task objects.
Successrate
Asapartofourfuturework,weareinterestedinapplying
themethodtomoreelaboratetaskswhichrequirebetterscene
understanding to manipulate objects, e.g., pushing objects
2) Generalizing to similar targets: In the picking task,
while avoiding obstacles. Also, our method can be applied
the model should apply the picking motion learned from
to sim-to-real transfer learning to learn a task completely in
the template object to a variety of other objects that are
simulation with a number of template objects and then to
different in terms of size, color and texture (Fig. 4). In this
transfer the policy to manipulate real objects.
experiment, ADDA performs better than for the previous
VI. ACKNOWLEDGMENTS
task, as the task objects are not of the same shape as the
task-irrelevant objects. However, due to the same reason This work is supported by the European Unions Horizon
discussed in the previous task, GPLAC still suffers from 2020researchandinnovationprogram,thesocSMCsproject
confusion with the task-irrelevant objects. Similar to the (H2020-FETPROACT-2014), and also by the Academy of
pouring task, adding extra information to the source domain Finland through the DEEPEN project.
1147
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 05:05:50 UTC from IEEE Xplore.  Restrictions apply. REFERENCES [14] B. Hariharan, P. Arbela´ez, R. Girshick, and J. Malik, “Simultaneous
detection and segmentation,” in European Conference on Computer
[1] X.Chen,A.Ghadirzadeh,J.Folkesson,M.Bjo¨rkman,andP.Jensfelt,
Vision. Springer,2014,pp.297–312.
“Deep reinforcement learning to acquire navigation skills for wheel-
[15] Z.Zhang,P.Luo,C.C.Loy,andX.Tang,“Faciallandmarkdetection
leggedrobotsincomplexenvironments,”in2018IEEE/RSJInterna-
by deep multi-task learning,” in European conference on computer
tionalConferenceonIntelligentRobotsandSystems(IROS). IEEE,
vision. Springer,2014,pp.94–108.
2018,pp.3110–3116.
[16] A. Gupta, C. Devin, Y. Liu, P. Abbeel, and S. Levine, “Learning
[2] A.Singh,L.Yang,andS.Levine,“Gplac:Generalizingvision-based
invariantfeaturespacestotransferskillswithreinforcementlearning,”
robotic skills using weakly labeled images,” in Proceedings of the
arXivpreprintarXiv:1703.02949,2017.
IEEEInternationalConferenceonComputerVision,2017,pp.5851–
[17] E.Jang,C.Devin,V.Vanhoucke,andS.Levine,“Grasp2vec:Learning
5860.
object representations from self-supervised grasping,” arXiv preprint
[3] A.Ha¨ma¨la¨inen,K.Arndt,A.Ghadirzadeh,andV.Kyrki,“Affordance
arXiv:1811.06964,2018.
learning for end-to-end visuomotor robot control,” arXiv preprint
[18] E. Tzeng, J. Hoffman, N. Zhang, K. Saenko, and T. Darrell, “Deep
arXiv:1903.04053,2019.
domainconfusion:Maximizingfordomaininvariance,”arXivpreprint
[4] F. Zhang, J. Leitner, M. Milford, and P. Corke, “Sim-to-real transfer
arXiv:1412.3474,2014.
ofvisuo-motorpoliciesforreachinginclutter:Domainrandomization
[19] A.Rozantsev,M.Salzmann,andP.Fua,“Beyondsharingweightsfor
andadaptationwithmodularnetworks,”world,vol.7,no.8,2017.
deepdomainadaptation,”IEEEtransactionsonpatternanalysisand
[5] E. Tzeng, C. Devin, J. Hoffman, C. Finn, X. Peng, S. Levine,
machineintelligence,vol.41,no.4,pp.801–814,2018.
K. Saenko, and T. Darrell, “Towards adapting deep visuomotor rep-
[20] M. Long, Y. Cao, J. Wang, and M. I. Jordan, “Learning trans-
resentations from simulated to real environments,” arXiv preprint
ferable features with deep adaptation networks,” arXiv preprint
arXiv:1511.07111,vol.2,no.3,2015.
arXiv:1502.02791,2015.
[6] J.Tobin,R.Fong,A.Ray,J.Schneider,W.Zaremba,andP.Abbeel,
[21] B.Sun,J.Feng,andK.Saenko,“Returnoffrustratinglyeasydomain
“Domain randomization for transferring deep neural networks from
adaptation,” in Thirtieth AAAI Conference on Artiﬁcial Intelligence,
simulation to the real world,” in 2017 IEEE/RSJ International Con-
2016.
ferenceonIntelligentRobotsandSystems(IROS). IEEE,2017,pp.
[22] B. Sun and K. Saenko, “Deep coral: Correlation alignment for deep
23–30. domain adaptation,” in European Conference on Computer Vision.
[7] S. James, A. J. Davison, and E. Johns, “Transferring end-to-end
Springer,2016,pp.443–450.
visuomotor control from simulation to real world for a multi-stage
[23] G.Kang,L.Jiang,Y.Yang,andA.G.Hauptmann,“Contrastiveadap-
task,”arXivpreprintarXiv:1707.02267,2017. tation network for unsupervised domain adaptation,” in Proceedings
[8] A.Ghadirzadeh,“Sensorimotorrobotpolicytrainingusingreinforce- oftheIEEEConferenceonComputerVisionandPatternRecognition,
mentlearning,”Ph.D.dissertation,KTHRoyalInstituteofTechnology,
2019,pp.4893–4902.
2018.
[24] Y. Ganin and V. Lempitsky, “Unsupervised domain adaptation by
[9] C. Finn, X. Y. Tan, Y. Duan, T. Darrell, S. Levine, and P. Abbeel, backpropagation,”arXivpreprintarXiv:1409.7495,2014.
“Deep spatial autoencoders for visuomotor learning,” in 2016 IEEE
[25] Y.Ganin,E.Ustinova,H.Ajakan,P.Germain,H.Larochelle,F.Lavio-
InternationalConferenceonRoboticsandAutomation(ICRA). IEEE,
lette,M.Marchand,andV.Lempitsky,“Domain-adversarialtrainingof
2016,pp.512–519. neuralnetworks,”TheJournalofMachineLearningResearch,vol.17,
[10] A.Ghadirzadeh,A.Maki,D.Kragic,andM.Bjo¨rkman,“Deeppredic-
no.1,pp.2096–2030,2016.
tivepolicytrainingusingreinforcementlearning,”in2017IEEE/RSJ
[26] E.Tzeng,J.Hoffman,K.Saenko,andT.Darrell,“Adversarialdiscrim-
International Conference on Intelligent Robots and Systems (IROS). inativedomainadaptation,”inProceedingsoftheIEEEConferenceon
IEEE,2017,pp.2351–2358. ComputerVisionandPatternRecognition,2017,pp.7167–7176.
[11] C. Devin, P. Abbeel, T. Darrell, and S. Levine, “Deep object-centric
[27] I.Goodfellow,J.Pouget-Abadie,M.Mirza,B.Xu,D.Warde-Farley,
representations for generalizable robot learning,” in 2018 IEEE In-
S.Ozair,A.Courville,andY.Bengio,“Generativeadversarialnets,”in
ternationalConferenceonRoboticsandAutomation(ICRA). IEEE, Advancesinneuralinformationprocessingsystems,2014,pp.2672–
2018,pp.7111–7118.
2680.
[12] M.Jaderberg,V.Mnih,W.M.Czarnecki,T.Schaul,J.Z.Leibo,D.Sil-
[28] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,”
ver,andK.Kavukcuoglu,“Reinforcementlearningwithunsupervised arXivpreprintarXiv:1312.6114,2013.
auxiliarytasks,”arXivpreprintarXiv:1611.05397,2016.
[29] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,
[13] S. Levine, C. Finn, T. Darrell, and P. Abbeel, “End-to-end training “Proximal policy optimization algorithms,” arXiv preprint
of deep visuomotor policies,” The Journal of Machine Learning arXiv:1707.06347,2017.
Research,vol.17,no.1,pp.1334–1373,2016. [30] L.v.d.MaatenandG.Hinton,“Visualizingdatausingt-sne,”Journal
ofmachinelearningresearch,vol.9,no.Nov,pp.2579–2605,2008.
1148
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 05:05:50 UTC from IEEE Xplore.  Restrictions apply. 