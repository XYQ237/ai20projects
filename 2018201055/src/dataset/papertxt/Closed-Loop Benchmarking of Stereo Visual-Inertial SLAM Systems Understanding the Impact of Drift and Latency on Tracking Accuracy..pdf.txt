2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
Schmidt-EKF-based Visual-Inertial Moving Object Tracking
Kevin Eckenhoff, Patrick Geneva, Nathaniel Merrill, and Guoquan Huang
Abstract—In this paper we investigate the effect of tightly- approach[8].Intheformer,theego-motionandobjecttrack-
coupled estimation on the performance of visual-inertial lo- ing are formulated as a single joint estimation problem [10],
calization and dynamic object pose tracking. In particular,
while in the latter the processes are performed separately,
we show that while a joint estimation system outperforms its
often conditioning the target tracking on the output of the
decoupledcounterpartwhengivena“proper”modelforthetar-
get’smotion,inconsistentmodeling,suchaschoosingimproper localizationmodule[11].Inthispaper,buildingoffourprior
levels for the target’s propagation noises, can actually lead to work on tightly-coupled visual-inertial localization and rigid
a degradation in ego-motion accuracy. To address the realistic body target tracking [10], we further investigate the effect
scenario where a good prior knowledge of the target’s motion
that target motion modeling has on overall estimation per-
model is not available, we design a new system based on the
formance. In particular, we show that while tightly-coupling
Schmidt-Kalman Filter (SKF), in which target measurements
donotupdatethenavigationstates,howeverallcorrelationsare the systems leads to improved accuracy of both processes
stillproperlytracked.Thisallowsforbothconsistentmodeling (that is, the localization performance is improved when co-
of the target errors and the ability to update target estimates estimating the target rather than simply discarding these
whenever the tracking sensor receives non-target data such
measurements), this beneﬁt is only seen if the target motion
as bearing measurements to static, 3D environmental features.
noisevaluesare“properly”chosen.Iftheyareoverconﬁdent,
We show in extensive simulation that this system, along with
a robot-centric representation of the target, leads to robust thiswillactuallyleadtoadegradingoftheVIOperformance.
estimationperformanceeveninthepresenceofaninconsistent This can be particularly catastrophic in cases where an
target motion model. Finally, the system is validated in a real- autonomous vehicle performs the tracking and relies on the
world experiment, and is shown to offer accurate localization
accuracy of its VIO to maintain operation.
and object pose tracking performance.
Therefore, we propose to leverage Schmidt-Kalman Fil-
I. INTRODUCTION tering (SKF) [12], an extension to the standard Extended
The ability for a sensor-platform to track its ego-motion Kalman Filter (EKF), and employ a local (robot-centric)
and perceive its environment is a critical component in representation of the target, in particular, when there is low
many autonomous systems. Cameras and inertial measure- conﬁdence in the chosen target model. This allows for the
ment units (IMUs) have become the standard sensor de- VIO estimates to be provably the same as if target measure-
ployment for many of these applications such as unmanned ments were discarded, however all correlations between the
autonomous vehicles (UAVs) and mobile devices [1] due to target and IMU are still properly modeled. A summary of
the affordability and light-weight nature of these sensors. this work’s contributions are the following:
Assuch,visual-inertialodometry(VIO),whichfusesvisual- • We investigate the choice of dynamic object motion
inertial data to estimate the ego-motion of the platform, has noises in a joint visual-inertial localization and target
seen a recent explosion in research efforts [2]–[6]. tracking ﬁlter, and show through simulation that the
In many robotics applications, not only is the user inter- selection of these values can lead to either improved
ested in estimating their own motion, but also the tracking or degraded accuracy of the VIO.
of external bodies (targets) whose states are only indirectly • WeofferanewSchmidt-EKFformulationthatdoesnot
observed through exteroceptive sensors mounted on the allow target measurements to update navigation states
tracking robot. External object tracking may be necessary while still consistently tracking all correlations, and
for safe navigation in dynamic environments, as in au- show that this system can lead to robust estimation
tonomous driving [7], or may even be the overall goal of accuracy even with inconsistent model selection.
the sensor deployment, as in military surveillance. For these • We advocate a robot-centric representation of the tar-
reasons, the problem of simultaneous localization, mapping, get’s pose that is shown to offer improved performance
and moving object tracking (SLAMMOT) is important but for both the EKF and SKF formulations.
challenging [8]. However, estimating the poses and motion • Theproposedsystemisvalidatedinareal-worldexper-
of external bodies using visual-inertial sensing has received iment where it is shown to offer accurate localization
less attention, with a few notable exceptions [9], [10]. and dynamic object pose tracking estimation.
When performing SLAMMOT, a question remains open
II. RELATEDWORK
till date whether to use a tightly-coupled or loosely-coupled
While SLAMMOT has seen study in other works [8],
This work was partially supported by the University of Delaware (UD) [13], few leverage visual-inertial sensors coupled with a
CollegeofEngineering,theNSF(IIS-1924897),theARL(W911NF-19-2-
pose representation of the target. In particular, many target
0226,JWS10-051-003),BoschResearch,andGoogleARCore.Genevawas
partially supported by the Delaware Space Grant College and Fellowship tracking systems treat the target as a single 3D point [14].
Program(NASAGrantNNX15AI19H)). With this, only measurements of this representative feature
The authors are with the Robot Perception and Navigation
can be used to update the target, thereby ignoring the addi-
G{roup (RPNG), University of Delaware, N}ewark, DE 19716, USA.
keck,pgeneva,nmerrill,ghuang @udel.edu tional information provided by other features that typically
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 651
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 15:56:03 UTC from IEEE Xplore.  Restrictions apply. exist on the object’s body. Another limitation of a point motionassumptionsbyleveragingtheSchmidt-KalmanFilter
particle model is that it requires continuous observation [12] formulation, allowing for consistent estimation of the
of this single point in order to update. However, in many target while preventing corruption of the tracking robot’s
scenariosthetrackingrobotmayviewthetargetfromvaried VIO.
viewing angles thereby occluding this feature despite the The SKF formulation updates estimates for only a cer-
object remaining in view of the sensor. Treating the target tain subset of variables, keeping the estimates of a set of
as a pose with a rigidly connected point cloud relaxes this “nuisance”parametersﬁxed,whilestillconsistentlytracking
assumption as we can gain information of the target state all correlations. While this leads to computational gains, as
when any of its features are viewed. has been investigated within the VIO community to reduce
Works that do perform full target pose estimation often complexity [21]–[23], here we leverage the SKF to prevent
leverage more advanced sensors such as LIDAR [15] or inconsistent target models from corrupting the trajectory
RGB-D cameras [16], limiting application for lightweight estimates of the tracking robot. We note the SKF has been
Micro-Aerial Vehicles (MAVs) due to weight and cost con- previously investigated in the context of target tracking to
straints. Other target pose-based estimation methods require address this issue or to avoid estimating navigation errors,
additional prior knowledge to handle ambiguity of the es- however these works do not consider the visual-inertial
timation problem, such as the target’s scale in monocular domain or object pose-tracking as in this work [24]–[26].
vision. For example, Li et. al [11] used a dimensional prior
III. VISUAL-INERTIALESTIMATION
ofthetargetandconditionedontheoutputoftheego-motion
The proposed visual-inertial target tracking system serves
estimation (a decoupled approach), and thus did not account
asanextensiono(cid:2)fthestandardMSCKFframew(cid:3)ork[19].We
for the uncertainty in these estimates.
deﬁne the IMU state to be estimated as:
The work closest to ours, introduced by Qiu et al. [9],
(cid:62) (cid:62) (cid:62) (cid:62) (cid:62) (cid:62)
utilized a robust monocular visual-inertial batch-based esti- x = I q¯ Gp Gv b b (1)
I G I I ω a
mator [3] which was ﬁrst used to track the motion of the where I q¯is the JPL-convention quaternion [27] parametriz-
sensor platform. The target object was detected using the ing theGrotation matrix I R that rotates vectors from the
learning-based object recognition system YOLO [17], and a gravity-aligned world framGe into the local IMU frame. Gp
I
vision-only structure-from-motion problem was then used to and Gv are the position and velocity of the IMU as
I
estimate (up-to-scale) the point cloud of the target as well expressedintheglobalframe,andb andb arerespectively
ω a
as the relative pose between the camera and target, while the gyroscope and accelerometer biases that corrupt the
metricscalewasestimatedusingtracecorrelation.Whilethis correspondingsensors.Thevectorerrorstatex˜ isdeﬁnedby
system was shown to offer robust monocular pose-tracking themappingx=xˆ(cid:1)x˜ wherexˆ isthecurrentbestestimate,
performance, due to the lack of explicitly estimating the andthe (cid:1)operationmapsmanifoldestimates andcorrection
target motion parameters, it is unclear whether this method vectors to an updated manifold element [28]. For vectors
canbeusedforactivetrackingpurposesasfuturetargetstates this operation is standard addition, while for quaternions we
arenotpredicted.Bycontrast,oursystemfusesobservations utilize the left multiplicative quaternion error [27].
of the target in a tightly-coupled probabilistic formulation, In addition to this evolving inertial state, following the
and is able to improve overall trajectory accuracy through standard MSCKF formulation, we also keep a estimate of
target observation information, its assumed motion model, the past IM(cid:104)U poses at the last m imaging times. We d(cid:105)enote
and proper modeling of its uncertainty. As we additionally this set of clone states as:
estimate motion parameters, we are also able to both predict (cid:62)
thetarget’sposeatfuturetimestepsandprovideanassociated x = Ik−1q¯(cid:62) Gp(cid:62) ··· Ik−mq¯(cid:62) Gp(cid:62)
IC G Ik−1 G Ik−m
uncertainty of this prediction that can be useful for active Thegoalofthesystemistotrackthemotionofanexternal
tracking scenarios [18]. object’s pose and its local point cloud of rigidly attached
In our previous work [10], which was built on the featuresthatcanbeseenandtrackedbythesensor’scamera.
light-weight Multi-State Constraint Kalman Filter (MSCKF) While here we only consider the local velocity model, as in
framework [19], we advocated the tight-coupling of the our prior work [10], one could model the target’s motion
tracker’s visual-inertial navigation and the target rigid-body in many different ways. In the chosen model, the estimated
pose estimation to improve the accuracy of both processes. target state is expr(cid:2)essed as a pose in some re(cid:3)ference frame
{ }
We represented the tracking robot and target in the same R and a local linear and angular velocity.
global frame and analyzed the impact of different assumed (cid:62) (cid:62) (cid:62) (cid:62) (cid:62)
targetmotionmodels.Bycontrast,inthispaper,weadvocate xT = TRq¯ RpT TvT TωT
the“local”representationofthetargetposethatisexpressed Note that the angular velocity, Tω is that of the target, and
T
in the tracking robot’s frame of reference. We additionally is distinct from the angular velocity of the IMU platform.
investigate how in certain scenarios the original tightly- We distinguish between two target pose representations in
coupled system may lead to a decreased trajectory accuracy this work: global and local. In the global representation, the
in the presence of poor target motion model assumptions. target pose is represented in the same global frame as the
While previous methods handle model uncertainty by using IMU (R = G), while in the local representation, the target
multiple estimators, each using a different model, these lead pose is expressed relative to the IMU (R=I).
to a large computational increase [20]. To address this issue, Lastly, assuming the target object moves as a rigid body,
we present an alternative solution that can handle incorrect we estimate its local point cloud that is made up by a set of
652
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 15:56:03 UTC from IEEE Xplore.  Restrictions apply. 3D features, Tp , that remain static in the target frame. In B. General Target Update
fi
order to better add these points into our state using delayed
During the motion of the tracking sensor platform and
(cid:104) (cid:105)
initialization [29], we will also maintain a set of target pose target,camerabearingmeasu(cid:0)remen(cid:1)tstofeaturesonthetarget
clones:
are collected. The measurement function is given by:
(cid:62)
xTC = TRkk−−11q¯(cid:62) Rk−1p(cid:62)Tk−1 ··· TRkk−−mmq¯(cid:62) Rk−mp(cid:62)Tk−m zif =Π Cipf +nif (7)
Note that the target clones in the local formulation are Here Cipf denotes the position of the 3D feature in the
expressed with respect to the corresponding IMU clone, measuring camera frame, Π is the projection function
(cid:62) (cid:62)
rather than the e(cid:2)volving IMU state. The full st(cid:3)ate estimated Π([x y z] ) = [x/z y/z] mapping a feature position ex-
for a single target feature is then given by: pressed in the camera frame into the corresponding normal-
∼N
(cid:62) (cid:62) (cid:62) (cid:62) (cid:62) (cid:62) izedpixelcoordinatemeasurement,andn (0,R )is
x= x x x x Tp (2) if if
I T IC TC f the Gaussian white-noise corrupting the measurement. The
A. System Dynamic Models position of the featur(cid:0)e in the camera frame u(cid:1)sing the global
The visual-inertial system under consideration contains and local represe(cid:0)ntations is given(cid:1)respectively by:
−
boththeIMU(i.e.,sensorplatform)andthetargetdynamics. Cip =CRI R GRTp +Gp Gp +Cp (8)
The inertial dynamics is standard and can be found in [27]. f I G T f T I I
=CR IRTp +Ip +Cp (9)
Unlike the sensor platform, we are not privy to any direct I T f T I
{ }
proprioceptive information for the target, and must perform where CR,Cp represents the relative pose between the
I I
prediction only using its estimated motion parameters. The IMU and camera. After linearizing our target feature mea-
dynamics of the global target model are given by: surement (7) about the current state estimate we perform an
TR˙ =−(cid:98)Tω ×(cid:99)TR, Gp˙ =GRTv (3) EKF update of the state estimate, xˆ, and covariance P:
G T G T T T (cid:9) (cid:62) − −
In the local representation the evolution of the target is K=P H S 1 :=LS 1 (10)
⊕ (cid:9)(cid:1) −
coupled with that of the IMU: xˆ =xˆ K(z Π(Cipˆ )) (11)
if f
⊕ (cid:9)− (cid:62) (cid:9)
TR˙ =TR˙GR+TRGR˙ P =P KH P (12)
I G I G I
−(cid:98) ×(cid:99) (cid:98) ×(cid:99) (cid:9) (cid:62)
=−(cid:98)T(cid:0)ωT×(cid:99)TGRGI R(cid:1)+(cid:98)TGRGI×R(cid:99)IωI whereHisthemeasurementJacobian,S=HP H +Rif,
= TωT −TIR(cid:0)+TIR⇒IωI (cid:1) (cid:0) ((cid:1)4) aunsidngCitpˆhef cisurtrheentrebseuslttsotafteeveasltuiamtiantges.evIanluaadtidnigtio(n8)woer u(9se)
Ip =I R Gp Gp (cid:9) ⊕
T G T I and to distinguish between the state/covariance before
−(cid:98) ×(cid:99) − −
Ip˙ = Iω I R Gp Gp +I R Gv Gv and after update, respectively. Besides these described target
T I G T I G T I
=−(cid:98)Iω ×(cid:99)Ip +IRTv −I RGv (5) measurements, the system additionally performs updates
I T T T G I using bearing measurements to static environmental features
Inbothrepresentationswemodelthelocallinearandangular
in the standard MSCKF manner [19].
velocities as random walks:
Tv˙ =n ,Tω˙ =n (6) IV. NUMERICALANALYSISOFMODELINGERRORS
T vt T ωt
While it has been shown that the co-estimation of target
Thus the amount that the target conforms to the assumed
tracking and localization within a tightly-coupled ﬁlter can
motion model is captured by controlling the values of its
improve performance [30], we have found through simula-
propagation noise (6).
tions that this conclusion is more nuanced. At the root of
1) CovariancePropagation: Basedontheabovetracking
the proposed framework is an assumption that the observed
robot and target motion models, we can deﬁne the propaga-
target will follow the selected motion model which we have
tion of the error state of the stacked system. Let x denote
S inordertoperformpredictionofthetarget’sfuturetrajectory.
the set of zero-dynamics static variables (whose true values
In the case that the target model is not accurate, e.g., too
do not evolve in time, such as static environmental or target
features, as well as possibleﬁxedcalibration parameters). small of noises are used in (6), inconsistencies will be
(cid:20) (cid:21)
introducedasweareoverconﬁdentinhowthetargetevolves
The overall linearized error state evolution is then given by:
      
and, in the worst case, negatively affect the accuracy of the
x˜˙I ≈ Fx 0 0 x˜I Gx 0 n trackingrobot’strajectory.Conversely,ifwepicktoolargeof
x˜˙T Ax AT 0 x˜T + Γx ΓT nI noise values, then the motion parameter estimation becomes
x˜˙S 0 0 0 x˜S 0 0 T unstable, and we do not gain much information to improve
where F is the Jacobian of the IMU’s error state evolution the tracking robot’s estimate.
x
withrespecttotheIMUerrors,A andA aretheJacobians To demonstrate this, a numerical simulation scenario was
x T
of the target’s error state evolution with respect to the IMU created, where a UAV equipped with an IMU and a stereo
and target errors. Lastly G ,Γ , and Γ are the Jacobians camerafollowsaplanartargetrobotwhichtravelsinasemi-
x x T
with respect to the IMU and target propagation noises (n circular pattern. A B-spline was ﬁtted to the tracking robot’s
I
andn ).Inthecaseoftheglobaltargetposerepresentation, trajectory to allow for calculation of the groundtruth angular
T
the IMU and target evolution will be decoupled, and thus velocities and linear accelerations. From these, the noisy
bothA andΓ willbezeromatrices.Fromthesecontinuous IMUmeasurements(andcorrespondingrandom-walkbiases)
x x
error-state dynamics, the standard EKF propagation can be were simulated at a rate of 200 Hz. A map of 3D points
performed [27]. was simulated along the ﬂoor and borders of the workspace.
653
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 15:56:03 UTC from IEEE Xplore.  Restrictions apply. Synthetic stereo images were generated at a frequency of
10 Hz by projecting the map points into the groundtruth
cameraposesandcorruptedwithonepixelnoise.Thetarget’s
pointcloudwassimulatedbyrigidlyattachingasetofpoints
lying on the surface of a one meter cube to the pose of the
groundtruth target at each timestep. Both occlusions due to
the target and randomly losing track of a given feature were
simulatedtomodelproblemsfacedbyreal-worldfront-ends.
As the target object operated on a plane, we modeled the
linear velocity as a random walk only in the local x and
y directions, while the angular velocity was a random walk
along local z-axis. We note that this type of target is very
common in real-world scenarios, such as when tracking a
terrestrial vehicle.
To evaluate the effect of different noise parameters on
estimation, we chose a single noise standard deviation σ =
t
σ = σ = σ to drive the assumed random walks,
wz vx vy
see Eq. (6), and performed a parameter sweep over a
series of values. Note that for all trials we utilized the same
groundtruth IMU and target trajectories, and only varied the
noise levels utilized by the ﬁlter during target propagation.
While a more sophisticated sweep may be performed that
varies the noise in all directions, we found that a single
Fig. 1: Estimation errors of IMU global pose when using VIO by
parameterwassufﬁcienttodemonstratethediscussedissues.
itself,tightly-coupledtrackingwitha“good”targetσ =0.1,aswell
t
For each assumed noise strength, 30 Monte Carlo runs asanoverconﬁdentσ =0.005.Inthetightly-coupledsystemwith
t
were simulated using the same groundtruth trajectory while “proper” noises, the error bounds are decreased and the estimate
each run had a different realization for the IMU and pixel remains consistent, showing that the fusion of target information
hasimprovedlocalizationperformance.Foranoverconﬁdentnoise,
measurement noises. The average Root Mean Squared Error
the estimator becomes inconsistent.
(RMSE) values shown in Table I clearly illustrate the issues
caused by inconsistencies when assuming incorrect model affecting the tracking robot’s trajectory.
noises. The standard VIO without estimating the external
target is able to achieve an average RMSE of 0.231 meters V. SCHMIDTEKFUPDATE
and 1.397 degrees for a trajectory of 165 meters. While lo- Inthecasethatwewanttopreventcorruptionofthetrack-
calization performance improves when using “proper” noise ing robot’s estimates, we leverage the SKF [12] to update
values, the overconﬁdence introduced by choosing target the target states; that is, when processing measurements to
propagation noises that are too small corrupts the overall thetarget,we“schmidt”theinertialstate(andcorresponding
system.Inaddition,wefoundthathavingtoolargeofnoises clones) and thus only update the target estimates and their
may also harm VIO accuracy. This is most likely because at correlation with the tracking robot’s states. More explicitly,
(cid:20) (cid:21)
these levels the motion parameter estimation become very wepartitionthestateintotwo,thetrackingrobot’sstatesx
(cid:62) (cid:62) (cid:62) R
unstable leading to poor target predictions that may cause and target parameters, x , such that x = [x x ] , and
T R T
large linearization errors. P P
decompose the covariance as: P = RR RT . Now,
Looking at a typical single run and its error bounds, as PTR PTT
shown in Figure 1, tightly-coupling of the target tracking let us consider a measurement residual r that is being used
to update the state. The SKF does not update the tracking
reduces the uncertainty compared to stand-alone VIO while
robot’s states by setting their Kalman gain to zero (i.e.
rpermovaiidneidngbcyontshiestecnotv(atrhiaatncise,)thienetrhreorscarseemaoifn“ingothoed”bonuonidses K = [K(cid:62) K(cid:62)](cid:62) →(cid:20)[0(cid:62)(cid:21)K(cid:62)](cid:62)). T(cid:20)he u(cid:21)pdate equations
R T T
become the following:
parameters. When the chosen noise is too small, the re-
sulting trajectory error is inconsistent, thereby showing the ⊕ (cid:9)(cid:1) (cid:20)0 (cid:9)(cid:1) 0 −(cid:21)
xˆ =xˆ r=xˆ S 1r (13)
corruptioncausedbyincorrectmodelingofthetargetmotion. K L
T T
Thus, the effectiveness of tightly-coupling the estimation − (cid:62)
(cid:2) ⊕ (cid:3)(cid:9)− 0 L S 1L
heavily depends on the choice of motion noise parameters. P =P − (cid:62) R − (cid:62)T (14)
L S 1L L S 1L
This has profound impact on the application of tightly- T R T T
(cid:62) (cid:62) (cid:62)
coupled target estimation algorithms in the real-world as the where L L = L from (10). Clearly, the tracking
R T
choice of correct noise values is difﬁcult and can leave the robotstatesx andmarginalcovarianceP donotchange
R RR
systemvulnerabletoinaccuraciesandintheworsecasemay in this update, preventing the corruption of these estimates
even cause ﬁlter divergence. This motivates the proposed in the case that the target estimate is inconsistent. Therefore
Schmidt-EKF formulation that can allow for modeling of the resulting VIO estimates are equivalent to if the target
the uncertainties between the tracking robot’s states and the measurements had been discarded. It is important to note
target’s without the inconsistencies of the target estimates thattheSKFstilltrackscorrelationsinthecovariancewhich
654
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 15:56:03 UTC from IEEE Xplore.  Restrictions apply. TABLE I: Tracking robot and target average RMSE in meters / degrees of the global (G) and local (L) tightly-coupled estimator for
different values of σ . We evaluate the target pose in the tracking robot’s IMU frame TT, the target pose in the global frame, TT, and
t I G
the IMU pose in the global frame IT.
G
σt TIT(G) TGT(G) IGT(G) TIT(L) TGT(L) IGT(L)
0.001 0.01172/1.069 2.149/12.843 2.030/12.948 0.01177/0.948 1.883/8.944 1.839/8.687
0.005 0.00720/0.545 0.277/1.519 0.271/1.678 0.00746/0.543 0.259/1.547 0.254/1.566
0.010 0.00697/0.499 0.241/1.525 0.236/1.690 0.00724/0.497 0.220/1.322 0.217/1.388
0.050 0.00646/0.319 0.211/1.478 0.207/1.504 0.00664/0.315 0.203/1.313 0.199/1.341
0.100 0.00657/0.339 0.211/1.500 0.207/1.502 0.00661/0.340 0.197/1.291 0.194/1.293
0.500 0.00835/0.687 0.284/1.719 0.279/1.779 0.00848/0.627 0.235/1.370 0.232/1.378
TABLE II: Tracking robot and target average RMSE for the proposed SKF formulation. For very overconﬁdent noise values, the global
representation causes target estimate divergence.
σt TIT×(G) TGT×(G) IGT(G) TIT(L) TGT(L) IGT(L)
0.001 0.231/1.397 0.01092/0.879 0.234/1.720 0.231/1.397
× ×
0.005 0.231/1.397 0.00763/0.528 0.234/1.524 0.231/1.397
0.010 0.0408/0.555 0.231/1.472 0.231/1.397 0.00732/0.488 0.234/1.511 0.231/1.397
0.050 0.0410/0.411 0.229/1.393 0.231/1.397 0.00665/0.334 0.234/1.399 0.231/1.397
0.100 0.0432/0.430 0.229/1.390 0.231/1.397 0.00664/0.364 0.234/1.413 0.231/1.397
0.500 0.0623/0.744 0.233/1.633 0.231/1.397 0.00853/0.639 0.234/1.617 0.231/1.397
canbeshowntobeaconservativeapproximationoftheEKF VI. EXTENSIONTOREAL-WORLDTRACKING
update [21]. We also highlight that the SKF is only used
We next evaluated the proposed local SKF-based local-
whenupdatingusingtargetmeasurements.Duringprocessing
ization and object tracking in a real-world scenario using
of the visual data corresponding to the static scene, the full
a stereo visual-inertial rig. In what follows we ﬁrst discuss
stateisupdatednormally.ThisallowscorrectionstotheIMU
thevisualfeaturefront-endneededtobothsegmentthetarget
tocorrectthetargetestimates,whichisonlypossiblebecause
andthentrackfeaturesonthattargetoveritstrajectory.From
of the consistently tracked correlations.
ahighlevel,weperformedsegmentationonincomingimages
using a variation of UNet [31] that generates a mask of the
Due to the fact that the SKF does not update a portion
target for each image. Using this mask, we then performed
of the state, the representation of the estimated variables
separatevisualtrackingforenvironmentalandtargetfeatures
becomescrucialtotheperformance[24].Intheglobalmodel,
which can then be fused in the proposed estimator.
the ﬁlter is free to fully update the global pose of the target,
whileinthelocalmodel,itfullyupdatestherelativeposebe-
A. Deep Learning-based Target Detection
tweentheIMUandtarget.Intuitively,astargetmeasurements
are relative to the sensing platform, it makes more sense Due to the absence of an annotated visual-inertial target
to fully update the relative relationship between the two. tracking dataset with groundtruth trajectories for the robot
To investigate the effect of this robot-centric representation, and target, we opted to collect and annotate our own dataset
we reran the noise sweep of the previous section using tracking a large remote controlled car. We collected a train-
the local formulation, as shown in Table I. These results ing, validation and testing dataset from the left camera at
illustratethatforthetightly-coupledsystemtherobot-centric 1Hz, resulting in 279 images for training and validation
ﬁlter outperforms its global counterpart in terms of both (with 10 percent randomly chosen for validation), and 85
target and VIO accuracy. This is most likely due to the for testing. These images were labeled by hand to create the
fact that linearization errors tend to be much smaller in the groundtruth masks. The camera used to collect the images
local frame. However, this system still displayed the same had a ﬁsheye lense, but the masks were generated without
problems when using a poor motion model. undistoring the images to avoid having to redistort later on.
However,duetothis,weonlyusedrandomleft-rightﬂipping
We tested the proposed SKF formulation with both the to augment the data and not rotations and random cropping.
global and local representations (Table II). As expected, the A variant of the popular UNet architecture was used to
systems cannot beneﬁt from the tightly-coupled estimation segment the target. In our test, we used sub-pixel convolu-
for the “proper” noise values, while the VIO cannot be tion [32] in place of the original upsample layer proposed
corrupted even when the target model is inconsistent. In forefﬁciency.Additionally,wereplacedtheReLUactivation
addition, we experimentally found that the global target with ELU [33] to avoid the need for batch normalization –
pose ﬁlter was more prone to target estimate divergence again, for efﬁciency. We also used zero padding to avoid the
at the lower noise levels (typically following large target need for croppingthe features used inskip connections. The
updates), while the robot-centric method was always able network can predict on average in less than 20 milliseconds
×
to provide accurate tracking performance, even when using onaGTX1080Tigraphicscardusinga320 240resolution.
aninconsistentmotionmodel.Thismotivatesourchoiceofa To test the network, we used the mean intersection over
localrepresentationforboththeEKFandSKFformulations, union (mIOU) and pixel-wise accuracy metrics. Pixel-wise
and shows that our local SKF ﬁlter offers robustness to poor accuracy in our case is deﬁned by TP+TN , where
TP+TN+FP+FN
target models. TP, TN, FP, and FN are the true positive, true negative,
655
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 15:56:03 UTC from IEEE Xplore.  Restrictions apply. IMU
TARGET
1
0.5 4
2
2 0
Fig. 2: Visual results of the network on the testing dataset. True 0 -2 -4 -2
positive pixels have been marked green, false positives blue, and
ffaallsseenpegoastiitvivese,piannkd. false negative predictions, respectively. Error (m)00..24 ITMAURGET Translation Errors Error (deg)105 Orientation Errors
0 0
0 20 40 60 80 0 20 40 60 80
The network achieved an mIOU of 0.628 and a pixel-wise Timestamp Timestamp
accuracy of 99.1% on the testing dataset (see Fig. 2). Fig. 3: True trajectory and estimation errors of IMU and target
global poses for the proposed SKF system. The total IMU and
Since the target typically occupies a small portion of the
target trajectories were 62 meters and 52 meters, respectively.
overall pixels, we opted to extract target features from the
boundingboxofthesegmentationmaskinsteadoftheentire
extrinsincs to handle the possibly poor prior calibration. As
image,whileenvironmentalfeatureswereextractedfromthe
this was a (mostly) planar scenario, we chose the noises
full image. To do this, we ﬁrst removed the majority of
driving the local z-axis linear velocity and the pitch/roll
falsepositivesfromthemaskthroughaseriesoferosionand
angular velocities to be zero, as in the simulated example.
dilation operations. The bounding box was then taken as the
For this experiment, images were collected at 10 Hz and the
rectangle about the blob with the largest area. A separate
entire system ran in real time on a commercial laptop.
Kalman ﬁlter was used to track the bounding box, thereby
In order to evaluate the accuracy, the tracking scenario
smoothing the noisy masks.
was performed in a large Vicon room, thus providing highly
B. Visual Feature Tracking accurate pose estimates of both the sensor platform and the
target.Apurposefullyoverconﬁdenttargetmotionmodelwas
We utilized FAST detection [34] across multiple grids of
selectedtoillustratetheproposedmethod’srobustness.Over
the image to ensure that we achieved a uniform distribution
30 Monte Carlo runs, the SKF based target tracking had
of features. Only features that fell within the mask were
a VIO RMSE of 0.153 m / 1.091 degrees, while the global
labeled as target features. We performed KLT tracking [35]
targeterrorswere0.183m/3.443degrees.Theerror-vs-time
through the implementation available in OpenCV [36] for
plots for an example run are shown in Fig 3. By contrast,
environmental features, and ORB descriptor tracking [37]
thetightly-coupledsystemachieved0.409m/1.640degrees
for target features. This tracking was done both from the
for the IMU and 0.492 m / 3.037 degrees for the target, thus
left-to-right images at the same timestamp as well as left-
showing the degradation of the VIO due to the poor motion
to-left and right-to-right tracking from the previous images.
model. These results indicate that the proposed system is
Outliers were rejected using 8-point RANSAC which was
able to accurately estimate both the target and navigation
performed independently for the static and target features.
states even when using an overconﬁdent target model. In
Target features that were tracked in this way for a certain
addition,themaskprovidedbythenetworkfailedforseveral
number of frames were initialized as permanent object fea-
sequential frames multiple times throughout the test, which
tures. In addition, we performed ORB descriptor matching
our system was able to handle due to the estimation of the
to the currently estimated target point cloud in order to
target motion parameters.
determine whether new tracks corresponded to previously
seen features. We have found that these target loop closing
events are vital to the performance of the estimator as they VII. CONCLUSIONS
greatly limit the target’s drift relative to the IMU (and
therefore globally). As mismatched features are common In this work we have addressed the issue of inconsistent
in real-world experiments, we utilized Mahalanobis distance target motion modeling in a tightly-coupled visual-inertial
tests to reject inconsistent measurements. localization and 3D rigid-body target tracking framework.
Wehaveshownthatwhenthemodelaccuratelydescribesthe
C. Experimental Results
target motion, tightly-coupled estimation leads to improved
Inthisexperiment,ahand-heldvisual-inertialrigtrackeda accuracy of both the localization and tracking tasks. We
remote-controlled car. The network was trained to detect the have also illustrated the dangers inherent to overconﬁdent
vehicle using a separately collected dataset in the manner modelselection,andhaveshownthatusinganSKFapproach
described previously. Note that different from the state of preventsinconsistentmotionnoisesfromcorruptingtheVIO
art [9], we do not require a high-resolution camera (which taskwhilestillproperlytrackingallcorrelationsbetweenthe
could certainly lead to higher accuracy). We allowed for target and navigation states. In addition, we have advocated
a maximum of 150 features to be tracked from the point a local representation of the target for improved estimation
cloud. To limit visual-inertial drift, we added up to 10 performance in both the EKF and SKF formulations. The
SLAM features which were generated from visual tracks proposed approach was validated in a real-world tracking
that reached the length of the window size (10 images in scenario and shown to offer accurate estimation of both
this test) [29], and were marginalized whenever they were the ego and target motions even without utilizing a high-
lost.WealsoperformedonlineestimationoftheIMU-camera resolution camera or continuous successful visual tracking.
656
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 15:56:03 UTC from IEEE Xplore.  Restrictions apply. REFERENCES [22] P.Geneva,K.Eckenhoff,andG.Huang,“Alinear-complexityEKFfor
visual-inertial navigation with loop closures,” in Proc. International
[1] K.J.Wu,A.M.Ahmed,G.A.Georgiou,andS.I.Roumeliotis,“A Conference on Robotics and Automation, Montreal, Canada, May
squarerootinverseﬁlterforefﬁcientvision-aidedinertialnavigationon 2019.
mobiledevices,”inRobotics:ScienceandSystemsConference(RSS), [23] P.Geneva,J.Maley,andG.Huang,“Anefﬁcientschmidt-ekffor3D
2015. visual-inertialSLAM,”inProc.ConferenceonComputerVisionand
[2] A. Mourikis, N. Trawny, S. Roumeliotis, A. Johnson, A. Ansar, and PatternRecognition(CVPR),LongBeach,CA,June2019,(accepted).
L. Matthies, “Vision-aided inertial navigation for spacecraft entry, [24] E. F. Brekke and E. F. Wilthil, “Suboptimal kalman ﬁlters for target
descent,andlanding,”IEEETransactionsonRobotics,vol.25,no.2, trackingwithnavigationuncertaintyinonedimension,”in2017IEEE
pp.264–280,2009. AerospaceConference,March2017,pp.1–11.
[3] T. Qin, P. Li, and S. Shen, “VINS-Mono: A robust and versa- [25] R. Y. Novoselov, S. M. Herman, S. M. Gadaleta, and A. B. Poore,
tile monocular visual-inertial state estimator,” IEEE Transactions on “Mitigating the effects of residual biases with schmidt-kalman ﬁlter-
Robotics,vol.34,no.4,pp.1004–1020,2018. ing,” in 2005 7th International Conference on Information Fusion,
[4] S. Leutenegger, S. Lynen, M. Bosse, R. Siegwart, and P. Furgale, vol.1,July2005,pp.8pp.–.
“Keyframe-based visual-inertial odometry using nonlinear optimiza- [26] C.Yang,E.Blasch,andP.Douville,“Designofschmidt-kalmanﬁlter
tion,”InternationalJournalofRoboticsResearch,vol.34,no.3,pp. for target tracking with navigation errors,” in 2010 IEEE Aerospace
314–334,2015. Conference,March2010,pp.1–12.
[5] C.Forster,L.Carlone,F.Dellaert,andD.Scaramuzza,“On-manifold [27] N. Trawny and S. I. Roumeliotis, “Indirect Kalman ﬁlter for 3D
preintegration for real-time visual-inertial odometry,” IEEE Transac- attitude estimation,” University of Minnesota, Dept. of Comp. Sci.
tionsonRobotics,vol.33,no.1,pp.1–21,Feb.2017. &Eng.,Tech.Rep.,Mar.2005.
[6] G. Huang, “Visual-inertial navigation: A concise review,” in Proc. [28] C. Hertzberg, R. Wagner, U. Frese, and L. Schro¨der, “Integrating
International Conference on Robotics and Automation, Montreal, generic sensor fusion algorithms with sound state representations
Canada,May2019. through encapsulation of manifolds,” Information Fusion, vol. 14,
[7] R. Wang, M. Schwo¨rer, and D. Cremers, “Stereo DSO: Large-scale no.1,pp.57–77,2013.
direct sparse visual odometry with stereo cameras,” in International [29] M. Li, “Visual-inertial odometry on resource-constrained systems,”
ConferenceonComputerVision(ICCV),Venice,Italy,2017. Ph.D.dissertation,UCRiverside,2014.
[8] C.-C.Wang,C.Thorpe,S.Thrun,M.Hebert,andH.Durrant-Whyte, [30] F.M.MirzaeiandS.I.Roumeliotis,“AKalmanﬁlter-basedalgorithm
“Simultaneouslocalization,mappingandmovingobjecttracking,”Int. forIMU-cameracalibration,”inProc.oftheIEEE/RSJInternational
J.Rob.Res.,vol.26,no.9,pp.889–916,Sept.2007. Conference on Intelligent Robots and Systems, San Diego, CA, Oct.
[9] K.Qiu,T.Qin,W.Gao,andS.Shen,“Tracking3-dmotionofdynamic 29-Nov.22007,pp.2427–2434.
objectsusingmonocularvisual-inertialsensing,”IEEETransactionson [31] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolu-
Robotics,vol.PP,pp.1–18,052019. tional networks for biomedical image segmentation,” CoRR, vol.
[10] K. Eckenhoff, Y. Yang, P. Geneva, and G. Huang, “Tightly-coupled abs/1505.04597,2015.
visual-inertial localization and 3D rigid-body target tracking,” IEEE [32] W. Shi, J. Caballero, F. Husza´r, J. Totz, A. P. Aitken, R. Bishop,
RoboticsandAutomationLetters(RA-L),vol.4,no.2,pp.1541–1548, D.Rueckert,andZ.Wang,“Real-timesingleimageandvideosuper-
2019. resolutionusinganefﬁcientsub-pixelconvolutionalneuralnetwork,”
[11] P. Li, T. Qin, et al., “Stereo vision-based semantic 3d object and CoRR,vol.abs/1609.05158,2016.
ego-motion tracking for autonomous driving,” in Proceedings of the [33] D. Clevert, T. Unterthiner, and S. Hochreiter, “Fast and accurate
European Conference on Computer Vision (ECCV), 2018, pp. 646– deepnetworklearningbyexponentiallinearunits(elus),”CoRR,vol.
661. abs/1511.07289,2015.
[12] S. F. Schmidt, “Application of state-space methods to navigation [34] E. Rosten, R. B. Porter, and T. Drummond, “Faster and better: A
problems,” ser. Advances in Control Systems, C. LEONDES, Ed. machine learning approach to corner detection,” IEEE Transactions
Elsevier,1966,vol.3,pp.293–340. on Pattern Analysis and Machine Intelligence, vol. 32, pp. 105–119,
[13] J.S.Ortega,“Towardsvisuallocalization,mappingandmovingobjects 2010.
trackingbyamobilerobot:ageometricandprobabilisticapproach,” [35] B. Lucas and T. Kanade, “An iterative image registration technique
E´coleDoctoraleSyste`mes,DocteurdelInstitutNationalPolitechnique with an application to stereo vision,” in Proc. of the International
deToulouse,PhDtheses,2007. JointConferenceonArtiﬁcialIntelligence,Vancouver,BC,1981,pp.
[14] M. Chojnacki and V. Indelman, “Vision-based dynamic target tra- 674–679.
jectory and ego-motion estimation using incremental light bundle [36] G.Bradski,“TheOpenCVLibrary,”Dr.Dobb’sJournalofSoftware
adjustment,”InternationalJournalofMicroAirVehicles,vol.10,no.2, Tools,2000.
pp.157–170,2018. [37] E. Rublee, V. Rabaud, K. Konolige, and G. Bradski, “Orb: An
[15] A. Azim and O. Aycard, “Detection, classiﬁcation and tracking of efﬁcientalternativetosiftorsurf,”in2011InternationalConference
movingobjectsina3denvironment,”in2012IEEEIntelligentVehicles onComputerVision,2011,pp.2564–2571.
Symposium,June2012,pp.802–807.
[16] A. Aldoma, F. Tombari, J. Prankl, A. Richtsfeld, L. Di Stefano,
and M. Vincze, “Multimodal cue integration through hypotheses
veriﬁcation for rgb-d object recognition and 6dof pose estimation,”
in2013IEEEInternationalConferenceonRoboticsandAutomation,
May2013,pp.2104–2111.
[17] J.Redmon,S.Divvala,R.Girshick,andA.Farhadi,“Youonlylook
once:Uniﬁed,real-timeobjectdetection,”inProceedingsoftheIEEE
conferenceoncomputervisionandpatternrecognition,2016,pp.779–
788.
[18] H. Wei, W. Lu, P. Zhu, G. Huang, J. Leonard, and S. Ferrari,
“Visibility-based motion planning for active target tracking and lo-
calization,” in Proc. of the IEEE/RSJ International Conference on
Intelligent Robots and Systems, Chicago, IL, Sept. 14-18 2014, pp.
76–82.
[19] A.I.MourikisandS.I.Roumeliotis,“Amulti-stateconstraintKalman
ﬁlterforvision-aidedinertialnavigation,”inProceedingsoftheIEEE
International Conference on Robotics and Automation, Rome, Italy,
Apr.10–14,2007,pp.3565–3572.
[20] X.RongLiandV.P.Jilkov,“Surveyofmaneuveringtargettracking.
partv.multiple-modelmethods,”IEEETransactionsonAerospaceand
ElectronicSystems,vol.41,no.4,pp.1255–1321,Oct2005.
[21] R. C. DuToit, J. A. Hesch, E. D. Nerurkar, and S. I. Roumeliotis,
“Consistent map-based 3d localization on mobile devices,” in 2017
IEEEInternationalConferenceonRoboticsandAutomation(ICRA),
May2017,pp.6253–6260.
657
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 15:56:03 UTC from IEEE Xplore.  Restrictions apply. 