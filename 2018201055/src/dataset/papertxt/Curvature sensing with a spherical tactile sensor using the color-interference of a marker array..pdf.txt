2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
Radar as a Teacher: Weakly Supervised Vehicle Detection using Radar
Labels
Simon Chadwick and Paul Newman
Abstract—Ithasbeendemonstratedthattheperformanceof
anobjectdetectordegradeswhenitisusedoutsidethedomain
ofthedatausedtotrainit.However,obtainingtrainingdatafor
a new domain can be time consuming and expensive. In this
work we demonstrate how a radar can be used to generate
plentiful (but noisy) training data for image-based vehicle
detection. We then show that the performance of a detector
trained using the noisy labels can be considerably improved
through a combination of noise-aware training techniques and
relabelling of the training data using a second viewpoint. In
ourexperiments,usingourproposedprocessimprovesaverage
precisionbymorethan17percentagepointswhentrainingfrom
scratchand10percentagepointswhenﬁne-tuningapre-trained
model.
I. INTRODUCTION
Supervised learning methods have achieved ever-higher
levels of performance in recent years. However, the require-
ment for large amounts of hand-labelled training data makes
Fig.1. Usingonlyradartargetsaslabelsweareabletotrainaneffective
it labour intensive to deploy them in new domains. While vehicle detector using a noise-aware training process. We automatically
much research effort has been devoted to improving the generatetheinitiallabelsfromradardata(top)thentrainamodelusinga
modiﬁedversionoftheco-teachingprocess[2][3]tohandlethenoise.The
generalisation of models between domains, there is still a
trainingdataisthenrelabelledusingtheﬁrstmodel,incorporatingdetections
signiﬁcant gap in performance if no labelled data from the fromazoomimageofthesamescenetomitigateoverﬁtting(middle).The
new domain is available [1]. An alternative is to generate new labels are used to train an improved model which provides effective
vehicledetectionfromimagesalone(bottom).
labels for a new domain using an automated method which
takes advantage of circumstances that are not available at
runtime (for example, additional sensors or running time only created for moving vehicles. However, as the velocity
backwards on recorded data). In this work we use a radar to information is not required at inference time (only images
automatically generate noisy labels for the task of detecting areused),thedetectorcanbedeployedonplatformswithout
vehicles in images. We show how we can clean these labels radar ﬁtted and is free to generalise to stationary vehicles
togivegooddetectorperformancewithouttheneedforhand- which would not be identiﬁed by the radar.
labelling. As the labels we generate are noisy we use two processes
Radar is an appealing sensor to work with as it has a to mitigate the effect of this noise. Firstly, we train our
number of characteristics that are complementary to more network using a modiﬁed version of the co-teaching process
commonly used sensors. For example, it is robust to most [2][3]whichattemptstopreventerroneouslabelsfrombeing
forms of environmental conditions (such as rain or fog). In used for parameter updates. Secondly, we use a relabelling
this work we leverage its long range and the direct radial process exploiting cameras of different focal lengths to both
velocity measurements provided by the Doppler effect. We clean the labels and mitigate the degree of over-ﬁtting that
make the naïve assumption that radar targets with velocities would occur if only a single image was used. An overview
thatcouldnotbecausedbythemotionofthedatacollection of the process is shown in Fig 1.
vehicle are generated by other vehicles moving nearby. This In a range of experiments presented below we show
allows us to inexpensively generate copious amounts of the beneﬁt provided by each element of the process and
training data without any hand-labelling effort. demonstrate that good performance can be achieved despite
Unlike LIDAR, radar provides velocity information with- the high level of noise in the labels.
out any need to associate between consecutive timestamps.
This velocity information is the distinguishing characteristic II. RELATEDWORK
that we use to identify what to label. As a result, labels are Supervised learning methods are the best performing ma-
chine learning techniques for many computer vision tasks.
Authors are from the Oxford Robotics Institute,
However, it is well known that the simplest way to achieve
Dept. Engineering Science, University of Oxford, UK.
{simonc,pnewman}@robots.ox.ac.uk good performance is to increase the amount of available
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 222
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:39:49 UTC from IEEE Xplore.  Restrictions apply. training data. Given the cost of creating labelled datasets,
there is a huge amount of research devoted to reducing
manuallabellingeffort,therebyallowingbiggertrainingsets
without the additional cost. Some work focuses on "domain
adaptation" - the practice of training a model using an
existing labelled dataset in such a way that when the model
is used in a new domain (for which no labels exist) any
reduction in performance is minimised. While this problem
is often studied in the context of semantic segmentation [4],
some methods are applicable regardless of the task [5][6].
In [7] the particular case of domain adaptation for object
detectionisaddressedbyintroducinglossestoencouragethe
featuresgeneratedtobeagnostictothedomainoftheimage.
Object detection is also targeted in [8] by incorporating
elementsofadversarialtraining,acommonthreadinmuchof
the recent domain adaptation work. One particularly popular
application is domain adaptation for models trained using Fig. 2. The good (top), the bad (middle) and the ugly (bottom) of using
radar for labelling. We leverage a forward facing cruise control radar and
synthetically generated data [9].
use a ﬁxed size prior to initially label those targets that do not match the
Asecondapproachistomakedowithdatathatislabelled platform’s ego-motion. It can be seen that the labelling method, though
inacheaperfashion.Inobjectdetectionthisso-calledweakly crude,isremarkablyaccurateinmanycases.However,itcanalsobeseen
that there is label noise, such as multiple bounding boxes for a single
supervised task often takes the form of image level text
vehicle(middle)andego-motionestimationerrorsleadingtotheinclusion
labels. It was noted in [10] that methods for tackling this ofstationarytargets(bottom).Thisnoisenecessitatestheuseofthetraining
problem can fail by detecting only the most salient part processthatwepropose.
of an object. This has led to efforts to counteract this
failure mode [11][12][13]. The weakly supervised detection
bethecase.Thecurrentstate-of-the-artintheseapproachesis
problem is tackled in [14] with the beneﬁt of full labels in
[21]whichlearnsarepresentationthatmaximisesthemutual
a neighbouring domain. This is exploited by using domain
information between the latent space and future samples
adaptation techniques to generate "pseudo labels" in the
in a sequence — for images a sequence is generated by
target domain. Many weakly supervised methods work from
predicting patches below the current patch. An extension of
a set of object proposals (such as those generated by [15] or
thisapproachthatmaybeapplicableinmulti-sensorsettings
[16])attemptingtodeterminewhichproposalcorrespondsto
(suchasours)isintroducedin[22]wherecontrastsaremade
the image-level label. The system of [17] aims to improve
between different views rather than different elements of
that process by using a model trained on motion cues to
a sequence. Another method of learning representations is
help rank the proposals. Regardless of the efﬁcacy of these
introduced in [23], where motion cues are used to segment
methods, some labelling effort is still required to generate
moving objects as training data.
the image-level labels.
Our work is poised between the weakly supervised meth-
When using alternative labelling techniques (for example,
ods, where the labels are known to be incomplete, and
so-called "webly" supervised learning where labels are har-
methods related to training models from noisy labels.
vested from the internet) it is highly likely that the labels
will contain some mistakes. This noise in the labels has
III. GENERATINGLABELSFROMRADAR
a detrimental effect on the learning process, reducing the
ﬁnal performance of the trained detector. There has been a The radar that we use, a Delphi ESR 2.5 pulsed Doppler
signiﬁcant amount of effort devoted to methods that aim to ADASradar,doesnotprovideaccesstotherawradarsignal.
reduce the effect of noisy labels. A number of these works Instead,itprovidesvariablelengthlistsoftargetswhereeach
focusonclassiﬁcationtaskssuchas[18].In[19],Sukhbaatar target consists of range, bearing, amplitude and range rate
et al. aim to learn the class noise transition matrix from the (radialvelocity).Withtheassumptionthateachtargetoccurs
data by inserting an additional linear layer. Recently, [20] intheplaneoftheradar(anassumptionthatdoesnotalways
SE
proposed a meta-learning objective to increase robustness to hold, see Fig 4) this allows the 3 position of each target
noisy labels. It also includes an iterative training process in relative to the radar to be calculated.
which those examples for which the previous model gives Targets may represent returns from stationary or moving
a low score to the supposed ground-truth are excluded from objects. As radar provides a clear velocity signal, we use
the re-training. thattodistinguishmovingvehiclesfrombackground(onthe
A different approach to reducing labelling effort is to use assumption that all moving objects are vehicles). We ﬁrst
unsupervised representation learning to train a model that subtract the ego-motion of the data collection vehicle, then
generates a representation of the input from which the ﬁnal take the subset of targets for which the resulting velocity is
task (in our case object detection) can be learned using a aboveasetthreshold.Giventhesetoftargetsthatcorrespond
muchsmallerquantityoflabelleddatathanwouldotherwise to moving vehicles these can then be approximately labelled
223
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:39:49 UTC from IEEE Xplore.  Restrictions apply. in image space by proposing a ﬁxed size cuboid at the shown in Fig. 4). Ordinarily, using a model to relabel its
location of the radar target and projecting that cuboid into own training data would lead to considerable over-ﬁtting.
the image. Although crude, this yields plausible bounding To reduce this effect we make use of a second camera
boxes in a high proportion of cases (see Fig 2). with a zoom lens, capturing the scene concurrently with the
The obvious failure cases for this labelling method are wide-angle camera (note that only the wide-angle camera
stationary vehicles or vehicles whose primary motion is is required at test time). We run our trained model over
tangential rather than radial relative to the ego-vehicle (even the training set images of both cameras and combine the
if those cases where motion is entirely tangential are quite detections from both cameras. Using this method, the labels
limited and brief). As a result of these failure cases and the are reﬁned using the more detailed zoom lens images. In
assumptions made (for example the ﬁxed size vehicle prior) addition,italsoactstoperturbthenewlabels(awayfromthe
the labels are sufﬁciently noisy that steps have to be taken wide-angleonlypredictions)reducingoverﬁtting(seeresults
to reduce the effect of this label noise. in Table I).
More speciﬁcally, we ﬁrst train a model using the wide-
IV. CO-TEACHING angle images of the training set using the labels derived
To improve trained detector performance when learning from radar. That model is then used to make predictions
from noisy labels, we make use of a modiﬁed version of on every image of the training set from both the wide-angle
the co-teaching framework [2]. Co-teaching aims to exclude images and the matching zoom lens images. The predictions
gradients originating from examples that are mislabelled. It from each wide-angle image and its matching zoom lens
is based on the observation that simpler patterns are more image are then thresholded based on the conﬁdence of each
easilylearnedduringtraining[24]andthatinthesetermsthe prediction and combined. This combination is performed
objectsthatweaimtodetectformasimplerpatternthanany using the procedure described in [27]. In brief, the zoom
noise present in the labels. The implication of this is that, lenspredictionsarereprojectedintothewide-angleimageby
at intermediate points during the training, the losses of the assuming the two cameras are coincident (in practice they
≈
twotypesofexampleswilldiverge(i.e.cleanlabelexamples are 30mm apart) which allows the image points to be
will be learned more quickly and so will have lower losses transferred without knowledge of distance. The combined
than examples with noisy labels). label set is formed by using the zoom lens predictions in
Toexploitthiseffect,co-teachingoperatesbytrainingtwo the overlapping region subject to an intersection metric [27]
identical but differently initialised networks in parallel. For at the boundary and the wide-angle labels elsewhere. In the
each batch, each network informs the other which examples case where co-teaching is used, two functional networks are
have the lowest loss and hence are the ones that should trainedsimultaneously.Consequently,weoptionallyuseboth
be used to provide gradients for parameter updates. The networks to make predictions on the zoom lens images. We
proportion of the batch used for updates reduces over time then combine these two zoom lens sets by concatenating the
from the full batch at the start of training (when no learning predictions and using non-maxima suppression to remove
has taken place so there is nothing to differentiate between highly overlapping predictions.
examples) down to a subset that excludes a proportion equal
to the estimated fraction of labels that are noisy. VI. NETWORKDETAILS
The modiﬁcations introduced in [3] are shown to im-
prove performance in an object detection setting. This is WeuseanetworkbasedonSSD[28],asingle-stageobject
done by adapting the framework to exclude updates on detection network. SSD operates by making predictions of
an object-by-object basis (which is more suitable to object classandboundingboxoffsetforeachoneofasetofanchor
detection) rather than an image-by-image basis (as used for boxesofdifferentsizesandaspectratios.Theseanchorboxes
classiﬁcation in the original paper), as shown in Figure 3. are repeated at each grid cell of a number of feature maps
In addition to the modiﬁcations of [3], instead of setting of different sizes. The dense predictions are then ﬁltered
a hyper-parameter to control the rate at which examples based on class probability (the class prediction includes a
are excluded from training, we make use of the percentile "no object" option) and non-maxima suppression. We use
moving average. Inspired by self-paced curriculum learning a relatively small ResNet18 backbone [29] extended with
from [25] and [26], after a burn-in period we exclude losses an additional block that provides input to a larger scale
abovethepercentilemovingaverage(wherethepercentileis feature map. In the case of single GPU training we are
setaccordingtotheexpectednoisefraction).Thisalsohelps constrainedinthesizeofnetworkthatcanbeusedbytheco-
when batch sizes are small as the high/low loss distinction teaching procedure that requires two networks to be trained
isthenmoreconsistentbetweeneachbatchregardlessofthe simultaneously. We use a batch size of 8 and the Adam
−
composition of a speciﬁc batch. optimiser [30] with an initial learning rate of 1e 4 reduced
− −
to1e 5 after55kiterations.WeuseL2weightdecayof1e 3
V. RELABELLING as well as standard image augmentation processes including
Having trained an initial model in our domain using the horizontal ﬂips, random crops and colour perturbations.
labels derived from radar we then use that model to relabel As the number of labelled images decreases after rela-
our examples to give a cleaner label set (the process is belling we train for a ﬁxed number of iterations (rather
224
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:39:49 UTC from IEEE Xplore.  Restrictions apply. Fig.3. Themodiﬁedco-teachingprocessintroducedin[3].Theindicesofthelowestlosselementsfromorderedlistsofthethreetypesofobjectdetector
loss (losses relating to positives, hard-negatives and bounding box regression) are exchanged between two identical networks and only those lower loss
elementsareusedtoprovideparameterupdates.
Fig.4. Therelabellingprocess.Theimagecropsshowexamplesofthelabelsatthedifferentstagesoftheprocess.Notehowtheoriginalradarlabelling
processdoesnotaccountfortheslopeoftheroadwhichresultsintheboundingboxesbeingplacedabovethevehicles.Duringrelabellingthisiscorrected
bydetectionsinthezoomlensimagethatarecombinedwiththewideimagedetectionstoformtheﬁnalcombinedlabelset.
than a number of epochs) for each model (pre- and post- matchedwithgroundtruthlabels)andlH beingcross-entropy
relabelling). losses for hard-negative instances. The bounding box losses
lB are calculated using a smooth L1 loss.
A. Co-teaching Loss Function
Ai− is the p(cid:40)ercentile exponential moving average up-
1 j,t
Whenco-teachingisenabled,theoveralllossfunctionthat dated at each step of training and calculated as
is optimised is calculated as
E(V,), t=0
L=L +L +L (1) At+1 = λE(V,)+(1−λ)A , t>0 (4)
P H B t
where L , L and L are the losses relating to the pos- where t is the training step, λ is the moving average rate
P H B −
itive, hard-negative and bounding box elements respectively. (we use λ = 0.9) and the i and 1 j have been left off
They are deﬁned as (cid:88)(cid:88) for readability. E(V,) is the loss value corresponding to
a given percentile where V is the vector of all loss values
Li = 1 Ni Lij,k (2) lu1is−ej,k=fo1r−a ηspwechieﬁrceiηainsdtheiesxptheecteredqeurirroedr fpraecrctieonnt)i.le (we
j=0k=1
VII. DATASET
where
I To examine the performance of our method we conduct
Li =li (li− <Ai− ) (3)
j,k j,k 1 j,k 1 j,t tests on a dataset comprising over three hours of driving
∈ { }
and i P,H,B . The loss for a speciﬁc instance k of near Oxford, UK, covering urban, sub-urban, highway and
I
type i for model j is li and is the indicator function. rural roads. The dataset includes six individual drives, four
j,k
The number of instances N varies between types (more used for training and one each for validation and testing.
i
hard-negatives than positives, for example). The losses li The validation and testing drives follow different routes to
j,k
are calculated in accordance with [28] with lP being cross- anyofthedrivesusedintraining.Intotal,aftersub-sampling,
entropy losses for the positive instances (those anchor boxes the dataset includes 8415 examples for training where each
225
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:39:49 UTC from IEEE Xplore.  Restrictions apply. examplecontainsawide-angleimage,azoomimage(which
TABLEI
covers the central portion of the wide-angle image), a set
VEHICLEDETECTIONPERFORMANCEUSINGDIFFERENTELEMENTSOF
of radar targets and an ego-motion estimate. In our case
THERELABELLINGPIPELINE
thisestimatewasobtainedusingstereovisualodometry[31]
(utilising an extra camera not used for network training), Relabellingconﬁguration
±
althoughotherego-motionestimationmethodscouldbeused. Co- Re- Wide- Zoom Both AP( 1SD)
Inaddition,inordertoenableclearevaluationofthelearning teaching label angle labels zoom
labels models
method, from the separate validation and test drives, 181
±
validation images and 228 test images have been hand- No No N/A N/A N/A 0.203(±0.0177)
No Yes Yes No N/A 0.236( 0.0085)
labelled with 2D bounding boxes around all vehicles. Wide- ±
No Yes Yes Yes N/A 0.264( 0.0042)
angle images are 512x1280 pixels in size, zoom images are ±
Yes No N/A N/A N/A 0.247( 0.0431)
960x1280. Yes Yes Yes No No 0.280(±0.0168)
±
Yes Yes Yes Yes No 0.363( 0.0378)
±
Yes Yes Yes Yes Yes 0.377( 0.0240)
VIII. EXPERIMENTALRESULTS
To examine the performance of our method we perform
a range of experiments on our dataset. For all experiments
we evaluate using the hand-labelled test set. We use average
precision with an IOU threshold of 0.5 and, in line with
theKITTIevaluationprocedure[32],weexcludelabels(and
related detections) of vehicles less than 25 pixels high.
In the ﬁrst set of experiments we show the effect of each
part ofour relabelling process.We start by simplytraining a
model without any additional processes which demonstrates
the issues caused by the noisy radar labels. We then add
relabelling but use only the wide-angle images to generate
thelabelsforthesecondtrainingrun.Inthenextexperiment
we again use relabelling but generate the labels for the
second run by combining labels from both the wide-angle
and zoom images. We then run the same experiments again
Fig. 5. Average precision curves showing the performance of a model
but this time using the co-teaching process in each training trained using the standard training process against one trained using our
loop. Finally, as co-teaching produces two usable models fullrelabellingprocess.
on each training run, in the relabelling process we use both
models to contribute zoom labels to the relabelled data. The
IX. CONCLUSIONSANDFURTHERWORK
results of these experiments are shown in Table I and in
Figure5withsomesampledetectionsshowninFigure6.All Inthispaperwehaveproposedamethodforimprovingthe
results are the average of two runs with those experiments performance of an object detector when using automatically
that make use of co-teaching presented as the average over labelled data. The combination of a modiﬁed version of co-
bothrunsandbothmodels(foratotaloffour).Itcanbeseen teaching(introducedin[3])withtherelabellingprocessusing
that the combination of co-teaching and relabelling provides two cameras introduced in this work has been shown to be
a large increase in detector performance. It can also be effective.Thisallowstheuseoflabellingmethodsthatwould
seen that the use of labels from the zoom image contributes previously have been thought to be far too unreliable.
signiﬁcantly. Our full process yields an improvement of 17 Nevertheless, there are downsides to this method: having
points over the standard training process. to train two models effectively doubles training time. In
In a further set of experiments, we evaluate the beneﬁt of addition, as is the case with our radar labelling method,
our method in a scenario where a network has already been biases in the original labels cannot be fully overcome — as
trained for the task on a different domain. In this case we theradarcannotdeterminethetangentialvelocityofcrossing
pre-train our network on the KITTI object detection dataset. vehicles they are rarely included in the label set which
The pre-trained network is then used as the initialisation for correspondingly reduces the overall detector performance.
bothstagesoftraining(i.e.whenusingonlytheoriginalradar Similarly, the ﬁxed-size vehicle prior is not suitable for
labels and additionally when using the cleaned labels in the larger vehicles such as buses — we ﬁnd that while the
second training run). The results are shown in Table II. It detector may detect the vehicles, the proposed bounding
can be seen that while the results in general are higher than boxes are too small (and are consequently evaluated as
those when training from scratch, the relabelling process is incorrect detections). It is possible that an improvement to
still highly beneﬁcial. The effect of the second zoom model our method would be to base the initial boxes on proposals
islimitedtoatighteningofthestandarddeviationratherthan such as those provided by [15].
an increase in performance. One facet that we have yet to explore is the effect of
226
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:39:49 UTC from IEEE Xplore.  Restrictions apply. TABLEII REFERENCES
VEHICLEDETECTIONPERFORMANCEUSINGCO-TEACHINGANDA
[1] J.Tremblay,A.Prakash,D.Acuna,M.Brophy,V.Jampani,C.Anil,
PRE-TRAINEDMODELFORINITIALISATION T.To,E.Cameracci,S.Boochoon,andS.Birchﬁeld,“Trainingdeep
networks with synthetic data: Bridging the reality gap by domain
Relabellingconﬁguration randomization,”inProceedingsoftheIEEEConferenceonComputer
± VisionandPatternRecognitionWorkshops,2018,pp.969–977.
Relabel Wide- Zoom Both AP( 1SD [2] B. Han, Q. Yao, X. Yu, G. Niu, M. Xu, W. Hu, I. Tsang, and
angle labels zoom
M.Sugiyama,“Co-teaching:Robusttrainingofdeepneuralnetworks
labels models
with extremely noisy labels,” in Advances in neural information
±
No N/A N/A N/A 0.321( 0.0168) processingsystems,2018,pp.8527–8537.
±
Yes Yes No N/A 0.344( 0.0107) [3] S. Chadwick and P. Newman, “Training object detectors with noisy
±
Yes Yes Yes No 0.427( 0.0388) data,”in2019IEEEIntelligentVehiclesSymposium(IV),June2019,
±
Yes Yes Yes Yes 0.422( 0.0192) pp.1319–1325.
[4] Y.Zou,Z.Yu,B.VijayaKumar,andJ.Wang,“Unsuperviseddomain
adaptationforsemanticsegmentationviaclass-balancedself-training,”
training data quantity on the efﬁcacy of our method. It in Proceedings of the European Conference on Computer Vision
(ECCV),2018,pp.289–305.
could be that large amounts of additional data provide
[5] M. Wulfmeier, A. Bewley, and I. Posner, “Addressing appearance
further performance improvements, or alternatively, in the change in outdoor robotics with adversarial domain adaptation,” in
experiments above our method may be purely compensating 2017 IEEE/RSJ International Conference on Intelligent Robots and
Systems(IROS). IEEE,2017,pp.1551–1558.
for a paucity of data. This could be potentially be explored
[6] J.Hoffman,E.Tzeng,T.Park,J.-Y.Zhu,P.Isola,K.Saenko,A.A.
using the nuScenes dataset [33] which also includes radar Efros, and T. Darrell, “Cycada: Cycle-consistent adversarial domain
data although it does not include zoom lens images which adaptation,”arXivpreprintarXiv:1711.03213,2017.
[7] Y. Chen, W. Li, C. Sakaridis, D. Dai, and L. Van Gool, “Domain
our results indicate contribute considerably to the ﬁnal per-
adaptivefasterr-cnnforobjectdetectioninthewild,”inProceedings
formance. of the IEEE conference on computer vision and pattern recognition,
2018,pp.3339–3348.
[8] X.Zhu,J.Pang,C.Yang,J.Shi,andD.Lin,“Adaptingobjectdetectors
via selective cross-domain alignment,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, 2019, pp.
687–696.
[9] S. Sankaranarayanan, Y. Balaji, A. Jain, S. Nam Lim, and R. Chel-
lappa, “Learning from synthetic data: Addressing domain shift for
semantic segmentation,” in Proceedings of the IEEE Conference on
ComputerVisionandPatternRecognition,2018,pp.3752–3761.
[10] H. Bilen and A. Vedaldi, “Weakly supervised deep detection net-
works,” in Proceedings of the IEEE Conference on Computer Vision
andPatternRecognition,2016,pp.2846–2854.
[11] X. Zhang, Y. Wei, G. Kang, Y. Yang, and T. Huang, “Self-produced
guidance for weakly-supervised object localization,” in Proceedings
of the European Conference on Computer Vision (ECCV), 2018, pp.
597–613.
[12] J. Choe and H. Shim, “Attention-based dropout layer for weakly
supervisedobjectlocalization,”inProceedingsoftheIEEEConference
onComputerVisionandPatternRecognition,2019,pp.2219–2228.
[13] C. Ge, J. Wang, Q. Qi, H. Sun, and J. Liao, “Fewer is more: Image
segmentation based weakly supervised object detection with partial
aggregation.”inBMVC,2018,p.136.
[14] N. Inoue, R. Furuta, T. Yamasaki, and K. Aizawa, “Cross-domain
weakly-supervisedobjectdetectionthroughprogressivedomainadap-
tation,” in Proceedings of the IEEE conference on computer vision
andpatternrecognition,2018,pp.5001–5009.
[15] J.R.Uijlings,K.E.VanDeSande,T.Gevers,andA.W.Smeulders,
“Selective search for object recognition,” International journal of
computervision,vol.104,no.2,pp.154–171,2013.
[16] C. L. Zitnick and P. Dollár, “Edge boxes: Locating object proposals
fromedges,”inEuropeanconferenceoncomputervision. Springer,
2014,pp.391–405.
[17] K.K.SinghandY.J.Lee,“Youreapwhatyousow:Usingvideosto
generatehighprecisionobjectproposalsforweakly-supervisedobject
detection,”inProceedingsoftheIEEEConferenceonComputerVision
andPatternRecognition,2019,pp.9414–9422.
Fig.6. Exampledetectionsfromthetestsetusingthemodeltrainedwith [18] S. Reed, H. Lee, D. Anguelov, C. Szegedy, D. Erhan, and A. Ra-
thefullrelabellingprocess. binovich, “Training deep neural networks on noisy labels with boot-
strapping,”arXivpreprintarXiv:1412.6596,2014.
[19] S. Sukhbaatar, J. Bruna, M. Paluri, L. Bourdev, and R. Fergus,
“Training convolutional networks with noisy labels,” arXiv preprint
ACKNOWLEDGEMENTS arXiv:1406.2080,2014.
[20] J. Li, Y. Wong, Q. Zhao, and M. S. Kankanhalli, “Learning to learn
We gratefully acknowledge the JADE-HPC facility for fromnoisylabeleddata,”inProceedingsoftheIEEEConferenceon
ComputerVisionandPatternRecognition,2019,pp.5051–5059.
providing the GPUs used in this work. Paul Newman is
[21] A. v. d. Oord, Y. Li, and O. Vinyals, “Representation learning with
funded by the EPSRC Programme Grant EP/M019918/1. contrastivepredictivecoding,”arXivpreprintarXiv:1807.03748,2018.
227
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:39:49 UTC from IEEE Xplore.  Restrictions apply. [22] Y. Tian, D. Krishnan, and P. Isola, “Contrastive multiview coding,”
arXivpreprintarXiv:1906.05849,2019.
[23] D. Pathak, R. Girshick, P. Dollár, T. Darrell, and B. Hariharan,
“Learningfeaturesbywatchingobjectsmove,”inProceedingsofthe
IEEEConferenceonComputerVisionandPatternRecognition,2017,
pp.2701–2710.
[24] D. Arpit, S. Jastrze˛bski, N. Ballas, D. Krueger, E. Bengio, M. S.
Kanwal, T. Maharaj, A. Fischer, A. Courville, Y. Bengio et al., “A
closer look at memorization in deep networks,” in Proceedings of
the 34th International Conference on Machine Learning-Volume 70.
JMLR.org,2017,pp.233–242.
[25] L. Jiang, Z. Zhou, T. Leung, L.-J. Li, and L. Fei-Fei, “Mentornet:
Learning data-driven curriculum for very deep neural networks on
corruptedlabels,”arXivpreprintarXiv:1712.05055,2017.
[26] M. P. Kumar, B. Packer, and D. Koller, “Self-paced learning for
latentvariablemodels,”inAdvancesinNeuralInformationProcessing
Systems,2010,pp.1189–1197.
[27] S.Chadwick,W.Maddern,andP.Newman,“Distantvehicledetection
usingradarandvision,”in2019InternationalConferenceonRobotics
andAutomation(ICRA),May2019,pp.8311–8317.
[28] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu,
and A. C. Berg, “Ssd: Single shot multibox detector,” in European
conferenceoncomputervision. Springer,2016,pp.21–37.
[29] K.He,X.Zhang,S.Ren,andJ.Sun,“Deepresiduallearningforimage
recognition,” in Proceedings of the IEEE conference on computer
visionandpatternrecognition,2016,pp.770–778.
[30] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimiza-
tion,”arXivpreprintarXiv:1412.6980,2014.
[31] W. Churchill, “Experience based navigation: Theory, practice and
implementation,” Ph.D. dissertation, University of Oxford, Oxford,
UnitedKingdom,2012.
[32] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous
driving?thekittivisionbenchmarksuite,”in2012IEEEConference
onComputerVisionandPatternRecognition. IEEE,2012,pp.3354–
3361.
[33] H.Caesar,V.Bankiti,A.H.Lang,S.Vora,V.E.Liong,Q.Xu,A.Kr-
ishnan,Y.Pan,G.Baldan,andO.Beijbom,“nuscenes:Amultimodal
dataset for autonomous driving,” arXiv preprint arXiv:1903.11027,
2019.
228
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:39:49 UTC from IEEE Xplore.  Restrictions apply. 