2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
The OmniScape Dataset
Ahmed Rida Sekkat1, Yohan Dupuis2, Pascal Vasseur1 and Paul Honeine1.
Abstract—Despite the utility and beneﬁts of omnidirectional have been conﬁrming the need of omnidirectional image
images in robotics and automotive applications, there are no databases [16], [17], [18], [19], [20], [21], [22], [23].
datasets of omnidirectional images available with semantic In this paper, we propose a framework that can be ap-
segmentation, depth map, and dynamic properties. This is due
plied to any simulator or virtual environment to generate
tothetimecostandhumaneffortrequiredtoannotateground
omnidirectional images. We present the data acquired from
truth images. This paper presents a framework for generating
omnidirectional images using images that are acquired from a simulator and describe its use to generate omnidirectional
a virtual environment. For this purpose, we demonstrate the images. We present in detail two well-known types of
relevance of the proposed framework on two well-known sim- omnidirectionalimages,ﬁsheyeandcatadioptricimages.The
ulators: CARLA Simulator, which is an open-source simulator
models can be computed from a calibrated camera, using a
for autonomous driving research, and Grand Theft Auto V
largeclassofmodelsproposedbyGeyerandDaniilidis[24],
(GTAV),whichisaveryhighqualityvideogame.Weexplainin
detailsthegeneratedOmniScapedataset,whichincludesstereo Barreto and Araujo [25], Mei and Rives [26] or Scaramuzza
ﬁsheye and catadioptric images acquired from the two front et al. [27].
sides of a motorcycle, including semantic segmentation, depth Whiletheproposedframeworkisgeneric,wedemonstrate
map, intrinsic parameters of the cameras and the dynamic
its relevance in two famous simulators: CARLA Simulator
parameters of the motorcycle. It is worth noting that the case
and Grand Theft Auto V (GTA V). CARLA is an open-
of two-wheeled vehicles is more challenging than cars due to
the speciﬁc dynamic of these vehicles. source simulator for urban autonomous driving. It gives the
possibility to generate datasets with several ground truth
I. INTRODUCTION modalities [28]. Grand Theft Auto V (GTA V) is a very
Perceivingandunderstandingtheenvironmentisanessen- high quality AAA video game. Both simulators provide an
tial task for a mobile robot or an autonomous vehicle. One environment similar to real life, thanks to dynamic weather,
of the main issues for the development of these vehicles seasons,regulatedroadtrafﬁc,trafﬁclights,signaling,pedes-
is the existence of datasets. Among the datasets of pinhole trians, different types of vehicles, ... It is worth noting that
camera images dedicated to the development and study of there is no support in these simulators or any other simula-
autonomous vehicles, mention may be made of KITTI [1], tor for omnidirectional images, which makes the proposed
Cityscape [2], Berkeley DeepDrive [3], CamVid [4] and framework of great interest for researchers working on om-
Mapillary Vistas Dataset [5]. Omnidirectional cameras can nidirectionalimagesinroboticsandautomotiveapplications.
perceive the surrounding environment with a ﬁeld of view We show the relevance of this work by releasing the
that can reach 360°. For this reason, they are increas- OmniScape dataset1, which is a dataset of a motorised
ingly used in the ﬁeld of intelligent vehicles, including two-wheeler in the aforementioned simulators. OmniScape
ﬁsheye cameras due to their compactness and inexpensive comprises stereo ﬁsheye and stereo catadioptric images ac-
design. Several datasets contain ﬁsheye images, such as quired from the two front sides of a motorcycle, with the
CVRG [6], LMS [7], LaFiDa [8], SVMIS [9], "Go Stan- corresponding depth maps, semantic segmentation, intrinsic
ford" [10], GM-ATCI [11], and RTH Zurich multi-FoV syn- parameters of the cameras and dynamic parameters of the
thetic datasets [12]. However, it is noted that there is a lack motorcycle, such as velocity, angular velocity, acceleration,
of road scenes omnidirectional images datasets embedded in locationandrotation.SeeFig.1.TheOmniScapedatasetwill
a vehicle dedicated for computer vision applications. Recent be progressively augmented with more omnidirectional data
work on semantic segmentation of ﬁsheye images of road using the same principle with different vehicles, modalities
scenes had been performed on perspective images to which and environments. We have chosen to provide data enquired
a distortion simulating the ﬁsheye effect is applied [13], from a motorcycle because motorcycles present challenging
[14],[15].Suchdeformationinducesartefactsintheresulting problemsthatwerenotaddressedbefore.Indeed,thedynam-
images. There is a growing need to generate more reliable ics of a motorcycle are totally different from the dynamics
datasets of omnidirectional images, without the need to of cars. As we know, a car is almost all the time parallel
go through simple image rectiﬁcation. Much recent work, to the road. In addition to the distortion in spherical or
especially in deep learning applied on spherical images, omnidirectional images, motorcycles undergo rotations yaw,
pitch and roll on the three axes, which make the task even
1Normandie Univ, UNIROUEN, LITIS, Rouen, France harder, due to the inadaptability of classical methods to
{ahmed-rida.sekkat, pascal.vasseur, paul.honeine} changes of orientation without a particular learning.
@univ-rouen.fr
2Normandie Univ, UNIROUEN, ESIGELEC, IRSEEM, Rouen, France
yohan.dupuis@esigelec.fr 1https://github.com/ARSekkat/OmniScape
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 1603
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 16:03:36 UTC from IEEE Xplore.  Restrictions apply. Fig. 1: Recording platform and a representation of the different modalities.
The remainder of this paper is organized as follows. truthdataforlow-levelandhigh-levelvisiontasks,including
Next section provides a survey on work made using virtual optical ﬂow, instance segmentation, detection and objects
environments as a data source. Section 3 introduces the tracking, as well as visual odometry.
proposed framework to generate omnidirectional images. Johnson-Roberson et al. used in [35] synthetic data gen-
Section 4 presents the OmniScape dataset. Finally, Section erated by GTA V, to show that state-of-the-art algorithms
6 concludes the paper. trained only by this data, work better than if they are driven
on manually annotated real-world data when tested on the
II. RELATEDWORK KITTI dataset for vehicle detection. We can note that all
In the literature, there are several works conducted on these works considered perspective images and, until now,
virtualenvironmentsforthedevelopmentorvalidationofau- there is neither a dataset for omnidirectional images, nor
tonomousdrivingsystems.Virtualenvironmentshaveseveral a dataset for motorcycles or any powered two-wheeler. The
advantages, mainly the inexpensiveness to generate realistic presentpaperseekstoﬁllthisgap,byproposingaframework
data, as well as the variety of the nature of the data that can for omnidirectional data generation from a virtual environ-
be generated, such as depth maps, semantic segmentation or ment, and generating speciﬁcally motorcycles datasets.
detailsofthedynamicpropertiesofthevehicle.Thesevirtual
environments allow to simulate different sensors. We also III. PROPOSEDFRAMEWORK
do not have to deal with the problem of data protection and The proposed framework generates omnidirectional im-
privacy of individuals. Currently several datasets were gen- agesfromavirtualenvironmentusing360°cubemapimages.
erated from virtual environments, such as SYNTHIA [29], To create 360° images, six images are extracted in the six
VEIS [30], and Playing for benchmark [31]. different directions. Using the appropriate omnidirectional
Thanks to advantages offered by the reverse engineering camera model, each pixel of the omnidirectional image can
and modding tools, several recent works have been carried be associated with a 3D direction on the unit sphere. We
out on the generation of data from GTA V. We can mention thencomputethecubethatpresentsthesiximagesunderthe
the method proposed by Doan et al. in [32] for generating cubemapprojection.Usingraytracing,weconstructalookup
perspective images using a virtual camera with six degrees tablethatstorescorrespondencesbetweentheomnidirection-
of freedom. In [33], Richter et al. used GTA V to capture nal image and the cubemap images. It corresponds to the
pixel-by-pixel semantic segmentation using an open source intersectionofthe3Ddirectionassociatedtoeachpixelinthe
middlewarecalledrenderdocbetweenthegameandtheGPU. omnidirectional image with the cubemap images. Then we
In [34], Angus et al. also extracted semantic segmentation justneedtoaffecttoeachpixelintheomnidirectionalimage
images by changing the textures and shaders of the game in the corresponding relevant information from the cubemap
the game ﬁles. Richter et al. generated in [31] a benchmark image (RGB, depth or semantic segmentation), as sketched
ofseveraldatatypesfromGTAV,allannotatedwithground in Fig. 2.
1604
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 16:03:36 UTC from IEEE Xplore.  Restrictions apply. Fig. 2: Lookup table construction to set the omnidirectional image pixel values.
The parameters of the model are calculated from a cali-
brated camera. To generate omnidirectional images, the pro-
posed framework can use well-known camera models, such
asthemodelsproposedbyGeyerandDaniilidis[24],Barreto
and Araujo [25], Mei and Rives [26] and Scaramuzza et
al.[27].Withoutlossofgenerality,wedetailinthefollowing
the model proposed by Scaramuzza et al. in [27]. It is a
calibration model for omnidirectional cameras, considering
theomnidirectionalimagingsystemasacompactandunique
systemcomposedbyapinholecameraandamirror.Itallows
to compute the intrinsic parameters of the omnidirectional
camera. This means that it provides the relation between a
given 2D pixel and the corresponding 3D vector, from the
point of view of the unit sphere, as illustrated in Fig. 3. Let
(u,v) be the metric coordinates of a pixel p with respect to
Fig. 3: The omnidirectional camera model proposed in [27].
the center of the omnidirectional image, and (x,y,z) those
of the corresponding 3D vector P, according to
x  u 
y = v , (1)
   
z f(w)
with w = √u2+v2. Within this model, the function f(w)
is considered to be a polynomial function, namely of the
following form Fig. 4: Mapping of the six images in the ﬁsheye (left) and
catadioptric (right) images.
f(w)=a +a w+a w2+a w3+a w4+... (2)
0 1 2 3 4
The calibration parameters a are estimated by the least-
i
squares method on data acquired with a real camera, as directions of the road in the images, which means that the
described in [36]. pinhole camera is on the top of the mirror or the contrary.
Since the above model is general for omnidirectional Fig. 4 shows the mapping of the six images in the ﬁsheye
images, not just ﬁsheye images, we use the same method images and the catadioptric images. The colors red, orange,
to generate also catadioptric images. We mean by catadiop- yellow, blue, green and purple represent respectively the six
tric images, in this paper, the images taken by a camera sides, front, back, left, right, up and down.
composed by a pinhole camera (perspective camera) and a The computational cost to render one omnidirectional
hyperboloidal mirror [24], [26]. The catadioptric images we frame does not exceed 4.6ms on Ubuntu 18.04.3 64-bit
generate in this work are made in a way that includes all running on an Intel Core i7-8750H CPU @ 2.20GHz.
1605
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 16:03:36 UTC from IEEE Xplore.  Restrictions apply. Fisheye images Catadioptric images
(a) RGB left image. (b) RGB right image. (c) RGB left image. (d) RGB right image.
(e) Semantic segm. left im- (f) Semantic segm. right im- (g) Semantic segm. left im- (h)Semanticsegm.rightim-
age. age. age. age.
(i) Depth map left image. (j) Depth map right image. (k) Depth map left image. (l) Depth map right image.
Fig. 5: Examples of ﬁsheye (left panel) and catadioptric (right panel) images generated from a single capture.
IV. OMNISCAPEDATASET For more insights on the extraction of data from GTA V,
we refer the interested reader to our previous work [37].
The OmniScape2 dataset contains, for each capture, ﬁsh-
eye and catadioptric stereo RGB images from the two front
sidesofamotorcycle,withsemanticsegmentationanddepth To generate the images, we used 5 towns available in
mapgroundtruth,aswellasthedynamicsofthevehiclewith CARLA Simulator. An example of images generated from
its velocity, angular velocity, acceleration and orientation. a single capture is given in Fig. 5. The RGB images are
See Fig. 1 for an overview. The OmniScape dataset will be available for 14 different weather conditions and time of
progressivelyaugmentedwithmoreomnidirectionaldataus- the day and this for each capture. Fig. 6 shows an example
ing the described framework with different vehicles, modal- of a capture with 4 different weather conditions in ﬁsheye
ities and environments. The dataset contains data generated and catadioptric. CARLA Simulator gives a semantic seg-
from GTA V and CARLA, and can be extended to other mentation into 13 classes, namely Building, Fence, Other,
simulators. However, due to space limitation, we present in Pedestrian, Pole, Road line, Road, Sidewalk, Vegetation,
the following data extracted only from CARLA. Vehicle, Wall, Trafﬁc sign, Unlabeled. Fig. 7 shows the
distribution of pixel of all images in the dataset for both
2https://github.com/ARSekkat/OmniScape ﬁsheye images and catadioptric images.
1606
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 16:03:36 UTC from IEEE Xplore.  Restrictions apply. Default Hard rain sunset
Fig. 7: Percentage of pixels representing each class in the
dataset for both ﬁsheye and catadioptric images.
V. CONCLUSION
This paper presented a general framework to generate
datasets of omnidirectional images from virtual environ-
Soft rain noon Clear noon
ments, and provided the OmniScape dataset. We demon-
stratedtherelevanceofthisframeworkbygeneratingﬁsheye
Fig. 6: Examples of ﬁsheye and catadioptric images gener-
and catadioptric images with depth map, semantic seg-
ated from a single capture with four different weather con-
mentation and dynamic parameters. Two simulators were
ditions and time. The motorcycle in this capture undergoes
investigated with success, GTA V and open-source CARLA
rotations.
Simulator.
TABLEI:Statisticsconcerningthedynamics(yaw,pitchand There are many possible extensions to this application,
roll, in degrees) of the motorcycle in a tested route including the generation of other types of datasets, using
different types of omnidirectional camera models and dif-
ferent vehicles like drones. These datasets can be used as
mean std min median max
evaluation credentials for different vision and deep learning
Yaw 1.15 110.54 -179.99 0.85 179.99 applications,whosealgorithmsappliedtoperspectiveimages
Pitch -0.15 1.75 -10.26 -0.07 18.94 havelimitedperformanceonomnidirectionalimages.Awide
variety of applications include Simultaneous Localization
Roll 0.14 3.62 -24.83 0.00 25.91
And Mapping (SLAM), visual odometry, depth estimation,
object recognition and classiﬁcation, detection and tracking.
Moreover, they can also be used to evaluate or even train
In complement to these omnidirectional images, the Om- semantic segmentation algorithms developed for omnidirec-
niScape dataset contains also the dynamics of the vehicle at tional images.
each capture, such as velocity, angular velocity, acceleration
and orientation. As we explained before, the case of two-
wheelers is more challenging because of the dynamics of
these vehicles. We computed statistics concerning these VI. ACKNOWLEDGEMENTS
dynamics in CARLA Simulator. As presented in TABLE I,
The authors would like to thank Vincent Vauchey for his
we can see that the roll and the pitch change considerably.
valuable suggestions on improving the optimisation of the
These alterations will surely affect classical tasks such as
computational cost.
visual odometry and semantic segmentation. This is due to
This work was mainly supported by a RIN grant, Région
thefactthatmostcomputervisionandmachinelearningtasks
Normandie, France. It was partially supported by ANR
are often trained on perspective data acquired with cars as
CLARA(ANR-18-CE33-0004-02)andDAISIprojectfunded
autonomousvehicles,whilethesevehiclesdonotsufferfrom
withthesupportfromtheEuropeanUnionwiththeEuropean
modiﬁcations in these dynamics.
Regional Development Fund (ERDF) and from the Regional
In complement to the images given in this paper, more
Council of Normandy.
examples from CARLA Simulator and GTA V can be found
in OmniScape GitHub.
1607
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 16:03:36 UTC from IEEE Xplore.  Restrictions apply. REFERENCES [21] C. Esteves, C. Allen-Blanchette, A. Makadia, and K. Daniilidis,
“Learning so(3) equivariant representations with spherical cnns,” in
[1] J.Fritsch,T.Kuehnl,andA.Geiger,“Anewperformancemeasureand The European Conference on Computer Vision (ECCV), September
evaluationbenchmarkforroaddetectionalgorithms,”inInternational 2018.
ConferenceonIntelligentTransportationSystems(ITSC),2013. [22] R.Kondor,Z.Lin,andS.Trivedi,“Clebsch-gordannets:afullyfourier
[2] M.Cordts,M.Omran,S.Ramos,T.Rehfeld,M.Enzweiler,R.Benen- spacesphericalconvolutionalneuralnetwork,”inNeurIPS,2018.
son, U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset for [23] Q. Zhao, C. Zhu, F. Dai, Y. Ma, G. Jin, and Y. Zhang, “Distortion-
semanticurbansceneunderstanding,”inProc.oftheIEEEConference aware cnns for spherical images,” in Proceedings of the Twenty-
onComputerVisionandPatternRecognition(CVPR),2016. Seventh International Joint Conference on Artiﬁcial Intelligence,
[3] F.Yu,W.Xian,Y.Chen,F.Liu,M.Liao,V.Madhavan,andT.Darrell, IJCAI-18. International Joint Conferences on Artiﬁcial Intelligence
“BDD100K:Adiversedrivingvideodatabasewithscalableannotation Organization,72018,pp.1198–1204.
tooling,”CoRR,vol.abs/1805.04687,2018. [24] C.GeyerandK.Daniilidis,“Aunifyingtheoryforcentralpanoramic
[4] G.J.Brostow,J.Fauqueur,andR.Cipolla,“Semanticobjectclassesin systems and practical implications,” in Computer Vision — ECCV
video: A high-deﬁnition ground truth database,” Pattern Recognition 2000,D.Vernon,Ed. Berlin,Heidelberg:SpringerBerlinHeidelberg,
Letters,vol.30,pp.88–97,2009. 2000,pp.445–461.
[5] G. Neuhold, T. Ollmann, S. Rota Bulo, and P. Kontschieder, “The [25] J. P. Barreto and H. Araujo, “Issues on the geometry of central
mapillaryvistasdatasetforsemanticunderstandingofstreetscenes,” catadioptricimageformation,”inProceedingsofthe2001IEEECom-
in The IEEE International Conference on Computer Vision (ICCV), puterSocietyConferenceonComputerVisionandPatternRecognition.
Oct2017. CVPR2001,vol.2,Dec2001,pp.II–II.
[6] I.BarisandY.Bastanlar,“Classiﬁcationandtrackingoftrafﬁcscene [26] C. Mei and P. Rives, “Single view point omnidirectional camera
objectswithhybridcamerasystems,”in2017IEEE20thInternational calibrationfromplanargrids,”inProceedings2007IEEEInternational
Conference on Intelligent Transportation Systems (ITSC), Oct 2017, ConferenceonRoboticsandAutomation,April2007,pp.3945–3950.
pp.1–6. [27] D.Scaramuzza,A.Martinelli,andR.Siegwart,“Atoolboxforeasily
[7] A.EichenseerandA.Kaup,“Adatasetprovidingsyntheticandreal- calibratingomnidirectionalcameras,”in2006IEEE/RSJInternational
world ﬁsheye video sequences,” in IEEE Int. Conf. on Acoustics, Conference on Intelligent Robots and Systems, Oct 2006, pp. 5695–
SpeechandSignalProcessing(ICASSP),Mar2016,pp.1541–1545. 5701.
[8] S.UrbanandB.Jutzi,“Laﬁda-alaserscannermulti-ﬁsheyecamera [28] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun,
dataset,”J.Imaging,vol.3,p.5,2017. “CARLA: An open urban driving simulator,” in Proceedings of the
[9] G. Caron and F. Morbidi, “Spherical Visual Gyroscope for Au- 1stAnnualConferenceonRobotLearning,2017,pp.1–16.
tonomous Robots using the Mixture of Photometric Potentials,” in [29] G.Ros,L.Sellart,J.Materzynska,D.Vazquez,andA.M.Lopez,“The
IEEEInternationalConferenceonRoboticsandAutomation,Brisbane, synthia dataset: A large collection of synthetic images for semantic
Australia,May2018,pp.820–827. segmentationofurbanscenes,”in2016IEEEConferenceonComputer
[10] N. Hirose, A. Sadeghian, M. Vázquez, P. Goebel, and S. Savarese, VisionandPatternRecognition(CVPR),June2016,pp.3234–3243.
“Gonet: A semi-supervised deep learning approach for traversability [30] F.S.Saleh,M.S.Aliakbarian,M.Salzmann,L.Petersson,andJ.M.
estimation,”CoRR,vol.abs/1803.03254,2018. Alvarez, “Effective use of synthetic data for urban scene semantic
[11] D. Levi and S. Silberstein, “Tracking and motion cues for rear-view segmentation,”inECCV,2018.
[31] S. R. Richter, Z. Hayder, and V. Koltun, “Playing for benchmarks,”
pedestriandetection,”in2015IEEE18thInternationalConferenceon
IntelligentTransportationSystems,Sep.2015,pp.664–671. in IEEE International Conference on Computer Vision, ICCV 2017,
[12] Z.Zhang, H. Rebecq, C. Forster, and D. Scaramuzza, “Beneﬁt of Venice,Italy,October22-29,2017,2017,pp.2232–2241.
[32] A. Doan, A. M. Jawaid, T. Do, and T. Chin, “G2D: from GTA to
large ﬁeld-of-view cameras for visual odometry,” in 2016 IEEE
International Conference on Robotics and Automation (ICRA), May data,”CoRR,vol.abs/1806.07381,2018.
[33] S. R. Richter, V. Vineet, S. Roth, and V. Koltun, “Playing for data:
2016,pp.801–808.
[13] Á.Sáez,L.M.Bergasa,E.Romera,M.E.L.Guillén,R.Barea,and Ground truth from computer games,” in European Conference on
R.Sanz,“Cnn-basedﬁsheyeimagereal-timesemanticsegmentation,” Computer Vision (ECCV), ser. LNCS, B. Leibe, J. Matas, N. Sebe,
andM.Welling,Eds.,vol.9906. SpringerInternationalPublishing,
2018IEEEIntelligentVehiclesSymposium(IV),pp.1039–1044,2018.
2016,pp.102–118.
[14] L. Deng, M. Yang, Y. Qian, C. Wang, and B. Wang, “Cnn based
[34] M. Angus, M. ElBalkini, S. Khan, A. Harakeh, O. Andrienko,
semanticsegmentationforurbantrafﬁcscenesusingﬁsheyecamera,”
C.Reading,S.L.Waslander,andK.Czarnecki,“Unlimitedroad-scene
in 2017 IEEE Intelligent Vehicles Symposium (IV), June 2017, pp.
231–236. synthetic annotation (URSA) dataset,” CoRR, vol. abs/1807.06056,
2018.
[15] L. Deng, M. Yang, H. Li, T. Li, B. Hu, and C. Wang, “Restricted
[35] M.Johnson-Roberson,C.Barto,R.Mehta,S.N.Sridhar,K.Rosaen,
deformableconvolutionbasedroadscenesemanticsegmentationusing
andR.Vasudevan,“Drivinginthematrix:Canvirtualworldsreplace
surroundviewcameras,”CoRR,vol.abs/1801.00708,2018. human-generatedannotationsforrealworldtasks?”inIEEEInterna-
[16] X.Yin,X.Wang,J.Yu,M.Zhang,P.Fua,andD.Tao,“Fisheyerecnet:
tionalConferenceonRoboticsandAutomation,2017,pp.1–8.
Amulti-contextcollaborativedeepnetworkforﬁsheyeimagerectiﬁ-
[36] Y.Dupuis,X.Savatier,J.Ertaud,andP.Vasseur,“Robustradialface
cation,” in Computer Vision – ECCV 2018, V. Ferrari, M. Hebert, detection for omnidirectional vision,” IEEE Transactions on Image
C. Sminchisescu, and Y. Weiss, Eds. Cham: Springer International
Processing,vol.22,no.5,pp.1808–1821,May2013.
Publishing,2018,pp.475–490.
[37] A. R. Sekkat, Y. Dupuis, P. Vasseur, and P. Honeine, “Génération
[17] Y.-C. Su and K. Grauman, “Learning spherical convolution for fast
d’imagesomnidirectionnellesàpartird’unenvironnementvirtuel,”in
featuresfrom360°imagery,”inAdvancesinNeuralInformationPro- 27-èmeColloqueGRETSIsurleTraitementduSignaletdesImages,
cessingSystems30,I.Guyon,U.V.Luxburg,S.Bengio,H.Wallach, Lille,France,Aug.2019.
R.Fergus,S.Vishwanathan,andR.Garnett,Eds. CurranAssociates,
Inc.,2017,pp.529–539.
[18] N.Perraudin,M.Defferrard,T.Kacprzak,andR.Sgier,“Deepsphere:
Efﬁcientsphericalconvolutionalneuralnetworkwithhealpixsampling
forcosmologicalapplications,”CoRR,vol.abs/1810.12186,2018.
[19] B. Coors, A. P. Condurache, and A. Geiger, “Spherenet: Learning
spherical representations for detection and classiﬁcation in omnidi-
rectional images,” in Computer Vision – ECCV 2018, V. Ferrari,
M. Hebert, C. Sminchisescu, and Y. Weiss, Eds. Cham: Springer
InternationalPublishing,2018,pp.525–541.
[20] W. Boomsma and J. Frellsen, “Spherical convolutions and their
application in molecular modelling,” in Advances in Neural Infor-
mation Processing Systems 30, I. Guyon, U. V. Luxburg, S. Bengio,
H.Wallach,R.Fergus,S.Vishwanathan,andR.Garnett,Eds. Curran
Associates,Inc.,2017,pp.3433–3443.
1608
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 16:03:36 UTC from IEEE Xplore.  Restrictions apply. 