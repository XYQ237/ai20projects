2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
Active Reward Learning for Co-Robotic Vision Based Exploration in
Bandwidth Limited Environments
Stewart Jamieson1,2, Jonathan P. How2, Yogesh Girdhar3
Abstract—WepresentanovelPOMDPproblemformulation
for a robot that must autonomously decide where to go to
collect new and scientiﬁcally relevant images given a limited
ability to communicate with its human operator. From this
formulationwederiveconstraintsanddesignprinciplesforthe
observationmodel,rewardmodel,andcommunicationstrategy
ofsucharobot,exploringtechniquestodealwiththeveryhigh-
dimensionalobservationspaceandscarcityofrelevanttraining
data. We introduce a novel active reward learning strategy
based on making queries to help the robot minimize path
“regret” online, and evaluate it for suitability in autonomous
visualexplorationthroughsimulations.Wedemonstratethat,in
some bandwidth-limited environments, this novel regret-based
criterionenablestheroboticexplorertocollectupto17%more
Fig. 1: Proposed approach to co-robotic exploration that
reward per mission than the next-best criterion.
models the interest of the operator over a low bandwidth
communication channel and uses the learned reward model
I. INTRODUCTION
toplanthemostrewarding(intermsofinterest)robotpaths.
Images of exotic biological and geological phenomena
from remote and dangerous locations have tremendous sci-
entiﬁc value but are extraordinarily challenging and costly
spatially-varyingscalar quantities,suchastemperature[12],
to collect. Robots have been at the forefront of collecting
[13].Toreachafutureofefﬁcientroboticexplorersinremote
visual scientiﬁc observations in such environments, which
environments, robots will need to autonomously: recognize
include Mars [1], deep space [2], the Earth’s oceans [3]–[5],
visual phenomena that might be scientiﬁcally interesting,
and under Arctic ice sheets [6]. Communication bandwidth
transmit images of them to scientists for clariﬁcation as
constraints are perhaps the biggest bottleneck to exploration
needed, model where more of them might be found, and
in these remote environments [7], [8]. As such, common
plan a trajectory accordingly (Figure 1).
current approaches to autonomous exploration are to either
The primary contributions of this work are a partially
deploy the vehicles on a predeﬁned path or to deploy them
observable Markov decision process (POMDP) formulation
with adaptive path plans based on tracking low-dimensional
for vision-based scientiﬁc exploration and a solution that is
observations from some other sensor. This paper proposes a
generalizable to many environments. Note that we explicitly
novel approach to vision-guided exploration using a human-
constrain the focus of this work to dealing with high-
robot team that is effective even in the presence of strong
dimensional observation spaces when solving the POMDP.
bandwidth constraints such as those imposed by acoustic
The proposed exploration approach uses the limited com-
underwater communications [7].
munication bandwidth to query the operator for the value
Mostrecentprogresstowardsincreasingthesciencereturn
of representative images and uses the responses to learn
of autonomous exploration missions has been made through
an interest function that informs the robot about the value
enabling“opportunisticscience”1 aswellasaddressingchal-
of an exploration path. The proposed approach is suitable
lengesinnavigation,taskplanningandschedulingautonomy
for deployment in completely unknown environments, and
[9]–[11]. The progress in adaptive sampling and exploration
it can use (but does not require) prior knowledge about
algorithms for robots has primarily focused on observing
the environment and the phenomena being observed. Our
ﬁnal contribution is an analysis and comparison of active
*ThisworkwassupportedbyNSF-NRIAwardNumber1734400
learning decision criteria that a robot could use for deciding
1S. Jamieson is with the MIT-WHOI Joint Program in Applied Ocean
ScienceandEngineeringsjamieson@whoi.edu which observations to send to the operator. That analysis
2S. Jamieson and J.P. How are with the Department of Aeronautics is supported by simulations of a scientiﬁc exploration task
and Astronautics at the Massachusetts Institute of Technology (MIT)
{ } using both real and artiﬁcial data.
sjamieson,jhow @mit.edu
3Y. Girdhar is with the Applied Ocean Physics and Engineer-
ing Department at the Woods Hole Oceanographic Institution (WHOI) II. RELATEDWORK
yogi@whoi.edu This work contributes to the ﬁeld of autonomous science;
1This refers to an autonomous action, such as targeting and using a
previous works in this area include AEGIS [1] and OASIS
particularsensor,thatpreemptstherobot’scurrenttaskafterbeingtriggered
byaspeciﬁcphenomenarecognizedbyanonboarddetectionalgorithm. [11], which enabled robots to opportunistically recognize
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 1806
Authorized licensed use limited to: Carleton University. Downloaded on September 22,2020 at 00:58:14 UTC from IEEE Xplore.  Restrictions apply. ∈O
scientiﬁcallyrelevantimageobservations,givenapredeﬁned latestobservationistheimageI .L isthesetofindices
t t
model, and schedule more detailed observations with other ofimagessenttoandlabeledbytheoperator.Y containsthe
t
sensors. However, these algorithms required domain-speciﬁc reward labels for all images, including those that have not
featureengineeringandlackedspatialobservationmodels,so been sent; most of these are unknown, making the robot’s
the adaptive path planning was limited to moving the robot state partially observable.
closertoatargetthathadalreadybeendetected.Attheother The partial observability comes from the robot’s limited
extreme, “curious” robots use a generic unsupervised vision ability to query the operator during a mission; in bandwidth
model and autonomously move towards anything in their constrained environments the robot sends images at a much
environment that is surprising or novel to the model [14]; slower rate than it collects them, so it must decide which
the lack of operator input makes it impossible to directly labels to observe. We assume that only the operator can
specify particular scientiﬁc objectives using this approach. evaluate the unknown, but deterministic, binary “interest”
I I
OurworkiscloselyrelatedtotheworkofAroraetal.[15], function (I) such that (Y ) = (I ). Further, it is as-
t i i
which modeled operator’s domain knowledge with a pre- sumedthattheoperatorcannotexpresstheirinterestfunction
deﬁned Bayesian Network (BN) that was used by the robot analytically (otherwise it would be computed onboard the
to estimate the reward for a trajectory. They introduced a robot), and would instead train an approximate model based
spatialobservationmodelinthesystem,enablinginformative on their labels for various example images. However, since
path planning using Monte-Carlo Tree Search (MCTS) to exploration typically occurs in remote and unstudied envi-
explore an action tree composed of movement and sensing ronments, the operator does not have a fully representative
actions [15]. Their approach requires the operator to specify dataset of what the robot will observe and is unable to
I ·
the domain-speciﬁc BN a priori, and has limited utility as provide the robot with a complete model of () a priori.
a general purpose exploration tool that can be deployed in The entire POMDP is characterized by the tuple
S A O
unknown environments. In contrast, our solution learns the ( , , ,T,O,R,γ,b ):
0
rewardmodelonline,andhenceallowstherobottodealwith
Component Deﬁnition OurAssumptions
unexpected observations efﬁciently during exploration. S Statespaceoftherobot S=(X,Y,L)
A
Active learning algorithms interactively query an oracle Discrete set of robot ac- Motionprimitives2
to produce samples in the training set, such that the model tions
O Observationspace Naturalimages
can be trained with far fewer labeled examples than would
andbinarylabels
normallyberequired[16].Activerewardlearningalgorithms T Transitionfunction S×A(cid:55)→S
have efﬁciently learned reward models representing human O Observationmodel S×A(cid:55)→O
ratings or preferences for robot behaviours by making on R Rewardmodel S∈×A×O(cid:55)→R
γ Discountfactor γ [0,1]
the order of 10-100 reward queries [17], [18]. Doshi-Velez
b0 Initialbeliefstate Initiallocationx0
et al. [19] considered a query to be an action that could be
taken if it helped the robot to gain additional reward. This
Given these speciﬁcations, it is typical for the robot to
is online active learning and our approach is most related to
use an online POMDP planner to approximate an optimal
theirswiththemaindifferencethatweuserewardqueriesto S (cid:55)→ A
policy π(cid:63) : in real-time. Algorithm 1 presents our
learn a mapping from observations to reward, whereas [19]
approachtoco-roboticexplorationbasedontheassumptions
used policy queries to learn optimal actions directly.
listed above.
Due to the high-dimensionality of natural images, even
There are three key decisions to fully specify the co-
with active learning, it can take hundreds of queries to learn
robotic visual exploration POMDP that we will consider.
a reward model [20]. In bandwidth limited environments
The ﬁrst is deﬁning an observation model over the space
suchasthedeepsea,sendingthatmanyimagesforlabelling
of natural images. The second is deﬁning a reward model,
duringthespanofamissionisnotfeasible.Deepfeaturesare
andthethirdischoosinganeffectiveactivelearningstrategy.
relatively low-dimensional representations of images which
are very helpful for learning new classiﬁcation tasks with A. Spatial Observation Model for Images
few examples [21], [22]. Topic models, especially when A spatial observation model is required for adaptive path
combined with deep features, can be used to provide a low- planningbecausetherobot’srewardisdeterminedbywhatit
dimensional semantic representation of the visual environ- observes,sotoevaluateacandidatetrajectorytherobotmust
ment [23]. Our proposed POMDP approach leverages both predictwhatitwillobservealongthattrajectory.Thisshould
active learning and low-dimensional image representations bepossiblebecausethesemanticcontentsofnaturalimages,
to enable interactive visual exploration over low bandwidth. such as terrain types and species present, often have strong
spatialcorrelation[23],[24].However,thesecorrelationsare
III. THECO-ROBOTICVISUALEXPLORATIONPOMDP
hardtomodelinthepixelspace,whereevennearlyidentical
We present the co-robotic visual exploration problem as images can be made distant by effects like sensor noise
a POMDP. We model the state of the robot at time t as
{ }
St = (Xt,Yt,Lt). Xt = (xi,Ii) ti=1 is the sequence of in2AQuweirtyhinsgomtheecoopsetraotfocroismomftuennicmatoiodne,llesudc(he.ags.,einne[r1g9y])usaasgaen,oitnhcelrudaecdtioinn
locations the robot has visited, with corresponding image
∈R therewardmodel.Forsimplicity,weassumethiscostisnegligibleandthat
observations, where the current location is x 3 and the therobotperformsqueriesconcurrentlywithotheractions.
t
1807
Authorized licensed use limited to: Carleton University. Downloaded on September 22,2020 at 00:58:14 UTC from IEEE Xplore.  Restrictions apply. Algorithm 1: Co-Robotic Exploration of topics are smooth (spatially correlated) and the topic
1 Give←n:∅(S,A,O,T,O,R,γ,x0), tmax dsiiostnraibliutyti.oTnhsethloeyw-udsiemteonsrieopnraelsietyntoifmthaegseesrheapvreesleonwtatdioimnsenis-
2 X // Stores the path and observations
0←{ } a critical requirement for learning the reward function from
3 τ x // The current trajectory plan
← 0 few examples; this is much more challenging with higher
4 q null // Index of next observation to label
← dimensional representations such as deep features [25].
5 t 1 // The current timestep
6 while t←<tmax: B. Learning a Reward Model Online over Low Bandwidth
7 xt← NEXT STEP(τ) We deﬁne the robot’s reward to be the total number of
8 It ←OBSER∪VE{(xt) } unique and interesting o(cid:88)bservations i(cid:88)t has collected
9 X X − x ,I
Ot← t 1 t t O
10 UPDATE OBSERVATION MODEL( ,X ) t I t
11 if LABEL READY(I ) t R(Xt)= (Ii)= (Yt)i. (1)
← q
12 y QUERY RESULT(I ) i=1 i=1
q ← ∪{ } q Thiscanonlybecomputedaftertheoperatorseesallimages
13 Y Y − I ,y
t← t 1 q q (i.e., after the mission). Since the robot models observations
14 R← UPDATE REWARD MODEL(R,Yt) in the semantic space Z, trajectory planning requires it
15 q null
endif to estimate the reward as a function of the semantic ﬁeld
16 Z (cid:55)→
17 τ ← PLAN TRAJECTORY(X ,S,A,T,O,R,γ) Zt(x). For this, the robot learns a model gθ : [0,1]
t
18 if q =←null O R(x)≈g(Zt(x);θ), (2)
19 q QUERY SELECTOR(X , ,R)
20 REQUEST LABEL(I ) t where θ is a set of parameters for the model family. Recall
endif q that L is the set of labeled image indices at time t, and
21 t { }
← let D = (I ,(Y ) ) ∈ be the corresponding training set.
22 t t+1 t i t i i Lt L
We choose θ to min(cid:0)imize the cr(cid:1)oss-entropy loss on D ,
t
resulting in the ﬁnal reward model
(cid:88)
Algorithm 2: PLAN TRAJECTORY R(x;D )≈g Z(x);θ(cid:63) (3)
1 Input: X ,S,A,T,O,R,γ t Dt L
t θ(cid:63) =argmin (y,g(z(I);θ)). (4)
2 GTiv←en: n // Number of trajectoSriAes to test Dt θ ∈
3 GENERATE TRAJECTORIES(X , , ,T,n) (I,y) Dt
t
4 for i=1,...,n: The number of labeled examples that a model must be
← T
5 s SCORE TRAJECTORY( (i),O,R,γ) trained on in order to generalize well is proportional to the
←iT
sample complexity of the model family [30], and for simple
6 τ (argmax s )
7 return τ i i models (e.g., logistic regression) the sample complexity is
typicallylinearinthenumberofinputdimensions[31].Thus,
it is desirable to jointly pick a semantic representation and
a model g such that the total number of examples required
and slight changes in illumination [25]. Further, due to the θ
to train g is less than the number of examples that can be
high dimensionality of the image space, there are no spatial θ
labelledduringthemission.Thisfurthermotivatestheuseof
models with which it is computationally tractable to predict
BNP-ROST [29] as the semantic feature extractor, since the
the image that would be observed in an unvisited location.
dimensionality of its semantic representation grows as logt,
Toovercomethesechallenges,therobotcomputesseman-
Z logarithmic in the number of images t, while the number
tic representations of images in the space , which is low- ≥
of labelled images grows linearly at t, where n 1 is
dimensional compared to the space of natural images O. n
set by the bandwidth constraint. Thus, when using BNP-
The robot builds a spatial observation model over semantic
ROST in combination with a simple reward model, then the
R (cid:55)→ Z
rtheperoebsesenrtvataitoionns,sdXetnOo=te{d(cid:55)→(xasiZ,ZIit)}(xti=)1:and3 these,mtraanitnicedfeuastiunrge tgroaoindinpgarparmoecteesrssfθo,revgeθnisweitxhpfeecwtedtratoiniqnugicekxlaymcpolnevs.erge to
extractor z(I) : . This approach requires that
the semantic representations of two images z(I ),z(I ) C. Query Selection for Low Bandwidth Reward Learning
1 2
are similar (typically measured by Euclidean distance) if When the robot observes novel phenomena, it needs to
and only if the human-perceived similarity of I and I is query the operator’s interest in collecting more observations
1 2
high.Semanticrepresentationsderivedfromcomputervision of the phenomena. The only type of query the robot can
modelsdevelopedforunsupervisednaturalimageclustering, perform in an unknown environment is sending an image
such as deep feature extractors [22], [25] and spatial topic to the operator and receiving an interest label in return; the
models (STMs) [26], [27] have this property. operatorcannotdeterminetheirinterestinanimagefromthe
STMs such as BNP-ROST [28], [29] are a strong class image’ssemanticrepresentation,anddoesnothaveaccessto
of candidates for the spatial observation model because enoughinformationtoadvisetherobotontheoptimalpolicy.
the priors they use to represent the spatial distributions This is a unique challenge for active learning.
1808
Authorized licensed use limited to: Carleton University. Downloaded on September 22,2020 at 00:58:14 UTC from IEEE Xplore.  Restrictions apply. IV. ONLINEACTIVEREWARDLEARNINGFORPOMDPS Suppose that the robot is considering a ﬁnite set of
T { }
Here we will consider active learning strategies to learn trajectories = τi Ni=τ1: it uses the observation model to
theparametersofaPOMDPrewardmodelonline.Wedenote predict what it will observe along each trajectory τ, predicts
U eachtrajectory’sreward,andﬁnallychoosestheonewiththe
the set of unlabelled image indices at time t as , and the
t highest reward (see Algorithm 2). However, given limited
active learning metric as h(z), such that the next image to
training data, the robot has signiﬁcant uncertainty in the
request a label for is chosen as
predicted rewards and thus is unlikely to have chosen the
i(cid:63) =argmaxh(z(I )). (5) true optimal trajectory. This motivates a question for each
∈U i
i t unlabeledimage:ifthisimagewerelabeled,wouldtherobot
A. Non-Adaptive Query Selection have chosen a different trajectory? If the answer is yes, then
Thesimplestapproachestoselectingimagestobelabelled it must mean that, given this additional label, a different
donotdependonz,andthusaregoodbaselinestoconsider. trajectory would be predicted to have greater reward and
Randomselectionchoosesunlabelledobservationsuniformly thus the robot would “regret” not knowing the label. If it is
at random. Uniform selection instead chooses every nth no, then the robot would h(cid:104)ave no immediate regret f(cid:105)or not
image, where n is determined by the bandwidth constraint. knowing it. We formalize this in the following objective:
(cid:88)
B. Informative Query Selection hRegret(z)=EDt(cid:48)|Dt R(τD(cid:63)t(cid:48);Dt(cid:48))−R(τD(cid:63)t;Dt(cid:48)) (8)
Informative query selection involves deﬁning some un-
R(τ;D )= g(Z(x);θ(cid:63) ) (9)
certainty metric on the model, and choosing to label the t ∈ Dt
observation which results in the greatest reduction of un- x τ
τ(cid:63) =argmaxR(τ;D) (10)
certainty. There are many query selection strategies that fall D ∈T
τ
into this category and are effective at learning a function
Equation 8 may be interpreted as the expected reward in-
in few examples [32]. A common uncertainty metric for
crease (regret decrease) given a label for z. An approach to
classiﬁcation problems is entropy, where the highest entropy
valuesoccurwhenanobservationisonadecisionboundary. computing hRegret is presented in Algorithms 3 and 4.
A widely-used approach to informative query selection is
“uncertainty samplin(cid:0)g”, wh(cid:1)ich typ(cid:2)ica(cid:0)lly me(cid:1)a(cid:3)ns picking the Algorithm 3: Regret-Based Query Selection
S A O
observation with the maximum entropy [32] 1 Given: X , , ,T, ,R,γ
U t
hEntropy z;θD(cid:63)t =H g z;θD(cid:63)t . (6) 23 Iτnp←ut:PLtAN TR/A/JESCetTOoRfY(uXnla,bSe,lAed,Ti,mOa,gRe,iγn)dices
0 ∈U t
An issue with uncertainty sampling is that labeling the most 4 foreach i :
← t
uncertain observation might not have much effect on the 5 z SEMANTIC REPRESENTATION(I )
← i
modelparametersθ–ifthemodelparametersdonotchange, 6 ypre←d PREDICT REWARD(z)
thenthemodelperfo(cid:0)rmance(cid:1)doesnotinc(cid:104)rease.T(cid:16)hissugg(cid:17)e(cid:105)sts 7 r COMPUTE REGRET(τ ,z, 0)
0 ← 0
maximizing “error reduction” [32] instead 8 r COMPUTE REGRET(τ ,z, 1)
1 ← − 0
hInfo(z)=hEntropy z;θD(cid:63)t −EDt(cid:48)|Dt hEntropy z;θD(cid:63)t(cid:48) (7) 109 returrengraetrigmaxyip∈reUdtr1re+gr(e1ti ypred)r0
(cid:48) ∪
D =D (z,y)
t (cid:48) | t ≈
p(D D ) g(z;θ(cid:63) ).
t t Dt Algorithm 4: COMPUTE REGRET
This Information Gain query selection method prioritizes 1 Given: X ,S,A,T,O,R,γ
t
labeling an observation by how much a new label y is 2 Input: τ ,z,y // Reference trajectory,
0
expectedtoreducetheentropyofsimilarfutureobservations.
observation to label, and temporary label
O
This should maximize the rate at which entropy is reduced 3 ADD TEMPORARY LABEL( ,z,y)
← S A O
and thus the rate at which the reward function is learned. 4 τ(cid:63) PLAN TRAJECTORY(X , , ,T, ,R,γ)
← t O
C. Regret Minimizing Query Selection 5 s(cid:63) ← SCORE TRAJECTORY(τ(cid:63),O,R,γ)
6 s SCORE TRAJECTORY(τ , ,R,γ)
Here we introduce a novel Regret minimizing query se- 0 0 O
7 REMOVE TEMPORARY LABEL( ,z)
lector that focuses on identifying labels that maximize the −
8 return (s(cid:63) s ) // Regret given the temp label
expected reward collected during the mission, rather than 0
information gained about the reward function. Regret is
typically deﬁned for POMDPs as the difference in utility
between the chosen action and the true optimal action V. EXPERIMENTS
based on complete information. To our knowledge, this is We evaluate the proposed query selection techniques
the ﬁrst work that compares a regret-based heuristic with through two experiments, each simulating the co-robotic
information-theoretic heuristics in online active learning. explorationtaskwithvariousbandwidthconstraints.Theﬁrst
1809
Authorized licensed use limited to: Carleton University. Downloaded on September 22,2020 at 00:58:14 UTC from IEEE Xplore.  Restrictions apply. (a) Top: A topic map where each location is described (b)Bestviewedonascreen.Sampletrajectoriesfollowedbyrobotsstartingat
∈
by the semantic representation z(i,j) ∆8. The color thecenterofthemapin(a)withdifferentqueryselectors.Alongeachtrajectory,
ofeachpixelindicatesthelargestcomponentofz(i,j). red-orange pixels correspond to no reward, and blue pixels to reward. Bright
Bottom: The reward at each location is randomly sam- orange/bluepixelsrepresentobservationsforwhichthequeryselectorrequested
pledasR(i,j) ∼Bernoulli(pTz(i,j)),wherep∈[0,1]k the label. The greyscale background intensities represent g(Z(x);θD(cid:63)): reward
represents how “interesting” each component of z is. estimatesofobservationsateachlocation,basedonalllabeledsamples.Query
Here, the pink and black topics are most interesting. Selectors: (top row) Random, Uniform; (bottom row) Info Gain, Regret.
(c)Acomparisonofthequeryselectorperformancefordifferentbandwidthavailability;thex-axisrepresentslabelingperiod(timebetween
making a call to REQUEST LABEL and LABEL READY returning true in Algorithm 1), which is inversely proportional to bandwidth.
Each datapoint represents the mean of 1080 simulations (36 trials on 30 unique maps) and bars represent the 68% conﬁdence bound of
the mean. Top: The mean amount of reward collected by each robot per unit time (higher is better). Lawnmower is not a query selector,
butratherrepresentsthemeanrewardcollectedby8preplannedboustrophedonictrajectories[33]thateachstartatthecenterofthemap
andmovetowardsacorner.Bottom:Themeancross-entropylossbetweenthegroundtruthinterestmaps,asin(a),andthecorresponding
robots’ predictions of the reward at each location, as in (b), at the end of each simulation (lower is better).
Fig. 2: Stages of the simulation procedure, and performance comparison of the query selectors on fully simulated data.
1810
Authorized licensed use limited to: Carleton University. Downloaded on September 22,2020 at 00:58:14 UTC from IEEE Xplore.  Restrictions apply. Fig. 3: Left: A crop of the KAH 2016 3 photomosaic im-
age from the 100 Islands Challenge [34], showing a coral
reef near Kaho’olawe. Center: The photomosaic annotations
where each color represents an expert label [34]. Right: One
of 30 unique interest maps generated (cf. Figure 2a).
experiment (see Figure 2) used 30 artiﬁcial “topic maps”
Fig. 4: The Regret query selector continues to outperform
(cf. [28]) created by randomly generating Voronoi partitions
the other active learning heuristics when the topic map is
of a 100×100 image, assigning each cell a topic label, and
derived from a real image (see Figure 3).
then assigning each pixel’s topic distribution as a distance-
weighted mean over cell labels. This produced continuous
topic maps with topics in varying concentrations, and each
one was associated with a unique interest map (see Figure presented in Figures 2c and 4. The Regret query selector
2a).Inthesecondexperiment,asingletopicmapwasderived matches, or outperforms, every other selection criterion at
fromtheexpertannotationsofanactualcoralreefimage,and collecting reward, at any bandwidth availability, in these
30 interest maps were generated for it (see Figure 3). The simulation conﬁgurations. The relative gains of non-random
procedure for both experiments was: query selection are smaller when the time between queries
∈ is short (high-bandwidth) and thus almost every image is
1) Generate a map of topic distributions z(x) ∆d
labeled, orwhen itis so long(low-bandwidth) thatthe robot
which represent the observations at each location x;
∈ barely learns anything before the mission ends. The results
2) Generateaninterestproﬁlep [0,1]dsothatp=pTz
also demonstrate the vast improvement of autonomous ex-
is the probability that the operator is interested in an
ploration over preplanned trajectories: the adaptive planners
observation with feature representation z;
∼ collected up to 29.7% more reward at very low bandwidth,
3) Generateabinary“interestmap”bysamplingR(x)
and up to 230% more reward at high bandwidth.
Bernoulli(p(z(x))ateachlocationxinthetopicmap;
Theregret-basedmethoddidnotlearntherewardfunction
4) Foreachbandwidthlimitationandeachqueryselection
as well as the information gain query selector, based on its
algorithm: perform 36 rollouts of algorithm 1 for a
higher map log-loss. This exempliﬁes the difference in the
simulated robot making reward queries according to
designcriteria:theinformationtheoreticcriterionfocuseson
the bandwidth limitation and query selector
useful labels for learning a function, which is appropriate
Each rollout in step (4) had a duration of 300 timesteps;
for active reward learning ofﬂine, during training. The regret
robot movement was one pixel per timestep and bandwidth
criterion instead optimizes for the robot’s reward, making
constraints were simulated by changing the number of
it better suited for online active reward learning, which
timesteps for a label to be received after being requested.
describes our usage of queries during a live mission.
State transitions and observations were deterministic and
noiseless.Therobotstartedwithnotrainingdataandusedlo-
VII. CONCLUSIONSANDFUTUREWORK
gisticregression(from[35])asitsrewardmodel.Trajectories
The Co-Robotic Visual Exploration POMDP provides a
weregeneratedbyrandomlysamplingsequencesof5motion
primitives.3 50 trajectories were generated at each timestep structured approach to managing human-robot collaboration
and high-dimensional observation spaces in autonomous
and scored using the sum of the predicted rewards along the
science. We provide general principles for choosing the
trajectory, less the scores of locations already visited. The
POMDP’s observation model, reward model, and active
highest scoring trajectory was followed.
learning criterion, and demonstrate that the novel Regret-
VI. RESULTS&DISCUSSION based active learning criterion can greatly improve the
amount of reward collected. Some next steps are: exploring
We compared the Random, Uniform, Information Gain,
spatial observation models capable of longer-range topic
and Regret query selectors described in Section IV over a
prediction (e.g. [36]), extending the reward model and ac-
total of 69120 simulations; the mean reward collection rates
tive learning formulation to non-binary rewards, and using
and interest map prediction losses for each experiment are
higher-ﬁdelity simulations and ﬁeld deployments to better
understand the performance increases that can be achieved
3The primitives were 13 straight lines, each 5 units long and at angles
spaceduniformlybetween-135◦to135◦fromtherobot’scurrentdirection. in real-world autonomous exploration.
1811
Authorized licensed use limited to: Carleton University. Downloaded on September 22,2020 at 00:58:14 UTC from IEEE Xplore.  Restrictions apply. REFERENCES [20] F. Shkurti, “Algorithms and Systems for Robot Videography from
HumanSpeciﬁcations,”Ph.D.dissertation,McGillUniversity,2018.
[1] T. A. Estlin, B. J. Bornstein, D. M. Gaines, R. C. Anderson, D. R.
[21] J.Donahue,Y.Jia,O.Vinyals,J.Hoffman,N.Zhang,E.Tzeng,and
Thompson, M. Burl, R. Castan˜o, and M. Judd, “AEGIS Automated
T. Darrell, “DeCAF: A Deep Convolutional Activation Feature for
Science Targeting for the MER Opportunity Rover,” ACM Transac- GenericVisualRecognition,”inProceedingsofthe31stInternational
tions on Intelligent Systems and Technology, vol. 3, no. 3, pp. 1–25, Conference on Machine Learning, ser. Proceedings of Machine
2012.
Learning Research, E. P. Xing and T. Jebara, Eds., vol. 32, no. 1.
[2] Y. Gao and S. Chien, “Review on space robotics: Toward top-level
Bejing,China:PMLR,2014,pp.647–655.
science through space exploration,” Science Robotics, vol. 2, no. 7,
[22] A. Romero, C. Gatta, and G. Camps-Valls, “Unsupervised Deep
p.eaan5074,62017. Feature Extraction for Remote Sensing Image Classiﬁcation,” IEEE
[3] R. D. Ballard, “WHOI-93-34: The JASON Remotely Operated Ve- Transactions on Geoscience and Remote Sensing, vol. 54, no. 3, pp.
hicle System,” Woods Hole Oceanographic Institution, Woods Hole,
1349–1362,32016.
Massachusetts,Tech.Rep.,1993.
[23] G. Flaspohler, N. Roy, and Y. Girdhar, “Feature discovery and
[4] B.P.Foley,R.M.Eustice,K.Dellaporta,D.Evagelistis,D.Sakellar-
visualization of robot mission data using convolutional autoencoders
iou,V.L.Ferrini,B.S.Bingham,K.Katsaros,R.Camilli,D.Kourk- and Bayesian nonparametric topic models,” in 2017 IEEE/RSJ
oumelis,A.Mallios,H.Singh,P.Micha,D.S.Switzer,D.A.Mindell, International Conference on Intelligent Robots and Systems (IROS).
T. Theodoulou, and C. Roman, “The 2005 Chios ancient shipwreck
IEEE,92017,pp.1–8.
survey:Newmethodsforunderwaterarchaeology,”Hesperia,vol.78,
[24] H.Reiss,S.Cunze,K.Ko¨nig,H.Neumann,andI.Kro¨ncke,“Species
no.2,pp.269–305,2009.
distribution modelling of marine benthos: A North Sea case study,”
[5] M.E.Clarke,N.Tolimieri,andH.Singh,“UsingtheSeabedAUVto MarineEcologyProgressSeries,vol.442,no.December,pp.71–86,
AssessPopulationsofGroundﬁshinUntrawlableAreas,”TheFuture
122011.
ofFisheriesScienceinNorthAmerica,pp.357–372,2009.
[25] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, “The
[6] G.Williams,T.Maksym,J.Wilkinson,C.Kunz,C.Murphy,P.Kim-
UnreasonableEffectivenessofDeepFeaturesasaPerceptualMetric,”
ball, and H. Singh, “Thick and deformed Antarctic sea ice mapped in 2018 IEEE/CVF Conference on Computer Vision and Pattern
with autonomous underwater vehicles,” Nature Geoscience, vol. 8, Recognition. SaltLakeCity,UT,USA:IEEE,62018,pp.586–595.
no.1,pp.61–67,2015.
[26] X. Wang and E. Grimson, “Spatial Latent Dirichlet Allocation,”
[7] J. W. Kaeli, J. J. Leonard, and H. Singh, “Visual summaries in Proceedings of the 20th International Conference on Neural
for low-bandwidth semantic mapping with autonomous underwater Information Processing Systems. Vancouver, British Columbia,
vehicles,” in 2014 IEEE/OES Autonomous Underwater Vehicles
Canada:CurranAssociatesInc.,2007,pp.1577–1584.
(AUV). IEEE,102014,pp.1–7.
[27] Y. Girdhar, P. Gigue`re, and G. Dudek, “Autonomous adaptive
[8] G. Burroughes and Y. Gao, “Ontology-Based Self-Reconﬁguring
exploration using realtime online spatiotemporal topic modeling,”
Guidance,Navigation,andControlforPlanetaryRovers,”Journalof The International Journal of Robotics Research, vol. 33, no. 4, pp.
AerospaceInformationSystems,vol.13,no.8,pp.316–328,82016.
645–657,42014.
[9] S.Chien,R.Sherwood,D.Tran,B.Cichy,G.Rabideau,R.Castano,
[28] Y.Girdhar,L.Cai,S.Jamieson,N.McGuire,G.Flaspohler,S.Suman,
A. Davis, D. Mandl, S. Frye, B. Trout, S. Shulman, and D. Boyer,
andB.Claus,“StreamingSceneMapsforCo-RoboticExplorationin
“UsingAutonomyFlightSoftwaretoImproveScienceReturnonEarth BandwidthLimitedEnvironments,”in2019InternationalConference
Observing One,” Journal of Aerospace Computing, Information, and on Robotics and Automation (ICRA). Montreal, Canada: IEEE, 5
Communication,vol.2,no.April,pp.196–216,2005.
2019,pp.7940–7946.
[10] T.Estlin,D.Gaines,C.Chouinard,R.Castano,B.Bornstein,M.Judd,
[29] Y. Girdhar, Walter Cho, M. Campbell, J. Pineda, E. Clarke, and
I.Nesnas,andR.Anderson,“IncreasedMarsRoverAutonomyusing
H. Singh, “Anomaly detection in unstructured environments using
AI Planning, Scheduling and Execution,” in Proceedings 2007 IEEE Bayesiannonparametricscenemodeling,”in2016IEEEInternational
International Conference on Robotics and Automation, no. April. Conference on Robotics and Automation (ICRA). Stockholm,
IEEE,42007,pp.4911–4918.
Sweden:IEEE,52016,pp.2651–2656.
[11] R. Castano, T. Estlin, D. Gaines, A. Castano, C. Chouinard,
[30] M.MitzenmacherandE.Upfal,“SampleComplexity,VCDimension,
B. Bornstein, R. Anderson, S. Chien, A. Fukunaga, and M. Judd, RademacherComplexity,”inProbabilityandComputing:Randomiza-
“Opportunistic Rover Science: Finding and Reacting to Rocks, tion and Probabilistic Techniques in Algorithms and Data Analysis,
Clouds and Dust Devils,” in 2006 IEEE Aerospace Conference, vol.
2nded. CambridgeUniversityPress,2017,ch.14,pp.361–391.
2006. IEEE,2006,pp.1–16.
[31] A. Y. Ng and M. I. Jordan, “On Discriminative vs. Generative
[12] G.Hitz,E.Galceran,M.-E`.Garneau,F.Pomerleau,andR.Siegwart,
Classiﬁers:AComparisonofLogisticRegressionandNaiveBayes,”
“Adaptive continuous-space informative path planning for online in Proceedings of the 14th International Conference on Neural In-
environmentalmonitoring,”JournalofFieldRobotics,vol.34,no.8, formationProcessingSystems,Vancouver,BritishColumbia,Canada,
pp.1427–1449,122017.
2001,pp.841–848.
[13] G. Flaspohler, V. Preston, A. P. M. Michel, Y. Girdhar, and
[32] Y. Yang and M. Loog, “A benchmark and comparison of active
N.Roy,“Information-GuidedRoboticMaximumSeek-and-Samplein learning for logistic regression,” Pattern Recognition, vol. 83, pp.
Partially Observable Continuous Environments,” IEEE Robotics and
401–415,2018.
AutomationLetters,vol.4,no.4,pp.3782–3789,102019.
[33] H. Choset and P. Pignon, “Coverage Path Planning: The
[14] Y. Girdhar and G. Dudek, “Modeling curiosity in a mobile robot Boustrophedon Cellular Decomposition,” in Field and Service
for long-term autonomous exploration and monitoring,” Autonomous Robotics, A. Zelinsky, Ed. London: Springer London, 1998, pp.
Robots,vol.40,no.7,pp.1267–1278,102016.
203–209.
[15] A. Arora, R. Fitch, and S. Sukkarieh, “An approach to autonomous
[34] J.E.Smith,R.Brainard,A.Carter,S.Grillo,C.Edwards,J.Harris,
sciencebymodelinggeologicalknowledgeinaBayesianframework,”
L.Lewis,D.Obura,F.Rohwer,E.Sala,P.S.Vroom,andS.Sandin,
in 2017 IEEE/RSJ International Conference on Intelligent Robots
“Re-evaluating the health of coral reef communities: baselines and
andSystems(IROS). IEEE,92017,pp.3803–3810. evidence for human impacts across the central Paciﬁc,” Proceedings
[16] M. F. Balcan, S. Hanneke, and J. W. Vaughan, “The true sample of the Royal Society B: Biological Sciences, vol. 283, no. 1822, 1
complexity of active learning,” Machine Learning, vol. 80, no. 2-3,
2016.
pp.111–139,2010.
[35] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,
[17] C. Daniel, M. Viering, J. Metz, O. Kroemer, and J. Peters, “Active O.Grisel,M.Blondel,P.Prettenhofer,R.Weiss,V.Dubourg,J.Van-
Reward Learning,” in Proceedings of Robotics: Science and Systems derplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and
(RSS). Robotics:ScienceandSystemsFoundation,72014. E. Duchesnay, “Scikit-learn: Machine Learning in Python,” Journal
[18] D. Sadigh, A. Dragan, S. Sastry, and S. Seshia, “Active Preference- ofMachineLearningResearch,vol.12,pp.2825–2830,2011.
Based Learning of Reward Functions,” in Robotics: Science and [36] J. E. San Soucie, H. M. Sosik, and Y. Girdhar, “Gaussian-Dirichlet
SystemsXIII. Robotics:ScienceandSystemsFoundation,72017. Random Fields for Inference over High Dimensional Categorical
[19] F. Doshi-Velez, J. Pineau, and N. Roy, “Reinforcement learning Observations,” in 2020 International Conference on Robotics and
with limited reinforcement: Using Bayes risk for active learning Automation(ICRA). Paris,France:IEEE,52020.
in POMDPs,” Artiﬁcial Intelligence, vol. 187-188, pp. 115–132, 8
2012.
1812
Authorized licensed use limited to: Carleton University. Downloaded on September 22,2020 at 00:58:14 UTC from IEEE Xplore.  Restrictions apply. 