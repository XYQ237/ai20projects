2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
Underwater Image Super-Resolution using Deep Residual Multipliers
Md Jahidul Islam1, Sadman Sakib Enan2, Peigen Luo3, and Junaed Sattar4
640 x 480 640 x 480
Abstract—Wepresentadeepresidualnetwork-basedgenera-
tivemodelforsingleimagesuper-resolution(SISR)ofunderwa-
terimageryforusebyautonomousunderwaterrobots.Wealso
provideanadversarialtrainingpipelineforlearningSISRfrom
Input 
paired data. In order to supervise the training, we formulate 160 x 120
an objective function that evaluates the perceptual quality of
an image based on its global content, color, and local style Generated: SRDRM-GAN Generated: SRDRM
information. Additionally, we present USR-248, a large-scale (a)Zoom-incapability:HRimagegenerationfromLRimagepatches
×
datasetofthreesetsofunderwaterimagesof‘high’(640 480)
× × ×
and‘low’(80 60,160 120,and320 240)resolution.USR-
× ×
248containspairedinstancesforsupervisedtrainingof2 ,4 ,
×
or8 SISRmodels.Furthermore,wevalidatetheeffectiveness
of our proposed model through qualitative and quantitative
Input
experiments and compare the results with several state-of-
160 x 120
the-art models’ performances. We also analyze its practical
feasibility for applications such as scene understanding and
attention modeling in noisy visual conditions.
I. INTRODUCTION Scaled for comparison SRDRM: 4x output G. truth (640 x 480)
(b)RealisticHRimagegeneration:comparisonwiththegroundtruths
Visually-guidedunderwaterrobotsrequireimagesynthesis
andsceneunderstandinginmanyimportantapplicationssuch Fig. 1: Demonstration of underwater image super-resolution using
as the monitoring of marine species and coral reefs [1], our proposed models: SRDRM and SRDRM-GAN.
inspection of submarine cables and wreckage [2], human-
robot collaboration [3], and more. Autonomous Underwater
Vehicles (AUVs) and Remotely Operated Vehicles (ROVs) adopting such models for underwater imagery. First, the un-
are widely used in these applications, where they harness derwater images suffer from a set of unique distortions. For
the synthesized images for visual attention modeling to instance, they tend to have a dominating green or blue hue
make navigation decisions such as ‘where to look or go becausetheredwavelengthsgetabsorbedindeepwater[10].
next’, ‘which snapshots should be recorded’, etc. However, Other factors such as the lighting variations in different
despiteoftenusinghigh-endcameras,underwaterimagesare depths,amountofparticlesinthewater,andscatteringcause
often greatly affected [4] by poor visibility, absorption, and irregular non-linear distortions which result in low-contrast
scattering. Consequently, the objects of interest may appear and blurry images [4]. Consequently, the off-the-shelf SISR
blurred as the images lack important details. This problem models trained on arbitrary images fail to generate realistic
exacerbates when the camera (i.e., robot) cannot get close higher resolution underwater images. Secondly, the lack of
to the objects to get a closer view, e.g., while following large-scale underwater dataset restricts extensive research
a fast-moving target, or surveying distant coral reefs or attemptsforthetrainingandperformanceevaluationofSISR
seabed.FastandaccuratetechniquesforSingleImageSuper- modelsonunderwaterimages.Becauseofthehighcostsand
Resolution (SISR) can alleviate these problems by restoring difﬁculties associated with acquiring real-world underwater
the perceptual and statistical qualities of the low-resolution data, the existing datasets (that were originally proposed for
image patches. training object detection and image enhancement models)
TheexistingliteraturebasedondeepConvolutionalNeural oftencontainsyntheticimages[10]and/ortheirresolutionare
×
Networks (CNNs) provides good solutions for automatic typically limited to 256 256 [4]. Due to these challenges,
SISR [5], [6]. In particular, several Generative Adversar- designing SISR models for underwater imagery and inves-
ial Network (GAN)-based models provide state-of-the-art tigating their applicability in real-world underwater robotic
(SOTA) performance [7], [8] in learning to enhance image applicationshavenotbeenexploredin-depthintheliterature.
resolution from a large collection of paired or unpaired We attempt to address these challenges by designing a
× ×
data [9]. However, there are a few challenges involved in novel SISR model that can learn to generate 2 , 4 , or
×
8 higher resolution (HR) underwater images from the
The authors are with the Interactive Robotics and Vision Laboratory, respective low-resolution (LR) inputs. We also present a
Department of Computer Science and Engineering, Minnesota Robotics
large-scale underwater dataset that provides the three sets
Institute,UniversityofMinnesota-TwinCities,US.
E-mail:{1islam034,2enan0001,3luo00034,4junaed}@umn.edu of LR-HR pairs of images used to train the proposed model.
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 900
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 05:02:15 UTC from IEEE Xplore.  Restrictions apply. In addition, we perform thorough experimental evaluations rathercomplextrainingpipelines,andareoftenpronetopoor
×
of the proposed model and demonstrate its effectiveness performance for large scaling factors (i.e., 4 and higher).
compared to several existing SOTA models. Speciﬁcally, we Thusfar,researchershavebeentryingtoaddresstheseissues
make the following contributions in this paper: byusingLaplacianpyramid-basednetworks(LapSRN)[29],
(a) We present a fully-convolutional deep residual network- dense skip connections (SRDenseNet) [30], deep residual
based generative model for underwater SISR, which networks (RDN) [31], etc.
we refer to as SRDRM. We also formulate an adver- The CNN-based SISR models learn a sequence of non-
sarial training pipeline (i.e., SRDM-GAN) by design- linear ﬁlters from a large number of training images. This
ing a multi-modal objective function that evaluates the end-to-endlearningofLR-HRmappingprovidesigniﬁcantly
perceptual image quality based on its global content, better performance [32] compared to using hand-crafted
color,andlocalstyleinformation.Inourimplementation, ﬁlters, or traditional methods based on bicubic interpola-
SRDRM and SRDM-GAN can learn to generate 640× tion. On the other hand, Generative Adversarial Networks
480 images from respective inputs of size 320×240, (GANs) [33] employ a two-player min-max game where the
80×60,or160×120.Themodelandassociatedtraining ‘generator’triestofoolthe‘discriminator’bygeneratingfake
pipelines are available at https://github.com/ images that appear to be real (i.e., sampled from the HR
xahidbuffon/srdrm. distribution). Simultaneously, the discriminator tries to get
better at discarding fake images and eventually (in equilib-
(b) In addition, we present USR-248, a collection of over
rium) the generator learns the underlying LR-HR mapping.
1.5K samples (i.e., paired HR-LR images) that facilitate
GANs are known to provide SOTA performance for style
large-scale SISR training. It has another 248 test images
transfer [34] and image-to-image translation [35] problems
for benchmark evaluation. These images are rigorously
in general. As for SISR, the GAN-based models can recover
collected during oceanic explorations and ﬁeld experi-
ﬁner texture details [36], [37] while super-resolving at large
ments, and also from a few publicly available online re-
up-scaling factors. For instance, Ledig et al. showed that
sources. We make this available at http://irvlab.
SRGAN [8] can reconstruct high-frequency details for an
cs.umn.edu/resources/usr-248-dataset.
up-scaling factor of 4. Moreover, ESRGAN [7] incorporates
(c) Furthermore, we perform a number of qualitative and
a residual-in-residual dense block that improves the SISR
quantitative experiments that validate that the proposed
performance.Furthermore,DeblurGAN[38]usesconditional
modelcanlearntoenhanceunderwaterimageresolution
GANs [39] that allow constraining the generator to learn
from both traditional and adversarial training. We also
a pixel-to-pixel mapping [35] within the LR-HR domain.
analyze its feasibility and effectiveness for improving
Recently, inspired by the success of CycleGAN [40] and
visual perception in underwater robotic applications; a
DualGAN [41], Yuan et al. [9] proposed a cycle-in-cycle
few sample demonstrations are highlighted in Fig. 1.
GAN-based model that can be trained using unpaired data.
However,suchunpairedtrainingofGAN-basedSISRmodels
II. RELATEDWORK
arepronetoinstabilityandoftenproduceinconsistentresults.
A. Single Image Super-resolution (SISR)
B. SISR for Underwater Imagery
SISR has been studied [11], [12], [13] for nearly two
decadesintheareaofsignalprocessingandcomputervision. SISR techniques for underwater imagery, on the other
SomeoftheclassicalSISRmethodsincludestatisticalmeth- hand, are signiﬁcantly less studied. As mentioned in the
ods [14], [15], [16], patch-based methods [17], [18], [19], previoussection,thisismostlyduetothelackoflarge-scale
sparse representation-based methods [20], random forest- datasetsthatcapturethedistributionoftheuniquedistortions
based method [21], etc. In recent years, with the rapid prevalent in underwater imagery. The existing datasets are
development of deep learning-based techniques, this area of only suitable for underwater object detection [3] and image
researchhasbeenmakingincredibleprogress.Inthepioneer- enhancement [4] tasks, as their image resolution is typically
×
ingwork,Dongetal.[5]proposedathree-layerCNN-based limited to 256 256, and they often contain synthetic
end-to-endmodelnamedSRCNN,thatcanlearnanon-linear images[10].Consequently,theperformanceandapplicability
LR-HRmappingwithoutrequiringanyhand-craftedfeatures. of existing and novel SISR models for underwater imagery
Soon after, Johnson et al. [22] showed that replacing the have not been explored in depth.
per-pixel loss with a perceptual loss (that quantiﬁes image Nevertheless, a few research attempts have been made
quality) gives better results for CNN-based SISR models. for underwater SISR which primarily focus on reconstruct-
On the other hand, Kim et al. proposed deeper networks ing better quality underwater images from their noisy or
such as VDSR [23], DRCN [24] and used contemporary blurred counterparts [42], [43], [44]. Other similar ap-
techniques such as gradient clipping, skip connection, and proaches have used SISR models to enhance underwater
recursive-supervisioninordertoimprovethetrainingfurther. image sequence [45], and to improve ﬁsh recognition per-
Moreover, the sparse coding-based networks [25], residual formance [46]. Although these models perform reasonably
block-based networks (e.g., EDSR [6], DRRN [26]), and well for the respective applications, there is still signiﬁcant
other CNN-based models [27], [28] have been proposed that room for improvement to match the SOTA performance. We
outperformSRCNNforSISR.Thesemethods,however,have attempt to address these aspects in this paper.
901
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 05:02:15 UTC from IEEE Xplore.  Restrictions apply. ×
(a) A few instances sampled from the HR set; the HR images are of size 640 480.
80 x 60 175
150
160 x 120 B)125
K
x 240 Avg. file size (1750500
0 
2 25
3
0
HR image (640 x 480) LR image (scaled for comparison) LR images HR LR-2x LR-4x LR-8x
(b) A particular HR ground truth image and its corresponding LR images are shown. (c) Comparison of ﬁles sizes.
Fig. 2: The proposed USR-248 dataset has one HR set and three corresponding LR sets of images; hence, there are three possible
× × ×
combinations (i.e., 2 , 4 and 8 ) for supervised training of SISR models.
III. USR-248DATASET Coral-reefs 43.75%
The USR-248 dataset contains a large collection of HR Fish 38.98%
underwater images and their respective LR pairs. As men-
Humans 16.34%
tioned earlier, there are three sets of LR images of size
80×60, 160×120, and 320×240; whereas, the HR images Humans+animals 13.89%
×
are of size 640 480. Each set has 1060 RGB images for Humans+robots 1.65%
trainingandvalidation;another248testimagesareprovided Turtles 0.84%
for benchmark evaluation. A few sample images from the
Wrecks/ruins 0.80%
dataset are provided in Fig. 2.
0 10 20 30 40
To prepare the dataset, we collected HR underwater
Fig.3:ModalityintheUSR-248datasetbasedonmajorobjectsof
images: i) during various oceanic explorations and ﬁeld interest in the scene.
experiments,andii)frompubliclyavailableFlickrTM images
andYouTubeTM videos.Theﬁeldexperimentsareperformed
in a number of different locations over a diverse set of IV. SRDRMANDSRDRM-GANMODEL
visibility conditions. Multiple GoPros [47], Aqua AUV’s
A. Deep Residual Multiplier (DRM)
uEyecameras[48],low-lightUSBcameras[49],andTrident
The core element of the proposed model is a fully-
ROV’sHDcamera[50]areusedtocollectHRimagesduring ×
the experiments. We also compiled HR underwater images convolutional deep residual block, designed to learn 2 in-
terpolationintheRGBimagespace.Wedenotethisbuilding
containing natural scenes from FlickrTM, YouTubeTM, and
block as Deep Residual Multiplier (DRM) as it scales the
other online resources1. We avoided multiple instances of
inputfeatures’spatialdimensionsbyafactoroftwo.Asillus-
similar scenes and made sure they contain different objects
of interest (e.g., coral reefs, ﬁsh, divers, wrecks/ruins, etc.) tratedinFigure4a,DRMconsistsofaconvolutional(conv)
inavarietyofbackgrounds.Fig.3showsthemodalityinthe layer, followed by 8 repeated residual layers, then another
data in terms of object categories. Once the HR images are conv layer, and ﬁnally a de-convolutional (i.e., deconv)
× layer for up-scaling. Each of the repeated residual layers
selectedandresizedto640 480,threesetsofLRimagesare
generatedbycompressingandthengraduallydownsizingthe (consistingoftwoconvlayers)isdesignedbyfollowingthe
× × × principlesoutlinedintheEDSRmodel[6].Severalchoicesof
images to 320 240, 160 120, and 80 60; a comparison
hyper-parameters,e.g.,thenumberofﬁltersineachlayer,the
of the average ﬁle sizes for these image sets are shown in
Fig. 2c. Overall, USR-248 provides large-scale paired data use of ReLU non-linearity [51], and/or Batch Normalization
× × × (BN) [52] are annotated in Fig. 4a. As a whole, DRM is a
for training 2 , 4 , and 8 underwater SISR models. It
also includes the respective validation and test sets that are 10 layer residual network that learns to scale up the spatial
dimension of input features by a factor of two. It uses a
used to evaluate our proposed model. ×
series of 2D convolutions of size 3 3 (in repeated residual
×
1Detailedinformationandcreditsfortheonlinemediaresourcescanbe block) and 4 4 (in the rest of the network) to learn this
foundinhttps://arxiv.org/abs/1909.09437(AppendixI). spatial interpolation from paired training data.
902
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 05:02:15 UTC from IEEE Xplore.  Restrictions apply.    8w x 8h x 3 
x f (input) RCeoLnUv,    CoRnevL, UBN ,  CoRnevL, UB N , + CBoNn v ,  + DReeCLoUn v ,  x 256 (output) w Ixn phu xt  3   BDlRocMk  2 w3  x 2 h x 3  BDlRocMk   43w  x 4h x 3   BDlRocMk   3 
h  h   Conv   Conv   Conv 
w x  64 64 64 8x 64 256 w x 2 Tanh  Tanh  Tanh 
2
(a)Architectureofadeepresidualmultiplier(DRM)block (b) Generator: SRDRM model with multiple DRM blocks
Input: 640 x 480 x 6 
320 x 240 320 x 240  Conv, Leaky-ReLU, BN  
160 x 120  160 x 120
Output:
80 x 60  80 x 60  40 x 30 40 x 30 40 x 30 x 1 
64 128 128 256 256 512 512 1024
×
(c) Discriminator: a Markovian PatchGAN [35] with nine layers and a patch-size of 40 30
Fig. 4: Network architecture of the proposed model.
B. SRDRM Architecture 2) Image content loss: being inspired by the success of
As Fig. 4b demonstrates, the SRDRM makes use of n ∈ existing SISR models [3(cid:2)2(cid:12)(cid:12)](cid:12)(cid:12), we also formulat(cid:12)(cid:12)e(cid:12)(cid:12) t(cid:3)he content
{1,2,3} DRM blocks in order to learn to generate 2n× loss as:
HR outputs. An additional conv layer with tanh non- L E −
(G)= Φ(Y) Φ(G(X)) .
linearity [53] is added after the ﬁnal DRM block in order to C X,Y· 2
reshapetheoutputfeaturestothedesiredshape.Speciﬁcally, Here, the function Φ() denotes the high-level features
× × extracted by the block5 conv4 layer of a pre-trained
it generates a 2nw 2nh 3 output for an input of size
× × VGG-19 network.
w h 3.
Finally, we formulate the multi-modal objective function
C. SRDRM-GAN Architecture L L L
for the generator as: G(G) = λ (G) + λ (G) +
L c C p P
For adversarial training, we use the same SRDRM model λ (G). Here, λ , λ , and λ are scalars that are empiri-
2 2 c p 2
as the generator and employ a Markovian PatchGAN [35]- cally tuned as hyper-parameters. Therefore, the generator G
based model for the discriminator. As illustrated by Fig. 4c, needs to solve the following minimization problem:
× ×
nine conv layers are used to transform a 640 480 6 ∗ L
× × G =argmin G(G). (1)
input (real and generated image) to a 40 30 1 output
G
that represents the averaged validity responses of the dis- On the other hand, adversarial training requires a two-
×
criminator.Ateachlayer,3 3convolutionalﬁltersareused player min-max(cid:2)game [3(cid:3)3] betwe(cid:2)en the generator G(cid:3) and
with a stride size of 2, followed by a Leaky-ReLU non- discriminator D, which is expressed as:
linearity[54]andBN.AlthoughtraditionallyPatchGANsuse
70×70 patches [35], [41], we use a patch-size of 40×30 L(G,D)=EX,Y logD(Y) +EX,Y log(1−D(X,G(X))) . (2)
L
as our input/output image-shapes are of 4:3. Here,thegeneratortriestominimize (G,D)whilethedis-
criminator tries to maximize it. Therefore, the optimization
D. Objective Function Formulation
problem for adversarial training becomes:
Atﬁrst,wedeﬁnetheSISRproblemaslearningafunction
{ } → ∗ L L
or mapping G : X Y, where X (Y) represents the G =argminmax GAN(G,D)+ G(G). (3)
LR (HR) image domain. Then, we formulate an objective G D
E. Implementation
function that evaluates the following properties of G(X)
compared to Y: We use TensorFlow libraries [56] to implement the pro-
1) Global similarity and perceptual loss: existing meth- posed SRDRM and SRDRM-GAN models. We trained both
ods have shown that adding an L (L ) loss to the objective the models on the USR-248 dataset up to 20 epochs with
1 2
function enables the generator to learn to sample from a a batch-size of 4, using two NVIDIATM GeForce GTX
(cid:2)(cid:12)(cid:12) (cid:12)(cid:12) (cid:3)
globally similar(cid:12)(cid:12)space in an(cid:12)(cid:12)L (L ) sense [35]. In our 1080 graphics cards. We also implement a number of SOTA
1 2
implementation, we measure the global similarity loss as: generative and adversarial models for performance com-
L (G)=E Y −G(X) . Additionally, as suggested parison in the same setup. Speciﬁcally, we consider three
in2[55], weXd,Yeﬁne(cid:2)(cid:12)(cid:12)a perceptua2l loss function based o(cid:12)n(cid:12) (cid:3)the generative models named SRCNN [5], SRResNet [8], [32],
(cid:12)(cid:12) (cid:12)(cid:12)
per-channel disparity between G(X) and Y as: and DSRCNN [57], and three adversarial models named
L E − SRGAN [8], ESRGAN [7], and EDSRGAN [6]. We already
(G)= (512+¯r)r2+4g2+(767 ¯r)b2 .
P X,Y 2 provided a brief discussion on the SOTA SISR models in
Here,r,g,andbdenotethenormalizednumericdifferences Section II. Next, we present the experimental results based
of the red, green, and blue channels between G(X) and Y, on qualitative analysis and quantitative evaluations in terms
respectively; whereas ¯r is the mean of their red channels. of standard metrics.
903
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 05:02:15 UTC from IEEE Xplore.  Restrictions apply. TABLEI:ComparisonofaveragePSNR,SSIM,andUIQMscores
V. EXPERIMENTALRESULTS × × ×
for 2 /4 /8 SISR(cid:0)on USR(cid:1)-248 test(cid:0)set. (cid:1) (cid:0) (cid:1)
A. Qualitative Evaluations
PSNR SSIM UIQM
Model G(x),y G(x),y G(x)
At ﬁrst, we analyze the sharpness and color consistency
SRResNet 25.98/24.15/19.26 0.72/0.66/0.55 2.68/2.23/1.95
in the generated images of SRDRM and SRDRM-GAN. SRCNN 26.81/23.38/19.97 0.76/0.67/0.57 2.74/2.38/2.01
As Fig. 5 suggests, both models generate images that are DSRCNN 27.14/23.61/20.14 0.77/0.67/0.56 2.71/2.36/2.04
× SRDRM 28.36/24.64/21.20 0.80/0.68/0.60 2.78/2.46/2.18
comparable to the grou×nd truths for 4 SISR. We observe SRDRM-GAN 28.55/24.62/20.25 0.81/0.69/0.61 2.77/2.48/2.17
evenbetterresultsfor2 SISR,asitisarelativelylesschal- ESRGAN 26.66/23.79/19.75 0.75/0.66/0.58 2.70/2.38/2.05
lenging problem. We demonstrate this relative performance EDSRGAN 27.12/21.65/19.87 0.77/0.65/0.58 2.67/2.40/2.12
SRGAN 28.05/24.76/20.14 0.78/0.69/0.60 2.74/2.42/2.10
margins at various scales in Fig. 6. This comparison shows
thattheglobalcontrastandtextureismostlyrecoveredinthe
× ×
2 and4 HRimagesgeneratedbySRDRMandSRDRM-
× M
GAN. On the other hand, the 8 HR images miss the DR
R
ﬁner details and lack the sharpness in high-texture regions. S
The state-of-the-art SISR models have also reported such
×
difﬁculties beyond the 4 scale [32]. N
A
Next, in Fig. 7, we provide a qualitative perf×ormance M-G
comparison with the state-of-the-art models for 4 SISR. DR
× R
We select multiple 160 120 patches on the test images S
2X 4X 8X
containing interesting textures and objects in contrasting
Fig. 6: Global contrast and texture recovery by SRDRM and
background. Then, we apply all the SISR models (trained × × ×
× SRDRM-GAN for 2 , 4 , and 8 SISR.
on 4 USR-248 data) to generate respective HR images of
×
size 640 480. In the evaluation, we observe that SRDRM
performsatleastaswellasandoftenbettercomparedtothe structure.Inaddition,weconsiderUnderwaterImageQuality
generative models, i.e., SRResNet, SRCNN, and DSRCNN. Measure (UIQM) [60], which quantiﬁes underwater image
Moreover, SRResNet and SRGAN are prone to inconsistent colorfulness, sharpness, and contrast. We evaluate all the
coloring and over-saturation in bright regions. On the other SISR models on USR-248 test images, and compare their
hand, ESRGAN and EDSRGAN often fail to restore the performance in Table I. The results indicate that SRDRM-
sharpness and global contrast. Furthermore, SRDRM-GAN GAN, SRDRM, SRGAN, and SRResNet produce compara-
generates sharper images and does a better texture recovery ble values for PSNR and SSIM, and perform better than
than SRDRM (and other generative models) in general. We other models. SRDRM and SRDRM-GAN also produce
postulatethatthePatchGAN-baseddiscriminatorcontributes higherUIQMscoresthanothermodelsincomparison.These
tothis,asitforcesthegeneratortolearnhigh-frequencylocal statistics are consistent with our qualitative analysis.
texture and style information [35].
C. Practical Feasibility
B. Quantitative Evaluation
The qualitative and quantitative results suggest that SR-
We consider two standard metrics [58], [4] named Peak DRM and SRDRM-GAN provide good quality HR visual-
Signal-to-Noise Ratio (PSNR) and Structural Similarity izationsfor LRimage patches,which ispotentiallyuseful in
(SSIM) in order to quantitatively compare the SISR models’ trackingfast-movingtargets,attentionmodeling,anddetailed
performances. The PSNR approximates the reconstruction understanding of underwater scenes. Therefore, AUVs and
quality of generated images compared to their respective ROVs can use this to zoom in a particular region of interest
ground truth, whereas the SSIM [59] compares the image (RoI) for detailed and improved visual perception. One
patches based on three properties: luminance, contrast, and operationalconsiderationforusingsuchdeeplearning-based
models in embedded robotic platforms is the computational
SRDRM SRDRM-GAN G. T. (HR)   |    Input (LR)  complexity. As we demonstrate in Table II, the memory
requirement for the proposed model is only 3.5-12 MB and
it runs at 4-7 fps on NVIDIATM Jetson TX2. Therefore, it
essentially takes about 140-246 milliseconds for a robot to
take a closer look at a LR RoI. These results validate the
feasibility of using the proposed model for improving real-
time perception of visually-guided underwater robots.
VI. CONCLUSION
In this paper, we present a fully-convolutional deep
residual network-based model for underwater image super-
× × × ×
Fig. 5: Color consistency and sharpness of the generated 4 HR resolution at 2 , 4 , and 8 scales. We also provide
images compared to the respective ground truth. generative and adversarial training pipelines driven by a
904
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 05:02:15 UTC from IEEE Xplore.  Restrictions apply. SRResNet SRCNN DSRCNN SRDRM SRDRM-GAN ESRGAN EDSRGAN SRGAN
SRResNet SRCNN DSRCNN SRDRM SRDRM-GAN ESRGAN EDSRGAN SRGAN
SRResNet SRCNN DSRCNN SRDRM SRDRM-GAN ESRGAN EDSRGAN SRGAN
Fig. 7: Qualitative performance comparison of SRDRM and SRDRM-GAN with SRCNN [5], SRResNet [8], [32], DSRCNN [57],
SRGAN [8], ESRGAN [7], and EDSRGAN [6]. (Best viewed at 400% zoom)
TABLE II: Run-time and memory requirement of SRDRM (same
[6] B. Lim, S. Son, H. Kim, S. Nah, and K. Mu Lee, “Enhanced Deep
as SRDRM-GAN) on NVIDIATM Jetson TX2 (optimized graph).
Residual Networks for Single Image Super-resolution,” in Proc. of
Model 2× 4× 8× the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)workshops,2017,pp.136–144.
Inference-time(ms) 140.6ms 145.7ms 245.7ms [7] X. Wang, K. Yu, S. Wu, J. Gu, Y. Liu, C. Dong, Y. Qiao, and
Framespersecond(fps) 7.11fps 6.86fps 4.07fps C. Change Loy, “Esrgan: Enhanced Super-resolution Generative Ad-
Model-size 3.5MB 8MB 12MB versarialNetworks,”inProc.oftheEuropeanConferenceonComputer
Vision(ECCV),2018,pp.0–0.
multi-modal objective function, which is designed to eval- [8] C.Ledig,L.Theis,F.Husza´r,J.Caballero,A.Cunningham,A.Acosta,
A.Aitken, A.Tejani,J.Totz, Z.Wangetal.,“Photo-realistic Single
uate image quality based on its content, color, and texture
Image Super-resolution using a Generative Adversarial Network,”
information. In addition, we present a large-scale dataset in Proc. of the IEEE Conference on Computer Vision and Pattern
namedUSR-248whichcontainspairedunderwaterimagesof Recognition(CVPR),2017,pp.4681–4690.
[9] Y. Yuan, S. Liu, J. Zhang, Y. Zhang, C. Dong, and L. Lin, “Un-
various resolutions for supervised training of SISR models.
supervised Image Super-resolution using Cycle-in-cycle Generative
Furthermore, we perform thorough qualitative and quantita- AdversarialNetworks,”inProc.oftheIEEEConferenceonComputer
tive evaluations which suggest that the proposed model can Vision and Pattern Recognition (CVPR) Workshops, 2018, pp. 701–
710.
learn to restore image qualities at a higher resolution for an
[10] C. Fabbri, M. J. Islam, and J. Sattar, “Enhancing Underwater Im-
improvedvisualperception.Inthefuture,weseektoimprove ageryusingGenerativeAdversarialNetworks,”inIEEEInternational
×
its performance for 8 SISR, and plan to further investigate Conference on Robotics and Automation (ICRA). IEEE, 2018, pp.
7159–7165.
its applicability in other underwater robotic applications.
[11] W.T.Freeman,T.R.Jones,andE.C.Pasztor,“Example-basedSuper-
resolution,” IEEE Computer Graphics and Applications, no. 2, pp.
REFERENCES
56–65,2002.
[1] O. Hoegh-Guldberg, P. J. Mumby, A. J. Hooten, R. S. Steneck, [12] H. Chang, D.-Y. Yeung, and Y. Xiong, “Super-resolution Through
P. Greenﬁeld, E. Gomez et al., “Coral Reefs under Rapid Climate NeighborEmbedding,”inProc.oftheIEEEConferenceonComputer
Change and Ocean Acidiﬁcation,” Science, vol. 318, no. 5857, pp. VisionandPatternRecognition(CVPR),vol.1. IEEE,2004,pp.I–I.
1737–1742,2007. [13] D. O. Melville and R. J. Blaikie, “Super-resolution Imaging through
[2] B.Bingham,B.Foley,H.Singh,R.Camilli,K.Delaporta,R.Eustice aPlanarSilverLayer,”OpticsExpress,vol.13,no.6,pp.2127–2134,
et al., “Robotic Tools for Deep Water Archaeology: Surveying an 2005.
AncientShipwreckwithanAutonomousUnderwaterVehicle,”Journal [14] J. Sun, Z. Xu, and H.-Y. Shum, “Image Super-resolution using
ofFieldRobotics(JFR),vol.27,no.6,pp.702–717,2010. GradientProﬁlePrior,”inIEEEConferenceonComputerVisionand
[3] M. J. Islam, M. Ho, and J. Sattar, “Understanding Human Motion PatternRecognition(CVPR). IEEE,2008,pp.1–8.
and Gestures for Underwater Human-Robot Collaboration,” Journal [15] K.I.KimandY.Kwon,“Single-imageSuper-resolutionusingSparse
ofFieldRobotics(JFR),pp.1–23,2018. Regression and Natural Image Prior,” IEEE Transactions on Pattern
[4] M. J. Islam, Y. Xia, and J. Sattar, “Fast Underwater Image Analysis and Machine Intelligence, vol. 32, no. 6, pp. 1127–1133,
Enhancement for Improved Visual Perception,” arXiv preprint 2010.
arXiv:1903.09766,2019. [16] M. Protter, M. Elad, H. Takeda, and P. Milanfar, “Generalizing the
[5] C. Dong, C. C. Loy, K. He, and X. Tang, “Image Super-resolution Nonlocal-means to Super-resolution Reconstruction,” IEEE Transac-
using Deep Convolutional Networks,” IEEE Transactions on Pattern tionsonImageProcessing,vol.18,no.1,pp.36–51,2008.
AnalysisandMachineIntelligence,vol.38,no.2,pp.295–307,2015. [17] D. Glasner, S. Bagon, and M. Irani, “Super-resolution from a Sin-
905
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 05:02:15 UTC from IEEE Xplore.  Restrictions apply. gle Image,” in IEEE International Conference on Computer Vision work,”inInternationalConferenceonMedicalImageComputingand
(ICCV). IEEE,2009,pp.349–356. Computer-AssistedIntervention. Springer,2018,pp.91–99.
[18] J. Yang, Z. Wang, Z. Lin, S. Cohen, and T. Huang, “Coupled [38] O. Kupyn, V. Budzan, M. Mykhailych, D. Mishkin, and J. Matas,
Dictionary Training for Image Super-resolution,” IEEE Transactions “Deblurgan: Blind Motion Deblurring using Conditional Adversarial
onImageProcessing,vol.21,no.8,pp.3467–3478,2012. Networks,”inProc.oftheIEEEConferenceonComputerVisionand
[19] J.-B.Huang,A.Singh,andN.Ahuja,“SingleImageSuper-resolution PatternRecognition(CVPR),2018,pp.8183–8192.
from Transformed Self-exemplars,” in Proc. of the IEEE Conference [39] M.MirzaandS.Osindero,“ConditionalGenerativeAdversarialNets,”
onComputerVisionandPatternRecognition(CVPR),2015,pp.5197– arXivpreprintarXiv:1411.1784,2014.
5206. [40] J.-Y.Zhu,T.Park,P.Isola,andA.A.Efros,“UnpairedImage-to-image
[20] J.Yang,J.Wright,T.S.Huang,andY.Ma,“ImageSuper-resolution TranslationusingCycle-consistentAdversarialNetworks,”inProc.of
viaSparseRepresentation,”IEEETransactionsonImageProcessing, theIEEEInternationalConferenceonComputerVision(ICCV),2017,
vol.19,no.11,pp.2861–2873,2010. pp.2223–2232.
[21] S. Schulter, C. Leistner, and H. Bischof, “Fast and Accurate Image [41] Z. Yi, H. Zhang, P. Tan, and M. Gong, “DualGAN: Unsupervised
UpscalingwithSuper-resolutionForests,”inProc.oftheIEEECon- DualLearningforImage-to-imageTranslation,”inProc.oftheIEEE
ference on Computer Vision and Pattern Recognition (CVPR), 2015, InternationalConferenceonComputerVision(ICCV),2017,pp.2849–
pp.3791–3799. 2857.
[42] Y. Chen, B. Yang, M. Xia, W. Li, K. Yang, and X. Zhang, “Model-
[22] J. Johnson, A. Alahi, and L. Fei-Fei, “Perceptual Losses for Real-
based Super-resolution Reconstruction Techniques for Underwater
time Style Transfer and Super-resolution,” in European Conference
Imaging,”inPhotonicsandOptoelectronicsMeetings(POEM):Opto-
onComputerVision(ECCV). Springer,2016,pp.694–711.
electronicSensingandImaging,vol.8332. InternationalSocietyfor
[23] J. Kim, J. Kwon Lee, and K. Mu Lee, “Accurate Image Super-
OpticsandPhotonics,2012,p.83320G.
resolution using Very Deep Convolutional Networks,” in Proc. of
[43] F.Fan,K.Yang,B.Fu,M.Xia,andW.Zhang,“ApplicationofBlind
the IEEE Conference on Computer Vision and Pattern Recognition
Deconvolution Approach with Image Quality Metric in Underwater
(CVPR),2016,pp.1646–1654.
Image Restoration,” in International Conference on Image Analysis
[24] ——, “Deeply-recursive Convolutional Network for Image Super-
andSignalProcessing. IEEE,2010,pp.236–239.
resolution,”inProc.oftheIEEEConferenceonComputerVisionand
[44] Y. Yu and F. Liu, “System of Remote-operated-vehicle-based Un-
PatternRecognition(CVPR),2016,pp.1637–1645.
derwater Blurred Image Restoration,” Optical Engineering, vol. 46,
[25] D.Liu,Z.Wang,B.Wen,J.Yang,W.Han,andT.S.Huang,“Robust no.11,p.116002,2007.
SingleImageSuper-resolutionviaDeepNetworkswithSparsePrior,” [45] E.Quevedo,E.Delory,G.Callico´,F.Tobajas,andR.Sarmiento,“Un-
IEEE Transactions on Image Processing, vol. 25, no. 7, pp. 3194– derwater Video Enhancement using Multi-camera Super-resolution,”
3207,2016. OpticsCommunications,vol.404,pp.94–102,2017.
[26] Y. Tai, J. Yang, and X. Liu, “Image Super-resolution via Deep [46] X. Sun, J. Shi, J. Dong, and X. Wang, “Fish Recognition from
Recursive Residual Network,” in Proc. of the IEEE Conference on Low-resolutionUnderwaterImages,”inInternationalCongressonIm-
Computer Vision and Pattern Recognition (CVPR), 2017, pp. 3147– ageandSignalProcessing,BioMedicalEngineeringandInformatics
3155. (CISP-BMEI). IEEE,2016,pp.471–476.
[27] W. Shi, J. Caballero, F. Husza´r, J. Totz, A. P. Aitken, R. Bishop, [47] GoPro, “GoPro Hero 5,” https://gopro.com/, 2016, accessed: 8-15-
D. Rueckert, and Z. Wang, “Real-time Single Image and Video 2019.
Super-resolution using an Efﬁcient Sub-pixel Convolutional Neural [48] G. Dudek, P. Giguere, C. Prahacs, S. Saunderson, J. Sattar, L.-A.
Network,”inProc.oftheIEEEConferenceonComputerVisionand Torres-Mendez, Jenkin et al., “Aqua: An Amphibious Autonomous
PatternRecognition(CVPR),2016,pp.1874–1883. Robot,”Computer,vol.40,no.1,pp.46–53,2007.
[28] C.Dong,C.C.Loy,andX.Tang,“AcceleratingtheSuper-resolution [49] BlueRobotics, “Low-light HD USB Camera,” https://www.
ConvolutionalNeuralNetwork,”inEuropeanConferenceonComputer bluerobotics.com/,2016,accessed:3-15-2019.
Vision(ECCV). Springer,2016,pp.391–407. [50] OpenROV, “TRIDENT,” https://www.openrov.com/, 2017, accessed:
[29] W.-S.Lai,J.-B.Huang,N.Ahuja,andM.-H.Yang,“DeepLaplacian 8-15-2019.
Pyramid Networks for Fast and Accurate Super-resolution,” in Proc. [51] V.NairandG.E.Hinton,“RectiﬁedLinearUnitsImproveRestricted
oftheIEEEConferenceonComputerVisionandPatternRecognition Boltzmann Machines,” in Proc. of the International Conference on
(CVPR),2017,pp.624–632. MachineLearning(ICML),2010,pp.807–814.
[30] T. Tong, G. Li, X. Liu, and Q. Gao, “Image Super-resolution using [52] S. Ioffe and C. Szegedy, “Batch Normalization: Accelerating Deep
DenseSkipConnections,”inProc.oftheIEEEInternationalConfer- Network Training by Reducing Internal Covariate Shift,” CoRR,
enceonComputerVision(ICCV),2017,pp.4799–4807. abs/1502.03167,2015.
[31] Y. Zhang, Y. Tian, Y. Kong, B. Zhong, and Y. Fu, “Residual Dense [53] T.Raiko,H.Valpola,andY.LeCun,“DeepLearningMadeEasierby
NetworkforImageSuper-resolution,”inProc.oftheIEEEConference LinearTransformationsinPerceptrons,”inArtiﬁcialIntelligenceand
onComputerVisionandPatternRecognition(CVPR),2018,pp.2472– Statistics,2012,pp.924–932.
2481. [54] A.L.Maas,A.Y.Hannun,andA.Y.Ng,“RectiﬁerNonlinearitiesIm-
[32] W.Yang,X.Zhang,Y.Tian,W.Wang,J.-H.Xue,andQ.Liao,“Deep proveNeuralNetworkAcousticModels,”inInternationalConference
learning for Single Image Super-resolution: A Brief Review,” IEEE onMachineLearning(ICML),vol.30,no.1,2013,p.3.
TransactionsonMultimedia,2019. [55] CompuPhase, “Perceptual Color Metric,” https://www.compuphase.
[33] I.Goodfellow,J.Pouget-Abadie,M.Mirza,B.Xu,D.Warde-Farley, com/cmetric.htm,2019,accessed:12-12-2019.
S.Ozair,A.Courville,andY.Bengio,“GenerativeAdversarialNets,” [56] M.Abadi,P.Barham,J.Chen,Z.Chen,A.Davis,J.Deanetal.,“Ten-
inAdvancesinNeuralInformationProcessingSystems(NIPS),2014, sorFlow: A System for Large-scale Machine Learning,” in USENIX
pp.2672–2680. SymposiumonOperatingSystemsDesignandImplementation(OSDI),
2016,pp.265–283.
[34] L.A.Gatys,A.S.Ecker,andM.Bethge,“ImageStyleTransferusing
[57] X.-J.Mao,C.Shen,andY.-B.Yang,“ImageRestorationusingCon-
ConvolutionalNeuralNetworks,”inProc.oftheIEEEConferenceon
volutional Auto-encoders with Symmetric Skip Connections,” arXiv
Computer Vision and Pattern Recognition (CVPR), 2016, pp. 2414–
preprintarXiv:1606.08921,2016.
2423.
[58] A. Hore and D. Ziou, “Image Quality Metrics: PSNR vs. SSIM,” in
[35] P.Isola,J.-Y.Zhu,T.Zhou,andA.A.Efros,“Image-to-imageTrans-
International Conference on Pattern Recognition. IEEE, 2010, pp.
lation with Conditional Adversarial Networks,” in Proc. of the IEEE
2366–2369.
Conference on Computer Vision and Pattern Recognition (CVPR),
[59] Z.Wang,A.C.Bovik,H.R.Sheikh,E.P.Simoncellietal.,“Image
2017,pp.1125–1134.
Quality Assessment: from Error Visibility to Structural Similarity,”
[36] C. K. Sønderby, J. Caballero, L. Theis, W. Shi, and F. Husza´r,
IEEETransactionsonImageProcessing,vol.13,no.4,pp.600–612,
“AmortisedMapInferenceforImageSuper-resolution,”arXivpreprint
2004.
arXiv:1610.04490,2016.
[60] K. Panetta, C. Gao, and S. Agaian, “Human-visual-system-inspired
[37] Y. Chen, F. Shi, A. G. Christodoulou, Y. Xie, Z. Zhou, and D. Li, Underwater Image Quality Measures,” IEEE Journal of Oceanic
“Efﬁcient and Accurate MRI Super-resolution using a Generative Engineering,vol.41,no.3,pp.541–551,2016.
Adversarial Network and 3D Multi-level Densely Connected Net-
906
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 05:02:15 UTC from IEEE Xplore.  Restrictions apply. 