2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
Gradient and Log-based Active Learning
for Semantic Segmentation of Crop and Weed for Agricultural Robots
Rasha Sheikh Andres Milioto Philipp Lottes Cyrill Stachniss Maren Bennewitz Thomas Schultz
Abstract(cid:151)Annotated datasets are essential for supervised
learning. However, annotating large datasets is a tedious and
time-intensive task. This paper addresses active learning in the
context of semantic segmentation with the goal of reducing the
human labeling effort. Our application is agricultural robotics
andwefocusonthetaskofdistinguishingbetweencropandweed
plantsfromimagedata.Akeychallengeinthisapplicationisthe
transferofanexistingsemanticsegmentationCNNtoanew(cid:2)eld,
inwhichgrowthstage,weeds,soil,andweatherconditionsdiffer.
Weproposeanovelapproachthat,givenatrainedmodelonone
(cid:2)eld together with rough foreground segmentation, re(cid:2)nes the
network on a substantially different (cid:2)eld providing an effective
method of selecting samples to annotate for supporting the Fig.1. SampleimagesfromtheBonn,Stuttgart,andZurichsugarbeet
transfer.Weevaluatedourapproachontwochallengingdatasets datasetsinthe(cid:2)rst,second,andthirdcolumn,respectively.The(cid:2)rstrow
fromtheagriculturalroboticsdomainandshowthatweachieve showstheRGBimagesandthesecondrowshowstheirannotations(green
a higher accuracy with a smaller number of samples compared denotescropwhilereddenotesweed).Ascanbeseen,theappearancediffers
to random sampling as well as entropy based sampling, which substantially.
consequently reduces the required human labeling effort.
need to be executed at the end-users site, one is interested in
I. INTRODUCTION keeping this effort as low as possible. Given annotated data
The ability to interpret the scene in front of a robot is key on one agricultural (cid:2)eld and a CNN that was trained on it,
for intelligent behavior in several applications. For example, weaddresstheproblemoftransferringthisknowledgetonew
precision farming robots need to know which type of plant (cid:2)elds with minimum effort. Datasets from different (cid:2)elds
they perceive or autonomous cars need to know which object reveal different crop and weed statistics. They often differ
in theirsurroundings is acar,a pedestrian, ora cyclist.These by soil type, weather condition, or various small objects that
classi(cid:2)cation or semantic segmentation tasks are typically can be found on the ground, such as stones, dried vegetation,
tackledusingconvolutionalneuralnetworks(CNNs)operating or marks from agricultural machines, i.e., patterns that are
on image data. In order to perform well, neural networks neither crop nor weed. Additionally, the robot can acquire
need to be trained with appropriately annotated datasets. images of plants at a certain growth stage in one (cid:2)eld, while
The performance of most supervised learning approaches thegrowthstateonthetarget(cid:2)eldisdifferent.Lastly,artifacts
and especially deep learning systems is related to the such as contrast changes can be found in the camera images
quality and quantity of training data. Annotated training data, captured from the various locations. As illustrated by Lottes
however, has a high cost as often a larger number of labeled et al. [19], [20], these conditions make it dif(cid:2)cult to simply
trainingdataisrequired.Inthiswork,wefocusonoptimizing reuse a previously trained network from one (cid:2)eld and infer
thetrainingsetgenerationforsemanticsegmentationofimage the labels on another.
data obtained from a mobile robot. Semantic segmentation The contribution of this work is to introduce and compare
refers to the task of computing a pixel-wise labeling of the three active learning strategies that intelligently pick images
images. More concretely, we address the agricultural robotics taken under new conditions to re-train an existing network:
application in which robots should perform automated weed The (cid:2)rst one picks samples based on a log-space ranking
control. For the semantic segmentation, this means that we of their loss with respect to pseudo labels. The second and
need to compute the semantic label (cid:147)crop(cid:148), (cid:147)weed(cid:148), or third approaches select training samples that are expected
(cid:147)misc(cid:148) for each pixel in the image. This task is particularly to have a maximum effect on the network weights. Even
challenging as the (cid:2)eld conditions often change substantially thoughsimilarideashavebeenexploredforactivelearningin
between years, regions, weather, and soil conditions as can other application contexts, it is non trivial to apply them for
be seen in Figure 1. semantic segmentation. An important technical novelty in our
One solution to adapt and re(cid:2)ne existing semantic segmen- work is to exploit a pseudo ground truth, which we obtain
tation systems to new (cid:2)eld conditions is through additional with very weakly supervised segmentation. Our approach
labeled data from the new (cid:2)eld. As these new annotations selects samples in batches, each time re(cid:2)ning the network,
then computing a new ranking of the unlabeled data. The
All authors are with the University of Bonn, Germany. This work has
best samples are then selected and the network is re-trained.
partlybeensupportedbytheGermanResearchFoundationunderGermany’s
ExcellenceStrategy,EXC-2070-390732324(PhenoRob). To compute the real gradients, corresponding ground truth
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 1350
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 07:05:33 UTC from IEEE Xplore.  Restrictions apply. data is needed. Thus, in our approach, we approximate the
very weakly supervised segmentation
ground truth as the result of unsupervised segmentation to
estimate the gradient. We evaluated our framework using
three distinctive sugar beet datasets [5] that have different
characteristics. Our results indicate that our method produces
a higher accuracy on the datasets with a fewer number of
samples compared to random sampling for annotation as well encoder-decoder network
as entropy based sampling.
reﬁne network
II. RELATEDWORK
annotation
Several works focusing on the elimination or reduction
of herbicide use, through the incorporation of autonomous
ranking
ground robots in crop (cid:2)elds, have been introduced to the
community in the last years [7], [16], [21]. A key component Fig.2. Overviewofourapproach.Thekeyideaisthatwe(cid:2)rstperforma
veryweaklysupervisedsegmentationtoobtainpseudogroundtruth.Given
of each of these unmanned platforms is a core perception
thelabelsanddifferentrankingmeasuresobtainedfromthenetwork,we
systemthathastheabilitytoaccuratelydistinguishcropsfrom ranktheunlabeledsamplesandpickthemaccordinglyforannotation.Those
weedsinordertoeffectivelyandselectivelyapplythedesired samplesarethenusedtore(cid:2)netheentirenetwork.
individualtreatment[18],[22],[23],[24],[27].Thesesystems
by the model are then used as the target labels for the next
allow autonomous robots to perform actuation in the (cid:2)elds
iteration of the process.
without human supervision, treating each plant individually.
The works mentioned previously and the current state-
Alloftheworksreferenced,however,arebasedonsupervised
of-the-art methods for active learning including [10], [3],
learning approaches which take large amounts of pixel-
[28], [36] are either more suitable for tasks other than pixel-
accurate hand-labeled images for training. Accordingly, one
wise semantic segmentation of images with CNNs and/or
of the main bottlenecks of these visual processing pipelines
are memory and computationally expensive. Differently,
is the amount of expensive labeled training data required to
we experiment with approaches that directly measure how
deploytheminrealagricultural(cid:2)elds,whichoftenlimitstheir
annotated samples can affect the gradients. We use labels
applicability. In order to tackle this data starvation problem,
obtained with very weak supervision as pseudo ground truth
we propose an active learning based solution.
and compute the gradients w.r.t the weights. We then re(cid:2)ne a
Numerous works on general active learning have been
pre-trained network with the newly annotated samples in an
presented in the community [30], [11], [12], [36]. The
iterative manner. Our intuition for using gradients is driven
most common measures for selecting samples are based
by the observation that the greater the mismatch is between
on the uncertainty of the network [38], [35], [10], [33]
the predicted segmentation and the ground truth, the larger
and diversity [38], [8], [14]. Sener et al. [29] assert based
the change is to the weights. This is in contrast to most of
on the experiments they performed that uncertainty based
the approaches mentioned earlier that rely on the con(cid:2)dence
approaches are not effective for active learning with CNNs.
of the network which may not be the best indication of the
Theyhypothesizethatthisisnotduetotheinaccurateestimate
best samples to choose for annotation, as the network output
of uncertainty by the network, rather to the ineffectiveness
might actually be correct although the network is uncertain
of uncertainty based approaches to cover the space of image
about it.
features. The Expected Model Output Change Principle
Previous work, such as the Expected Gradient Length
(EMOC) developed by Freytag et al. [9] tries to avoid
(EGL) [13], [31], has explored how changes in model
selecting samples that are redundant and Ka¤ding et al. [14]
parameters can be exploited for sample selection. However,
followthisapproachwithdeepneuralnetworks.Thisprinciple
it computes the expectation of the gradient norm over all
measures how a model would perform with and without
possible annotations, which would be prohibitively expensive
the candidate sample. Given that the labels are unknown, a
for pixel-wise semantic segmentation of images. We instead
marginalizationoverthepossiblelabelsisneeded.Uncertainty
compute gradients from rough foreground/background seg-
estimation for active learning can be performed using Monte-
mentation. Du et al. [6] use gradient similarity to determine
Carlodropoutasin[10]orwithanensembleofdeepnetworks.
when an auxiliary task is helpful for transfer learning to the
Beluch et al. [3] compare both of these approaches on
main task and when it can be hurtful. Although in our work,
different datasets. They found that an ensemble of deep
theweaklysupervisedsettingcanbeseenasanauxiliarytask,
classi(cid:2)ers has a superior performance even with a smaller
we only use the gradients computed there as a guidance to
number of models. They conclude that Monte-Carlo dropout
choose samples for annotations. These gradients are not used
approaches suffer from a lower diversity and a smaller model
to measure similarity with those of the main task nor are the
capacity.
parameters of the main task updated with those gradients.
Weakly supervised segmentation is an active research
topic [34], [1], [32], [15]. In the context of self-learning, III. OURAPPROACHTOEFFECTIVESAMPLESELECTION
Zhangetal.[37]uselabelsobtainedwithK-meansgraphcuts Figure 2 shows an overview of our framework. The key
as ground truth for their network. The predictions produced idea of our approach is to perform a very weakly supervised
1351
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 07:05:33 UTC from IEEE Xplore.  Restrictions apply. a complete new dataset. In accordance with previously used
terminology [37], we refer to this as very weak supervision.
Figure 3 shows an image, its ground truth and the foreground
segmentation (pseudo ground truth) provided by clustering. It
isanimportant(cid:2)ndingfromourexperimentsthataroughand
easy to compute segmentation is suf(cid:2)cient for the purpose
Fig. 3. Very weakly supervised segmentation used as pseudo ground of selecting images for annotation. This makes our proposed
truthbyourapproach.Left:Inputimage.Middle:Groundtruthsemantic
gradient-based approach feasible in practice.
segmentation; Right: Foreground segmentationofvegetationprovidedby
k-means clustering. Note that only such a rough segmentation as pseudo In order to compute a loss from the network output, which
groundtruthisenoughforourapproach. includes three classes, and the pseudo ground truth, which
merely includes two, one might combine crop and weed into
segmentation to obtain pseudo ground truth. Given the labels a single foreground class, or treat the foreground class as a
and different measures produced by the network, we rank the speci(cid:2)c type of vegetation (i.e., crop or weed). We tried all
unlabeled samples and pick them accordingly for annotation. three options and found that treating the foreground from the
These are then used to re(cid:2)ne the entire network. pseudo ground truth as crop empirically produced the best
Our CNN for semantic segmentation relies on Bonnet [25]. result. We emphasize that the pseudo ground truth is only
The used network is based on SegNet [2] and ENet [26]. It used to select training samples that should be annotated; the
has an encoder-decoder structure with a total of 25 [5x5] network weights are updated based on manual annotations
convolutional layers. It uses batch normalization, residual of the selected samples, which include all three classes.
connections, ReLU as the non-linearity layer, and the focal In our agricultural application, the number of true classes
loss function [17]. As input to our network, we only use the (3) is not much higher than the number of classes (2) in
standard RGB channels of a camera. our pseudo-ground truth. Naturally, in a different semantic
In order to perform the semantic segmentation in sugar segmentation task, the number of classes could be higher and
beet (cid:2)eld for agricultural robotics tasks, we train our model might require generating a pseudo-ground truth with a larger
ontheBonnsugarbeetdataset[5].Wethenre(cid:2)nethetrained number of classes. Our method here uses a simple clustering
model on other datasets by incrementally selecting batches mechanism but other unsupervised or weakly supervised
of samples. The datasets differ in their crop/weed statistics methods can also be used to generate pseudo-labels with
and the images acquired with the cameras also differ in their a higher number of classes that can be later used to compute
illumination. Therefore, simply running the trained model to the gradients for sample selection.
segment the vegetation in other (cid:2)elds does not work.
C. Sample Selection Using Loss
We compare three different approaches to sample selection
for active learning. Our main technical contribution is the Thelossofthenetworkisanindicationofthesegmentation
generation of a pseudo ground truth (Sec. III-B) and its use error. Given that training neural networks with backpropaga-
for loss-based (Sec. III-C), as well as two gradient-based tion is driven by the loss, it also provides a useful cue as
approaches for sample selection (Sec. III-D and Sec. III-E). to which samples the network will most bene(cid:2)t from. We
computethefocalloss[17]basedonthepseudogroundtruth.
A. Setup
We found that training only on the images with the highest
We evaluate our different approaches by (cid:2)rst training a loss values did not generalize well. This could indicate that
networkontheBonnsugarbeetdatasetthenre(cid:2)ningitonthe they are not representative enough of the overall dataset.
StuttgartandZurichdatasetsseparately.Tore(cid:2)nethenetwork Therefore, we instead employ a scheme that samples images
we pick unlabeled samples in batches of 10 using one of with a diverse range of loss values, but prefers those with
the methods described in this section. Once the samples are higher losses. To this end, we sort the images by their loss
annotated, they are given to the network. We repeat this in a descending order, and then select them uniformly on a
process iteratively, each time re(cid:2)ning the network on all of logarithmic scale. Speci(cid:2)cally, we compute index i of the
the newly annotated samples. n-th sample as:
bj j j j(cid:0) c(cid:0) 2f j j(cid:0) g
B. Generation and Use of Pseudo Ground Truth i= P n=(S 1) 1; n 0;1;:::; S 1 (1)
j j j j
Our three main methods make use of (cid:147)pseudo ground where S is the number of samples to be selected and P
truth(cid:148) foreground-background segmentation masks, which we is the size of the images pool. Since the samples are sorted,
obtain by clustering the values of the RGB channels. An this approach would more heavily select those that have
example is shown in Figure 3. We run k-means to determine higher loss values while not completely discarding images
20 cluster representatives from 10 randomly selected images. that the network is performing well on.
After viewing a single image that contains all 20 clusters, a
D. Sample Selection Using Norm of Gradients
human annotator chooses which clusters represent vegetation.
In our experiments, it was enough to select two clusters. For this approach and the following one, we pick those
Therefore, the human annotation effort that is required to samples for annotation that might have the largest impact on
obtain the pseudo ground truth amounts to a few seconds for the network weights. The norm of the network gradients is a
1352
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 07:05:33 UTC from IEEE Xplore.  Restrictions apply. measure that is indicative of which samples will affect the
TABLEI
weights more than others. Although the loss and norm of
DATASETSSTATISTICSOFCROPANDWEEDPLANTS
gradients are correlated, there are instances where the loss
could be high for certain samples, yet the gradient is locally Bonn Stuttgart Zurich
small. This depends on the loss function and the state of the Images 8230 2584 2577
current network parameters.
Croppixels 2.0% 1.5% 0.4%
As in the previous approach, we use labels from very
Weedpixels 0.3% 0.7% 0.1%
weakly supervised segmentation as pseudo ground truth. We
run the network on the training images for one epoch (to TABLEII
maintaincomputationalef(cid:2)ciency)andcomputethegradients. IOUWITHOUTANYREFINEMENT(LOWERBOUND)ANDIOUWHEN
Again we note that this step is only used to compute the TRAININGONTHEWHOLEDATASET(UPPERBOUND).
gradients but the network weights remain unchanged. Once
NoRe(cid:2)nement Fullysupervised
we have the gradients, we compute the L norm of those in
2 Stuttgart 0.3429 0.7989
the last two layers of the network (the classi(cid:2)er layer and
(cid:13) (cid:13)
the one immediately before(cid:13)it): (cid:13) Zurich 0.3595 0.7024
r L A. Datasets
n (x)= (x) ; (2)
g wf ThedatasetsweusedwereacquiredwithaBoschDeep(cid:2)eld
where x is the image and w are the weights of the (cid:2)nal two Robotics UGV. The robot was developed to assist in several
layers. The images are sorted based on this measure in a agricultural applications, including mechanical weed control
descending order and again we pick samples on a log-space and selective herbicide spraying [5]. It is equipped with
scale afterwards as explained earlier. multiple sensors such as cameras, GPS trackers, and 3D laser
sensors. For our experiments we use the RGB data provided
by the JAI AD-130GE camera.
E. Sample Selection Using Gradient Projection
The data was captured in three different (cid:2)elds: Bonn and
The log-space in the previous approaches was used to StuttgartinGermany,andZurichinSwitzerland.Thedatasets
ensure there is enough diversity among the samples so that have weed and crop plants at different stages of growth.
the network does not over(cid:2)t on them and can generalize to Figure1showssampleimagesfromthedifferentdatasets.The
unseen data. Here we use a different method that relies on imagesvaryintheirillumination,soiltype,andclassstatistics,
the space spanned by the gradients where we project onto hence the need for transfer learning. The images have been
the orthogonal complement of the gradients of the selected annotatedintothreeclasses:weed,crop,andsoil/misc.TableI
samples.Foreverypickedsample,weprojectthegradientsof shows the number of images in each dataset and the ratio of
all remaining samples onto the selected sample gradient. We foreground pixels. It can be clearly seen that there is a high
thensubtracttheprojectedgradientfromtheoriginalgradients. imbalance of classes in the data. We follow the approach of
The residual we are left with indicates which samples have [24]andsplitthenewdatasetintothreesets:40%fortraining,
thestrongestremainingeffectontheweightsafteraccounting 10% for validation, and 50% for testing. The samples are
(cid:13) (cid:13)
for the already selected(cid:13)samplXes. This can b(cid:13)e formulated as: picked from the training set. All experiments were conducted
(cid:13) (cid:13)
(cid:13) (cid:13) on four Nvidia Titan X GPUs.
(cid:13) h i (cid:13)
(cid:0) S g ;g B. Re-Training Performance
n (x)= g h i xig ; (3)
p x g ;g i The experiments in this section are designed to show
i i
i=1
how the proposed sample selection strategies impact the
where x is the image, g is the gradient of the ith sample performance of the network in the new environment. For
i
out of S previously selected samples, and g is the gradient quantifying the performance, we use the mean Intersection
x
of the current sample. We select samples one by one, each over Union (mIoU) as the performance measure. To provide
time sorting them according to this measure and choosing thelowerandupperboundsforthemethods,welistinTableII
the one with the highest norm of the residual. To pick the the mIoU for each dataset when running the model without
(cid:2)rst sample, we choose that with the highest norm of the anyre(cid:2)nementaswellaswhentrainingonallofthesamples.
gradient. Figures 4 and 5 show the performance on the Stuttgart and
Zurich datasets when selecting samples for annotation with
different methods. As baselines we include random sampling,
IV. EXPERIMENTALEVALUATION XX
andselectingsamplesthathavethehighestentropy([4],[38]):
In this section, we demonstrate the effectiveness of the (cid:0) 1 N j j
approaches we designed for active learning and evaluate the H(x)= p(c x )logp(c x ); (4)
N i i
performance of the different sample selection methods on i=1 c
different datasets, and compare them to random and entropy where x is pixel i in image x, c is the class and N is the
i
based approaches. number of pixels in the image.
1353
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 07:05:33 UTC from IEEE Xplore.  Restrictions apply. TABLEIII
0.775
OBJECT-WISEPERFORMANCEONTHESTUTTGARTANDZURICH
0.750 DATASETSRESPECTIVELY.EACHROWSHOWSTHEPERFORMANCEAFTER
0.725 SELECTING10SAMPLESWITHTHEDIFFERENTMETHODSANDREFINING
0.700 THENETWORK.RUNNINGTHEMODELWITHOUTANYNEWANNOTATIONS
YIELDSANACCURACYOF0:15ONSTUTTGARTAND0:33ONZURICH.
IoU0.675
0.650 grad_proj Samples Random Entropy Loss Gradient Gradient
grad_norm No. Norm Proj.
0.625 loss_log
0.600 entropy 10 0.6920 0.6890 0.7882 0.8040 0.8196
random
20 0.7402 0.8050 0.7769 0.8350 0.8404
0 20 40 60 80 100
number of samples 30 0.8138 0.8300 0.7950 0.8461 0.8470
Fig.4. Pixel-wisemeanIoUontheStuttgartdataset.Runningthemodel 40 0.8254 0.8463 0.8555 0.8682 0.8252
withoutanynewannotationsyieldsanIoUof0:34.Runningthemodelon
the whole dataset yields an IoU of 0.79. Gradient-based approaches can 50 0.8225 0.8405 0.8523 0.8599 0.8278
reach90%ofthefullysupervisedperformancewith10samples.
Samples Gradient Gradient
Random Entropy Loss
No. Norm Proj.
0.60 10 0.7552 0.7879 0.7697 0.8354 0.8025
20 0.7971 0.8212 0.8189 0.8768 0.8170
0.55
30 0.8591 0.7884 0.8321 0.8553 0.8299
IoU0.50 40 0.8575 0.8711 0.8610 0.8711 0.8479
grad_proj 50 0.8593 0.8688 0.8636 0.8852 0.8784
0.45 grad_norm
loss_log
entropy
0.40 random
0 20 40 60 80 100 Tofurtherquantifytheperformanceofourapproach,weuse
number of samples
the object-wise metric de(cid:2)ned by Milioto et al. [24], where
Fig.5. Pixel-wisemeanIoUontheZurichdataset.Runningthemodel the accuracy is measured for objects larger than 50 pixels.
withoutanynewannotationsyieldsanIoUof0:36.Runningthemodelon
Since the target application is weeding with agricultural
the whole dataset yields an IoU of 0.70. Gradient-based approaches can
reach77%ofthefullysupervisedperformancewith10samples. robotics, this metric is more directly useful than pixel-wise
performance. Table III shows how our approach performs on
A few observations can be made from the (cid:2)gures: the the Stuttgart and Zurich datasets. Each row shows the mean
effect of the sampling method is stronger when only a few accuracy when selecting n samples with different methods.
images are selected. As the model is trained on more and For comparison, random and entropy based sampling are
more samples, the accuracy plateaus as expected and the shown in the (cid:2)rst and second columns respectively.
variation between the different methods decreases. It can be
C. Comparison to Other Baselines
notedhoweverthatrandomsamplinghasalowerperformance
even with a greater number of images. To gain more insight into what our baselines are, we ran
The overall performance on the Stuttgart dataset is better additional experiments with the results shown in Table IV.
than that on the Zurich dataset. This can be attributed to the Inthe(cid:2)rstrow,werananexperimentwherewetrainedthe
differentclassstatisticsofthetwodatasets.Ascanbeseenin model with the pseudo ground truth (cid:2)rst and picked samples
Table I, the Stuttgart dataset has a larger percentage of crop randomlyafterwards.Wefoundthatitperformsslightlybetter
and weed pixels compared to the Zurich dataset. This allows than when picking random samples directly (0:64 vs. 0:61)
the model to better distinguish between the different classes. but still worse than our log and gradient based methods (e.g.
This observation is also supported by the fully supervised 0:64 vs. 0:71 for the gradient-norm approach). Although pre-
performance shown in Table II where a higher IoU can be training with the pseudo ground truth allows the network
obtained on the Stuttgart dataset. to distinguish foreground vegetation from background, the
Whentrainingthemodelwithonlyahandfulofimages,10 task at hand is to learn three classes and more importantly
or 20 images, the methods that take into account the impact distinguish crop from weed. Therefore for all experiments,
of the samples on the weights lead to better generalization we re(cid:2)ne the model without pre-training on the foreground
to the rest of the unseen data. In particular, ranking samples masks.
by projecting out gradients results in higher mIoU on both In the second row, we run an (cid:148)oracle(cid:148) experiment. We
datasets. With 10 samples, which would amount to roughly compute the difference between the parameters of the model
1% of the training dataset size, we can achieve 90% of without any re(cid:2)nement and the parameters of the fully
the fully supervised performance (Table II) on the Stuttgart supervised model. We then (cid:2)nd samples with gradients that
dataset, compared to 76% with random selection. On the align with the parameters difference. This experiment is not
Zurich dataset, we can achieve 77% of the fully supervised intendedforsampleselection,rathertoknowiftheframework
performance compared to 63% with random selection. had complete knowledge of how the gradients should look
1354
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 07:05:33 UTC from IEEE Xplore.  Restrictions apply. TABLEIV TABLEV
ADDITIONALBASELINESFORTRAININGWITH10SAMPLESONTHE PRECISIONANDRECALLONTHESTUTTGARTDATASETAFTER
STUTTGARTDATASET.COMPAREWITHFIG.4. SELECTINGTHEFIRST10SAMPLES.THEFIRSTTABLESHOWSTHE
PIXEL-WISEPERFORMANCEANDTHESECONDTABLESHOWSTHE
Method mIoU OBJECT-WISEPERFORMANCE.THEHIGHESTVALUESALONGACOLUMN
AREINBOLDANDTHELOWESTINITALICS.
Random-pseudogroundtruth 0.6448
Alignwithparametersdifference(oracle) 0.7010 Precision Recall
Weed Crop Weed Crop
Random 0.4095 0.7278 0.4851 0.6946
40 Entropy 0.4158 0.7334 0.4786 0.5894
20 LossLog 0.5331 0.8025 0.6179 0.8112
GradientNorm 0.5970 0.8259 0.6136 0.8402
0
GradientProjection 0.5745 0.8365 0.6564 0.8212
−20
Precision Recall
−40
grad_norm Weed Crop Weed Crop
grad_proj
−60 loss Random 0.8723 0.5740 0.6587 0.6474
−50 −40 −30 −20 −10 0 10 20 30 Entropy 0.8851 0.5238 0.6122 0.7399
LossLog 0.9005 0.6898 0.7811 0.7351
Fig.6. t-SNEoftheimagesgradientsontheStuttgartdataset.Eachpoint
representsthe2-Dembeddingofthegradientvector.The(cid:2)rst10samples GradientNorm 0.9090 0.7390 0.7970 0.7536
selectedbyeachmethodareshownindifferentcolors.
GradientProjection 0.9030 0.7308 0.8289 0.7375
like, would it be able to pick better samples. We found
Projection have a high recall and precision of the crop
that the oracle performance is similar to our gradient-based
class without degrading those of the weed class. The object-
approachesafterseeing10newsamples.Thisimpliesthatthe
wise performance in the second table further illustrates the
gradient-based approaches are bounded by this performance.
effectiveness of these methods. Gradient Norm and Gradient
Substantiallyimprovingupontheirperformancemightrequire
Projection produce high precision and recall for both classes.
exploiting additional knowledge from the model, possibly
We observed the same behavior on the Zurich dataset (not
with the aid of unsupervised segmentation.
included here).
D. Inspecting t-SNE of Samples Gradients V. CONCLUSION
To further analyze the ranking methods and inspect In this paper, we introduced and compared several active
potential patterns in the different sampling approaches, we learning approaches that support the adaptation of semantic
plotthet-distributedStochasticNeighborEmbedding(t-SNE) segmentation networks to new environments. Our approaches
of the gradients in Figure 6. Each circle denotes the 2-D effectively select samples from the new environment for user
embeddingofthegradientofasingleimagebeforepickingthe annotation with the goal of maximizing the bene(cid:2)t from a
(cid:2)rst 10 samples. Samples selected by each method are shown small number of annotated examples. We applied sample
in different colors. As explained in Section III-D and III-E, selection strategies to the task of crop/weed classi(cid:2)cation for
we combined the idea of gradient-based selection with two agricultural robots, as the appearance between agricultural
alternative approaches to achieving diversity in the selected (cid:2)elds often changes substantially such that re-training is
images: picking on a log scale, or projecting out gradients needed. We compute pseudo ground truth labels using very
that have been selected previously. In our experiments, both weakly supervised segmentation and use those labels to
strategies performed well (see Fig. 4 and Fig. 5). When estimate how new, unlabeled samples will affect the weights
inspecting the gradients of the samples selected, we found of the CNN if selected for training. We select the training
that the strongest gradients cluster together, near the top left. samples for user annotation based on the estimated effect on
Additionally, the gradient projection method selects many the weights and use them to re(cid:2)ne the network.
points at the boundary of the distribution, suggesting that it We evaluated the performance gain of our gradient-based
might be improved further by adding a mechanism to ensure and log-based approaches on two agricultural datasets for
that selected images are representative of a larger subset in weed detection. The datasets reveal different characteristics
the overall dataset. from the dataset on which the network was pretrained.
Our results show the effectiveness of our method as it
E. Performance on Weed and Crop Classes
produces higher semantic segmentation accuracies with a
A more detailed breakdown of the methods performance small number of training samples, compared to random
is shown in Table V. The (cid:2)rst table shows the pixel-wise sampling as well as entropy based sampling. As a result
precisionandrecallontheStuttgartdatasetafterselectingthe of that, the effort in human annotation is reduced without
(cid:2)rst 10 samples. Both methods, Gradient Norm and Gradient compromising performance.
1355
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 07:05:33 UTC from IEEE Xplore.  Restrictions apply. REFERENCES arrangement.InProc.oftheIEEE/RSJIntl.Conf.onIntelligentRobots
andSystems(IROS),2017.
[1] David Acuna, Huan Ling, Amlan Kar, and Sanja Fidler. Ef(cid:2)cient
[21] ChrisMcCool,JamesBeattie,JenniferFirn,ChrisLehnert,JasonKulk,
interactiveannotationofsegmentationdatasetswithpolygon-rnn++. In
Raymond Russell, Tristan Perez, and Owen Bawden. Ef(cid:2)cacy of
Proc.oftheIEEEConf.onComputerVisionandPatternRecognition
mechanicalweedingtools:Astudyintoalternativeweedmanagement
(CVPR),pages859(cid:150)868,2018. strategiesenabledbyrobotics. IEEERoboticsandAutomationLetters
[2] VijayBadrinarayanan,AlexKendall,andRobertoCipolla. Segnet:A (RA-L),2018.
deep convolutional encoder-decoder architecture for image segmen-
[22] ChrisMcCool,TristanPerez,andBenUpcroft.MixturesofLightweight
tation. IEEETrans.onPatternAnalalysisandMachineIntelligence
DeepConvolutionalNeuralNetworks:AppliedtoAgriculturalRobotics.
(TPAMI),39(12):2481(cid:150)2495,2017.
IEEERoboticsandAutomationLetters(RA-L),2017.
[3] WilliamH.Beluch,TimGenewein,AndreasNu¤rnberger,andJanM.
[23] AndresMilioto,PhilippLottes,andCyrillStachniss. Real-timeblob-
Ko¤hler. The power of ensembles for active learning in image
wisesugarbeetsvsweedsclassi(cid:2)cationformonitoring(cid:2)eldsusing
classi(cid:2)cation. In Proc. of the IEEE Conf. on Computer Vision and
convolutionalneuralnetworks.InProc.oftheIntl.Conf.onUnmanned
PatternRecognition,pages9368(cid:150)9377,2018.
AerialVehiclesinGeomatics,2017.
[4] ShayokChakraborty,VineethBalasubramanian,QianSun,Sethuraman
[24] Andres Milioto, Philipp Lottes, and Cyrill Stachniss. Real-time
Panchanathan, and Jieping Ye. Active batch selection via convex
semantic segmentation of crop and weed for precision agriculture
relaxationswithguaranteedsolutionbounds. IEEETrans.onPattern
robotsleveragingbackgroundknowledgeincnns. InProc.oftheIEEE
AnalalysisandMachineIntelligence(TPAMI),37(10):1945(cid:150)1958,2015.
Intl. Conf. on Robotics and Automation (ICRA), pages 2229(cid:150)2235,
[5] NivedChebrolu,PhilippLottes,AlexanderSchaefer,WeraWinterhalter,
2018.
WolframBurgard,andCyrillStachniss. Agriculturalrobotdatasetfor
[25] AndresMiliotoandCyrillStachniss. Bonnet:Anopen-sourcetraining
plantclassi(cid:2)cation,localizationandmappingonsugarbeet(cid:2)elds. The
anddeploymentframeworkforsemanticsegmentationinroboticsusing
Intl.JournalofRoboticsResearch,36(10):1045(cid:150)1052,2017.
cnns.Proc.oftheIEEEIntl.Conf.onRoboticsandAutomation(ICRA),
[6] YunshuDu,WojciechMCzarnecki,SiddhantMJayakumar,Razvan
2019.
Pascanu, and Balaji Lakshminarayanan. Adapting auxiliary losses
[26] AdamPaszke,AbhishekChaurasia,SangpilKim,andEugenioCulur-
usinggradientsimilarity. arXivpreprintarXiv:1812.02224,2018.
ciello. Enet:Adeepneuralnetworkarchitectureforreal-timesemantic
[7] TomDuckett,SimonPearson,SimonBlackmore,andBruceGrieve.
segmentation. arXivpreprintarXiv:1606.02147,2016.
Agriculturalrobotics:Thefutureofroboticagriculture. arXivpreprint,
[27] InkyuSa,MarijaPopovic,RaghavKhanna,ZetaoChen,PhilippLottes,
abs/1806.06762,2018.
Frank Liebisch, Juan Nieto, Cyrill Stachniss, Achim Walter, and
[8] Suyog Dutt Jain and Kristen Grauman. Active image segmentation
RolandSiegwart. WeedMap:ALarge-ScaleSemanticWeedMapping
propagation. In Proc. of the IEEE Conf. on Computer Vision and
Framework Using Aerial Multispectral Imaging and Deep Neural
PatternRecognition,pages2864(cid:150)2873,2016.
NetworkforPrecisionFarming. RemoteSensing,10,2018.
[9] Alexander Freytag, Erik Rodner, and Joachim Denzler. Selecting
[28] Ozan Sener and Silvio Savarese. Active learning for convolutional
in(cid:3)uential examples: Active learning with expected model output
neuralnetworks:Acore-setapproach.arXivpreprintarXiv:1708.00489,
changes. In European Conf. on Computer Vision, pages 562(cid:150)577,
2017.
2014.
[29] Ozan Sener and Silvio Savarese. A geometric approach to active
[10] Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep bayesian
learning for convolutional neural networks. arXiv preprint arXiv,
activelearningwithimagedata.InProc.ofthheIntl.Conf.onMachine
1708:1,2017.
Learning,pages1183(cid:150)1192,2017.
[11] Isabelle Guyon, Gavin Cawley, Gideon Dror, and Vincent Lemaire. [30] BurrSettles.Activelearningliteraturesurvey.Technicalreport,Univ.of
Resultsoftheactivelearningchallenge.InProc.oftheAISTATSActive Wisconsin-Madison,Dep.ofComputerSciences,2009.
LearningandExperimentalDesignWorkshop,pages19(cid:150)45,2011. [31] BurrSettles,MarkCraven,andSoumyaRay. Multiple-instanceactive
[12] AlexHolub,PietroPerona,andMichaelC.Burl. Entropy-basedactive learning.InAdvancesinNeuralInformationProcessingSystems,pages
learningforobjectrecognition. InIEEEComputerSocietyConf.on 1289(cid:150)1296,2008.
ComputerVisionandPatternRecognitionWorkshops,pages1(cid:150)8,2008. [32] MengTang,AbdelazizDjelouah,FedericoPerazzi,YuriBoykov,and
[13] JiajiHuang,RewonChild,VinayRao,HairongLiu,SanjeevSatheesh, ChristopherSchroers. Normalizedcutlossforweakly-supervisedcnn
andAdamCoates. Activelearningforspeechrecognition:thepower segmentation. In Proc. of the IEEE Conf. on Computer Vision and
ofgradients. arXivpreprintarXiv:1612.03226,2016. PatternRecognition(CVPR),pages1818(cid:150)1827,2018.
[14] Christoph Ka¤ding, Erik Rodner, Alexander Freytag, and Joachim [33] Keze Wang, Dongyu Zhang, Ya Li, Ruimao Zhang, and Liang
Denzler. Activeandcontinuousexplorationwithdeepneuralnetworks Lin. Cost-effective active learning for deep image classi(cid:2)cation.
andexpectedmodeloutputchanges. arXivpreprintarXiv:1612.06129, IEEE Transactions on Circuits and Systems for Video Technology,
2016. 27(12):2591(cid:150)2600,2017.
[15] SuhaKwak,SeunghoonHong,andBohyungHan. Weaklysupervised [34] YunchaoWei,HuaxinXiao,HonghuiShi,ZequnJie,JiashiFeng,and
semanticsegmentationusingsuperpixelpoolingnetwork. InThirty- ThomasSHuang. Revisitingdilatedconvolution:Asimpleapproach
FirstAAAIConferenceonArti(cid:2)cialIntelligence,2017. forweakly-andsemi-supervisedsemanticsegmentation. InProc.of
[16] Frank Liebisch, Pfeifer Johannes, Raghav Khanna, Philipp Lottes, theIEEEConf.onComputerVisionandPatternRecognition(CVPR),
CyrillStachniss,TillmannFalck,SlawomirSander,RolandSiegwart, pages7268(cid:150)7277,2018.
Achim Walter, and Enric Galceran. Flourish (cid:150) A robotic approach [35] LinYang,YizheZhang,JianxuChen,SiyuanZhang,andDannyZ.
forautomationincropmanagement. InInProc.oftheWorkshopfu¤r Chen. Suggestiveannotation:Adeepactivelearningframeworkfor
Computer-Bildanalyseundunbemannteautonom(cid:3)iegendeSystemein biomedicalimagesegmentation. InProc.oftheIntl.Conf.onMedical
derLandwirtschaft,2016. ImageComputingandComputer-AssistedIntervention,pages399(cid:150)407,
[17] Tsung-YiLin,PriyaGoyal,RossB.Girshick,KaimingHe,andPiotr 2017.
Dolla·r. Focalloss for dense objectdetection. In Proc. ofthe IEEE [36] DonggeunYooandInSoKweon. Learninglossforactivelearning. In
Intl.Conf.onComputerVision(ICCV),pages2999(cid:150)3007,2017. Proc.oftheIEEEConf.onComputerVisionandPatternRecognition
[18] Philipp Lottes, Jens Behley, Nived Chebrolu, Andres Milioto, and (CVPR),pages93(cid:150)102,2019.
CyrillStachniss. JointStemDetectionandCrop-WeedClassi(cid:2)cation [37] LingZhang,VissaganGopalakrishnan,LeLu,RonaldMSummers,
for Plant-speci(cid:2)c Treatment in Precision Farming. In Proc. of the JoelMoss,andJianhuaYao. Self-learningtodetectandsegmentcysts
IEEE/RSJIntl.Conf.onIntelligentRobotsandSystems(IROS),2018. inlungctimageswithoutmanualannotation. InIEEEIntl.Symposium
[19] PhilippLottes,JensBehley,AndresMilioto,andCyrillStachniss.Fully onBiomedicalImaging(ISBI2018),pages1100(cid:150)1103,2018.
convolutionalnetworkswithsequentialinformationforrobustcropand [38] Zongwei Zhou, Jae Shin, Lei Zhang, Suryakanth Gurudu, Michael
weeddetectioninprecisionfarming. IEEERoboticsandAutomation Gotway,andJianmingLiang.Fine-tuningconvolutionalneuralnetworks
Letters(RA-L),3:3097(cid:150)3104,2018. forbiomedicalimageanalysis:activelyandincrementally. InProc.of
[20] Philipp Lottes and Cyrill Stachniss. Semi-supervised online visual theIEEEConf.onComputerVisionandPatternRecognition(CVPR),
crop and weed classi(cid:2)cation in precision farming exploiting plant pages7340(cid:150)7351,2017.
1356
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 07:05:33 UTC from IEEE Xplore.  Restrictions apply. 