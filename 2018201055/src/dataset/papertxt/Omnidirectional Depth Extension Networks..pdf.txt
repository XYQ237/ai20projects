2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
What’s in my Room? Object Recognition on Indoor Panoramic Images
(cid:3) (cid:3)
JuliaGuerrero-Viu 1,ClaraFernandez-Labrador 1;2,Ce·dricDemonceaux2,andJoseJ.Guerrero1
Abstract(cid:151)Inthelastfewyears,therehasbeenagrowinginterest
(cid:14)
intakingadvantageofthe360 panoramicimagespotential,while
managingthenewchallengestheyimply.Whileseveraltaskshave
beenimprovedthankstothecontextualinformationtheseimages
offer,objectrecognitioninindoorscenesstillremainsachallenging
problemthathasnotbeendeeplyinvestigated.Thispaperprovides
an object recognition system that performs object detection and
semantic segmentation tasks by using a deep learning model - painting -chair
- bed -sofa
adaptedtomatchthenatureofequirectangularimages.Fromthese - bed -door
- table -door
results, instance segmentation masks are recovered, refined and - table -door
- mirror -door
transformed into 3D bounding boxes that are placed into the 3D - window -cabinet
- window -cabinet 
modeloftheroom.Quantitativeandqualitativeresultssupportthat - window -bedside
ourmethodoutperformsthestateoftheartbyalargemarginand
showacompleteunderstandingofthemainobjectsinindoorscenes. Fig.1:What’sinmyroom?:fromasinglesphericalRGBimage,we
obtainlocalizationandpixel-wiseclassificationoftheobjectspresent
I. INTRODUCTION inthescene.Weusethisinformationtorecoverinstancesegmentation
masksofallobjectsandfinallyplacetheminsidethe3Droomlayout.
The increasing interest in autonomous mobile systems, like
drones, robotic vacuum cleaners or assistant robots, makes
detectionandrecognitionofobjectsinindoorenvironmentsavery advantageoftheirstrengthsandallowworkingwithpanoramic
importantanddemandedtask. imagesinanefficientandeffectiveway.
Since recognizing a visual concept is relatively trivial for a In this paper, we propose an object recognition system that
human, it is worth considering the hard challenges inherently providesacompleteunderstandingofthemainobjectsinanindoor
involved.Objectsinimagescanbeorientedinmanydifferentways, scenefromasingle360(cid:14)imageinequirectangularprojection.Our
varytheirsize,beoccluded,blendedintotheenvironmentbecause methodextendstheBlitzNetmodel[1]toperformbothobject
oftheircolororappearance,oraffectedbydifferentillumination detectionandsemanticsegmentationtasksbutadaptedtomatch
conditions, which changes drastically their aspect on the pixel thenatureoftheequirectangularimageinput.Wetrainthenetwork
level.Moreover,theconceptbehindanobject’snameissometimes to predict 14 different classes of main indoor scenes related
broad,includingnon-clearfrontierstootherconcepts.Forexample, objects.ResultsoftheCNNarepost-processedtoobtaininstance
wheredoyouconsiderthelimitsbetweenasofaandanarmchair? segmentation masks, which are successfully refined by taking
Convolutional Neural Networks (CNNs) have already advantage of the spatial contextual clues that the room layout
demonstrated to be the best known models to perform object provides.Inthiswork,wenotonlyshowthepotentialofexploiting
recognition,astheyarecapableofdealingwiththosechallenges the2Droomlayouttoimprovetheinstancesegmentationmask,
byautomaticallylearningobjects’inherentfeaturesandcorrectly butalsothepossibilityofleveragingthe3Dlayouttogenerate
identifytheirintrinsicconcepts. 3Dobjectboundingboxesdirectlyfromtheimprovedmasks.
However, images from conventional cameras have a small
II. PREVIOUSWORK
fieldofview,muchsmallerthanhumanvision,whichimplies
thatcontextualinformationcannotbeasusefulasitshould.To Object detection field has been mainly dominated by two
overcomethislimitation,arealimpactcamewiththearrivalofthe different approaches: one-stage and two-stage detectors. Two-
360(cid:14)full-viewpanoramicimages,whicharerecentlyarisingmore stage detectors, as the first R-CNN[2] architecture followed
andmoreinterestintheroboticsandcomputervisioncommunity, by its variants Fast R-CNN[3], Faster R-CNN[4] and Mask
astheyallowustovisualize,inasingleimage,thewholesceneat R-CNN[5]achievegreataccuracybutlowerspeed.Theyrequire
thesametime.Togetherwithalloftheirpotentialwehavetodeal firstlytorefineproposalstoobtainthefeaturesneededtoclassify
withchallengesproducedbytheirownsphericalprojection,such the objects. On the other hand, one-stage detectors, following
asdistortion,orthelackofcomplete,labeledandmassivedatasets. YOLO[6] and SSD[7] simultaneous bounding box refinement
This requires the development of specific techniques that take andclassification,significantlyreducecomputationalcost.They
achievereal-timeperformingmaintaininghighaccuracy,which
(cid:3)Equalcontribution isneededformostapplicationsinautonomousmobilesystems.
1J. Guerrero-Viu, C. Fernandez-Labrador and J.J. Guerrero. Instituto de SSDmulti-scalepyramidideaprovestohelpinconductingmore
Investigacio·nenIngenier·(cid:17)adeArago·n(I3A),UniversidaddeZaragoza,Spain
accurate detections and manage widely various object sizes,
2C.Fernandez-LabradorandCe·dricDemonceaux.VIBOTERLCNRS6000,
ImViA,Universite·BourgogneFranche-Comte·,France. approachfollowedinmoststate-of-the-artobjectdetectors.
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 567
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 15:46:35 UTC from IEEE Xplore.  Restrictions apply. Whileallthosemodelsoptimizeboundingboxdetection,notso
manyintegrateintheirpipelinethepixel-wiserecognitionneeded
for many applications. In this way, BlitzNet [1] is a one-stage
multi-scalemodelthataddssemanticsegmentationandtherefore
recognizesobjectsatpixellevel.Italsoprovestheadvantagesof
jointlylearningtwosceneunderstandingtasks:objectdetection
and semantic segmentation, which benefit from each other by
sharingalmostthecompletenetworkarchitecture. Fig.2:Resultofourmethodtocreatesemanticsegmentationmasks,
However,state-of-the-artresearchmainlyfocusesonusingcon- assuming hypothesis of occlusion. Notice on the left the differences
ventionalimages.Theirlimitedfieldofviewpreventscontextual betweencreatingstraightcontoursonimagedomain(top)vs.spherical
informationfrombeingascrucialasitisinsceneunderstanding domain(bottom).
forhumans.Differentlyfromoutdoorobjectrecognition,where
thankstotheincreasingresearchonautonomousdriving,there
biggestoneisselected.Withitsevidentlimitations,thishypothesis
arerecentworksusingpanoramicimages[8][9],thereisnowide
experimentallyprovestoworkwellinmostofthecases,allowing
researchonobjectrecognitionfromindoorpanoramas.Arecent
tocorrectlysegmentmostofthevisibleandcuboid-shapedobjects
workthataddressesthisproblemis[10],whereDenget.aluse
inimagesasshowninFigure2.
aR-CNNapproach,andalsoevaluatetheirownimplementation
Thecompletedatasetusedinthisworkisreleasedforpublic
ofDPM[11]onpanoramas.Themostrelevantworkonindoor
accessandcanbefoundintheprojectwebpage1.
panoramic object recognition is PanoContext [12]. It includes
2D object detection and semantic segmentation among other
IV. MODEL
3D scene understanding tasks, proving the potential of having
Inthissectionwepresentourobjectrecognitionmodel,called
a larger field of view for recognition problems. Their method,
PanoramicBlitzNet,thatisbasedontheoriginalCNNBlitzNet[1]
nevertheless,isbasedongeometricalreasoningandtraditional
butadaptedtoworkspecificallywithcompleteequirectangular
computer vision feature extractors and can be still considered
images. It addresses both object detection and semantic
as state-of-the-art in indoor object recognition on panoramic
segmentation tasks, following BlitzNet architecture: a Fully
images.Recentresearchonthiskindofimagesincludes3Dlayout
Convolutionalmodelthatfollowstheencoder-decoderapproach
recovery [13] [14] [15] [16] and scene modeling [17], which
with skip connections. It performs multi-scale recognition and
providesglobalcontextandgivesa3Dinterpretationofthescene
takesadvantageofjointlearning.Mainchangestotheirbaseimple-
from a single view. In [18], they show that this tasks can also
mentationincludetheuseofthecompleterectangularpanorama,
benefitandaugmentanomnidirectionalSLAM.Combiningobject
modifyingtheinputaspectratio.Wealsochangetheanchorboxes
recognition and 3D layout recovery motivates our proposal to
proposals,asthenewinputshapeneedstobeconsideredbecause
obtainthe3Drecognitionandlocationofmainobjectsinourroom.
theyarecenteredonpixelsgrid.Ourboundingboxesproposals
III. DATASETEXTENSION aredonebyfirstlyconvertingimagetoaregulargrid,coveringthe
wholerectangular-shapedimage.Gridhasdifferentdimensionsin
Panoramicimagesdatasetswithobjectrecognitionlabelsarenot
eachlayer,from128x256to1x2,becauseoftheiterativelylower
asstandardorcompleteasconventionalimagesones[19][20][21].
scaleofthefeaturemaps.Ineachgridcell5differentproposals
Therefore, in this work we decide to extend the SUN360
are created with 5 different aspect ratios: 1, 2, 1/2, 3 and 1/3,
database[19]withsegmentationlabels.Foreverypanorama,we
allowingthenetworktomanagedifferentobjectshapes.
generateindividualmasksencodingeachobject’sspatiallayout.
Specialmentiondeservesdataaugmentationasanimportant
Additionally, we combine all the masks obtaining a semantic
technique to avoid overfitting, particularly on non-massive
segmentationpanoramicimagewithper-pixelclassification.
datasets like in our case. Here, we modify the original data
Bedroom and living room sets, formed by 418 and 248
augmentationbyremovingrandomcropsonimages(contextual
images respectively, are used and 14 different object classes (cid:14)
informationisimportant)andaddinghorizontalrotationfrom0
are considered. The dataset is divided into 85% for train and (cid:14)
to360 tocoveralldifferentpositionsonthesphere.
validationand15%fortest.
(cid:14)
Wegeneratesegmentationmasksbasedon2Dboundingpoints A. Howcanwedealwith360 imagesdistortion?
of the objects, taken from PanoContext [12] work. We project
Weexploitthepotentialofomnidirectionalimagescovering
them on the spherical domain to follow distortion patterns in (cid:14) (cid:14)
360 horizontal and 180 vertical field of view represented in
contours and to correctly manage objects that appear cropped
equirectangularprojection.Whiletheseimagesallowustoanalyse
on the horizontal image limits. To combine the binary masks
thewholesceneatoncetakingadvantageofallthecontext,they
andcreatethesemanticsegmentationpanorama,withthelackof
presentgreatdistortionsduetotheirprojectionofthesphere.Here,
depthorother3Dinformation,anhypothesisofocclusionamong
wereplaceallstandardconvolutionsofourPanoramicBlitzNet
objectsisneeded.Weconsidertheassumptionthatobjectsare
byequirectangularconvolutions(EquiConvs[16]),tostudytheir
notingeneralcompletelyoccluded,andthereforeforeachpair
impact on the task of recognizing objects. With this kind of
ofobjectsinconflicttheirareaofoverlapandsizearecomputed. convolutions,thekerneladaptsitsshapeandsizeaccordingly
Ifareaofoverlapisbiggerthanathreshold,thesmallestobject
is considered closer and completely visible and otherwise the 1Availableathttps://webdiis.unizar.es/(cid:24)jguerrer/roomOR/
568
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 15:46:35 UTC from IEEE Xplore.  Restrictions apply. tothedistortionsproducedbytheequirectangularprojection.As asinglepanorama,theyrecoverthemainstructureoftheroom(cid:150)i.e.
mentionedin[16],thedistortionpresentedislocationdependent, dispositionofthewalls,ceilingandfloor,notonlyintheimage
specifically,itdependsonthepolarangle.Theydemonstratehow domain,butalsoacomplete3Dreconstructionmodeluptoscale.
EquiConvs can be really convenient to generalize to different Theintuitionisthatobjectslocationandposeinsidea3Dindoor
camera positions since the layout shape can suffer from many spacearenotrandomlydistributed.Followingthelawofphysics,
variations. For the specific task of object recognition, the use objectswillbefairlyconstrainedtolieonatleastonesupporting
of EquiConvs is definitely convenient even if the camera is plane,instableconfigurationsand,inseveralcases,alignedwith
alwaysatthesameplace,sinceobjectscanbeatmanydifferent theroomwalls.Thismeansthattheroomlayoutprovidesstrong
locationsinsidethescene(cid:150)e.g.objectsclosertothecamerawill spatialcontextualcluesastowhereandhowobjectscanbefound.
have greater distortions than objects around the horizon line. Thus,inordertoprovideagreaterunderstandingofthescene,
EquiConvs here play an important role since they can learn hereweanalyzethepotentialofusingtheroomlayoutasaprior
ignoringthisdistortionpatternsandthus,beingmoreabletolearn combinedwiththeobjectrecognitiontask:
real objects appearance. Additionally, one important challenge 1)Wefoundthatwecaneasilyleveragetheroomlayoutinthe
toaccomplishourgoalisrepresentedbytheneedofextensive image domain to improve the instance segmentation masks.
annotationsfortrainingobjectrecognition.Tothisend,EquiConvs Basedonthecontextualinformationgivenbythelayout,there
makethepre-trainingmuchmoreeffectivesincethistypeof areaseriesoflogicalassumptionsthatwecanimmediatelymake
convolutionsimplicitlyhandleequirectangulardistortion,being (cid:150)e.g.itisveryunlikelytofinddoorsnotrestingontheflooror
abletousepreviousweightsfromconventionalimagesasifthey paintingshanginginbetweentwowalls.Duringthisprocesswe
werelearntonthesamekindofimages.Wecanthereforeexploit alsodetectholesinmasksandfillthem.
thewealthofpubliclyavailableperspectivedatasetsfortraining 2)Theaforementionedmethodsprovide3Dlayoutmodelsofthe
(cid:150)SUNRGB-D[22]datasetinthiscase,whichreducesthecostof rooms.Thisallowsustoplacetheidentifiedobjectsinsidethe
annotationsandallowstrainingunderalargervarietyofscenarios. 3Dmodeloftheroomaslongastheylieonthewalls,orrest
Moreover,standardconvolutionsdonotunderstandthattheimage onthefloor/ceilingalignedwiththewalls.Inthisway,onlyby
wrapsaroundthesphere,loosingthecontinuityofthescene,while detectingthemasksoftheobjectsandwithagoodlayoutprior,
EquiConvs,workingdirectlyonthesphericalspace,avoidpadding wecanobtainaveryprecise2Drepresentationoftheobjectsand
andexploitthisidea.Thismakesthemalsoverysuitableforthe aninitialestimateofthe3Dunderstandingofthescene.
objectsthatappearcutbetweentheleftandrightsideoftheimage. Toobtaintheroomlayout,herewechoosetheworkofFernan-
B. Fromsemantictoinstancesegmentation dezetal.[16].Additionally,wewillneedtoassumeManhattan
World [25] whereby there exist three dominant orthogonal
Semanticsegmentationmasksallowustopixel-wiseclassify
directionsdefiningthescene.Here,wecomputethevanishing
scenes in object categories. One step further goes instance
pointsofthescenefollowingtheapproachof[23].Theyproposea
segmentation,whichclassifieseachpixelnotonlytoitscategory
RANSAC-basedalgorithmthatworksdirectlyonomnidirectional
butalsodifferentiatingitsconcreteobjectinstance,anessential
imagesrunningupto5timesfasterthanotherapproaches.
stage to correctly locate them into the 3D reconstruction of
Forthe3Dobjectrecognitiontask,inPanoContext[12]they
theroom.Withoutusinganyinstancesegmentationpriortobe
generate many cuboid hypotheses combining two approaches.
learnt, we add a simple post-processing to obtain pixel-wise
First,theyperformrectangledetectioninsixaxis-alignedviews
instanceclassificationofthescene,basedontheoutputsofthe
projected from the original panorama. Then, they sample rays
network.ConsideringeachboundingboxdetectionasaGaussian
fromthevanishingpointstofitimagesegmentationboundaries
distribution, it is assumed that 99% of the object is contained
obtainedbyselectivesearch.Finally,theychoosethebestcuboid
onit,sostandarddeviationineachdimensionistakenas(cid:27) =
w whose projection has the largest intersection over union score
width;(cid:27) = height,giving3(cid:27) ateachsideofthemean,which
6 h 6 withthesegment.Hereinstead,wedirectlyapproximateevery
isdefinedasthecenteroftheboundingbox.Then,eachpixel
objectmaskwithfourlinesbyaRANSACapproachandclassify
fromthesemanticsegmentationmaskisassignedtotheinstance
these lines accordingly to the vanishing points, vp, (cid:150)i.e. we
distributionwiththeminimumMahalanobisdistance.Whennone
obtaintheorientationoftheobjectsinsidethescene.Weconsider
of the distances to distributions exceeds the chi-squared test
that a line belongs to one concrete direction, k=x;y;z, when
threshold,thepixelisleftastheclassificationintheinitialsemantic
itsnormaldirectioninthe3Dspacen fulfillstheconditionof
segmentation,aproposalthatshownthebestexperimentalresults. i
perpendicularity with the direction under an angular threshold
This assumption gives, consequently, more importance to (cid:6) (cid:14) j (cid:1) (cid:0) j(cid:20)
bigger objects, which are usually better segmented by our of 0:5 experimentallyobtained, arccos(ni vpk) (cid:25)2 (cid:18)th.
Then,wedeterminewithwhichplanesoftheroomeachobject
model.Inadditiontoprovidinganapproachtocreateinstance
interactsbyobtainingtheintersectionsbetweentheobjectmask
segmentation masks, we analyze the impact it can have in
and the 2D room layout segmentation map. The room layout
improvingourinitialsegmentation,asshowninexperiments.
segmentationmapisdirectlygeneratedfromthepredictedlayout
C. Can we convert instance segmentation masks into 3D corners [16], encoding each plane accordingly to the main
boundingboxes? directionsofthescene.Ifwepredictthattheobjectliesonawall
Ifthereisataskthathasexperimentedadisruptiveinnovation (cid:150)i.e.doors,windows,mirrors,pictures,etc.,wesimplyprojectthe
(cid:14)
withtheemergenceof360 images,ithasbeentheroomlayoutes- objectmasktoitspositioninthewallinthe3Droomlayout.For
timationproblem[14],[15],[23],[16],[24].Intheseworks,from cuboidobjectslikebeds,sofasorbedsidetables,wecanobtain
569
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 15:46:35 UTC from IEEE Xplore.  Restrictions apply. TRAIN TEST TRAIN TEST
mAPw mAPw meanIoU meanIoU
wall Fromscratch 0.911 0.468 0.872 0.419
wall ImageNet(ResNet) 0.896 0.479 0.788 0.432
SUNRGB-D 0.728 0.516 0.742 0.461
TABLEI:Effectofinitialization:ResultstestedonoriginalBlitzNetwith
differentweightsinitializations.NoticehowinitializingwithSUNRGB-D
ﬂoor
weightsgivesclearlybettertestresults.Evaluationontrainsetisshownto
ﬂoor
observetheoverfittingeffect,speciallyclearintrainedfromscratchmodel.
Fig.3:Frommaskto3D:Wefindthelinesthatbestfittheobjectmaskby mAPw mAP meanIoU
aRANSACalgorithmandorientthemaccordinglytothemaindirections BlitzNet 0.516 0.688 0.461
ofthescene.Theobjectdimensionsareobtainedtrustinginthelineresting PanoramicBlitzNet 0.632 0.768 0.530
onthewallandthelinerestingonthefloor(redlinesinthefigure).
TABLEII:EffectofadaptingtheCNNforpanoramas:Comparison
betweenresultsonpanoramicimageswithBlitzNetvs.ourproposed
PanoramicBlitzNet.
theobjectdimensions(cid:150)i.e.length,widthandheight,usingthe
linesinteractingwiththeroomplanes.Figure3showshowwe
dealwiththeseobjects.Withtheseideas,wecanplacemostofthe classesweuseaweightedmeanasdefinedinequation1,beingM
objectsinsidethe3Dsceneandobtainagoodunderstandingofthe thenumberofclasses,AP theaverageprecisionperclass,d the
i i
scenefromthe2Dsegmentationmasks.Somequalitativeresults numberofdetectionsofclassiandnthetotalnumberofdetections.
areshowninFigure4aswellasintheteaserimage,Figure1. Itisconsideredasamorerepresentativemetricbecauseresultscal-
V. EXPERIMENTS culatedfromobjectswithaminimumnumberofsamplesinthetest
setshouldbelesssignificantwhenanalyzingaglobalperformance.
We evaluate our model by different experiments conducted
Segmentationperformanceismeasuredwithmeanintersection
onSUN360[19]extendeddataset,whicharepresentedinthis
section. Experimental setup is explained in order to make our overunion(mIoU),asstatedinequation2,beingAi thearea
workreproducible,togetherwiththedetailedevaluationmetrics. formedbyallpixelsofclassiinthegroundtruthsegmentation
Experimentalsetup: ThewholemodeliscodedinPython3.5 map,andA^iXinthepredictedsegmentationmXap.
usingtheframeworkTensorflowv1.13.1.Allexperimentswere M d 1 M A \A^
conAdsucitned[1o]n,awseingulseeNRviedsiNaeGt-e5F0oracseGfeTaXtur1e08e0xtGraPcUto.r, Adam mAP=i=1 niAPi (1) mIoU=Mi=1Aii[A^ii (2)
stochasticalgorithm[26]foroptimizationandlearningrateset A. Initialization
(cid:0)
to10 4anddecreasedtwiceduringtraining.Experimentswere
First experiment was conducted before the development of
conducted by changing that learning rate without noticeable
our model, to verify the importance of pre-training. It uses
influence.Weusestride4inthelastlayeroftheup-scalingstream
original BlitzNet300 architecture, without modifying the base
andvaryingmini-batchsizes,whicharestatedineachexperiment.
implementationtoadaptforpanoramas.Batchsizeissetto16
Allmodelsaretraineduntilconvergence,measuredwitharandom
anditistraineduntilconvergence.Whentraining,threedifferent
validationsubset.
initializations are executed to compare: random initialization
Basedonourdatasetcharacteristics,wedecidetopre-trainour
(trainedallfromscratch),ImageNetinitializationofthefeature
networkinsteadofinitializingitrandomly,thatwouldconducttoa
extractor (ResNet) and pre-trained SUN RGB-D initialization
clearoverfittoourdata,asstudiedinthefirstexperiment.Because
ofthecompletemodel.ResultsarecomparedinTableI,which
ofthelackofmassivepanoramicdatasets,conventionalimagesare
showsthatpre-trainingwithSUNRGB-Dfollowedbyfine-tuning
usedforpre-training:Firstly,weusethepubliclyavailableweights
the complete network with panoramic dataset, gives the best
ofResNet50backboneonImageNet[27]dataset.Withthatini-
performanceresultsandgeneralizesbetter.ImageNetinitialization
tialization,wethentrainthewholenetworkonSUNRGB-D[22],
convergesfaster,butitdemonstrateshigheroverfittingthanSUN
pre-processedtohavethesamecommonclasses.Thiswaywehave
RGB-Dpre-training.Thisexperimentshowsthatgivenarelatively
aninitializationforthecompletemodeltobeabletofine-tunewith
smallpanoramicdataset,theuseofamassiveoneofconventional
thepanoramicimages,possiblethankstoaFullyConvolutionalnet-
imagesforpre-trainingallowsthenetworktolearnhigherlevel
workwhereweightscanbesharedwithvariableinputdimensions.
characteristicsoftheobjects,avoidingoverfittingandbeingone
Evaluationmetrics: Toevaluatedetectionperformanceweuse
ofthekeysforthesuccessofthesystem.
typicalmeanaverageprecision(mAP),consideringthatapre-
B. Squarevs.Panoramic
dictedboundingboxiscorrectifitsintersectionoverunionwiththe
groundtruthishigherthan0.3,asexactlocalizationisbetterpre- ThisexperimentcomparestheperformanceofourPanoramic
dictedinthesegmentationbranch.Theaverageprecisionevaluates BlitzNetnetworkwiththeoriginalmodeldesignedforconventional
interpolatedprecisionatalldifferentrecalllevels,initssimplest images. In this case, batch size is reduced to 4, for memory
definition,butcanalsobewidelyfoundinliteratureasweighted limitationsinourGPU.AsseeninTableII,ouradaptedmodel
by the recall area that they represent. In this paper we mostly demonstrates clearly better results, improving performance by
usesimpleAP andwhenusingthesecondversionitisreferred awidemarginandsupportingourassumptionoftheimportant
asAPw.Finally,whencalculatingthemeanamongalldifferent benefitsofaconcretemodeldesignedforpanoramas.
570
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 15:46:35 UTC from IEEE Xplore.  Restrictions apply. Fig.4:Examplesof3Dmodelsobtainedfrominstanceobjectsmasksandroomlayoutknowledge[16].
input mIoU backgrd. bed painting table mirror windowcurtain chair light sofa door cabinet bedside tv shelf
Initialseg.map 53.0 90.7 61.7 32.1 75.2 42.3 55.8 54.0 55.1 31.4 34.6 63.6 48.5 40.7 52.2 57.4
Post-processedseg.map 53.1 90.7 63.3 30.1 75.0 41.5 56.7 55.3 55.5 34.0 31.8 62.9 48.8 40.2 53.5 57.5
TABLEIII:Semanticsegmentationresultsbeforeandafterapplyingtheinstancesegmentationpost-processing.Initialsemanticsegmentationis
takenfromourCNNoutput.
Apartfromavoidingdistortionsandcropstomakeitfittoa
square shape, it also shows the benefits of using a wider field
ofview,whichallowstoconsiderthewholecontextoftheroom.
Thisideaisastrongsupportforthepotentialofpanoramicimages,
notonlyinobjectrecognitionbutinmanyothervisualtasks,at
leastinthoserelatedtoindoorsceneunderstandingproblem.
C. StandardConvsvs.EquiConvs
Evaluationoftheinfluencethatequirectangularconvolutions
have on object recognition task is a key point in this work.
ComparisonofbothmodelscanbeseeninTablesIVandVand
Fig. 5: Instance segmentation post-processing results. Top is initial
Figure7.ThereweshowthatourmodelwithEquiConvs,adapting
semanticsegmentation(outputofCNN)andbottomisresultofpost-
the kernel to manage the equirectangular distortion, perform
processing.Noticethatapartfromcorrectlydifferentiateamonginstances
betterinbothobjectdetectionandsemanticsegmentationtasks. (highlightedinblue)itimprovesoriginalsegmentation(highlightedin
Additionally,equirectangularconvolutionshaveotheradvantages redandgreenforfailedandimprovedsegmentationrespectively).
over standard convolutions. They make our pre-training on
conventionalimagesmoremeaningful,asmanagingdistortionsby Before correction After correction
thekernelallowstosharetheweightsasiftheyweretrainedon
thesamekindofimages.Therefore,westronglybelievethatthis
typeofconvolutionshelpinavoidingoverfittingtotrainingdata,
whichduetotheparticularitiesoftheSUN360dataset(camera
pose does not vary and scenes are relatively similar) does not
drasticallydamagetestresults,butwillprobablybecrucialwhen
working on different datasets. Finally, EquiConvs also prove
tomakedetectionswithhigherconfidenceas,whenraisingthe Fig.6:Aftercombiningtheroomlayoutwithoursegmentationmasks,
themodelexperiencesaclearimprovementasawhole.However,here
confidencethresholdto0:95,theirrecallismaintainedover40%
wewanttoshowafailurecasewhere,whenassumingthatdoorsmust
comparedto28%achievedwithstandardconvolutions.Further reachthefloor,wemayhaveoverlappingwithotheroccludingobjects
qualitativeresultscanbefoundinourprojectwebpage1. intheimage,damagingsegmentationresultsbutimprovingthedoor3D
localization.
D. Instancesegmentationpost-processing
In this experiment we compare the semantic segmentation
output of the network with the result of applying our instance However,qualitativeresultssupportourintuitiveideabyshowing
segmentationmethodtocreateimprovedsemanticsegmentation someclearlyimprovingcasesthatareremarkedinFigure5.
maps.Althoughthisisnottheobjectiveofthepost-processing(the Ourapproachprovestoworkwellonseveraldifferentscenes
goalistodifferentiateamongdifferentinstances),itisevaluatedto by correctly separating same category objects, that initially
provetheinfluencethatitcanhaveinsegmentationperformance. overlappedinsemanticmaps,intodifferentinstances.Limitations
Ourintuitionwasthattheinstancesegmentationmethodwould ofthemethodcanbeseenwhenthenetworkfailsdetectingan
implyanimprovementtotheinitialsegmentationmapsbecause object,whichisthereforenotdifferentiatedasaninstanceonthe
itgiveshigherconfidencetodetections,whoseperformanceis finalmapandwhenmanagingobjectswithcomplexshapesthat
clearly higher than segmentation’s one in our model. Results, cannotbemodelledwithagaussiandistribution.
showninTableIII,areverysimilarandleadustoconcludethat Here,wefinallyanalyzetheimprovementoveroursegmen-
thepost-processingdoesnotprovetobeinfluentialinthisway. tation masks by leveraging the contextual information of the
571
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 15:46:35 UTC from IEEE Xplore.  Restrictions apply. model mAP bed painting table mirror windowcurtain chair light sofa door cabinet bedside tv shelf
(cid:3)
DPM 29.4 35.2 56.0 21.6 19.2 21.8 29.5 26.0 (cid:151) 22.2 31.9 (cid:151) (cid:151) 31.0 (cid:151)
(cid:3)
Dengetal. 68.7 76.3 68.0 73.6 58.7 62.6 69.5 68.0 (cid:151) 72.5 67.3 (cid:151) (cid:151) 70.0 (cid:151)
PanoBlitzNetStdConvs 76.8 94.9 85.0 83.3 71.9 72.2 72.2 71.9 35.0 89.3 75.5 57.9 87.9 91.1 30.5
PanoBlitzNetEquiConvs 77.8 95.3 83.9 82.1 76.2 70.9 75.9 80.9 41.0 85.4 72.5 55.6 91.4 93.3 40.2
TABLE IV: Object detection results on SUN360 test set with our method Panoramic BlitzNet using different convolutions (Standard vs.
(cid:3)
Equirectangular). Resultstrainedandevaluatedonacombinationofdatasets(includingSUN360)byDengetal.[10]
model mIoU backgrd. bed painting table mirror windowcurtain chair light sofa door cabinet bedside tv shelf
PanoContext 37.5 86.9 78.6 38.7 29.6 38.2 35.6 (cid:151) 09.6 (cid:151) 11.1 19.4 27.4 39.7 34.8 (cid:151)
PanoBlitzNetStdConvs 53.0 90.7 61.7 32.1 75.2 42.3 55.8 54.0 55.1 31.4 34.6 63.6 48.5 40.7 52.2 57.4
PanoBlitzNetEquiConvs 54.4 91.3 62.1 61.2 72.3 41.1 53.4 53.7 55.2 26.5 32.9 63.8 51.1 36.6 52.3 61.9
TABLEV:SemanticsegmentationresultsonSUN360extendedtestsetwithourproposedmodelPanoramicBlitzNetusingdifferentconvolutions
(Standardvs.Equirectangular)andcomparisonwithPanoContext[12].
Panoramic	BlitzNet	StdConvs Panoramic	BlitzNet	EquiConvs
mAP=77.8%. For completeness, we include here the results
of [10], recent work on indoor panoramic object recognition
with deep learning, together with their evaluation of DPM on
panoramas. Our method achieves the best results in detection
forall10commonclassescomparedtothem.Itisworthnoting
(cid:24)
thatourapproachachievestheseresultsjusttrainingwith 400
panoramasfromtheSUN360datasetwhiletheyuseadditional
panoramastotraintheirmodel.Sincetheirdatasetisnotpublicand
nocodeisavailable,wereportdirectlytheresultscollectedin[10].
Segmentation:TableVsummarizesthesemanticsegmentation
resultsontheSUN360extendeddataset.Adirectcomparisonis
possiblewiththeworkofPanoContext[12].Theresultsclearly
show that our method significantly improves over the state of
theart.Inparticular,weaddthreenewobjectclassesandboost
mIoU=54.4%,whichrepresentsanimprovementof16:9%over
PanoContext’smethod.
Fig. 7: Qualitative evaluation of object detection and semantic VI. CONCLUSION
segmentation:ExamplesofresultsobtainedwithourPanoramicBlitzNet
From a single panoramic image, we propose a method that
usingbothstandardconvolutionsandEquiConvs[16].
provides a complete understanding of the main objects in an
indoor scene. By managing the inherent characteristics and
challengesthatequirectangularpanoramasinvolve,weoutperform
roomlayout.Inourexperiment,logicalassumptionsusedforthis
stateoftheartinadditiontocreatingamorecompletesystem,
refinemententailasignificantimprovementofupto7:2%mIoU
whichnotonlyobtains2Ddetectionandpixel-wisesegmentation
withrespecttothesegmentationoutputofPanoramicBlitzNetwith
ofobjectsbutalsoplacesthemintoa3Dreconstructionofthe
StdConvs,achievingafinalmIoU=60.3%.Itshouldbenoted
room.Exploitingtheadvantagesofhavingawiderfieldofview
thattheclassesthatcontributemosttothisimprovementaremirror,
inindoorenvironments,thisvisualsystembecomesapromising
windowandpainting.However,whileonewouldalsoexpecta
keyelementforfutureautonomousmobilerobots.Futurework
clearimprovementinthedoorcategory,wehaveseenadropin
includestheinclusionofinstancesegmentationpredictionsinto
performanceinsomecasessuchastheoneshowninFigure6,
thedeeplearningpipelineandafurtherstudyofthepotentialin
althoughitdefinitelyhasapositiveeffectonitslocationinthe3D
combininglayoutrecoveryandobjectrecognitiontasks.
roomspace.Asalreadysupportedbythispreliminaryexperiment,
weproposeapromisingmethodtonoticeablybenefit2Dand3D ACKNOWLEDGMENT
objectrecognitiontasksfromroomlayoutknowledge,andencour-
ThisworkwassupportedbyProjectsRTI2018-096903-B-I00
agetheideathatitisworthcontinuingtoworkinthisdirection.
(AEI/FEDER,UE),theRegionalCouncilofBourgogneFranche-
E. ComparisonwiththeStateoftheArt Comte(2017-9201AAO048S01342)andMobilityCity.
Detection: In Table IV we show our detection results on
the SUN360 extended dataset. Our Panoramic BliztNet with
EquiConvs achieves very satisfactory results, with a global
572
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 15:46:35 UTC from IEEE Xplore.  Restrictions apply. REFERENCES [17] Y.Yang,S.Jin,R.Liu,S.BingKang,andJ.Yu,(cid:147)Automatic3dindoorscene
modelingfromsinglepanorama,(cid:148)inTheIEEEConferenceonComputer
[1] N.Dvornik,K.Shmelkov,J.Mairal,andC.Schmid,(cid:147)Blitznet:Areal-time VisionandPatternRecognition(CVPR),June2018.
deepnetworkforsceneunderstanding,(cid:148)inICCV,2017,pp.4154(cid:150)4162. [18] R.Lukierski,S.Leutenegger,andA.J.Davison,(cid:147)Roomlayoutestimation
[2] R.Girshick,J.Donahue,T.Darrell,andJ.Malik,(cid:147)Richfeaturehierarchies fromrapidomnidirectionalexploration,(cid:148)in2017IEEEInternationalCon-
foraccurateobjectdetectionandsemanticsegmentation,(cid:148)inProceedings ferenceonRoboticsandAutomation(ICRA). IEEE,2017,pp.6315(cid:150)6322.
oftheIEEEconferenceoncomputervisionandpatternrecognition,2014, [19] A.O.J.Xiao,K.A.EhingerandA.Torralba,(cid:147)Recognizingsceneviewpoint
pp.580(cid:150)587. usingpanoramicplacerepresentation,(cid:148)Proceedingsof25thIEEEConference
[3] R. Girshick, (cid:147)Fast r-cnn,(cid:148) in Proceedings of the IEEE international onComputerVisionandPatternRecognition,2012.
conferenceoncomputervision,2015,pp.1440(cid:150)1448. [20] S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and T. Funkhouser,
[4] S.Ren,K.He,R.Girshick,andJ.Sun,(cid:147)Fasterr-cnn:Towardsreal-time (cid:147)Semanticscenecompletionfromasingledepthimage,(cid:148)Proceedingsof30th
objectdetectionwithregionproposalnetworks,(cid:148)inAdvancesinneural IEEEConferenceonComputerVisionandPatternRecognition,2017.
informationprocessingsystems,2015,pp.91(cid:150)99. [21] I.Armeni,S.Sax,A.R.Zamir,andS.Savarese,(cid:147)Joint2d-3d-semanticdata
[5] K.He,G.Gkioxari,P.Dolla·r,andR.Girshick,(cid:147)Maskr-cnn,(cid:148)inProceedings forindoorsceneunderstanding,(cid:148)arXivpreprintarXiv:1702.01105,2017.
of the IEEE international conference on computer vision, 2017, pp. [22] S.L.S.SongandJ.Xiao.,(cid:147)SUNRGB-D:ARGB-DSceneUnderstanding
2961(cid:150)2969. BenchmarkSuite,(cid:148)Proceedingsof28thIEEEConferenceonComputer
[6] J. Redmon and A. Farhadi, (cid:147)Yolo9000: better, faster, stronger,(cid:148) in VisionandPatternRecognition(CVPR),2015.
Proceedings of the IEEE conference on computer vision and pattern [23] C.Fernandez-Labrador,A.Perez-Yus,G.Lopez-Nicolas,andJ.J.Guerrero,
recognition,2017,pp.7263(cid:150)7271. (cid:147)Layoutsfrompanoramicimageswithgeometryanddeeplearning,(cid:148)IEEE
[7] W.Liu,D.Anguelov,D.Erhan,C.Szegedy,S.Reed,C.-Y.Fu,andA.C. RoboticsandAutomationLetters,vol.3,no.4,pp.3153(cid:150)3160,2018.
Berg,(cid:147)Ssd:Singleshotmultiboxdetector,(cid:148)inEuropeanconferenceon [24] C. Sun, C.-W. Hsiao, M. Sun, and H.-T. Chen, (cid:147)Horizonnet: Learning
computervision. Springer,2016,pp.21(cid:150)37. roomlayoutwith1drepresentationandpanostretchdataaugmentation,(cid:148)
inProceedingsoftheIEEEConferenceonComputerVisionandPattern
[8] X. Meng, X. Zhang, K. Yan, and H. Zhang, (cid:147)Real-time detection and
Recognition,2019,pp.1047(cid:150)1056.
recognitionoflivepanoramictrafficsignsbasedondeeplearning,(cid:148)in2018
[25] J.M.CoughlanandA.L.Yuille,(cid:147)Manhattanworld:Compassdirection
IEEE9thInternationalConferenceonSoftwareEngineeringandService
fromasingleimagebybayesianinference,(cid:148)inProceedingsoftheSeventh
Science(ICSESS). IEEE,2018,pp.584(cid:150)588.
IEEEInternationalConferenceonComputerVision,vol.2. IEEE,1999,
[9] W.Yang,Y.Qian,J.-K.Ka¤ma¤ra¤inen,F.Cricri,andL.Fan,(cid:147)Objectdetection
pp.941(cid:150)947.
inequirectangularpanorama,(cid:148)in201824thInternationalConferenceon
[26] D.P.KingmaandJ.Ba,(cid:147)Adam:Amethodforstochasticoptimization,(cid:148)
PatternRecognition(ICPR). IEEE,2018,pp.2190(cid:150)2195.
arXivpreprintarXiv:1412.6980,2014.
[10] F. Deng, X. Zhu, and J. Ren, (cid:147)Object detection on panoramic images
[27] O.Russakovsky,J.Deng,H.Su,J.Krause,S.Satheesh,S.Ma,Z.Huang,
basedondeeplearning,(cid:148)in20173rdInternationalConferenceonControl,
A.Karpathy,A.Khosla,M.Bernstein,A.C.Berg,andL.Fei-Fei,(cid:147)ImageNet
AutomationandRobotics(ICCAR). IEEE,2017,pp.375(cid:150)380.
Large Scale Visual Recognition Challenge,(cid:148) International Journal of
[11] D.A.Forsyth,(cid:147)Objectdetectionwithdiscriminativelytrainedpart-based
ComputerVision(IJCV),vol.115,no.3,pp.211(cid:150)252,2015.
models,(cid:148)IEEEComputer,vol.47,pp.6(cid:150)7,2014.
[12] Y.Zhang,S.Song,P.Tan,andJ.Xiao,(cid:147)Panocontext:Awhole-room3d
contextmodelforpanoramicsceneunderstanding,(cid:148)inEuropeanconference
oncomputervision. Springer,2014,pp.668(cid:150)686.
[13] G.Lo·pez-Nicola·s,J.Omedes,andJ.J.Guerrero,(cid:147)Spatiallayoutrecovery
fromasingleomnidirectionalimageanditsmatching-freesequentialprop-
agation,(cid:148)inRoboticsandAutonomousSystems,2014,pp.62(9):1271(cid:150)1281.
[14] H.JiaandS.Li,(cid:147)Estimatingstructureofindoorscenefromasinglefull-view
image,(cid:148)in2015IEEEInternationalConferenceonRoboticsandAutomation
(ICRA). IEEE,2015,pp.4851(cid:150)4858.
[15] C.Zou,A.Colburn,Q.Shan,andD.Hoiem,(cid:147)Layoutnet:Reconstructingthe
3droomlayoutfromasinglergbimage,(cid:148)inProceedingsIEEEConference
onComputerVisionandPatternRecognition,2018,pp.2051(cid:150)2059.
[16] C.Fernandez-Labrador,J.M.Facil,A.Perez-Yus,C.Demonceaux,J.Civera,
andJ.J.Guerrero,(cid:147)Cornersforlayout:End-to-endlayoutrecoveryfrom
360images,(cid:148)IEEERoboticsandAutomationLetters,5(2),pp:1255-1262,
April2020.
573
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 15:46:35 UTC from IEEE Xplore.  Restrictions apply. 