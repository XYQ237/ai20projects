2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
Center-of-Mass-based Robust Grasp Planning for Unknown Objects
Using Tactile-Visual Sensors
∗
Qian Feng1,3, Zhaopeng Chen1 ,2, Jun Deng1, Chunhui Gao1,
Jianwei Zhang2, and Alois Knoll3
Abstract—An unstable grasp pose can lead to slip, thus material in one object, are hard to grasp and manipulate
an unstable grasp pose can be predicted by slip detection. A because of uneven mass distributions towards simple robot
regraspisrequiredafterwardstocorrectthegraspposeinorder
end effectors for example a two-jaw gripper.
to ﬁnish the task. In this work, we propose a novel regrasp
A grasp is unstable if the grasp pose is not force-
planner with multi-sensor modules to plan grasp adjustments
withthefeedbackfromaslipdetector.Thenaregraspplanner closure/antipodal [18] or the grasp force is too small to
is trained to estimate the location of center of mass, which provide enough friction [16], resulting in insufﬁcient con-
helps robots ﬁnd an optimal grasp pose. The dataset in this tact or slip. Slip detection has been studied since the late
work consists of 1025 slip experiments and 1347 regrasps
1980s till now with either analytical methods [19]–[22] or
collected by one pair of tactile sensors, an RGB-D camera and
data-driven methods [23]–[27]. With the feedback of slip
oneFrankaEmikarobotarmequippedwithjointforce/torque
sensors.Weshowthatouralgorithmcansuccessfullydetectand detection, regrasp can be planned accordingly by applying
classify the slip for 5 unknown test objects with an accuracy more grasp force [16], [28], when the grasp pose is stable
of 76.88% and a regrasp planner increases the grasp success and the contact situation is unchanged. Or with a grasp
rate by 31.0% compared to the state-of-the-art vision-based
stability estimator, regrasp can be chosen by scoring the
grasping algorithm.
regrasp poses [11], [29]. Regrasp planning using feedback
of slip detection has advantages of regulating grasp forces
I. INTRODUCTION
and more robustness by identifying a bad grasp with a slip
Robotic grasping and manipulating of unknown objects
detector instead of pure information from touch. To the best
bring enormous proﬁt to human society. Leveraging the
ofourknowledge,sofar,thereisnoregraspplannerwhichis
fast advances in deep learning and computer vision, robotic
based on the slip detection to plan a new stable grasp pose.
grasping draws increasing interests from industries, e.g., the
In this paper we propose a novel center-of-mass-based
AmazonRoboticChallenges[1].However,toobtainarobust
robotic grasping algorithm using tactile-visual sensors to
performance, vision based methods have some limitations.
grasp unknown objects. The grasp failures are found by slip
First, the currently proposed methods either learned from
detection and then a regrasp is planned to improve grasp
realistic data [1]–[5] or from synthetic data [6]–[8], only
stability. The proposed algorithm takes object shapes and
involve object shapes or geometries, ignoring many other
object mass into consideration using multi-sensor modules.
aspectssuchas materialpropertiesandobject mass.Second,
Weclosetheloopofroboticgraspingtomakeitmorerobust
the vision-only method is open-loop without information
withbetterperformance.WeemploySupportVectorMachine
of contacts with objects, thus the robustness is hard to
(SVM) and Long short-term Memory (LSTM) model to
guarantee.
extract the tactile features from tactile sensors to detect
In consequence, one sensory modality cannot provide
slip as well as to plan regrasp, and we collect a dataset of
enough context to plan a robust grasp. Several novel fu-
1039 slips and 1347 regrasps from 19 experimental objects.
sion methods of multi-sensor modules are proposed, to
To the best of our knowledge, this is the ﬁrst work that
learn to detect object geometries [9], to help robotic grasp
utilizes multi-sensor modules to plan regrasp based on slip
planning [10]–[12], to execute manipulation tasks like peg
prediction.
insertion[13].Hereweproposeanovelalgorithmwithfusion
We list the main contributions of our paper as follows:
of multi-sensor modules for unknown objects.
1) A novel center-of-mass-based robust grasp planning
Meanwhile, grasping and manipulating unknown objects
to grasp unknown objects. Then to close the loop of
are essential for service robots in the future to execute
grasping, a novel slip detection method is proposed
challenging tasks. For object manipulation, a stable grasp
and a novel regrasp planner is trained to estimate the
is prerequisite but grasping objects with uncertainties of
grasp stability by estimating center of mass.
surface textures [14], [15] and center of mass [16], [17], are
2) Amulti-modalroboticgraspingdatasetcontainstactile
still challenging for robotic systems, such as some common
sensors and RGB-D images for daily objects.
tools,e.g.,axesandhammerswhichcombinemorethanone
3) The proposed algorithm is proved to be feasible to
{ grasp daily objects with good performance, even for
ju1nA.gdileengR,obcohtsunhAuGi,.gaGoil}ch@ianggilGee-rrmoanbyo.ts.qcioamn.feng, a low-resolution tactile sensor with only 4×4 tactile
2TAMS (Technical Aspects of Multimodal Systems), Department of taxels.
Informatics,Universita¨tHamburg
3Chair for Robotics and Embedded Systems, Technische Universita¨t II. RELATEDWORK
Mu¨nchen,Germany.knoll,qian.feng @tum.de
Recently grasp planning has been extensively researched
*Corresponding author to provide e-mail: zhaopeng.chen@agile-
robots.com in the past years in the robotic ﬁeld [3], [10], [30]–[33].
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 610
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 15:47:05 UTC from IEEE Xplore.  Restrictions apply. Analytical methods. These methods require an explicit simulated and predicted. The proposed algorithm simpliﬁes
model of the objects and the robot kinematics. First, robots the regrasp actions only as translational action, which is not
need to “know” this object previously. To provide this feasible when the object has shape of e.g., cylinder. Further,
data, some precomputed databases such as Columbia Grasp anaction-conditionalmodelwithinputsofvision,tactileand
Database [34] of 3D objects have been established, where grasp action is proposed to plan the regrasp [11].
each object is labeled with grasp quality metrics such as All the proposed regrasping methods are based on the
graspwrenchspace(GWS)analysis[35].Afterwardsthebest tactile data collected by ”touching” the static object, which
grasp is chosen out of the precomputed grasp poses with the has limitations for certain objects e.g., tool objects, where
highest score according to the quality metric in [30]. Ana- the single tactile data may not be sufﬁcient for a robust
lytical method only works well if the real-world system ﬁts prediction.Ourproposedmethodscollecteddatabynotonly
the assumption of an analytic model without uncertainties, ”touching” but also ”lifting” the object.
such as unseen objects, un-structured environment. To solve more speciﬁc problems in the domain of robotic
Empirical methods with vision sensors.Givensufﬁcient grasping, a center-of-mass based grasp planning method is
data,empiricalmethodsaremorerobustagainstuncertainties proposedby[17].Usingaforce/torquesensoranda3Drange
during robotic grasping [36]. Recently empirical methods sensor, the regrasp pipeline towards center of mass follows
are able to predict grasp pose using RGB-D cameras by themeasurementoftorqueusingahumanoidhand.Similarly,
eitherend-to-endlearningmethods[1],[3],[5],[31],[37]or aregrasppolicybyaddingforcein[16]isintroducedtograsp
learning a grasp pose scoring function [6], [30], [32], [38]. objectswithdynamiccenterofmassbutonlyforceinsteadof
Empirical methods with tactile sensors. Tactile sensing the grasp pose needs to be adjusted. In advance, we design
inroboticgraspingcanbeusedtodetectobjectmaterial[39], a regrasp planner which predicts the grasp adaption based
estimate force [11], detect slip [24], [26] and estimate grasp on slip detection using two-jaw gripper where objects are
stability. Considering the limitations of vision, researchers previously unknown.
arealsodiscoveringhowmuchatactilesensorcancontribute
to the grasping performance in combination with vision. III. PROBLEMSTATEMENT
Since accurate modeling of contact physics is complex and We attempt to solve the problem of planning a stable
parameter-dependent, analytical methods [19], [21], [40] are grasp pose with a parallel-jaw gripper for unknown objects
notabletogeneralizetounknownobjects.Thetactilesensor laying with unknown mass distribution on the table inside
can contribute to the grasping performance in two ways, the robot workspace, using a commercial RGB-D camera,
either using the data collected during closing the robot end tactile sensors and force/torque sensors.
effector to estimate grasp stability [10], [33], [41] or using
the data to detect slip [19], [21], [22], [26], [42].
Camera Start
Slip detection. Analytical slip detection methods detect
thederivativeofthenormalforcein[19],[21],[40],estimate Depth
the ratio between friction and the normal force according to
Segmentation
the Coulomb friction model by [19], [21], [22] as well as
analyze vibrations features [19]–[21]. Torque
Empirical methods become more popular in research be- Antipodal Grasp  Sensors
cause they can handle uncertainties in the environment and Sampler
can be generalized to unknown objects. Machine learning Initial grasp New grasp  Regrasp 
methodsareusede.g.,HiddenMarkovModels(HMM)[23],
Planner
SVM [25] and Random Forest [24], deeper architecture
Inverse Kinematic
of network e.g., convolutional network by treating tactile
Solver
feedback as images [42] and LSTM [26]. In most literature, Yes
the experiment regarding slip detection is conducted to keep
either the object or the robot end effector static except [25], Robot Control Slip Detector
which is not the typical scenarios for robot grasping appli-
cation. No Slip
Meanwhile, all methods mentioned above to predict grasp
Stable 
stability are still open-loop. Without a regrasp planner, the
Grasp
grasping process has to iteratively ”touch” objects until a
stable enough grasp pose is found.
Regrasp.Oneideaoftactilebasedregraspingistodirectly Fig.1. Thewholepipelineoftheproposedmethod.
apply more force when the slip is detected only under IV. METHODS
the assumption that the grasp pose is correct. There are The proposed grasping method is illustrated in Figure 1.
some model-based methods to adjust the grasping force
A. Antipodal Grasp Sampler
considering the feedback of tactile sensing [12], [16], [28],
[43]. The object is segmented out in a depth image using
A more common case for regrasping is the unstable grasp RANSAC plane segmentation method implemented in Point
pose. In this case, the ﬁrst step is to correct the grasp pose. CloudLibrary(PCL)[44].Afterwardswiththeobjectbound-
The regrasp action is assumed as a translational action [29] ary, an antipodal grasp sampler is implemented to sample
and the tactile imprints after the regrasp action can be poses out of the object boundary and identify if the grasp
611
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 15:47:05 UTC from IEEE Xplore.  Restrictions apply. ×
Fig.2. Thestructureoftheslipdetectionmodel.Theinputtactiledataisvisualizedwitha4 4GUIexhibitingthecolorintensitybetweenblackand
white.ThenthesequentialtactiledataXnT isprocessedwith2classiﬁersbasedonSVMandLSTMmodeltopredicttheslip.ForSVM-basedclassiﬁer,
thesequentialdataisﬁrstﬂattenedinaone-dimensionalvectorandthenclassiﬁedwithSVMclassiﬁer.ForLSTM-basedclassiﬁer,thesequentialdatais
processed with a LSTM layer of 75 memory cells. The extracted features are ﬁrst fed into an average-pooling layer and then classiﬁed with two Fully
Connected(FC)layers.
poseisforce-closure[18].Thefrictioncoefﬁcientisassumed more hyperparameters. Additionally, in [46] the sigmoid
started with 0 and then it increases step by step to 1 with kernel has very similar behavior like RBF kernel for certain
interval of 0.2 until a certain number of grasp samples are parameters.
found. The depth value “z” of grasp pose is also sampled 2) LSTM: Long short-term memory (LSTM) is one re-
between the depth of table and the depth of the grasp center current neural network which has shown great performance
point.Afterchoosinganinitialgrasppose,therobotexecutes in processing sequential data, for example speech recogni-
the pose using an inverse kinematic solver and robot joint tion [47], [48] or video recognition [49].
motion control with the pipeline shown in Figure 1. The main problems of recurrent neural networks are the
gradientvanishandgradientexplosionproblem[50].During
B. Slip Detector
the backpropagation, the gradient ﬂow could vanish if it
·
We attempt to learn a classiﬁer f() named slip detec- is frequently multiplied by a small value and it could also
{
tor, given the n tactile sensor data sequence Xn : explodeifoftenmultipliedbyalargevalue.Thestructureof
}th T
xn ,xn ,...xn withavariablelengthT,whichiscollected LSTM model with three gate units could protect the access
1 2 T
during lifting the objects. The slip is classiﬁed with four totheerrorﬂow,toenforcetheerrorﬂowconstant.Therefore
{
classes s :“no slip”, s :“clockwise rotational slip”, we can keep the value of gradient in a proper range to avoid
no cw
s :“counterclockwise rotational slip”, s :“translational the problem of vanishing or exploding gradients [51].
ccw} tra
slip” , the ﬁrst case is detected by determining if the object Considering that the input tactile sequences may have
is still in contact with the tactile sensor and the other three variable lengths thus we resize all the sequences with the
cases are predicted by our classiﬁer. same length and then we employ a Masking layer from
Tensorﬂow Keras before LSTM layers to mask those time
s =f(Xn ), (1)
t T steps for all downstream layers. The structure follows the
∈
where s [s ,s ,s ,s ]. pipeline shown in Figure 2.
t no cw ccw tra
We comparethe performance of twoclassiﬁcation models
to predict the slip. C. Learning a regrasp planner
1) Support Vector Machine: For the classiﬁcation tasks,
We attempt to utilize multiple sensor modules such as
Support Vector Machine (SVM) ﬁnds a decision func-
tactile sensors and torque sensors mounted on the robot arm
tion efﬁciently with the maximal margin [45]. Given
to learn the regrasp. The unstable grasps in our experiment
the training set of T examples x with labels y :
i i often lead to rotational slip because of too much torque
(x ,y ),(x ,y ),...,(x ,y ), where y = 1 means x
1 1 2 2 T T − k k generated from gravity. In other word, the grasp pose is
belongs to class A and(cid:88)yk = 1 stands for class B. The far away from the center of mass. And the regrasp planner
algorithm ﬁnds the decision function f(x) [45]:
attempts to predict a stable grasp pose which is as close as
T possible to the center of mass.
f(x)= w K(x ,x)+b. (2)
k k The regrasp action is formalized as an one dimensional
k=1 adjustment. We use a variable µ to deﬁne the location of
InEquation(2)w istheweight,K isthekernelfunction,b center of mass (i.e., the regrasp pose) based on the current
k
is the bias and x is a support vector. For 3-class prediction grasp pose c and the object boundary point a visualized
k (cid:48)
inourcase,SVMdrawsdecisionfunctionbetweeneachpair in Figure 3. The object boundary points a and a can be
ofclassessointotal3.Herewecomparetwokernelsnamely obtainedbyﬁndingthepointsofintersectionfromtheobject
linear kernel and Radial Basis Function(RBF) kernel. boundary and the grasp normal which is depicted as dashed
Other kernels such as polynomial and sigmoid kernels line in Figure 3. One of the object boundary points will be
are less applied considering their computational reasons and chosen according to the rotational slip.
612
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 15:47:05 UTC from IEEE Xplore.  Restrictions apply. UDP  Sensor Housing
a Franka Gripper
b
x
c
a'
y
Silicon with 
Tactile Sensor 4x4 taxels 
Fig. 3. Given the current grasp pose c, the object boundary point a, the
regraspposeislocatedwithavariablecalledregraspratioµ.Regardingthe Fig. 5. The Franka gripper and the tactile sensors employed for our
point a as the reference point, the distance between a and c is y and the experiment.
distancebetweenaandbisx.Thustheregraspratioµ= x.
y laptopinstalledwithUbuntu18.04witha4.1GHzIntelCore
i7-8750H 6-Core CPU and an NVIDIA GeForce GT 1060
graphic card.
TheFrankaparallel-jawgripperinFigure5hasonedegree
offreedomwithanadjustablegraspof0.0cmto8.0cm.The
force applied on gripper can be controlled in the range of
−
20 100N. The gripper during experiment is controlled to
close until the required force is reached.
AcommercialRGB-DcameraRealsenseD415ismounted
at a height of 80cm above the table and we capture depth
images to sample the initial grasp poses.
B. Tactile and Torque Sensors
The tactile sensor in Figure 5 provided by the Kinﬁnity
UG1 is a 3D-printed low-resolution pressure tactile sensor
×
with 4 4 resolution, which provides the information about
Tactile Data pressure distribution on the contact surface. It consists of a
×
blue silicon cover and 4 4 tactile taxels embedded inside,
×
where each taxel has a spatial dimension of 4 4mm.
Fig. 4. The structure of the regrasp planner. The model takes the tactile
The sensor output is transmitted through User Datagram
sequential data, torque/force data, regrasp ratio and grasp force as inputs
anditoutputsthegrasprobustnessofthenewgraspposelocatedaccording Protocol (UDP) protocol via Ethernet cable. An inﬁnite
toregraspratio. impulse response (IIR) digital ﬁlter is designed to remove
ThewholeregraspplannermodelisillustratedinFigure4. high-frequency noise in tactile data, which also are capable
The ﬁrst input is tactile sequential data with 32 features for real-time applications.
from 2 tactile sensors. The second input is also a sequential The Franka Emika Panda platform uses 7-DOF joint
data of external forces and torques in 6-DOFs end effector torque sensors. The force/torque data in end effector frame
frame. The third input is one scalar variable, the regrasp is obtained by applying Jacobian matrix transformation.
The data is streamed at a frequency of 1kHz and with
ratio described in Figure 3. The fourth input is also a scalar
ﬁltering we collect the data at a frequency of 50Hz.
variable, the adjustment of grasp force.
We apply the LSTM model again to process the tactile C. Experimental objects
sequentialdataandthetorque/forcedataforregraspplanner.
The training dataset contains daily tool objects such as
For the third and the forth scalar inputs, we simply add
axes and hammers, box-like objects and also a modularized
one dense layer to enlarge the output size. Afterwards the
object, considering the limitations of the experimental hard-
two LSTM layers and two dense layers get concatenated in
ware and software.
the third dense layer along the axis of features. The whole
Withthelimitednumberofexperimentalobjects,wekeep
features are classiﬁed with the following two dense layers
the material of objects same by adding extra weight to
and ﬁnally the grasp robustness is estimated.
adjust the locations of center of mass as well as the object
With a learned regrasp planner, we sample the grasp ratio
mass, such as the different conﬁgurations for LEGO model
and choose one with the highest predicted grasp robustness.
in Figure 6(c).
Given the object boundary point a and the ﬁrst grasp pose In total, there are 6 conﬁgurations for training, namely
c, we can obtain the regrasp pose with equation : (0,0), (120,0), (120,120), (240,0), (240,120)g. Addition-
− ∗ ally, a conﬁguration of extra weights on left and right sides
pose =pose +(pose pose ) µ. (3)
b a c a with (60,300)g is used for the test phase.
V. EXPERIMENTS In summary, we collect totally 19 objects (originally 13
objects with different extra weights) including 12 objects of
A. Experimental setup
dailyuseandamodularizedobjectmadeofLEGOmodelto
The robotic grasping experiment is conducted using a
generate7differentconﬁgurations.Wesplitthose19objects
7-DOFs Franka Panda robot arm, equipped with a Franka
into 14 training objects and 5 test objects.
parallel-jawgripper.Thegripperismountedwithtwotactile
sensors on the ﬁngertip. The experiments are running on a 1http://kinfinity-solutions.com
613
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 15:47:05 UTC from IEEE Xplore.  Restrictions apply. 1.0
sno 0.90 0.05 0.05 0.67 0.10 0.24 0.8
el
uelab scw 0.13 0.78 0.09 0.04 0.83 0.13 00..46
Tr
sccw 0.05 0.05 0.90 0.00 0.00 1.00 0.2
0.0
(a) Theexperimentalobjects (b) Thetestedobjects. sno scw sccw sno scw sccw
Predictedlabel Predictedlabel
ofLinearSVM ofRBFSVM
Weights `````` Weights 1.0
sno 0.81 0.05 0.14 0.8
bel 0.6
ela scw 0.04 0.70 0.26 0.4
0g 120g Trusccw 0.10 0.25 0.65 0.2
(c) The“open”LEGO (d) Th“closed”LEGO. 0.0
sno scw sccw
Fig. 6. All objects used for our experiment. The objects are collected Predictedlabel
from tools of daily use, box-like objects and a modularized object made ofLSTM
of LEGO. The modularized object allows us to load extra weights on the
two sides. For example in (c) the weights on left and right side are 0g Fig. 7. Comparison of three classiﬁers using confusion matrix. All
and120g,or(0,120g.The”closed”LEGOin(d)showsnodifferencesin predictionaccuracyisnormalizedforeachclass.ThelinearSVMclassiﬁer
visual sensor but it may has totally different center of mass. Same works hasthebestvalidationresults.
forbox-likeobjects.
74.9%). For the classiﬁcation accuracy of each label, linear
D. Data collection SVM and RBF kernel SVM both achieve best classiﬁcation
Anautomaticdatacollectionprocessisdesignedtoobtain performance on the “counterclockwise slip” class each with
training data. The robot will have a ﬁrst trial to grasp the an accuracy of 90.0% and 100.0% respectively. Meanwhile
object and lift it to the constant height of 10cm. Then a LSTM model relatively is better at detecting stable grasp of
random regrasp ratio µ is chosen to deﬁne the regrasp pose the class “no slip” with a 81% accuracy.
as illustrated in Figure 3. Linear SVM performs slightly better than RBF kernel
After two grasp trials the robot will change the pose of SVM. Since we ﬂatten the tactile sequential data into an
grasped object to another random pose and then release it one-dimensional vector, the feature size increases from “16”
×
on the table for the next experiment. to “sample length 16”. In case where the feature size is
In the experiment there are only 14 samples labeled with larger than the instances, RBF kernel could not outperform
“translational slip” out of 1039 samples so we will ignore linear kernel because there is no need to project the data
this case in the result. Totally we collect 1039 grasps for the into a higher-dimensional feature space. Meanwhile, LSTM
slip detection from 12 objects and 1347 regrasps. classiﬁer does not perform as well as other classiﬁers,
Fortheregrasping,welabelthedatawith“1”iftheobject probably because of the limited dataset.
is grasped successfully without slip, otherwise we label the 1) Torque Sensors for Slip Detection: In Table II we
data with “0”. see that performance indeed drops signiﬁcantly when
force/torque data is involved. The possible reasons are on
E. Training
the one hand, torque sensors are known for inevitable noise
First, we preprocess the data with feature standardization and drift. On the other hand, the object can be grasped but
method.Then we split the training dataset objectwise ran- still lying on the table because of a rotational slip thus the
domlyintotrainingsetandvalidationsetwitharatioof5:1. external forces from gravity are relative small. Meanwhile
For the SVM based slip detector, using 5-fold cross with torque input, LSTM model outperforms other SVM
validation, the hyperparameter C for linear SVM and RBF modelswithmorethan10%accuracy.ItindicatesthatLSTM
kernelSVMarechosentobe1and1e3.Thehyperparameter model can process multi-sensor input modules and extract
γ in RBF kernel function. features better than SVM models.
The LSTM based slip detector has a batch size of 16, 75 2) Robot Grasp Evaluation: Afterwards we test our
−
LSTM memory cells, a learning rate of 1e 3 and a hidden trained classiﬁers on the robot towards novel test objects
layer size of 50. To avoid overﬁtting we deploy dropout and with the following test objects listed in Figure 6(b).
recurrent dropout [52] with rate of 0.2 for the LSTM model TheresultsfromonlinetestinTableIIIarequitesimilarto
meanwhile a dropout layer with rate of 0.5 after the hidden theofﬂineresults.LinearSVMoutperformsLSTMclassiﬁer
FC layer. We choose binary cross entropy method as loss with 10.10% but they have different performances on the
function and Adam optimizer as the optimization method. individual object. Linear SVM has better performance on
With similar hyperparameters, LSTM based regrasp planner mostobjectsthanLSTMclassiﬁerexcepthammerandLEGO
uses mean squared error as loss function. model.ThelinearSVMclassiﬁerwithbestperformancewill
be applied for our regrasp planner.
VI. RESULTS
A. Slip Detection B. Regrasp Planner
TheevaluationresultsareshowninFigure7.LinearSVM We evaluate the regrasp planner with different input mod-
outperforms other classiﬁers with an accuracy of 84.4% ules in Table IV. The result shows a better performance of
(RBF kernel SVM with 82.8% and LSTM classiﬁer with inputs with multi-sensor modules.
614
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 15:47:05 UTC from IEEE Xplore.  Restrictions apply. TABLEI
REGRASPPLANNERTESTRESULTS
Randomplanner Dex-Net4.0 SimplerRegraspPlanner OurRegraspPlanner
Algorithms
success successrate success successrate success successrate success successrate
Hammer 2 10.0% 0 0.00% 7 35.0% 16 80.0%
Woodenhammer 16 80.0% 17 85.0% 19 95.0% 20 100%
iPhonebox 5 25.0% 6 30.0% 8 40.0% 9 45.0%
LEGOmodel 11 55.0% 12 60.0% 16 80.0% 18 90.0%
Screwdriver 14 70.0% 14 70.0% 16 80.0% 17 85.0%
Mean 9.6 48.0% 9.8 49.0% 13.2 66.0% 16.0 80.0%
TABLEII
muchbetterperformancethanarandomplanner.Becausethe
K-FOLD(K=5)CROSS-VALIDATIONACCURACYOFDIFFERENTSLIP
Dex-Netistrainedonadatasetwhereallobjectsareassumed
DETECTIONMODELS
with even mass distribution. In our case, many objects e.g.,
SlipClassiﬁer LinearSVM RBFSVM LSTM hammer, LEGO model and screwdriver have uneven mass
tactileonly 88.2% 80.5% 82.1% distributions. Second, all experimental objects are relative
torqueonly 58.1% 61.8% 75.5% heavier with our experimental setup, which makes it critical
tactile+torque 68.8% 68.1% 78.1%
for a grasp pose to be close to the center of mass. However,
TABLEIII Dex-Net 4.0 predicts most of the poses close to the pixel-
THEFSCORE[53]OFSLIPDETECTIONFROMALINEARSVMANDA wise center in the depth image instead.
LSTMCLASSIFIER For the closed-loop policies, the learning based regrasp
planner with predicted regrasp ratio outperforms the Dex-
SlipClassiﬁer LinearSVM LSTM
Hammer 71.43% 82.98% Net 4.0 with 31% and the simpler regrasp planner with 14%
Woodenhammer 90.50% 75.00% success rate.
iPhonebox 82.35% 56.70%
LSEcGreOwdmriovdeerl 8567..2154%% 6527..0174%% VII. CONCLUSION&FUTUREWORK
Mean 76.88% 66.78% We present a novel learning based approach using multi-
TABLEIV sensor modules to predict stable grasp poses of unseen
DIFFERENTINPUTMODULESFORREGRASPPLANNER objects, based on the slip detection. The proposed approach
consists of two parts, a slip detector and a regrasp planner.
Inputmodules Tactileonly Torqueonly Tactile+torque
Both models are learned from real-world experiments with
Accuracy 66.8% 63.3% 75.2%
ground-truth label. An initial antipodal grasp pose is chosen
to be executed. Then the slip is detected by a slip detector
We use grasp success rate to compare different policies. during lifting the object. Afterwards a regrasp planner pre-
The deﬁnition of a successful grasp is that only if the dicts a new stable grasp pose by predicting the location of
object is lifted stably without any slip, determined by a centerofmassbasedonthefeedbackoftheslipdetectorand
humanexpert.Withthelimitedgraspforceandrelativeheavy alsoothersensormodules.Wedemonstratethatourlearning
experimental objects chosen by us, the grasp is likely to be based slip detector and regrasp planner effectively gener-
unstable if the grasp pose is not close enough to the center alizes learned knowledge to detect slip and regrasp while
of mass. grasping novel objects. We believe our proposed algorithm
• Random grasp planner: The grasp pose is randomly can potentially be an Add-on algorithm for general grasp
chosen from the antipodal grasp sampler. planners used for applications such as pick and place.
• Dex-Net 4.0 [6]: A grasp planning model trained on The proposed algorithm can only correct the grasp pose
Dex-Nex 4.0 dataset using a parallel-jaw gripper. in one direction where the rotational slip happens. This
• Our slip detector + a simple regrasp policy: A simple functionalityhaslimitationsforobjectswithcomplexshapes,
regrasp policy with a ﬁxed regrasp ratio of 0.5. thus the dataset and the number of objects are limited. In
• Our slip detector + our regrasp planner: Our learned the future, we will attempt to collect more data either by
regrasp planner using multi-sensor modules to plan a a better automatic self-supervised data collection process or
stable grasp based on slip detector. by generating synthetic data from simulation. Also a more
general regrasp algorithm will be studied for objects with
Theﬁrsttwopoliciesareopen-loopandtherestpolicieswith
complex geometries, to detect slip and plan regrasp.
regraspplannerareclosed-loopwiththefeedbackoftheslip
detection.
ACKNOWLEDGMENT
Each policy is evaluated with two grasp trials and each
objectisgraspedwith20times.Fortheregraspplanner,both This research has received funding from the European
the false prediction from slip detector and regrasp planner Unions Horizon 2020 research and innovation programme
will lead to a failure. under the Marie Sklodowska-Curie grant agreement No
In Table I, the result implies that the closed-loop policies 691154 STEP2DYNA and No 778602 ULTRACEPT. We
outperform the open-loop policies. The feedback from slip thank the products offered from Maximilian Maier and Dr.
detection in our case can help to regrasp and thus improve Maxime Chalon. We thank Yunlei Shi, Kaixin Bai, Lei
the grasp robustness. Zhang, Manuel Brucker and Karan Sharma for excellent
For the open-loop policies, Dex-Net 4.0 does not show advises and proofreading.
615
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 15:47:05 UTC from IEEE Xplore.  Restrictions apply. REFERENCES [24] F.Veiga,H.Hoof,J.Peters,andT.Hermans,“Stabilizingnovelobjects
by learning to predict tactile slip,” in International Conference on
[1] A. Zeng, S. Song, and K. Yu, “Novel objects in clutter with multi- IntelligentRobotsandSystems(IROS),2015.
affordance grasping and cross-domain image matching,” in IEEE
[25] L.Li,F.Sun,B.Fang,Z.Huang,C.Yang,andM.Jing,“Learningto
InternationalConferenceonRoboticsandAutomation(ICRA),2018. detectslipforstablegrasping,”inIEEE-ICRB,2017.
[2] L. Pinto and A. Gupta, “Supersizing self-supervision: Learning to
[26] K. V.Wyk and J. Falco,“Slip detection: Analysisand calibration of
grasp from 50k tries and 700 robot hours,” in IEEE International
univariatetactilesignals,”CoRR,vol.abs/1806.10451,2018.[Online].
ConferenceonRoboticsandAutomation(ICRA),2016.
Available:http://arxiv.org/abs/1806.10451
[3] D. Park and S. Y. Chun, “Classiﬁcation based grasp detection
[27] J.Li,S.Dong,andE.Adelson,“Slipdetectionwithcombinedtactile
usingspatialtransformernetwork,”CoRR,vol.abs/1803.01356,2018.
andvisualinformation,”CoRR,vol.abs/1802.10153,2018.[Online].
[Online].Available:http://arxiv.org/abs/1803.01356
Available:http://arxiv.org/abs/1802.10153
[4] X. Zhou, X. Lan, H. Zhang, Z. Tian, Y. Zhang, and N. Zheng,
[28] F.Veiga,J.Peter,andT.Hermans,“Gripstabilizationofnovelobjects
“Fully convolutional grasp detection network with oriented anchor
box,” CoRR, vol. abs/1803.02209, 2018. [Online]. Available: usingslipprediction,”inIEEETransactionsonHaptics,2018.
http://arxiv.org/abs/1803.02209 [29] F.Hogan,M.Bauza,O.Canal,E.Donlon,andA.Rodriguez,“Tactile
[5] F.Chu,R.Xu,andP.A.Vela,“Real-worldmulti-object,multi-grasp regrasp: Grasp adjustments via simulated tactile transformations,”
detection,”inIEEERoboticsandAutomationLetters,2018. CoRR,2018.[Online].Available:http://arxiv.org/abs/1803.01940
[6] J.Mahler,M.Matl,V.Satish,M.Danielczuk,B.DeRose,S.McKinley, [30] J.Mahler,J.Liang,S.Niyaz,M.Laskey,R.Doan,X.Liu,J.A.Ojea,
and K. Goldberg, “Learning ambidextrous robot grasping policies,” and K. Goldberg, “Dex-net 2.0: Deep learning to plan robust grasps
ScienceRobotics,vol.4,no.26,p.eaau4984,2019. with synthetic point clouds and analytic grasp metrics,” Robotics:
[7] K. Wada, S. Kitagawa, K. Okada, and M. Inaba, “Instance segmen- ScienceandSystems(RSS),2017.
tation of visible and occluded regions for ﬁnding and picking target [31] I. Lenz, H. Lee, and A. Saxena, “Deep learning for detecting
from a pile of objects,” in 2018 IEEE/RSJ International Conference robotic grasps,” The International Journal of Robotics Research,
onIntelligentRobotsandSystems(IROS),Oct2018,pp.2048–2055. vol. 34, no. 4-5, pp. 705–724, 2015. [Online]. Available: https:
[8] J. Cai, H. Cheng, Z. Zhang, and J. Su, “Metagrasp: Data //doi.org/10.1177/0278364914549607
efﬁcient grasping by affordance interpreter network,” CoRR, vol. [32] H. Liang, X. Ma, S. Li, M. Goerner, S. Tang, B. Fang, F. Sun, and
abs/1902.06554, 2019. [Online]. Available: http://arxiv.org/abs/1902. J. Zhang, “Pointnetgpd: Detecting grasp conﬁgurations from point
06554 sets,”inIEEEInternationalConferenceonRoboticsandAutomation
[9] J.Varley,D.Watkins-Valls,andP.K.Allen,“Multi-modalgeometric (ICRA),2019.
learningforgraspingandmanipulation,”CoRR,vol.abs/1803.07671, [33] A.Murali,Y.Li,D.Gandhi,andA.Gupta,“Learningtograspwithout
2018.[Online].Available:http://arxiv.org/abs/1803.07671 seeing,”2018.
[10] R.Calandra,A.Owens,M.Upadhyaya,W.Yuan,J.Lin,E.Adelson, [34] C.Goldfeder,M.Ciocarlie,H.Dang,andP.K.Allen,“Thecolumbia
and S. Levine, “The feeling of success: Does touch sensing help grasp database,” in Proceedings of the 2009 IEEE International
predictgraspoutcomes?”ConferenceonRobotLearning(CoRL),2017. ConferenceonRoboticsandAutomation,ser.ICRA’09. Piscataway,
[11] R. Calandra, A. Owens, D. Jayaraman, J. Lin, W. Yuan, J. Malik, NJ, USA: IEEE Press, 2009, pp. 3343–3349. [Online]. Available:
E. Adelson, and S. Levine, “More than a feeling: Learning to grasp http://dl.acm.org/citation.cfm?id=1703775.1703988
and regrasp using vision and touch,” in International Conference on [35] T.J.PrattichizzoD.,SpringerHandbookofRobotics.Springer,Berlin,
IntelligentRobotsandSystems(IROS),2018. Heidelberg. Springer,Berlin,Heidelberg,2008,vol.pages671700.
[12] S.-Q. Ji, M.-B. Huang, and h. Huang, “Robot intelligent grasp of [36] S.Caldera,A.Rassau,andD.Chai,“Reviewofdeeplearningmethods
unknownobjectsbasedonmulti-sensorinformation,”Sensors,vol.19, inroboticgraspdetection,”052018.
p.1595,042019. [37] J. Redmon and A. Angelova, “Real-time grasp detection using con-
[13] M. A. Lee, Y. Zhu, K. Srinivasan, P. Shah, S. Savarese, L. Fei-Fei, volutional neural networks,” in IEEE International Conference on
A. Garg, and J. Bohg, “Making sense of vision and touch: RoboticsandAutomation(ICRA),2015.
Self-supervised learning of multimodal representations for contact- [38] E. Johns, S. Leutenegger, and A. Davison, “Deep learning a grasp
rich tasks,” CoRR, vol. abs/1810.10191, 2018. [Online]. Available: functionforgraspingundergripperposeuncertainty,”inInternational
http://arxiv.org/abs/1810.10191 ConferenceonIntelligentRobotsandSystems(IROS),2016.
[14] S. S. Baishya and B. Buml, “Robust material classiﬁcation with a
[39] S. S. Baishya and B. Baum, “Robust material classiﬁcation with
tactile skin using deep learning,” in 2016 IEEE/RSJ International a tactile skin using deep learning,” in International Conference on
ConferenceonIntelligentRobotsandSystems(IROS),Oct2016,pp. IntelligentRobotsandSystems(IROS),2016.
8–15.
[40] C. Pasluosta, H. Tims, and A. Chiu, “Slippage sensory feedback
[15] F. Sun, C. Liu, W. Huang, and J. Zhang, “Object classiﬁcation and
and nonlinear force control system for a low cost prosthetic hand,”
graspplanningusingvisualandtactilesensing,”IEEETransactionson
AmericanJournalofBiomedicalSciences,2009.
Systems,Man,andCybernetics:Systems,vol.46,no.7,pp.969–979,
[41] D. Cockbum, J.-P. Roberge, T. H. L. Le, A. Maslyczyk, and
July2016.
V.Duchaine,“Graspstabilityassessmentthroughunsupervisedfeature
[16] M. Kaboli, K. Yao, and G. Cheng, in 2016 IEEE-RAS 16th Interna-
learningoftactileimages,”052017,pp.2238–2244.
tionalConferenceonHumanoidRobots(Humanoids),Nov2016,pp.
[42] M.Meier,F.Patzelt,R.Haschke,andH.Ritter,“Tactileconvolutional
752–757.
networksforonlineslipandrotationdetection,”inSpringerInterna-
[17] D. Kanoulas, J. Lee, D. G. Caldwell, and N. G. Tsagarakis,
tionalPublishingSwitzerland2016,2016.
“Center-of-mass-based grasp pose adaptation using 3d range and
force/torque sensing,” CoRR, vol. abs/1802.06392, 2018. [Online]. [43] E. Engeberg and S. Meek, “Adaptive sliding mode control for pros-
Available:http://arxiv.org/abs/1802.06392 thetichandstosimultaneouslypreventslipandminimizedeformation
[18] R. M. M. Z. Li and S. S. Sastry, A Mathematical Introduction to ofgraspedobjects,”inIEEE/ASME,2013.
RoboticManipulation. BocaRaton,FL,USA:CRCPress,Inc.,2005. [44] C.PapazovandD.Burschka,“AnEfﬁcientRANSACfor3DObject
[19] J. Reinecke, A. Dietrich, F. Schmidt, and M. Chalon, “Experimental Recognition in Noisy and Occluded Scenes,” in Computer Vision –
comparison of slip detection strategies by tactile sensing with the ACCV 2010, R. Kimmel, R. Klette, and A. Sugimoto, Eds. Berlin,
biotac on the dlr hand arm system,” in 2014 IEEE International Heidelberg:SpringerBerlinHeidelberg,2011,pp.135–148.
ConferenceonRoboticsandAutomation(ICRA),2014. [45] B. E. Boser, I. M. Guyon, and V. N. Vapnik, “A training algorithm
[20] C. Schuermann, M. Schoepfer, R. Haschke, and H. Ritter, “A high- for optimal margin classiﬁers,” in Proceedings of the Fifth Annual
speedtactilesensorforslipdetection,”inTowardsServiceRobotsfor WorkshoponComputationalLearningTheory. NewYork,NY,USA:
EverydayEnviron,2012. ACM,1992.
[21] Z. Su, K. Hausman, Y. Chebotar, A. Molchanov, G. E. Loeb, G. S. [46] L.Hsuan-TienandL.Chih-Jen,“Astudyonsigmoidkernelsforsvm
Sukhatme,andS.Schaal,“Forceestimationandslipdetectionforgrip and the training of non-psd kernels by smo-type methods,” Neural
controlusingabiomimetictactilesensor,”inIEEE-RAS,2015. Computation,062003.
[22] A. Ajoudani, E. Hocaoglu, A. Altobelli, M. Rossi, E. Battaglia, [47] J. T. Geiger, Z. Zhang, F. Weninger, B. Schuller, and G. Rigoll,
N.Tsagarakis,andA.Bicchi,“Reﬂexcontrolofthepisa/iitsofthand “Robust speech recognition using long short-term memory recurrent
during object slippage,” in Robotics and Automation (ICRA), 2016 neuralnetworksforhybridacousticmodelling,”2014.
IEEEInternationalConference,2016. [48] Z. Zhang, F. Ringeval, J. Han, J. Deng, E. Marchi, and B. Schuller,
[23] N. Jamali and C. Sammut, “Slip prediction using hidden markov “Facing realism in spontaneous emotion recognition from speech:
models: Multidimensional sensor data to symbolic temporal pattern Feature enhancement by autoencoder with lstm neural networks,”
learning,”inInternationalConferenceonIntelligentRobotsandSys- in Interspeech 2016, 2016, pp. 3593–3597. [Online]. Available:
tems(IROS),2012. http://dx.doi.org/10.21437/Interspeech.2016-998
616
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 15:47:05 UTC from IEEE Xplore.  Restrictions apply. [49] N. Srivastava, E. Mansimov, and R. Salakhutdinov, “Unsuper-
vised learning of video representations using lstms,” CoRR, vol.
abs/1502.04681,2015.
[50] S.HochreiterandJ.Schmidhuber,“Longshort-termmemory,”Neural
computation,vol.9,no.8,pp.1735–1780,1997.
[51] F. A. Gers, J. Schmidhuber, and F. Cummins, “Learning to
forget: Continual prediction with lstm,” Neural Comput., vol. 12,
no. 10, pp. 2451–2471, Oct. 2000. [Online]. Available: http:
//dx.doi.org/10.1162/089976600300015015
[52] Y. Gal and Z. Ghahramani, “A theoretically grounded application of
dropout in recurrent neural networks,” in Proceedings of the 30th
InternationalConferenceonNeuralInformationProcessingSystems,
ser. NIPS’16. USA: Curran Associates Inc., 2016, pp. 1027–
1035. [Online]. Available: http://dl.acm.org/citation.cfm?id=3157096.
3157211
[53] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,
O.Grisel,M.Blondel,P.Prettenhofer,R.Weiss,V.Dubourg,J.Van-
derplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay, “Scikit-learn: Machine learning in Python,” Journal
ofMachineLearningResearch,vol.12,pp.2825–2830,2011.
617
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 15:47:05 UTC from IEEE Xplore.  Restrictions apply. 