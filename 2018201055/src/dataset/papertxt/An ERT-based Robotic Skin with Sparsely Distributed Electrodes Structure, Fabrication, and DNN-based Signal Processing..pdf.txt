2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
Adversarial Feature Disentanglement for Place Recognition Across
Changing Appearance
Li Tang, Yue Wang, Qianhui Luo, Xiaqing Ding, Rong Xiong
Abstract—When robots move autonomously for long-term,
varied appearance such as the transition from day to night
and seasonal variation brings challenges to visual place recog-
nition. Deﬁning an appearance condition (e.g. a season, a
kind of weather) as a domain, we consider that the desired
representation for place recognition (i) should be domain-
unrelated so that images from different time can be matched
regardless of varied appearance, (ii) should be learned in a
self-supervised manner without the need of massive manually
labeled data, and (iii) should be able to train among multiple
domains in one model to keep limited model complexity. This Fig. 1: Translated and zero-appearance images of Nordland.
paper sets to ﬁnd domain-unrelated features across extremely Row 1: input images from D ; row 2: input images from
1
changing appearance, which can be used as image descriptors
D ; row 3: translated images from D to D ; row 4: zero
to match between images collected at different conditions. We 2 1 2
propose to use the adversarial network to disentangle domain- appearance images of D1. Column 1 to 5: D1 is winter and
unrelated and domain-related features, which are named place D2 isspring;column6to10:D1 isspringandD2 iswinter.
and appearance features respectively. During training, only
domain information is needed without requiring manually
aligned image sequences. Experiments demonstrated that our be invariant across different appearances. Meanwhile, some
method can disentangle place and appearance features in both
researches like [11] transfer query images to match the style
toycaseandimagesfromtherealworld,andtheplacefeatureis
of database images using neural networks. These methods
qualiﬁedinplace recognitiontasksunderdifferentappearance
conditions.Theproposednetworkisalsoadaptabletomultiple show realistic transferred images, but they are only able to
domainswithoutincreasingmodelcapacityandshowsfavorable match the two domains used in the training phase. When a
generalization. new domain comes, more models are needed. In addition,
[11] extracts features using existing featurization tool [12].
I. INTRODUCTION
We think such a process is indirect because style-transfer is
Visual place recognition is a vital task for mobile robots. unnecessary in place recognition.
Given sequences of images captured at different conditions,
its goal is to ﬁnd out pair of images corresponding to the In this paper, we set to ﬁnd a model that can disen-
sameplaces.Featureextractionisoneofthecoreproblemsin tangle domain-unrelated and domain-related features of an
place recognition while changing appearance is a challenge. image, where the domain-unrelated feature is used for place
Conventional methods use features which are designed tobe recognition(Fig.1).Speciﬁcally,adomaindenotesaspeciﬁc
robust against smallillumination variation like HOG [1] and appearancecondition,suchasdaytimeinspringornighttime
SIFT[2],orusestatisticsofhandcraftlocalfeaturesasglobal in summer. This idea is motivated by the hypothesis that an
descriptors like DBoW2 [3] and VLAD [4]. However, they imageiscomposedofplaceandappearance,wheretheplace
are not able to overcome extreme appearance changes, such isdomain-unrelatedcontentofascene(e.g.cornersoredges
as illumination changes from day to night and the visual of buildings), while appearance is domain-related properties
difference in different seasons. Besides, although LiDAR- (e.g. brightness of sunlight and type of season). Under this
based methods [5], [6], [7] are shown to be more robust in hypothesis, we disentangle place and appearance features
placerecognitionundersuchchanges,thehigh-costprevents using two encoders, following the widely used autoencoder
them from widely used. structure.Toensurethatplacefeaturescontainonlydomain-
Recentsuccessindeeplearningmakesresearchersstartto unrelated content, adversarial training is applied explicitly
study how to apply deep learning features in place recogni- among place features. Besides, another adversarial loss is
tion.Lotsofeffortsaredevotedtosupervisedfeaturelearning used to eliminate dependency between place features and
[8],[9],butthedependenceonmassivelabeleddataishardto appearancefeatures.Themodelistrainedinaself-supervised
ensure.Thus,self-supervisedmethodslike[10]arepreferred mannerwithoutmanuallyaligneddata.Finally,placefeature
insuchatask.Intheirwork,thewholefeatureisrequiredto is used as the descriptor in place recognition. Also, the
networkissharedacrossdifferentdomains,thusourmodelis
Authors are with State Key Laboratory of Industrial Control Tech- adaptabletodifferentdomainswithoutanincreasingnumber
nology, Zhejiang University, 310012 Hangzhou, P.R.China. Yue Wang is
of parameters. Main contributions of our method are listed
the corresponding author. Rong Xiong is the co-corresponding author.
{ltang,ywang24,qianhuiluo,xqding,rxiong}@zju.edu.cn. as follows:
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 1301
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 21,2020 at 05:57:36 UTC from IEEE Xplore.  Restrictions apply. • A self-supervised feature learning method is proposed Lrecon
to disentangle the place and appearance features from Lrecon
the given image. x′1 EA a′1 xˆ′1
x1 a1 xˆ1
Theproposedarchitectureisdesignedtobetrainedwith G
•
multipledomains without increasing model complexity. ES s′1
s1
A toy case study on MNIST is carried out to validate
• our hypothesis. Besides, experiments are taken on two !"#$%& !"#$%& Dpla Dapp !"#$%&
public datasets and show good performance. We also s2
ES
open the source code for reproduction 1. s′2
Theremainderofthispaperisorganizedasfollows:related G
x2 a2 xˆ2
work will be discussed and summarized in Sec. II and x′2 EA a′2 xˆ′2
our method will be presented thoroughly in Sec. III. We Lrecon
will introduce the experiments in detail and show results in Lrecon
Sec. IV. Conclusion will be made in Sec. V.
Dpla Dapp
s1 s′1 ’$(% s1 a1 )#*+%
II. RELATEDWORK
s1 s2 )#*+% s1 a2 ’$(%
Place recognition has been studied for years. Typical s2 s1 )#*+% s2 a1 ’$(%
pipeline for place recognition includes global feature extrac- s2 s′2 ’$(% s2 a2 )#*+%
tion and matching, optionally followed by temporal fusion
[13], among which this paper will focus on the ﬁrst part. Fig. 2: Proposed network architecture. ES: place encoder;
Handcraft features for place recognition can be classiﬁed EA: appearance encoder; G: decoder; Dpla: place domain
as global and local features. Traditional global-feature-based discriminator; Dapp: appearance compatibility discrimina-
methods try to ﬁnd appearance-invariant features, such as tor; si/ai: place/appearance feature from domain i; xi/xˆi:
aggregating gradient information using histograms [1] and input/reconstructed image from domain i; ⊗: concatenate
computing responses to artiﬁcial ﬁlters [14]. On the other operation. Any symbol with superscript “′” indicates that it
side, traditional local-feature-based methods extract local is sampled or generated from another image of the same
features [15], [16] as representation and use different statis- domain.
tics strategies [3], [4] to obtain descriptors. However, none
ofthemisfoundrobustacrossallenvironmentswithextreme
eigenvalues represent variations in the image. By removing
appearancechanges,thusnotpreferableinplacerecognition.
these components, the remaining information is suitable for
As deep neural network achieves success in the computer
place recognition. [23] use denoising autoencoder to learn
vision area, researchers in robotics community start to ex-
features that can reconstruct original images, while [10] try
plore how to integrate deep learning into existing research.
toreconstructHOGsimilarly.Theoutputofencodersinthese
[17]investigatediscriminativeabilityforplacerecognitionof
two methods exhibit robustness in place recognition. One
different layers from AlexNet [18] pre-trained on ImageNet
advantageofworkby[10]isthattheirtrainingsetisdifferent
ILSVRCdataset[19],andtheyﬁndthatfeaturesfrommiddle
from the testing set, demonstrating favorable generalization.
layers are robust against changing appearance. However,
Inspired by the work of style transfer [24], [25], some
its performance is not good enough as reported in [10].
researcherstrytotransferqueryimagestomatchthestyleof
[20] use a classiﬁcation network for location-speciﬁc place
databaseimages(suchas[11],[26],[27],[28]),followingby
recognitionandachievecomparableresults.Thisworkshows
local feature matching, global descriptor matching or dense
the potential for the neural network to distinguish places,
matching. For example, [11] transfers nighttime images into
but places are ﬁxed once training is ﬁnished. Therefore, it
daytime ones, and extract features using DenseVLAD [12].
cannotbeextendedtootherscenes.Toaddressthisproblem,
Each model in these methods is targeted at the two domains
[21] introduces a differential VLAD component into the
used in the training phase. When adding new domains, new
neuralnetwork,namelyNetVLAD,tocompactafeaturemap
models are needed, and the number of models increases
into a descriptor vector, which is optimized by triplet loss.
exponentially. [29] enhances the pretrained NetVLAD [21]
Labeled data from Google Street View Time Machine are
with semantic information, which is shown to be viewpoint-
needed to construct training tuple. [9] uses two datasets that
invariant. It demonstrates that appearance-based descriptors,
are manually aligned to illustrate the power of supervision.
such as the proposed method, can be improved to overcome
However, these supervised methods need massive labeled
changing viewpoints with techniques in [29].
data, which might be undesirable in fast deployment.
Another branch of machine learning methods, namely
III. ADVERSARIALFEATUREDISENTANGLEMENTFOR
self-supervised learning, do not rely on aligned data. [22]
PLACERECOGNITION
apply PCA to raw images or CNN features to obtain in-
variant descriptors. It is reported that components with large Under the hypothesis that an image is composed of place
and appearance, our method uses the convolutional network
1https://github.com/dawnos/fdn-pr as a feature extractor to disentangle place and appearance
1302
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 21,2020 at 05:57:36 UTC from IEEE Xplore.  Restrictions apply. features. Speciﬁcally, autoencoder is used as the backbone. adversarial paradigm against D , and its loss function can
pla
An image is embedded into feature space, and the feature be expressed as:
is transformed back to image space. In this paper, part of
1
feature encodes place and other encodes appearance, which LpAldav,1 =2Ex1∼p(x1),x′1∼p(x1)[(Dpla(ES(x1),ES(x′1))−0)2]+
are disentangled into place space S and appearance space A
1
respectively. E [(D (E (x ),E (x ))−1)2]
2 x1∼p(x1),x2∼p(x2) pla S 1 S 2
We begin from 2 domains, denoted by D and D . As (3)
1 2
shown in Fig. 2, for any image x ∈ D (i = 1,2), place Eq.(2)and(3)useleastsquareadversarialloss[30],where
i i
encoder E and appearance encoder E are used to extract L2lossisusedwithoutsigmoidfunction.It’sworthnoticing
S A
place feature s = E (x ) and appearance feature a = that E (x ), E (x′) and E (x ) in Eq. (3) are in fact s ,
i S i i S 1 S 1 S 2 1
E (x ) respectively. Decoder G jointly leverages these two s′ and s in Eq. (2). We replace them to remind the readers
A i 1 2
features to produce reconstructed image xˆ = G(s ,a ) that E is ﬁxed when updating D (Eq. (2)), while D
i i i S pla pla
(Sec. III-A). To ensure domain-unrelated property of place is ﬁxed when updating E (Eq. (3)).
S
latent space, place domain discriminator D is introduced Eq. (2) and (3) are formulated for the case that the ﬁrst
pla
for adversarial training (Sec. III-B). Besides, appearance input image is sampled from D . When the ﬁrst image is
1
compatibility discriminator D is designed to eliminate fromD ,Lpla,2andLpla,2andcanbederivedbyexchanging
app 2 D Adv
the dependency between these two features (Sec. III-C). two domains in Eq. (2) and (3).
A. Reconstruction Loss C. Appearance Compatibility Discriminator
As in many self-supervised feature learning literature, Only applying place domain discriminator is not enough.
encoders (ES and EA) and decoder G are used to re- To see it, one can assume that output features of EA might
construct original input image cooperatively. To measure carry some information about the place. In this case, the
reconstruction quality, L2 distance is used, thus the overall combination of s and a are still able to reconstruct the
i i
reconstruction loss is expressed as: original image, and D is not affected. However, place
pla
and appearance features are not independent. To eliminate
1
Lrecon = 2Ex1∼p(x1)kG(ES(x1),EA(x1))−x1k22+ (1) dependency of those features, we propose appearance com-
12Ex2∼p(x2)kG(ES(x2),EA(x2))−x2k22 papaptiebairliatnycedifsecartiumreinsaatorerinDdaepppentdoentte.ll if given place and
where x is image sampled from some prior distribution As images x1 and x2 sampled from different domains
p(x ) ofidomain D . are independent, place feature s1 = ES(x1) from D1 and
i i appearance feature a =E (x ) from D are also indepen-
2 A 2 2
B. Place Domain Discriminator dent. On the other hand, if place and appearance features
Pure autoencoder cannot guarantee that the place feature are not disentangled, s1 = ES(x1) and a1 = EA(x1) from
only captures domain-unrelated information. It may contain the same image x1 are not independent. Now that we have
appearancecontent,whichisnotpenalizedbyreconstruction positiveandnegativepairsforDapp,whichcanbeleveraged
loss.Toovercome thisproblem,weuseadversariallearning, to construct loss function for Dapp:
where place domain discriminator D is introduced to 1
constrain place features to lie in the spalmae latent space. In LaDpp,1 = 2Es1,a1∼p(s1,a1)[(Dapp(s1,a1)−1)2+ (4)
each training iteration, two place features are extracted from 1
E [(D (s ,a )−0)2]
two images, which may come from the same or different 2 s1∼p(s1),a2∼p(a2) app 1 2
domains. Dpla tries to tell whether they are from the same where p(s ,a ) is given by (E (x ),E (x )),x ∼p(x ),
domain, as shown in Fig. 2 (top right). For example, given 1 1 S 1 A 1 1 1
and p(a ) is given by E (x ),x ∼p(x ).
place feature s from domain D , we sample the other two 2 A 2 2 2
1 1 Similar to Sec.III-B, encoders E and E are encour-
place features s′ and s from D and D respectively. The S A
1 2 1 2 aged to confuse D by outputting independent place and
loss function for this case can be written as app
appearance features. When D fails, place and appearance
app
LpDla,1 = 12Es1∼p(s1),s′1∼p(s1)[(Dpla(s1,s′1)−1)2]+ (2) faeuattouernecsodweirllcbane binedeexppernedsesnedt. aAsgain, the loss function for
1
2Es1∼p(s1),s2∼p(s2)[(Dpla(s1,s2)−0)2] Lapp,1 = 1E [(D (E (x ),E (x ))−0)2]+
Adv 2 x1∼p(x1) app S 1 A 1
where p(si) is derived by ES(xi),xi ∼p(xi). The discrimi- 1
natorDpla outputs1ifthegiventwoplacefeaturesarefrom 2Ex1∼p(x1),x2∼p(x2)[(Dapp(ES(x1),EA(x2))−1)2]
the same domain and 0 for those from different domains. (5)
Simultaneously, place encoder E are encouraged to Similarly, we can have Lapp,2 and Lapp,2.
S D Adv
confuse D by outputting place features following the Like [10], we generate labels for Eq. (2)-(5) with un-
pla
same distribution across domains, so that they are invariant aligned data, thus we claim that the proposed method is
against varied appearance. The autoencoder is trained in an self-supervised.
1303
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 21,2020 at 05:57:36 UTC from IEEE Xplore.  Restrictions apply. D. Extension: Multiple Domains Case 12 12 12 12
3 3 3 3
4 4 4 4
The autoencoder in our method is shared across do- 5 5 5 5
6 6 6 6
7 7 7 7
mains. Besides, discriminators use information between two 8 8
9 9
domains, and they are domain-unrelated. Thus, it is easy 10 10
to extend to multiple domains. Assume that there are N
(a) Place feature. (b) Appearance feature.
domains, denoted by D ,D ,··· ,D . In each training it-
1 2 N
eration,twodomains D andD arerandomlydrawn,where
i j
i,j =1,2,··· ,N and i6=j. Then images from D and D
i j
are sampled respectively from training set, which are input
data of encoders (Eq. (3)). In testing phase, images from
difOfenreenatddvoamntaaignes aorfeofuerdminettohoEdSistothoabttaoinnlyplaocneefmeaotudreels.is Input 2
needed in a speciﬁc area for long-term deployment. When
collecting new data with different appearances in the same
area, one can improve the model by retraining with new
data without additional parameters. On the contrary, style-
transfer-based methods need new models to transfer new Input 1
data into known style. When new data come periodically, (c) Example translation between different domains. Each block
contains the ﬁrst input image (upper left), the second input image
this will lead to exponentially increasing parameters. Thus,
(upper right), the translated image (lower left) and the zero-
our method can be plugged into any long-term localization
appearanceimage(lowerright).Theﬁrstinputimagesofthesame
framework [31], [32] as the feature extractor. column share the same domain, while the second input images of
the same row share the same domain.
E. Implementation and Training
Fig. 3: Visualization of features and image translation be-
Training Strategy During training, discriminators (D
pla tween different domains of toy case experiment. Fig. 3a and
and D ) and autoencoder (E , E and G) are updated
app S A Fig. 3b are visualization of place and appearance features
alternatively by back-propagation:
usingt-SNErespectively,wherelabelsarecolors(left,1∼7)
minLpla,1+Lpla,2 (6) and digits (right, 1∼10).
D D
Dpla
minLapp,1+Lapp,2 (7)
Dapp D D ES. Then, the best-matched image xm in the database is
min L +λ (Lapp,1+Lapp,2)+λ (Lpla,1+Lpla,2) determined by
recon 1 Adv Adv 2 Adv Adv
ES,EA,G (8) m= argmax ( sQ · sDB,i ) (9)
ks k ks k
where λ and λ are hyper-parameters to balance recon- i=1,···,NDB Q DB,i
1 2
struction and disentanglement. In case of two domains, IV. EXPERIMENTALRESULTS
images x , x′, x , x′ are drawn from two domains in each We ﬁrstly carry out a toy case experiment to validate our
1 1 2 2
training iteration. For multiple domains, images are drawn hypothesis (Sec. IV-A). Later, experiments are conducted
as described in Sec. III-D. on two real datasets to illustrate its basic performance
Network Architecture Encoders (E and E ), decoder (Sec.IV-B)andgeneralizationability(Sec.IV-C)underplace
S A
G and discriminators (D and D ) are all convolu- recognition scenario with two domains. The last experiment
app app
tional networks. They are shared among different domains. demonstrates an extension to multiple domains (Sec. IV-D).
The autoencoder is bottleneck architecture, where encoders
A. Toy Case
downsample an image as two feature maps and the decoder
jointly upsamples them back to the original resolution. The We use MNIST, a handwritten digits dataset, to validate
appearance feature is a vector of dimension n . Input fea- our hypothesis that image is composed of place and appear-
A
tures of discriminators, (s ,s ) or (s ,a ), are concatenated ance.Tosimulatedifferentdomains,dataimagesarecolored
i j i j
together before going into the discriminators, and they are randomly in 7 colors. In this case, place refers to the digit,
downsampled to one dimension as output. while appearance refers to the color. Sample digits can be
PlaceRecognitionTodemonstratethediscriminativeabil- seen in Fig. 3c. Dimension of appearance feature n is 8.
A
ity of the learned features, we use place features as descrip- In training, λ and λ are set as 0.1.
1 2
tors in place recognition. Speciﬁcally, given a sequence of Weﬁrstlyinvestigatedisentanglementbyvisualizingplace
already observed images, we extract their place features by and appearance features with t-SNE [33] (Fig. 3). As shown
feeding those images into place encoder E , constituting in Fig. 3a, place features lie in the manifold where digits
S
the database feature set {s }, where i = 1,··· ,N can be distinguished easily, while colors are randomly dis-
DB,i DB
and N is number of database images. When a new tributed.Ontheotherside,appearancefeaturesareclustered
DB
query image comes, the query feature s is extracted from by colors, and they are unrelated to digits. These results
Q
1304
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 21,2020 at 05:57:36 UTC from IEEE Xplore.  Restrictions apply. Nordland Alderley
Method Aligned? AUC Accuracy AUC Accuracy 50000 500 1000 1500 2000 2500 3000 0.9 50000 500 1000 1500 2000 2500 3000 01..909090
DBoW2 ✗ 0.09 1.33% 0.00 0.22% 1000 0.8 1000 0.998
HOG ✗ 0.17 17.0% 0.02 1.85% 0.7 0.997
[10] ✗ 0.28 15.8% 0.10 1.26% 1500 0.6 1500 0.996
NetVLAD ✗ 0.22 19.9% 0.02 2.65% 2000 0.5 2000 0.995
Ours ✗ 0.70 49.4% 0.34 21.0% 23500000 00..34 23500000 00..999934
[9] ✓ N/A1 92% N/A1 7.82% 0.2 0.992
NetVLAD ✓ 0.74 83.0% 0.13 15.8% (a) Place. (b) Appearance.
1 NotAvailable(N/A)becausetheydonotuseAUCascriteria.
Fig. 4: Distance matrixes of place feature (left) and appear-
TABLE I: AUC and accuracy of different methods on Nord- ance (right) feature. Cosine distance is used.
land and Alderley.
mounted in a car in two sessions, one rainy nighttime, and
Night vs Day Day vs Night
Methods ToA1? AUC Accuracy AUC Accuracy one sunny daytime. Ground truths for both datasets are
DBoW2 ✗ 0.00 0.22% 0.00 0.13% determined by GPS, and the partition of training and testing
HOG ✗ 0.02 1.85% 0.02 0.43% setsfollows[9].Amatchisconsideredcorrectifthedistance
[10] ✗ 0.10 1.26% 0.02 1.02% between predicted and ground truth images is less than 3
[9] ✗ N/A2 1.73% N/A2 N/A2 frames, as used by [9]. Two criteria are used in quantity
[11] ✗ 0.01 0.6% 0.00 0.3% analysis: (i) AUC (Area Under Curve) and (ii) accuracy
NetVLAD ✗ 0.04 5.56% 0.02 2.65%
(overall true positive rate). The dimension of appearance
Ours ✗ 0.13 8.28% 0.10 3.17%
feature n is 8. In training, λ and λ are set as 0.003 and
[9] ✓ N/A2 7.82% N/A2 N/A2 A 1 2
0.01 respectively.
NetVLAD ✓ 0.16 19.2% 0.13 15.8%
Ours ✓ 0.39 23.7% 0.34 21.0% To demonstrate basic performance, the ﬁrst experiment
is targeted at two domains case. For Nordland, winter and
1 ShortforTrainonAlderley.
2 NotAvailablebecausetheydoprovidetheseresultsandthecode. springarechosenasD1 andD2.ForAlderley,bothdomains
are used, where nighttime and daytime are D and D
1 2
TABLEII:Generalizationresults.Modelsexceptforthelast respectively. In testing, images from D are used to build
1
3rows do notsee Alderley before testing.Resultsinthelast a database, while those from D are seen as query images.
2
3 rows are trained on Alderley, and they are placed here for
As a comparison, several methods are used to illustrate
comparison. We also try different orders of input domains.
the discriminative ability of our learned features. We use the
Forexample,“NightvsDay”meansD1 isnighttimeandD2 model provided by [10] without retraining on Nordland or
is daytime.
Alderley,asdoneintheirpaper.Besides,theimplementations
of DBoW and HOG in [10] are used. For [9], results are
obtained from their paper as they do not provide their code.
illustrate that our method can effectively disentangle place
ForNetVLAD,wenotonlyvalidateitbyusingthepretrained
and appearance features in this toy case.
model from [21], but also retrain it on those two datasets
To understand better what has place feature learned, the
for comparison. Results are shown in Tab. I. Our method
vector of appearance feature is set to zero, and the out-
shows large improvement over classical features including
put of the decoder is displayed in Fig. 3c (called zero-
DBoW2 and HOG in both datasets. It is also better than
appearance image). One can see that when the appearance
the self-supervised method by [10]. The supervised methods
feature is ﬁxed, all decoded images have the same color.
like [9], [21] can obtain better performance in some datasets
It illustrates that the decoupled place feature contains only
(such as Nordland), but it highly depends on the quality of
place information (digit) of any input digit image. Fig. 3c
ground truth. By watching images frame by frame, we ﬁnd
also presents reconstructed images decoded from place and
that two domains in Alderley dataset also have perspective
appearance features of different domains (called translated
differences, due to the motion characteristic of a car (such
image). Speciﬁcally, the decoder combines the place feature
as changing lane). That’s why our method exhibits higher
from the ﬁrst input image and appearance feature from the
accuracy than [9] and [21] in Alderley.
secondinputimagetoreconstructdigit.Resultsshowthatthe
To validate that appearance features only contain place-
digits of translated images are determined by place features,
unrelated information, we plot the distance matrix across
while colors are controlled by appearance features.
appearance features of two domains in Nordland (Fig. 4b).
B. Basic Performance As a comparison, we also plot the matrix of place features
We use Partitioned Nordland Dataset [34] and Alderley (Fig. 4a). It can be seen that the appearance feature shows
Day/Night Dataset [13] to validate our method in place nodifferenceindifferentplaces,whiletheplacefeaturesare
recognition. Nordland is collected on a train in four seasons more meaningful in the context of place recognition.
(spring, summer, fall, and winter), which are treated as four Similar to Sec.IV-A, some translated and zero-appearance
domains in this paper. Alderley is captured by a camera imagesfromNordlanddatasetarepresentinFig.1.Although
1305
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 21,2020 at 05:57:36 UTC from IEEE Xplore.  Restrictions apply. D
2 Spring Summer Fall Winter Mean
D
1
Spring - 0.86/0.92 0.92/0.94 0.78/0.73 0.85/0.86
Summer 0.86/0.91 - 0.98/0.98 0.68/0.71 0.84/0.87
Fall 0.88/0.92 0.98/0.98 - 0.66/0.76 0.84/0.89
Winter 0.70/0.59 0.64/0.58 0.63/0.63 - 0.66/0.60
(a) AUC.
D
2 Spring Summer Fall Winter Mean
D
1
Spring - 70.3%/77.7% 78.3%/81.3% 56.3%/52.3% 68.3%/70.4%
Summer 70.5%/77.8% - 96.1%/96.5% 44.8%/46.7% 70.5%/73.7%
Fall 76.9%/80.1% 96.2%/97.0% - 43.7%/50.2% 72.3%/75.8%
Winter 49.4%/37.9% 39.1%/38.6% 41.5%/40.0% - 43.3%/38.8%
(b) Accuracy.
TABLE III: Result of multiple domains case. Each item contains criterion (AUC or accuracy) of two/multiple domains.
Number of parameters: 213.6M/17.8M (two/multiple domains).
the reconstructed images are not as realistic as in [11], it is described inSec. III-E.AUC and accuracy are computed for
acceptablebecauseourgoalistoobtaindisentangledfeatures thetestingsetofeverytwoseasonsfromNordlandusingthis
instead of image reconstruction or style-transfer. From the model. Besides, to see beneﬁts brought by fusing multiple
lastrow,wecanseethatthelearnedplacefeatureisunrelated domains,wealsotrainedatwo-domainsmodelforeverytwo
to different appearance conditions. seasons and compute those criteria.
From Tab. III, we can see that by fusing more domains,
C. Generalization Performance
the model can get comparable AUC and accuracy in most
We also experiment to see the generalization performance
of the time. We should notice that the model trained from 4
of our disentangled features. Models trained on datasets
domains has the same number of parameters as one model
otherthanAlderleyareusedtoperformplacerecognitionon
trained from 2 domains. On the contrary, with only two-
Alderley(Tab.II).Methodscanbedividedinto3categories.
domain models (such as [24], [25]), we have to train 12
Theﬁrstkindofmethod(DBoW,HOG,NetVLAD,and[10])
networks. Thus, by fusing more domains, our method can
is designed for general purposes. The second one ([11]) is
achieve comparable results without increasing the capacity
targeted at the same appearance conditions (daytime and
of the model.
nighttime) but trained with data from other places from a
different dataset (Robotcar [35]). The third one is trained in V. CONCLUSION
different appearance conditions and places. Inthispaper,weproposeaself-supervisedfeaturelearning
It is found that our method achieves the highest perfor- method to disentangle the place and appearance features,
mance. Models by [9], [10] and [21] try to learn uniﬁed and the place feature is leveraged in the place recognition
features, which is, in fact, hard to fulﬁll across domains. task. An autoencoder, including place encoder, appearance
Thus,whentheyaretransferredtoanewdomain,thefeatures encoder, and decoder, is trained as a self-supervised feature
are not as robust as ours. The accuracy of our method extractor.Tomakeplacefeaturedomain-unrelated,placedo-
without training on Alderley is even slightly higher than maindiscriminatorisproposed.Besides,appearancecompat-
the method by [9] with Alderley as the training set. This ibilitydiscriminatorisusedtoeliminatedependencybetween
shows that our model is as robust as supervised learning place and appearance features. We start from a toy case
without disentanglement. In comparison with [11], one can to illustrate the disentanglement effect. Experiments on real
ﬁnd that our method is much stronger. It is because [11] datasets show that the disentangled place feature is suitable
tries to ﬁnd invariant representation in image space. Our fortheplacerecognitiontask.Itachievescomparableresults
methoddirectlyconstrainsthefeatures,whichismoreuseful to several existing methods. We also present the generaliza-
inﬁndinginvariantfeaturesforplacerecognition.Also,style- tion ability of our method. An extension strategy is shown
transfer-basedmethodsmaybenotgoodforgeneralization,as to prove that our method is easy to add new domains, which
errorbuiltontheimagerequiresthequalityofimagedetails, is also found to be beneﬁcial to reduce model complexity
which can be less useful for place recognition. These results without sacriﬁcing place recognition performance.
veriﬁed the superior generalization of the proposed method.
VI. ACKNOWLEDGMENT
D. Multiple Domains This work was supported by the National Key R&D
To show that the proposed network is easy to adapt to Program of China (2018YFB1309300), the National Nature
multiple domains, four domains from Nordland are used ScienceFoundationofChina(U1609210,61903332),andthe
to train a uniﬁed model, following the training strategy Fundamental Research Funds for the Central Universities.
1306
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 21,2020 at 05:57:36 UTC from IEEE Xplore.  Restrictions apply. REFERENCES [19] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei,
“Imagenet:Alarge-scalehierarchicalimagedatabase,”in2009IEEE
[1] N.DalalandB.Triggs,“Histogramsoforientedgradientsforhuman conferenceoncomputervisionandpatternrecognition. Ieee,2009,
detection,”in2005IEEEComputerSocietyConferenceonComputer pp.248–255.
VisionandPatternRecognition(CVPR’05),vol.1. IEEE,2005,pp. [20] Z. Chen, A. Jacobson, N. Su¨nderhauf, B. Upcroft, L. Liu, C. Shen,
886–893. I. Reid, and M. Milford, “Deep learning features at scale for visual
[2] D. G. Lowe et al., “Object recognition from local scale-invariant placerecognition,”in2017IEEEInternationalConferenceonRobotics
features.”iniccv,vol.99,no.2,1999,pp.1150–1157. andAutomation(ICRA). IEEE,2017,pp.3223–3230.
[3] D.Ga´lvez-Lo´pezandJ.D.Tardos,“Bagsofbinarywordsforfastplace [21] R.Arandjelovic,P.Gronat,A.Torii,T.Pajdla,andJ.Sivic,“Netvlad:
recognition in image sequences,” IEEE Transactions on Robotics, Cnn architecture for weakly supervised place recognition,” in Pro-
vol.28,no.5,pp.1188–1197,2012. ceedings of the IEEE Conference on Computer Vision and Pattern
[4] H. Je´gou, M. Douze, C. Schmid, and P. Pe´rez, “Aggregating local Recognition,2016,pp.5297–5307.
descriptorsintoacompactimagerepresentation,”inCVPR2010-23rd [22] S. Lowry and M. J. Milford, “Supervised and unsupervised linear
IEEEConferenceonComputerVision&PatternRecognition. IEEE learningtechniquesforvisualplacerecognitioninchangingenviron-
ComputerSociety,2010,pp.3304–3311. ments,”IEEETransactionsonRobotics,vol.32,no.3,pp.600–613,
[5] H. Yin, Y. Wang, X. Ding, L. Tang, S. Huang, and R. Xiong, “3d 2016.
lidar-based global localization using siamese neural network,” IEEE [23] X. Gao and T. Zhang, “Unsupervised learning to detect loops using
TransactionsonIntelligentTransportationSystems,2019. deep neural networks for visual slam system,” Autonomous robots,
[6] L. He, X. Wang, and H. Zhang, “M2dp: A novel 3d point cloud vol.41,no.1,pp.1–18,2017.
descriptor and its application in loop closure detection,” in 2016 [24] M.-Y. Liu, T. Breuel, and J. Kautz, “Unsupervised image-to-image
IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems translation networks,” in Advances in neural information processing
(IROS). IEEE,2016,pp.231–237. systems,2017,pp.700–708.
[7] G. Kim and A. Kim, “Scan context: Egocentric spatial descriptor [25] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, “Image-to-image
forplacerecognitionwithin3dpointcloudmap,”in2018IEEE/RSJ translationwithconditionaladversarialnetworks.arxiv,”2016.
International Conference on Intelligent Robots and Systems (IROS). [26] Y. Latif, R. Garg, M. Milford, and I. Reid, “Addressing challenging
IEEE,2018,pp.4802–4809. placerecognitiontasksusinggenerativeadversarialnetworks,”in2018
[8] Z.Chen,L.Liu,I.Sa,Z.Ge,andM.Chli,“Learningcontextﬂexible IEEEInternationalConferenceonRoboticsandAutomation(ICRA).
attentionmodelforlong-termvisualplacerecognition,”IEEERobotics IEEE,2018,pp.2349–2355.
andAutomationLetters,vol.3,no.4,pp.4015–4022,2018. [27] H. Porav, W. Maddern, and P. Newman, “Adversarial training for
[9] J.M.Facil,D.Olid,L.Montesano,andJ.Civera,“Condition-invariant adverseconditions:Robustmetriclocalisationusingappearancetrans-
multi-viewplacerecognition,”arXivpreprintarXiv:1902.09516,2019. fer,”in2018IEEEInternationalConferenceonRoboticsandAutoma-
[10] N. Merrill and G. Huang, “Lightweight unsupervised deep loop tion(ICRA). IEEE,2018,pp.1011–1018.
closure,”inProc.ofRobotics:ScienceandSystems(RSS),Pittsburgh, [28] L.ClementandJ.Kelly,“Howtotrainacat:learningcanonicalappear-
PA,June2018,(accepted). ance transformations for direct visual localization under illumination
[11] A.Anoosheh,T.Sattler,R.Timofte,M.Pollefeys,andL.VanGool, change,” IEEE Robotics and Automation Letters, vol. 3, no. 3, pp.
“Night-to-dayimagetranslationforretrieval-basedlocalization,”arXiv 2447–2454,2018.
preprintarXiv:1809.09767,2018. [29] S. Garg, N. Suenderhauf, and M. Milford, “Semantic–geometric
[12] A.Torii,R.Arandjelovic,J.Sivic,M.Okutomi,andT.Pajdla,“24/7 visual place recognition: a new perspective for reconciling oppos-
place recognition by view synthesis,” in Proceedings of the IEEE ing views,” The International Journal of Robotics Research, p.
Conference on Computer Vision and Pattern Recognition, 2015, pp. 0278364919839761,2019.
1808–1817. [30] X.Mao,Q.Li,H.Xie,R.Y.Lau,Z.Wang,andS.PaulSmolley,“Least
[13] M.J.MilfordandG.F.Wyeth,“Seqslam:Visualroute-basednaviga- squaresgenerativeadversarialnetworks,”inProceedingsoftheIEEE
tionforsunnysummerdaysandstormywinternights,”in2012IEEE InternationalConferenceonComputerVision,2017,pp.2794–2802.
InternationalConferenceonRoboticsandAutomation. IEEE,2012, [31] L. Tang, Y. Wang, X. Ding, H. Yin, R. Xiong, and S. Huang,
pp.1643–1649. “Topological local-metric framework for mobile robots navigation: a
[14] A.C.Murillo,G.Singh,J.Kosecka´,andJ.J.Guerrero,“Localization longtermperspective,”AutonomousRobots,vol.43,no.1,pp.197–
in urban environments using a panoramic gist descriptor,” IEEE 211,2019.
TransactionsonRobotics,vol.29,no.1,pp.146–160,2013. [32] W.ChurchillandP.Newman,“Experience-basednavigationforlong-
term localisation,” The International Journal of Robotics Research,
[15] D. G. Lowe, “Distinctive image features from scale-invariant key-
vol.32,no.14,pp.1645–1661,2013.
points,” International journal of computer vision, vol. 60, no. 2, pp.
[33] L.v.d.MaatenandG.Hinton,“Visualizingdatausingt-sne,”Journal
91–110,2004.
ofmachinelearningresearch,vol.9,no.Nov,pp.2579–2605,2008.
[16] E. Rublee, V. Rabaud, K. Konolige, and G. R. Bradski, “Orb: An
[34] D. Olid, J. M. Fa´cil, and J. Civera, “Single-view place recognition
efﬁcientalternativetosiftorsurf.”inICCV,vol.11,no.1. Citeseer,
underseasonalchanges,”inPPNIVWorkshopatIROS2018,2018.
2011,p.2.
[35] W.Maddern,G.Pascoe,C.Linegar,andP.Newman,“1year,1000km:
[17] N. Su¨nderhauf, S. Shirazi, F. Dayoub, B. Upcroft, and M. Milford,
The oxford robotcar dataset,” The International Journal of Robotics
“On the performance of convnet features for place recognition,” in
Research,vol.36,no.1,pp.3–15,2017.
2015 IEEE/RSJ International Conference on Intelligent Robots and
Systems(IROS). IEEE,2015,pp.4297–4304.
[18] A.Krizhevsky,I.Sutskever,andG.E.Hinton,“Imagenetclassiﬁcation
with deep convolutional neural networks,” in Advances in neural
informationprocessingsystems,2012,pp.1097–1105.
1307
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 21,2020 at 05:57:36 UTC from IEEE Xplore.  Restrictions apply. 