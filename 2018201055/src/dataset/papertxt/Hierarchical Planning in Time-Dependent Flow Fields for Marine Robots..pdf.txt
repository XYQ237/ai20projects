2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
Keypoint Description by Descriptor Fusion Using Autoencoders
Zhuang Dai1, Xinghong Huang1, Weinan Chen1, Chuangbing Chen1, Li He1, Shuhuan Wen2, and Hong Zhang*
Abstract—Keypoint matching is an important operation in as well. Any layer of these pre-trained CNNs would have
computervisionanditsapplicationssuchasvisualsimultaneous a length that is typically two or three orders of magnitude
localization and mapping (SLAM) in robotics. This matching
longer than that of hand-crafted or trained descriptors, and
operation heavily depends on the descriptors of the keypoints,
this length directly affects their time and storage complexity
and it must be performed reliably when images undergo
conditionalchangessuchasthoseinilluminationandviewpoint. for matching keypoints, and should be reduced for them
In this paper, a descriptor fusion model (DFM) is proposed to be computationally competitive with the other descriptor
to create a robust keypoint descriptor by fusing CNN-based choices.
descriptors using autoencoders. Our DFM architecture can be
Two main challenges to effective keypoint description are
adapted to either trained or pre-trained CNN models. Based
how to handle illumination and viewpoint changes of the
on the performance of existing CNN descriptors, we choose
HardNet and DenseNet169 as representatives of trained and keypoints being matched. In our recent work [13], we ﬁnd
pre-traineddescriptors.OurproposedDFMisevaluatedonthe that the trained CNN’s perform better than the pre-trained
latest benchmark datasets in computer vision with challenging CNN’s with respect to viewpoint changes, whereas the pre-
conditional changes. The experimental results show that DFM
trained CNN’s perform better than the trained CNN’s with
is able to achieve state-of-the-art performance, with the mean
respect to illumination changes for keypoint matching. In
mAP that is 6.45% and 6.53% higher than HardNet and
DenseNet169, respectively. ordertocreateadescriptorthatisrobustwithrespecttoboth
types of condition changes and has a compact dimension,
I. INTRODUCTION in this paper, we propose a descriptor fusion model (DFM)
Keypointmatchingisanimportantstepinmanycomputer to fuse the trained and the pre-trained CNN models using
vision and robotics applications, such as structure-from- autoencoders. The details of our proposed model are shown
motion (SfM) [1], multi-view stereo (MVS) [2], image re- in Fig. 1.
trieval [3] and visual simultaneous localization and mapping Our main motivation is to beneﬁt from the advantages of
(SLAM)[4].Theperformanceoftheseapplicationsstrongly the two classes of CNN descriptors to obtain a descriptor
depends on keypoint matching, which in turn depends on robust to both illumination and viewpoint changes. Descrip-
the quality of keypoint descriptors. Previous research in tor fusion is an effective and common method to improve
keypointdescriptionhaspursuedthreeclassesofdescriptors: performance. By optimizing and combining these multiple
hand-crafted, those speciﬁcally trained convolutional neural descriptors, a fusion method not only retains the effective
networks (CNN) for keypoint description, and those based discriminative information of the source descriptorss, but
on layers of pre-trained CNN’s. also eliminates redundant information that may exist.
SIFT[5]isperhapsthemostrepresentativeofhand-crafted Data fusion is a popular technique for improving perfor-
descriptors and has been popularly used in various ﬁelds of manceincomputervisonresearch.Yangetal.[14]proposed
computer vision and robotics reasearch. However, the recent two feature fusion strategies for image classiﬁcation, one
rise of deep learning has created the opportunity to develop called serial feature fusion, which directly concatenates two
learning-based, data-driven techniques of keypoint descrip- descriptors,andtheotherparallelfeaturefusion,whichturns
tion.AmongtrainedCNNsforkeypointdescription,themost two descriptors into a complex vector γ = α + iβ (i is
notable models include L2-Net [6], HardNet [7], GeoDesc imaginary unit). The complex vector showed a better result
[8] and ContextDesc [9], and they generate descriptors with than the concatenation method and also improved the image
a length of either 128 or 256 dimensions, similar to that classiﬁcation accuracy. However, the complex vector cannot
of the hand-crafted descriptors such as SIFT. In parallel, a deal with the situation where the dimensions of the two
number of popular pre-trained CNN models exist, such as descriptors are different. Campos et al. [15] proposed a
AlexNet [10], ResNet101 [11] and DenseNet169 [12], and weighted-based fusion method to fuse the local features for
their various layers are candidates as keypoint descriptors robot visual localization. This method added a weight to
each descriptor, and then fused these descriptors by sum
1 arewiththeBiomimeticandIntelligentRoboticsLab(BIRL),Guang- or product methods. Their results showed that both sum
dongUniversityofTechnology,Guangzhou,China,510006.H.Zhangisthe
and product fusion methods can improve the accuracy of
correspondingauthor(hzhang@ualberta.ca)andisalsowiththeDepartment
ofComputingScience,UniversityofAlberta,AB,Canada. localization and that the sum fusion method is better than
TheworkinthispaperissupportedbytheNaturalScienceFoundation product.However,thismethodhasthesamelimitationasthe
of China (Grant No. 61673125, 61703115, 61773333), and the Frontier
parallelfusionmethod[14].Wangetal.[16]alsoproposeda
andKeyTechnologyInnovationFundsofGuangdongProvince(GrantNo.
2016B090910003). weightingstrategytocombinemultipleimagedescriptorsfor
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 65
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 10:51:07 UTC from IEEE Xplore.  Restrictions apply. Fig.1:Thedetailsofourdescriptorfusionmodel(DFM).Thedescriptorfusionstepsareasfollows.First,weuseakeypoint
×
detector (SIFT in our experiments) to detect K keypoints and then extract 64 64 image patches around these keypoints.
Secondly,eachoftheimagepatchesisfedintoatrainedCNNmodelandapre-trainedCNNmodeltogeneratetwokeypoint
descripors, for all the K keypoints. Subsequently, we use a convolutional auto-encoder to compress the descriptor from the
pre-trained model. Finally the descriptor from the trained CNN model and the compressed pre-trained descriptor are fused
byafully-connectedautoencoder.Foreitherautoencoder,itistrainedwiththestandardreconstructionlossand,aftertraining
only the encoder half is used.
loop closure detection. Unlike [15], they fused the weighted both dimension reduction and performance improvement.
descriptors by concatenation, to be able to handle source Inspired by [18] and [19], we use a fully connected
descriptors of different dimensions. In our DFM, the dimen- autoencoder (FCAE) to combine the trained and the pre-
sions of the trained and the pre-trained desciptors are also trained descriptors. In our preliminary study, we found that
different.Therefore,wealsouseconcatenationonthetrained direct fusion of a high-dimensional pre-trained descriptor -
and the pre-trained descriptors. However, the dimension of on the order of a few hundred thousand - and the much
the descriptor after concatenation can be high, and compu- shorter trained descriptor of a low dimension (e.g., 128)
tationally costly to use. In addition, the fusion techniques does not perform well. Instead, in our DFM we ﬁrst reduce
reviewed above make no attempt to learn or discover latent the dimension of the pre-trained descriptors down to a few
features that may exist in a low dimensional space and did thousandbeforefusingitwiththetraineddescriptors.Among
not learn a deeper relationship between descriptors. thedimensionalityreductionmethods,suchasprincipalcom-
ponent analysis (PCA) [20], random projection (RP) [21],
In recent years, deep neural network has achieved state-
Isomap [22] and LE [23], autoencoder is better than these
of-the-artresultsinvariousimageprocessingtasksincluding
dimension reduce methods and has proven to enhance the
data fusion. A multi-feature fusion deep network (MFFDN)
accuracy in [24]–[26], as will be veriﬁed in our comparative
was proposed in [17] and it combined ﬁve hand-crafted
study. To perserve the spatial information in the feature
descriptors by MFFDN and achieved excellent performance
maps of the pre-trained CNN models, we use convolutional
inimageclassiﬁcationtastonMNIST,CIFAR-10andSVHN
autoencoder (CAE) to reduce the dimension of the pre-
datasets. In an image classiﬁcation task, Zhou et al. [18]
trained descriptors. Finally, we fuse the trained and the
trained a simple two-layer network (softmax and fully con-
compressed pre-trained decriptors and obtain a descriptor
nected layers) attached to a pre-trained CNN (CaffeNet)
with the dimension of 128 or 256 with a fully connected
to fuse the descriptor from the CNN and a hand-crafted
autoencoder.
descriptor. The combination of the two descriptors was
The remainder of this paper is organized as follows. In
able to improve the performance of image classiﬁcation.
Section II, we will describe the details of our proposed
Similarly, in an image retrieval task, Jun et al. [19] also
descriptor fusion model (DFM). The experimental results
trained a fully connected layer whose input is a sum or
and discussion are shown in Section III. In Section IV, we
concatenation of multiple global descriptors generated by
conclude the study, and outline the future work.
ResNet. The performance of the summation technique is
found to be better than that of concatenation, and both II. PROPOSEDDESCRIPTORFUSIONMODEL
techniquesareabletoimproveimageretrieval.Thesestudies Previous research in learning-based keypoint description
are limited to image classiﬁcation or retrieval tasks, and did has pursued two classes of CNN descriptors: those speciﬁ-
not attempt the use of autoencoder as a fusion technique for callytrainedCNN’sforkeypointdescription,andthosebased
66
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 10:51:07 UTC from IEEE Xplore.  Restrictions apply. on layers of pre-trained CNN’s. Different CNN models have layer and a parametric rectiﬁed linear unit as the activation
their own strengths in terms of the type of conditions they function.Theuseofconvolutionallayersallowustopreserve
are able to handle well. In our previous work [13], we spatial information in the feature maps of the pre-trained
ﬁnd that the trained CNN’s perform better than pre-trained CNN models. Similarly, each block of the decoder includes
CNN’swithrespecttoviewpointchanges,andthepre-trained a deconvolutional layer, a batch normalization layer and a
CNN’s perform better than the trained CNN’s with respect parametric rectiﬁed linear unit as the activation function.
toilluminationchangesforkeypointmatching.Basedonthis We select pool3 of DenseNet169 [12] as our pre-trained
observation,weproposeadescriptorfusionmodel(DFM)to descriptor model, because it performed the best among all
fuse descriptors genereated by trained and pre-trained CNN the pre-trained descriptors in our previous work [13]. The
models, so that we can combine the advantages of the two feature maps of the pool3 of DenseNet169 have dimensions
× ×
classes of CNN descriptors to obtain a descriptor robust of 1280 12 12. The ﬁlter sizes of three encoder blocks
× × ×
to both illumination and viewpoint changes. Our proposed are 4 4, 3 3 and 3 3, respectively. The strides of
model is summarized in Fig. 1 and described in detail in three encoder blocks are 1, 2 and 1. The number of feature
this section. maps of the ﬁrst two encoder blocks is 640 and 256. In
order to generate descriptors of different dimensions in our
A. Keypoint descriptor fusion model
experimental comparison, the number of feature maps for
The steps of generating a keypoint descriptor in DFM the third encoder block is set to be 32, 64, 128, 256, 512
are as follows. First, we use a keypoint detector (SIFT in and 1024. The hyper-parameters of the decoders mirror the
our experiments) to detect K keypoints and then extract a encoders. After the CAE is trained ofﬂine, its encoder half
×
64 64 image patch around each keypoint. Secondly, each is used online to perform descriptor compression.
image patch is fed into a trained CNN model and a pre- For training the CAE model, we use a reconstruction loss
trained CNN model, respectively, to generate two keypoint function in terms of the mean squared error, and we use the
descripors. Subsequently, we use a convolutional autoen- Adamoptimizationalgorithmtolearnthemodelparameters.
coder(CAE)tocompressthedescriptorfromthepre-trained The learning rate is set at 0.001 and the batch size equals
model. Finally the descriptor from the trained CNN model 256. The model is trained for 50 epochs. We extract all the
and the compressed pre-trained descriptor are fused by a pre-traineddescriptors,80%ofwhichareusedasthetraining
fully-connected autoencoder (FCAE). The two autoencoders dataset and 20% are used as the test dataset.
are trained independently by minimizing reconstruction loss 2) Fully connected autoencder (FCAE): Since there is
and, during online operation, only the encoder half is used. little spatial information to preserve among the two input
The details of the CAE and FCAE are described in Section descriptors, we use a fully connected autoencoder (FCAE)
II-B. after concatenation to fuse the trained descriptor and the
WeuseaCAEtoreducethedimensionsofthedescriptors compressed pre-trained descriptor. This decision allows us
extracted from the pre-trained CNN model. The use of the tohandletwosourcedescriptorsofdifferentdimensionsand
CAE allows us to reduce the memory requirement of the is supported by [18] and [19], which used fully connected
descriptor generated by the pre-trained CNN, which is typi- layertofusemultipledescriptorsafterconcatenation.Forthe
cally on the order of a few hundred thousand, and improve choiceofatraineddescriptorinourDFM,weadoptHardNet
the discriminating power of the descriptor at the same time, [7] because of its robustness to viewpoint changes [13]. We
as we have found in our experiments. In addition, instead of shouldnote,however,thatourproposedDFMarchitectureis
using one autoencoder to perform both dimension reduction abletohandleanypre-trainedandtraineddescriptormodels.
andfusion,wechoosetousetwoseparateautoencoders,one Similar to CAE, each block of our FCAE is composed
for compressing the pre-trained descriptor (CAE in Fig. 1) of a fully connected layer, a batch normalization layer and
and the other for fusion (FCAE in Fig. 1), as this separation a parametric rectiﬁed linear unit as the activation function.
allowseachautoencodertospecializeandprovidesanoverall The input sizes of three encoder blocks are 4224, 512 and
optimal result. 256, respectively. The output sizes of the ﬁrst two encoder
blocksare512and256.Theoutputsizeofthethirdencoder
B. Convolutional autoencoder (CAE) and fully connected
is 128 or 256. The hyper-parameters of the decoders mirror
autoencder (FCAE)
the encoders. After the FCAE is trained ofﬂine, its encoder
The design of the CAE and the FCAE in our DFM is half is used online to perform descriptor compression.
based on the traditional autoencoder architecture, and they For training the FCAE model, we also use a reconstrution
each contains three encoders and three decoders. loss function in terms of the mean squared error, and the
1) Convolutional autoencoder (CAE): Among the di- Adamoptimizationalgorithmtolearnthemodelparameters.
mensionality reduction methods, autoencoder has proven to The learning rate is set at 0.0001 and the batch size we
enhancetheaccuracyin[24]–[26]atthesametime,although choose64.Themodelistrained200epochsandallcodesrun
the autoencoder models may differ in design details. In our on a computer with 16 CPUs (2.1 GHz), 128 GB memory
DFM, we use a CAE to compress the descriptors from and a Nvidia TITAN Xp GPU. We concat all compressed
a pre-trained CNN model. Each block of the encoder is pre-trained descriptors and trained descriptors as the FCAE
composed of a convolutional layer, a batch normalization dataset, of which 80% are used for training and 20% are
67
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 10:51:07 UTC from IEEE Xplore.  Restrictions apply. used for testing.
III. EXPERIMENTALRESULTS
In this section, we ﬁrst introduce the experimental dataset
and evaluation metric. Second, we discuss the effect of the
keypointpatchsizesonmatchingperformance,andchoosea
suitablepatchsizeforallthecompareddescriptorextractors.
Then,wecompareconvolutionalautoencoder(CAE)asadi-
mensionreductionmethodwithprincipalcomponentanalysis
(PCA) and random projection (RP) on the pre-trained CNN
descriptors. Next, we compare the fully connected autoen-
coder (FCAE) with product, summation and concatenation
as alternative techniques to fuse the trained and the pre-
trained descriptors. Finally, we compare our proposed DFM
descriptor with state-of-the-art descriptors and discuss the
experimental results on keypoint matching .
Fig. 3: The performance of Densenet169 on different key-
point patch sizes. In our experiment, we select 6 different
A. Experimental dataset and evaluation metric × ×
keypoint patch sizes ranging from 16 16 to 96 96. We
For evaluation, we use the comprehensive Hpatches ﬁnd that the patch size impacts the accuracy of keypoint
dataset [27], which includes 57 image sequences with il- matching. For example, matching accuracy with the patch
×
lumination changes and 59 image sequences with viewpoint size of 64 64 is 20% higher than that with the patch size
×
changes. Some example sequences are shown in Fig. 2. The of 16 16.
performancemetricofkeypointmatchingweuseisthemean
average precision (mAP), which is the same as in Balntas et
al. [27] and, for the keypoint matching steps, one can refer B. Keypoint patch size
to [13]. The keypoint patch size extracted from an image of
×
Hpatches in the original study [27] is 16 16. However,
we found that the keypoint patch size has a great impact
on accuracy in our experiment. We have therefore evaluated
thepre-traineddescriptorDenseNet169ondifferentkeypoint
× ×
patch sizes ranging from 16 16 to 96 96, and the result
×
can be seen in Fig. 3.Based on this study, we select 64 64
as the keypoint patch size in our experiment.
C. Dimensionalityreductionofthepre-trainedCNNdescrip-
(a)illuminationsequences
tors
In order to reduce the memory requirement of the de-
scriptor generated by pool3 of DenseNet169 [12], which is
typically on the order of a few hundred thousand, and to
improve the discriminating power of the descriptor at the
same time, we use the convolutional autoencoder (CAE)
introduced in Section II-B to compress the pre-trained CNN
descriptors.WecompareourCAEwithprincipalcomponent
analysis (PCA) [20] and random projection (RP) [21], two
(b)viewpointsequences
popular dimensionality reduction methods. The comparison
Fig. 2: Example images in the Hpatches dataset of (a) three
results are shown in Fig. 4.
illumination sequences and (b) three viewpoint sequences.
From the experimental results we ﬁnd that (a) the CAE
ThesubsetsofHpatchesweusedinclude57illuminationse-
method performs much better than PCA and RP methods,
quencesand59viewpointsequences.Eachsequenceincludes
this conclusion that is consistent with that in [25], (b) with
six images where the ﬁrst image serves as the reference and
the increase of descriptor dimension, the accuracy of key-
the others target images. For each sequence, we match the
point matching increases before it saturates or even declines
ﬁrst image with other ﬁve, calculate the average precision
(not shown) and (c) the CAE method not only reduces
(AP) for each pair of matching images, and then calculate
the pre-trained descriptors dimension, but also improves the
the mean AP for the ﬁve pairs.
performance of the pre-trained descriptors as deﬁned by the
“baseline” in Fig 4.
68
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 10:51:07 UTC from IEEE Xplore.  Restrictions apply. (a)DenseNet169-illumination (b)DenseNet169-viewpoint
Fig. 4: Comparison of three dimensionality reduction methods in terms of mean average precision (mAP) on Hpatches
datasets.Wecompareconvolutionalautoencoder(CAE)withprincipalcomponentanalysis(PCA)[20]andrandomprojection
(RP) [21], two popular dimensionality reduction methods. The methods are applied to the descriptors computed by pool3
of DenseNet169 with dimensions of 184320. The red lines represent the mAP of the descriptor before dimension reduction.
We reduce the dimensions of the DenseNet169-pool3 to 128, 256, 512, 1024, 2048 and 4096. We ﬁnd the CAE to perform
the best among the three dimensionality reduction methods on both the illumination and viewpoint datasets. In our ﬁnal
DFM, we use 4096 as the output dimension of the CAE.
TABLE I: Comparision of different fusion methods in terms
D. Fusion of the trained and pre-trianed CNN descriptors
of mean average precision (mAP) [%] on Hpatches datasets.
We use the fully connected autoencoder (FCAE) intro-
In order to choose a fusion method, we select product,
duced in II-B to fuse the trained descriptor and the com-
summation, concatenation and autoencoder on HardNet and
pressed pre-trained descriptor. In order to choose a fusion
DenseNet169.Inordertocomparethesefourfusionmethods
method, we compare FCAE with product, summation and
fairly, we compress the descriptor from DenseNet169 to a
concatenation, which are proven to enhance performance in
dimension of 128 with CAE. We ﬁnd the FCAE method is
[14] and [15]. For the trained descriptor, we select HardNet
the most accurate among the four fusion methods. The dim
[7] which performs the best in terms of viewpoint changes
meansthedescripordimension,andillumandviewrepresent
[13]. For the pre-trained descriptors, in order to be able to
two sub-datasets of Hpatches. Bolder numbers represent the
comparethefourfusionmethods,weusethe128-dimension
best performance among all the compared fusion methods.
DenseNet169 so that summation and multiple of two source
descriptors can be performed. Descriptors dim illum view mean
The results of the comparison among fusion techniques DenseNet169[12] 128 69.14 42.49 55.82
Baseline
HardNet+[7] 128 64.04 51.57 57.81
are summarized in Table I. As can be seen (a) the FCAE
Product[15] 128 62.25 46.97 54.61
fusion method is better than product, summation and con-
Summation[15] 128 68.41 53.28 60.81
Fusionmethods
catenation fusion methods, and (b) concatenation is better Concatenation[14] 256 70.26 53.56 61.91
than summation and product and, summation is better than FCAE 256 72.93 53.83 63.38
product.Thisconclusionisconsistentwiththatin[14],[15].
Concatenationdirectlylinksthetwodescriptors,addingmore
information, but does not ﬁnd the relationship between the
ContextDesc [9]. The results are shown in Table II. We ﬁnd
two descriptors. The FCAE adopts a fully connected layer
that our DFM256 descriptor performs the best on both the
that not only combines these two descriptors, but also learns
illumination dataset and viewpoint dataset, as well as on the
global information in the latent space of the concatenated
combineddataset.Inaddition,ourDFM128alwaysperforms
DenseNet169 and HardNet.
the second best except for one case against a descriptor
that is three orders of magnitude longer. Compared with the
E. Comparison with the state-of-the-art descriptors
HardNet and DenseNet169 before fusion, the mean mAP of
We compare our proposed DFM method with three pre- ourfusion-baseddescriptorisimprovedby6.45%and6.53%
trained descriptors from pool4 of AlexNet [10], pool3 of respectively. In addition, our DFM outperforms GeoDesc
ResNet101 [11] and DenseNet169 [12] and four trained and ContextDesc, two state-of-the-art keypoint descriptors,
descriptors from L2-Net [6], HardNet [7], GeoDesc [8] and by over 10%.
69
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 10:51:07 UTC from IEEE Xplore.  Restrictions apply. TABLEII:Comparisionoftheproposeddescriptorswiththe
pre-trained and trained descriptors in terms of mean average
precision (mAP) [%] on Hpatches datasets. Our proposed
descriptor is obtained by the fusion the trained descriptor
and the compressed pre-trained by two autoenocders whose
details can be seen in Fig 1. For the trained descriptor
model, we choose HardNet because of its robustness to
viewpoint changes. For the pre-trained descriptor, we sle-
cet DenseNet169 because of its robustness to illumination
changes [13]. Our fusion-based descriptor combines the
advantagesofthesetwodescriptors.Theoutputofourmodel
canbe128or256indimension.Inthetableheading,thedim
meansthedescripordimension,andillumandviewrepresent
two sub-datasets of Hpatches. Bold and underlined numbers
represent the best and the second best performing among all Fig. 5: Qualitative comparison of ﬁve descriptors for key-
the compared descriptors. points matching. From top to bottom: SIFT [5], GeoDesc
[8], ContextDesc [9], HardNet [7] and our DFM. The ﬁrst
Descriptors dim illum view mean
two columns of images are from illumination sequences of
AlexNet-pool4 30976 68.05 43.11 55.58
Pre-Trained ResNetNet-pool3 147456 69.52 42.45 55.96 Hpatches and the last two columns of images are viewpoint
DenseNet169-pool3 184320 72.30 43.16 57.73 sequences. For the convenience of viewing, we only show
GeoDesc[8] 128 59.00 46.29 52.65 the20pairsofmatchingkeypointswiththehighestsimilarity
ContextDesc[9] 128 59.60 49.70 54.65
Trained for each image pair. The red lines show incorrect matches
L2-Net+[6] 256 66.97 50.15 58.56
HardNet+[7] 128 64.04 51.57 57.81 and the blue lines the correct matches. Our DFM descriptor
DFM128(ours) 128 70.53 53.14 61.84 achieves the best results in both the illumination sequences
Fusion
DFM256(ours) 256 74.13 54.38 64.26 and viewpoint sequences.
In Fig. 5, we show some qualitative results for keypoint dataset, respectively. The main reason for the improvement
matching of ﬁve descriptors: SIFT, GeoDesc, ContextDesc, may be that the information in the latent space deﬁned by
HardNet and our DFM. For visibility, we only show in each the two descriptors is learned by the FCAE. Our proposed
image pair 20 of the matched keypoints with the highest DFM architecture can handle any trained and pre-trained
similarities. Our DFM descriptor achieves the best results descriptornetworksincludingbutnotlimitedtoHardNetand
to handle either in the illumination or viewpoint changes DenseNet169.
environment.
IV. CONCLUSIONANDFUTUREWORK
F. Discussion
Inthispaper,adescriptorfusionmodel(DFM)isproposed
Basedonthedifferentpropertiesofthetrainedandthepre- to fuse two leading keypoint descriptors from the trained
trained descriptors for handling illumination and viewpoint and pre-trained CNN models using two autoencoders (CAE
changes [13], we propose a descriptor fusion model (DFM) and FCAE), leading to a keypoint description method that is
tocombinethetrainedandthepre-trainedCNNmodelsusing superior to the state-of-the-art. We have shown that, (a) our
twoautoencoders(CAEandFACE),sothatwecancombine fusion-based descriptor combines the advantages of trained
the advantages of the two classes of CNN descriptors to and pre-trained descriptors and improves the descriptor ac-
obtainadescriptorrobusttobothilluminationandviewpoint curacy in handling both illumination and viewpoint changes
changes. The CAE is used to reduce the dimension of the (b) the compressed pre-trained descriptors are better than
pre-trained descriptor. As can be seen in Fig. 4, when the theuncompresseddescriptorsfromconvolutionlayersofpre-
dimension of DenseNet169 is higher than 256, the CAE not trainedCNNmodels(c)theCAEisabetterdimensionreduc-
only reduces the dimension, but also improves the mAP of tion technique than principal component analysis (PCA) and
DenseNet169.Thisshowsthatthereisredundantinformation random projection (RP) in terms of matching performance,
in the high-dimensional descriptor extracted directly with and (d) the FCAE is a better fusion method than product,
pool3 of DenseNet169. We found that this conclusion also summation and concatenation. Although our proposed DFM
applies to other pre-trained descriptors in our experiments. uses pre-trained and trained CNN models to generate the
The FCAE is used to fuse the trained and the compressed input descriptors to the fusion process, it is not limited to
pre-traineddescriptors.AscanbeseeninTableI,theFCAE thesetwoclasses.Infact,anyexistingdescriptormodelscan
canwellintegratethetrainedandthecompressedpre-trained beusedwithinourDFMframeworkinconstructinganalter-
descriptors, and create a ﬁnal descriptor of low dimension nativecompetingdescriptorwithimprovedperformance.Our
(128 or 256). The mean mAP of using FCAE is 5.57% DFM is available at https://github.com/dytrong/
and 7.56% higher than HardNet and DenseNet on Hpatches Descriptor-Fusion-Model.
70
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 10:51:07 UTC from IEEE Xplore.  Restrictions apply. REFERENCES [23] M. Belkin and P. Niyogi, “Laplacian eigenmaps and spectral tech-
niquesforembeddingandclustering,”inAdvancesinneuralinforma-
tionprocessingsystems,2002,pp.585–591.
[1] J.L.SchonbergerandJ.-M.Frahm,“Structure-from-motionrevisited,”
in Proceedings of the IEEE Conference on Computer Vision and [24] Y. Wang, H. Yao, and S. Zhao, “Auto-encoder based dimensionality
PatternRecognition,2016,pp.4104–4113. reduction,”Neurocomputing,vol.184,pp.232–242,2016.
[25] J. Liu, C. Li, and W. Yang, “Supervised learning via unsupervised
[2] J.L.Scho¨nberger,E.Zheng,J.-M.Frahm,andM.Pollefeys,“Pixel-
sparseautoencoder,”IEEEAccess,vol.6,pp.73802–73814,2018.
wiseviewselectionforunstructuredmulti-viewstereo,”inEuropean
[26] S. Petscharnig, M. Lux, and S. Chatzichristoﬁs, “Dimensionality
ConferenceonComputerVision. Springer,2016,pp.501–518.
reduction for image features using deep learning and autoencoders,”
[3] L.Zheng,Y.Yang,andQ.Tian,“Siftmeetscnn:Adecadesurveyof
inProceedingsofthe15thInternationalWorkshoponContent-Based
instanceretrieval,”IEEEtransactionsonpatternanalysisandmachine
MultimediaIndexing. ACM,2017,p.23.
intelligence,vol.40,no.5,pp.1224–1244,2018.
[27] V. Balntas, K. Lenc, A. Vedaldi, and K. Mikolajczyk, “Hpatches: A
[4] F.FraundorferandD.Scaramuzza,“Visualodometry:Partii:Match-
benchmark and evaluation of handcrafted and learned local descrip-
ing, robustness, optimization, and applications,” IEEE Robotics &
tors,” in Conference on Computer Vision and Pattern Recognition
AutomationMagazine,vol.19,no.2,pp.78–90,2012.
(CVPR),vol.4,no.5,2017,p.6.
[5] D. G. Lowe, “Distinctive image features from scale-invariant key-
[28] K. Mikolajczyk and C. Schmid, “An afﬁne invariant interest point
points,” International journal of computer vision, vol. 60, no. 2, pp.
detector,”inEuropeanconferenceoncomputervision. Springer,2002,
91–110,2004.
pp.128–142.
[6] Y.Tian,B.Fan,andF.Wu,“L2-net:Deeplearningofdiscriminative
patch descriptor in euclidean space,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, 2017, pp.
661–669.
[7] A.Mishchuk,D.Mishkin,F.Radenovic,andJ.Matas,“Workinghard
to know your neighbor’s margins: Local descriptor learning loss,” in
AdvancesinNeuralInformationProcessingSystems,2017,pp.4826–
4837.
[8] Z. Luo, T. Shen, L. Zhou, S. Zhu, R. Zhang, Y. Yao, T. Fang, and
L.Quan,“Geodesc:Learninglocaldescriptorsbyintegratinggeometry
constraints,”inProceedingsoftheEuropeanConferenceonComputer
Vision(ECCV),2018,pp.168–183.
[9] Z. Luo, T. Shen, L. Zhou, J. Zhang, Y. Yao, S. Li, T. Fang, and
L. Quan, “Contextdesc: Local descriptor augmentation with cross-
modality context,” in Proceedings of the IEEE Conference on Com-
puterVisionandPatternRecognition,2019,pp.2527–2536.
[10] A.Krizhevsky,I.Sutskever,andG.E.Hinton,“Imagenetclassiﬁcation
with deep convolutional neural networks,” in Advances in neural
informationprocessingsystems,2012,pp.1097–1105.
[11] K.He,X.Zhang,S.Ren,andJ.Sun,“Deepresiduallearningforimage
recognition,” in Proceedings of the IEEE conference on computer
visionandpatternrecognition,2016,pp.770–778.
[12] G.Huang,Z.Liu,L.VanDerMaaten,andK.Q.Weinberger,“Densely
connectedconvolutionalnetworks.”inCVPR,vol.1,no.2,2017,p.3.
[13] Z. Dai, X. Huang, W. Chen, L. He, and H. Zhang, “A comparison
ofcnn-basedandhand-craftedkeypointdescriptors,”in2019Interna-
tionalConferenceonRoboticsandAutomation(ICRA). IEEE,2019,
pp.2399–2404.
[14] J.Yang,J.-y. Yang,D.Zhang, andJ.-f.Lu,“Feature fusion:parallel
strategy vs. serial strategy,” Pattern recognition, vol. 36, no. 6, pp.
1369–1381,2003.
[15] F. M. Campos, L. Correia, and J. M. Calado, “An evaluation of
local feature combiners for robot visual localization,” in 2013 13th
InternationalConferenceonAutonomousRobotSystems. IEEE,2013,
pp.1–6.
[16] X. Wang, G. Peng, and H. Zhang, “Combining multiple image de-
scriptionsforloopclosuredetection,”JournalofIntelligent&Robotic
Systems,vol.92,no.3-4,pp.565–585,2018.
[17] G. Ma, X. Yang, B. Zhang, and Z. Shi, “Multi-feature fusion deep
networks,”Neurocomputing,vol.218,pp.164–171,2016.
[18] Z. Tianyu, M. Zhenjiang, and Z. Jianhu, “Combining cnn with
hand-crafted features for image classiﬁcation,” in 2018 14th IEEE
InternationalConferenceonSignalProcessing(ICSP). IEEE,2018,
pp.554–557.
[19] H. Jun, B. Ko, Y. Kim, I. Kim, and J. Kim, “Combination
of multiple global descriptors for image retrieval,” arXiv preprint
arXiv:1903.10663,2019.
[20] C. Rodarmel and J. Shan, “Principal component analysis for hy-
perspectral image classiﬁcation,” Surveying and Land Information
Science,vol.62,no.2,pp.115–122,2002.
[21] X. Z. Fern and C. E. Brodley, “Random projection for high dimen-
sionaldataclustering:Aclusterensembleapproach,”inProceedings
ofthe20thinternationalconferenceonmachinelearning(ICML-03),
2003,pp.186–193.
[22] J.B.Tenenbaum,V.DeSilva,andJ.C.Langford,“Aglobalgeometric
frameworkfornonlineardimensionalityreduction,”science,vol.290,
no.5500,pp.2319–2323,2000.
71
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 10:51:07 UTC from IEEE Xplore.  Restrictions apply. 