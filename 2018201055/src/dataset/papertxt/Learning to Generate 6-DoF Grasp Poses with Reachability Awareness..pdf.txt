2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
Tightly-Coupled Single-Anchor Ultra-wideband-Aided
Monocular Visual Odometry System
Thien Hoang Nguyen, Thien-Minh Nguyen*, and Lihua Xie, Fellow, IEEE
Abstract—In this work, we propose a tightly-coupled odom- additional high-quality IMU sensor is still required, which
etry framework, which combines monocular visual feature addsamajorcostandextralayerofcomplexitytothesystem.
observations with distance measurements provided by a single
In exchange for the ﬂexibility and low demand on com-
ultra-wideband (UWB) anchor with an initial guess for its
putational resources, two particular challenges need to be
location.Firstly,thescalefactorandtheanchorpositioninthe
vision frame will be simultaneously estimated using a variant addressed when using a monocular VO/SLAM system. The
of Levenberg-Marquardt non-linear least squares optimization ﬁrst is scale ambiguity, whereby from a sequence of images
scheme. Once the scale factor is obtained, the map of visual provided by one camera, one can only obtain the knowl-
features is updated with the new scale. Subsequent ranging
edge of relative scale about 3D distances in the perceived
errorsinaslidingwindowarecontinuouslymonitoredandthe
environment and not metric scale. For monocular systems,
estimationprocedurewillbereinitializedtoreﬁnetheestimates.
Lastly, range measurements and anchor position estimates are scale ambiguity is inherent since depth information of 3D
fused when needed into a pose-graph optimization scheme to sceneislostwhenprojectedonto2Dframe.Allestimatesof
minimize both the landmark reprojection errors and ranging camerapositionsanda3Drepresentationoftheenvironment
errors,thusreducingthevisualdriftandimprovingthesystem
are therefore calculated “up to a scale”. The second issue is
robustness. The proposed method is implemented in Robot
“scale drift” [2] where the scale factor has to be adjusted in
Operating System (ROS) and can function in real-time. The
performance is validated on both public datasets and real-life different regions of the map. In principle, initial scale esti-
experiments and compared with state-of-the-art methods. mate in monocular odometry can be corrected by adjusting
thevaluesofparameters.However,sincescaleisarbitraryin
I. INTRODUCTION each run even with the same algorithm, this is not a viable
In recent years, it can be seen that vision-based localiza- solution for fast deployable platform like MAV.
To overcome these challenges, this work aims to leverage
tionmethodssuchasvisualodometry(VO)andsimultaneous
point-to-pointrangemeasurementsprovidedbyUWBsensor
localizationandmapping(VSLAM)havebecomeanintegral
such that not only a metric scale factor can be obtained, but
part of robotics research. As technology progresses, many
also the scale drift would be corrected since scale correc-
lightweight, energy-efﬁcient but high performance cameras
tion is performed continuously along the MAV’s trajector..
have come into the ﬁeld and offer strong advantages over
Among various wireless technologies such as ZigBee, BLE
more expensive and heavier alternative sensors for SLAM,
or Wiﬁ, UWB is chosen thanks to its properties of strong
such as laser and LiDAR. While a wide variety of sensors
multi-path resistance and accurate ranging measurement in
have been studied in various VO and SLAM systems, e.g.,
indoor, GPS-denied and cluttered environment [3], [4].
stereo camera, infrared (IR) and thermal cameras, laser
scanner LiDAR, etc., traditional monocular camera systems
II. LITERATUREREVIEW
are still attracting great interests from the community due
A. Methods for metric scale correction
to its high ﬂexibility and easy integration with many mo-
bile platforms. A detailed comparison of monocular, stereo Many methods have been proposed to address the scale
and multi-camera visual odometry pipelines [1] shows that problems in the literature. For metric scale recovery, the
among these approaches, monocular setup would be the commonsolutionistointroduceatleastoneadditionalsensor
most preferred solution for many real-world applications that provides metric measurements of some kind into the
where constraints on size and weight put hard restriction monocular system. Through bundle adjustment (BA) in the
on the kind of sensors and computational resources that can back-end task [5]–[7], one can ﬁnd an optimal solution for
be carried by the robot. More speciﬁcally, monocular VO the whole trajectory, then the drifts can be corrected, at the
setup does not require sufﬁciently large baseline between expenseofhighcomputationalcost.However,ifnoloopsare
cameras like stereo/multi-camera setups, and while visual- encountered in the trajectory of the robot, scale drift would
inertial alternatives can employ the ubiquitous IMU sensor still be an issue for the front-end odometry.
on Micro Aerial Vehicle (MAV), the low-cost nature of In stereo [8] or multi-camera [9] setups, since ﬁxed base-
the sensor often makes good performance unattainable. An line length is provided, depth estimates can be directly com-
putedandmetricscalecanthusbeobtained.However,depth
The authors are with School of Electrical and Electronic Engineering, estimation range is highly dependent on baseline length,
NanyangTechnologicalUniversity,Singapore639798,50NanyangAvenue. distances from objects to baseline, and calibration accuracy
*Emailofcorrespondingauthor:thienminh.npn@ieee.org
between cameras. When the scene is far away or objects are
Avideosummarizingthemainconceptsandexperimentalresultsofthis
papercanbeviewedathttps://youtu.be/Z8hDW6Zf8io. tooclosetooneofthecameras,stereovisionwilldeteriorate
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 665
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 16:49:59 UTC from IEEE Xplore.  Restrictions apply. to monocular case. Signiﬁcant computational resources are [25] or RGB-D-based [26]. Furthermore, when fused with
also required to perform various image processing tasks. other sensors, UWB is able to provide centimeter level of
A popular approach is incorporating sensors that can pro- localization accuracy while being robust to multipath and
vide range measurements such as 1D/2D/3D LiDAR [10]– non-line-of-sight (NLOS) effects [3], [27]. However, these
[12], RGB-D camera [13], ultrasonic altimeter, radar etc., approaches assume prior knowledge of anchor positions and
eachofwhicheffectivelyprovidesdistancemeasurementsin require at least four UWB anchors for full 3D localization
different dimensions. By exploiting the right properties in which might not be practical in cluttered, dynamic environ-
each conﬁguration, the scale can be accurately recovered. ments like industrial facilities and warehouses.
On the other hand, considering that each sensor comes Without a setup of sufﬁcient number of anchors for
with its own limitation, the applicable scenario might be full 3D localization, UWB distance measurements between
restricted after the fusion of odometry and range data [10], robotsinaformationhavebeenusedforcooperativerelative
[14]. For example, when 1D LiDAR is used, the camera is localization and control problem in [28]. The use of single
assumed to face a ﬂat surface. Most 2D and 3D LiDAR UWB anchor placed at an arbitrary unknown position was
sensors are relatively heavy, power hungry and require high studied in [29], [30] for a distance-based docking problem
computationalpower,whichisnotavailableonalightweight of MAVs without the need of visual information. In [31],
MAV.DepthresolutionandrangeofRGB-Dcameraisoften an optimization-based approach was proposed to perform
limited and deteriorate in outdoor scenes. scale and orientation correction of different trajectories with
It can be seen that the most successful method to acquire 1D UWB distance measurements between points on those
metric-scaled odometry data is the visual-inertial odome- trajectories. However, this method only considers movement
try (VIO), where IMU measurements and VO are fused in 2D case and required a height sensor. Furthermore, the
together to estimate both ego-motion and map coordinates whole trajectory was used to reach a desirable solution, thus
in true scale. Many state-of-the-art approaches [15]–[17] the performance of the system was only veriﬁed ofﬂine.
can demonstrate very high accuracy and robustness in a
wide range of environments. Nonetheless, coupling of IMU
C. Main Contributions
and camera often requires carefully supervised initial states
andaccuratemulti-sensorcalibration(includingintrinsicand In summary, the main contributions of this work include:
extrinsiccalibrationparameters,IMUbiasescharacterization,
• a variant of the Levenberg-Marquardt method in which
time-synchronization between sensors), so that local minima
given an initial guess of the UWB anchor position, the
[18] or divergence can be avoided. Notably, VINS-Mono
original up-to-scale position from monocular odometry
[15] proposed an in-ﬂight initialization routine to solve the
is tightly combined with distance measurements to si-
problemofobtainingprecisecamera-IMUcalibrations.Even
multaneously estimate the scale factor and anchor posi-
then, the performance is heavily dependent on the quality of
tion,effectivelyaddressingthescaleambiguityproblem;
the IMU sensor, of which the size and cost are often major
• a real-time monitoring process of ranging errors in
considerations in practical applications.
a sliding window to trigger the estimation procedure
Other approaches to recover scale include learning the
to continuously reﬁne the scale and anchor position
scene depth, dimensions of objects [19]–[21] or applying an
estimates, hence the scale drift problem is diminished;
adaptivecontrolstrategy[22].Depthlearning-basedmethods
• an extension of the ORB-SLAM [5] pose-graph op-
[19], [20] are applicable in a wide range of scenarios, but
timization scheme that aims to minimize both the
requires large amount of data and high computing power
landmark reprojection errors and ranging errors, which
platform for training and deployment. If a known target
aims to reduce the visual drift and improve the overall
is exploited as a priori information for initialization [21],
robustness of the system.
once the robot moves away from the original scene the
scale drift problem will not be addressed. An interesting The advantages of such system are: 1) a monocular
solution is controlling the robot’s movements to recover camerawouldgreatlyalleviatethecomplexityofmechanical
metric scale through observations on control gains [22], and software design, compared to stereo/multi-camera setup
in which neither additional sensors nor high computation that requires large baseline or good IMU-camera intrin-
resourcesareneeded.However,anaccuratemotionmodelof sic/extrinsic calibrations for VIO setup, 2) the proposed
the robot in the vertical axis is necessary, and the robot will approachrequirestheleastnumberofUWBsensorsandwith
have to spend its limited ﬂight time on correcting the scale unknown positions as opposed to UWB-only localization
with vertical motions multipletimes throughout the mission. methods. However, it is noted that the solution would be
limited by the maximum rangeof the UWB sensor and line-
B. UWB-based and UWB-Aided Localization
of-sight (LOS) conditions of the environment. Additionally,
Range-based localization such as UWB has the capability movements on a sphere centered at the anchor position, in
to overcome the shortcomings of vision-based methods in theory, would render the scale unobservable. As such, the
reﬂective or featureless environments [23]. Other methods system is most effective if the environment permits LOS
of localization can also employ UWB data to improve from the anchor to the robot, and the robot’s movement is
estimation accuracy, such as VO [23], [24], LiDAR-based established on multiple axes.
666
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 16:49:59 UTC from IEEE Xplore.  Restrictions apply. ∼N
III. PROBLEMFORMULATION with η (0,Ω ) is assumed to be a zero mean Gaussian
k k
A. System Overview noise [23]. dk is directly referred as distance from UWB
anchor to camera since the camera and UWB sensor are
attachedon arigid bodyand thetranslational offset between
UWB antenna and camera is considered neglectable. In this
work, two-way TOF UWB sensor is used for it does not
require clock synchronization between anchors.
2) Operating phases: The overall architecture of the
proposed system is illustrated in Fig. 2. A new thread for
estimatingthescalefactorandtheanchorpositionaswellas
monitoringrangingerrorsisaugmentedontopoftheexisting
ORB-SLAM [6] framework. The operation consists of the
following three phases:
1) After the monocular odometry pipeline is initialized,
N
a data is collected, which composes of m arbitrar-
m
ily scaled odometry positions and their nearest range
N { (cid:62) }
measurements, i.e. = (p ,d ) i=m.
N m i i i=1
2) Once is ﬁlled, an estimation procedure as de-
Fig. 1: Overview of the coordinate frames and the operating m
scribed in III-B is carried out to estimate both s and
phases of the system.
pa concurrently. Aninitial guess pa is providedfor the
0
anchor position but not for the scale factor (s = 1).
0
After each successful run, the visual map is updated
with the new scale factor, with the position of all the
keyframes and map points rescaled.
N
3) The sum of ranging errors of the data in is
m
monitoredtoreinitializephase2withthecurrentanchor
position estimates if it rises above a certain threshold.
As discussed in III-E, if the anchor position is stable
overapre-deﬁnednumberofruns,thepositionisadded
as a vertex with subsequent ranges as edges to the
current pose in the pose-graph optimization scheme.
B. Non-linear least squares regression
∈
Let s R1 be the scale factor, such that metric position
| |
can be recovered with s p . The absolute operator is used
k
to restrain sign ﬂip of scale e(cid:13)stimates without an(cid:13)explicit
(cid:13) (cid:13)
constraint s>0, as explained i(cid:13)(cid:13)nIII-D. We have (cid:13)(cid:13)
(cid:13) (cid:13)
(cid:13) pax px (cid:13)
Fig. 2: The architecture of the proposed system, based on dk =(cid:107)(cid:13)(cid:13)(cid:13)pak−|sk|pk(cid:107)=pkaky −(cid:13)(cid:13)(cid:13) |sk| pkyk
the monocular ORB-SLAM framework [5]. (cid:13) paz (cid:13) pz
(cid:13) |k |(cid:13) k
{ } (cid:13) − s (cid:13)
1) Basic deﬁnitions: As depicted in Fig. 1, let C and (cid:13) px 1 0 0 k (cid:13) (2)
{ } (cid:13) − k pax (cid:13)
V bethecam(cid:98)eraandvisioncoordinateframe,respectively. = py 0 1 0 k
{ } { } − k pay
Theoriginof V correspondsto C atthecamera’sinitial pz 0 0 1 k
{ } { } k paz
pose, i.e. V = C t=0. At time instance k, the odom(cid:62)etr∈y (cid:107) (cid:107) k
output{for}position of our system is pk = [pxk, pyk, pzk] (cid:107)∗(cid:107)= Akβk ,
R3 in V frame, the nearest associated range measurement with denotes Euclidean norm of the argument vec-
∈
from the UWB anchor to UWB sensor on the MAV is d tor, all the unknown parameters are grouped into a vector
Rarsapn,necagehnedodrmcppeaoakanssdi=utirtoie[mnpmakeiexnno,t{fpVdﬂakki}yg,ihsfrtp,aoakmbwzte]ha(cid:62),iincw∈heidtiRhsb3ymaniemsainutshuiltteriieapedllsigctfiaurmoteimasostnetpdhoa0efU.ltWiTikgmhhBeet β{(kpi=(cid:62),(dsik)},pii==akm1x,,pβoakuykr,=paakizm(s)(cid:62)ki,s.ptGakoxiv,ﬁpennakdya,thpesakezvt)e(cid:62)oc,ftomr ofdaptaaraNmmete(3=rs)
stamps TRx and TTx of when the UWB ranging signal is where k is the time index of the latest data added to the
sent andMre1ceived. MTa0king into account the processing time window, that minimizes the(cid:88)sum of squared errors
delay σ of the UWB sensors, we have
k − − k
TRx TTx σ Er = r 2(β ). (4)
dk =c M1 2M0 k +ηk (1) k i=k−m i k
667
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 16:49:59 UTC from IEEE Xplore.  Restrictions apply. (cid:107) (cid:107)
Let be y =d , f(β )= A β , the residuals are 2) Dynamic learning rate: To incorporate the static na-
i i k k k
ture of the anchor, we apply a time-varying learning rate
−
ri(βk)=yi2 f(βk)2. (5) γ =diag(1,γ(k),γ(k),γ(k))tothecomponentsoftheshift
vector ∆β corresponding to the anchor position pa, with
With the cost function (4), the optimal values for pa and
s can be obtained through the minimization of γ(k)=γ e−(k/τ), (10)
0
∗
βk =argminEkr. (6) where γ is an initial value, τ is a time constant and k is
βk the num0ber of run times. In essense, pa should be initially
Remark III.1. The following formulas for the residuals r estimatedwithlargegradientandﬁne-tunedwithsmallerand
i
have been validated in experiments: smaller gradient as pa becomes more precisely determined.
−(cid:107) −| | (cid:107) 3) Sign-bounded cost function: All distance measure-
r =d pa s p ,
i i −(cid:107)i −| | i (cid:107) ments would be unchanged if the trajectory and the position
ri =di2 pai s pi 2, of the anchor are mirrored on one plane or axis of {V}.
neither of which showed signiﬁcant and consistent improve- In such cases, signs of scale estimates s can be ﬂipped
but the cost function value will still be the same, resulting
ments compare to the other based on experimental results
in local minimas. To address this issue, one can keep the
of RMSE errors. However, the calculation of the Jacobian
matrix from the former involves a square root in the denom- sign of s positive or ﬁx the position of anchor as known
parameters, with the former being the only viable option as
inator, which might lead to the division by zero issue.
{ }
thetrueanchorpositionin V isunknown.Sincethetheory
C. Existence of solution of LM algorithm does not deﬁne a way to handle explicit
bound constraints, the scale-corrected position is formulated
ForUWBsensorsthatusetime-of-ﬂighttechnology,range | |
as s p so that ﬁnal position output never changes sign.
measurementd willbetheshortestwhenthereisdirectLOS k
k 4) Start and stop conditions: Except for the ﬁrst batch
path between the antennas. Taking multipath and non-LOS
of data, a condition is checked when new odometry data
effects into account, the actual measurements would tend to
p˜ arrives. If the new position data is outside a pre-deﬁned
be higher than ideal cases due to propagation delay. With N
diameter ρ from the last point in the window , i.e.
that insight, we can then rewrite equation (2) as m
(cid:107) − (cid:107)
d ≥(cid:107)A β (cid:107),or β (cid:62)A (cid:62)A β ≤d 2. (7) p˜ pm >ρ, (11)
k k k k k k k k
The problem can be viewed as ﬁnding a solution within the then it will be added to the window. Otherwise, p˜ is dis-
carded and previous estimations are carried on until the next
intersection of the convex regions created by each range
measurement constraints since Ak(cid:62)Ak is positive semi- daatthariesshreocldei,vtehde.FeostrimthaetiﬁornstprruonceodruwreheinsevtreirggEekrrerdiseasndabwovilel
deﬁnite. With d > 0, a feasible solution can be found
sreingcioentsh.eItphoainstbβekekn=sh0owisncthoanttatihneedopitnimeiazcahtioonf pthroebcleomnveoxf raupnreit-edreaﬁtinveedly(cid:12)(cid:12)(cid:12)(cid:104)liumnittilLthoernaumcobnevr(cid:105)eorgfeintecreatciro(cid:12)(cid:12)(cid:12)intesrilonhadseeﬁxnceededaesd
similar structure will have a convex hull deﬁned by convex − − −
constraints, and the global optima must lie on the convex Ekr(l 1) Ekr(l) /Ekr(l 1) <ζ, (12)
hull [31].
is met, where Er(l) is the sum of squared errors at data
k
D. Modiﬁed Levenberg-Marquardt algorithm samplek anditerationl,ζ =0.0001isanumericalconstant.
In this section we describe our variant and modiﬁcations
E. Joint optimization for UWB-aided visual odometry
of the Levenberg-Marquardt (L-M) algorithm [32] to solve
the optimization problem (6). Updating scale factor would not address the inevitable
1) Recursive LM algorithm: At time instance k, starting drift that exists in vision-only estimator. In this work, we
withaninitialguessβ(0),theparametervectorβ isreﬁned propose reducing the visual drift by utilizing camera and
k k
after each iteration l by applying: UWBmeasurementsinatightlycoupledmanner.Inaloosely
coupled scheme [33], image and range measurements are
β(l+1) =β(l)+γ∆β, (8) processed separately to produce a up-to-scale position and
k k
a scale factor, which are then multiplied to obtain the
where ∆β is the shift vector and γ is the dynamic learning ﬁnal position. The proposed tightly coupled approach would
rate. ∆β can be found via solving the equation: compute the odometry output directly from the images and
(cid:62) (cid:62) − rangemeasurements,whichwouldimprovetherobustnessof
(J J+λI)∆β =J [y f(β )], (9)
k the system since range data is a reliable source of constraint
where I is the identity matrix, J = [J ] with J = tofacilitatepositiontrackingaswellasreducingvisualdrift.
− ij ij
∂r /∂β being the Jacobian matrix, λ is a non-negative Every new j-th keyframe is augmented by the nearest
i j
damping parameter, adjusted at each iteration l. rangemeasurementd andestimatedanchorpositionpa.The
j j
668
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 16:49:59 UTC from IEEE Xplore.  Restrictions apply. costfunctionfortheoptimizationschemeinthelocalbundle
(cid:88)(cid:88) (cid:13) (cid:13)
adjustment thread would include t(cid:13)he range(cid:13)error constraints:
P K − −
argminEv+ ρ( pa p d ), (13)
j i j
pi i=0j=0
where P is the number of optimized camera poses, K is the
number of keyframes associated with range constraints. Ev
is the cost of visual measurements which uses the weighted
normofthereprojectionerror,asoriginallydescribedin[5],
and ρ(.) is the Huber loss function that ensures the effects
of outliers and noise can be diminished. Fig. 3: Results with MH 01 dataset: proposed (blue) and
ground truth (red) trajectories. The ﬁrst batch of data is
IV. EXPERIMENTALRESULTS
collected at t=54s, which is 4s after the MAV starts moving.
In this section, we present results performed on EuRoC
MAV datasets [34] and real-life experiments. The proposed VINS-
Sequence Proposed OKVIS ROVIO
system was implemented in ROS1 with loop-closure thread Mono
MH01 0.16 0.16 0.21 0.27
disabled. The results are validated with Root Mean Square
MH02 0.11 0.22 0.25 0.12
Error(RMSE)ofabsolutetranslationerror(ATE)andrelative MH03 0.15 0.24 0.25 0.13
poseerror(RPE)[35],whichindicatestheglobalconsistency MH04 0.25 0.34 0.49 0.23
MH05 0.24 0.47 0.52 0.35
anddriftoftheestimatedtrajectory,respectively.Thesystem
V101 0.18 0.09 0.10 0.07
runs in real-time for all of the experiments. V102 0.25 0.20 0.10 0.10
V103 0.32 0.24 0.14 0.13
A. Public datasets
TABLE I: Comparison of RMSE values of ATE (m) on
Since UWB distance measurements are not available in
EuRoC datasets. The best results are highlighted in bold.
the EuRoC datasets, UWB anchor is simulated as a virtual
anchor placed at the origin of the ground truth frame. ATE(m) RPE(m/s)
Sequence
Range data is the Euclidean norm of ground truth positions Proposed ORB-Stereo Proposed ORB-Stereo
HH01 0.23 0.25 0.04 0.2
added Gaussian noise with standard deviation Ω = 0.05. HH02 0.20 0.24 0.04 0.03
The simulated UWB runs at the same frequency as the HH03 0.09 0.24 0.02 0.03
groundtruthpositiondata(20Hz).InitialguessfortheUWB HH04 0.24 0.22 0.02 0.03
(cid:62) HH05 0.28 0.21 0.06 0.02
anchor position is always set at pa0 = (0.5,0.5,0) in all HH06 0.18 0.23 0.04 0.02
experiments, which would gradually approach to the true
location of the simulated anchor in {V} frame. The initial TABLE II: Comparison of RMSE of ATE (m) and RPE
guess for the scale factor is always set as s =1, since the (m/s) on indoor experiments. The best results are in bold.
0
original scale is arbitrary. The algorithm generally performs
better with increasing size of sliding window, but in our
N
evaluations the size of is ﬁxed to m=100 data points,
m
which typically spans over 4s of image streams. It is noted
that given the same initial parameters and over the same
dataset, the ﬁnal RMSE value might vary depends on how
far the initial arbitrary scale is from the true value. Thus, 5
trials are conducted on each dataset and the reported RMSE
results are the average of all the RMSEs obtained. Fig. 3
shows the simulated UWB range measurements, position ×
Fig.4:Hardwaresetupandexperimentalenvironments:6m
output on each axis and overall 3D trajectory in one of the
6m Vicon room (left) and 170m long corridor (right).
experimentsperformedonMH 01dataset.TableIshowsthat
the proposed method can achieve the same level of accuracy
forRMSE,withresultsofstate-of-the-artmethodsfrom[36]. ranges at 40Hz. While only the left image stream and UWB
data are used by the proposed system, stereo images are
B. Indoor and outdoor real-life experiments
recorded and tested ofﬂine with ORB-SLAM2 Stereo [6].
The proposed system was further validated with real-
Theindoortestingareaisa6mx6mroomequippedwith
life experiments where results are summarized in Table II.
Vicon system. All experiments have different trajectories
HardwaresetupconsistsofanIntelNUCi7,astereocamera2
and anchor positions. Potential outliers in range data are
and P440 UWB sensor3, which provide images at 30Hz and
rejected and a smoothing ﬁlter is applied before fusion. Size
N
of is set at m = 100, which corresponds to 3.5s of
1https://www.ros.org/ m
data. Initial guess for scale is identity, while initial guess for
2https://www.mynteye.com/
3https://www.humatics.com/products/scholar/ anchor position is measured from the starting point. Other
669
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 16:49:59 UTC from IEEE Xplore.  Restrictions apply. Fig. 5: Results with HH 06 experiment: proposed (blue),
ORB-SLAM2 Stereo (green), ground truth trajectories (red).
Fig. 8: Results with outdoor experiments: for test 03, (a)
showsthefulltrajectory,(b)-(c)areclose-upviewsofthetwo
ends of the trajectory, (d) is UWB distance measurements,
(e) shows the results of multiple trials on the same corridor.
axis, initial guess for paz often has a larger error to the true
value than for the other axes. Both methods work well since
the visual features in the environment and the experiments
are relatively small-scale. In Table II, it is evident that the
proposed method can achieve the same level of accuracy as
ORB-SLAM2 Stereo.
Outdoor experiments were carried out in a 170m long
corridor to showcase the ability to reduce drift and the
robustness of the system. In these experiments, the anchor
− (cid:62)
was placed at roughly [ 1.3,0.5,1.5] . The system was
initialized at one end of the corridor, then walked to the
Fig. 6: Estimates of the UWB anchor position in HH 06 other end and back. Since the movements were mostly on
experiment, which is visible along the trajectory in Fig. 5. a straight line, UWB range measurements are sufﬁcient as
ground truth for scale evaluation. The proposed system was
able to consistently follow 340m long trajectories with less
than 0.1m drift at the end, while the scale error between the
estimatedandgroundtruthtrajectoriesislessthan1.5%,with
the results can be seen in Fig. 8. ORB-SLAM2 Stereo failed
to complete any of the experiments, due to losing track at
someinstanceswithchallengingmovementsandillumination
condition during the trials, and thus was not available for
comparison.
V. CONCLUSIONSANDFUTUREWORKS
In this work, a tightly-coupled fusion scheme is proposed
tousebothup-to-scalemonocularvisualodometryandUWB
rangingmeasurementstoestimateandreﬁnemetricscaleand
anchor position. Range errors are monitored to incorporate
range measurements in the pose graph optimization scheme
Fig. 7: Trajectories and results of the indoor experiments.
to reduce visual drift in a tightly-coupled manner. The
accuracy is on par with state-of-the-art stereo and VIO
parametersaretunedselectively.Fig.5depictsthepositionin systems with datasets and indoor experiments, while drifts
HH 06 experiments. After the ﬁrst batch of data is collected in large-scale outdoor experiments are notably diminished.
at t = 3.5s, the odometry position on each axis as well as However, the proposed method still requires an initial
the anchor position can be seen to quickly approach the true guessfortheanchorpositionsandexcludesthespatialoffsets
values, oscillate for a short period before stabilizing with betweenthesensors.Eliminatingthenecessityforthisinitial
little change thereafter. Fig. 6 shows that with the initial guess, taking into account the extrinsic and temporal offsets
(cid:62)
guess pa =(0,1.5,0.5) , pa converges to the true position betweensensors,andextendingtomultipleanchorscasesare
0
in 3s. Since ﬁrst movement of the MAV is usually in z the main directions for future development.
670
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 16:49:59 UTC from IEEE Xplore.  Restrictions apply. REFERENCES oftheIEEEConferenceonComputerVisionandPatternRecognition
(CVPR),vol.2,2017.
[1] K.Mohta,M.Watterson,Y.Mulgaonkar,S.Liu,C.Qu,A.Makineni,
[20] S.Wang,R.Clark,H.Wen,andN.Trigoni,“Deepvo:Towardsend-
K. Saulnier, K. Sun, A. Zhu, J. Delmerico et al., “Fast, autonomous
to-endvisualodometrywithdeeprecurrentconvolutionalneuralnet-
ﬂight in gps-denied and cluttered environments,” Journal of Field
works,”inRoboticsandAutomation(ICRA),2017IEEEInternational
Robotics,vol.35,no.1,pp.101–120,2018.
Conferenceon. IEEE,2017,pp.2043–2050.
[2] H. Strasdat, J. Montiel, and A. J. Davison, “Scale drift-aware large
[21] D. Eberli, D. Scaramuzza, S. Weiss, and R. Siegwart, “Vision based
scalemonocularslam,”Robotics:ScienceandSystemsVI,vol.2,2010.
positioncontrolformavsusingonesinglecircularlandmark,”Journal
[3] T.M.Nguyen,A.H.Zaini,K.Guo,andL.Xie,“Anultra-wideband-
ofIntelligent&RoboticSystems,vol.61,no.1-4,pp.495–512,2011.
based multi-uav localization system in gps-denied environments,” in [22] S. H. Lee and G. de Croon, “Stability-based scale estimation for
2016InternationalMicroAirVehiclesConference,2016. monocularslam,”IEEERoboticsandAutomationLetters,vol.3,no.2,
[4] T.-M. Nguyen, A. H. Zaini, C. Wang, K. Guo, and L. Xie, “Robust pp.780–787,2018.
target-relative localization with ultra-wideband ranging and commu- [23] C. Wang, H. Zhang, T.-M. Nguyen, and L. Xie, “Ultra-Wideband
nication,” in 2018 IEEE International Conference on Robotics and Aided Fast Localization and Mapping System,” in 2017 IEEE/RSJ
Automation(ICRA). IEEE,2018,pp.2312–2319. International Conference on Intelligent Robots and Systems (IROS).
[5] M. J. M. M. Mur-Artal, Rau´l and J. D. Tardo´s, “ORB-SLAM: a IEEE,2017.
versatileandaccuratemonocularSLAMsystem,”IEEETransactions [24] J. Tiemann, A. Ramsey, and C. Wietfeld, “Enhanced uav indoor
onRobotics,vol.31,no.5,pp.1147–1163,2015. navigationthroughslam-augmenteduwblocalization,”in2018IEEE
[6] R.Mur-ArtalandJ.D.Tardo´s,“ORB-SLAM2:anopen-sourceSLAM InternationalConferenceonCommunicationsWorkshops(ICCWork-
system for monocular, stereo and RGB-D cameras,” IEEE Transac- shops). IEEE,2018,pp.1–6.
tionsonRobotics,vol.33,no.5,pp.1255–1262,2017. [25] Y. Song, M. Guan, W. P. Tay, C. L. Law, and C. Wen, “Uwb/lidar
[7] G.DubbelmanandB.Browning,“Cop-slam:closed-formonlinepose- fusion for cooperative range-only slam,” in 2019 International Con-
chain optimization for visual slam,” IEEE Transactions on Robotics, ferenceonRoboticsandAutomation(ICRA). IEEE,2019,pp.6568–
vol.31,no.5,pp.1194–1213,2015. 6574.
[8] J. Engel, J. Stu¨ckler, and D. Cremers, “Large-scale direct slam [26] F. J. Perez-Grau, F. Caballero, L. Merino, and A. Viguria, “Multi-
withstereocameras,”in2015IEEE/RSJInternationalConferenceon modalmappingandlocalizationofunmannedaerialrobotsbasedon
IntelligentRobotsandSystems(IROS). IEEE,2015,pp.1935–1942. ultra-wideband and rgb-d sensing,” in 2017 IEEE/RSJ International
[9] P.Liu,M.Geppert,L.Heng,T.Sattler,A.Geiger,andM.Pollefeys, ConferenceonIntelligentRobotsandSystems(IROS). IEEE,2017,
“Towardsrobustvisualodometrywithamulti-camerasystem,”in2018 pp.3495–3502.
IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems [27] T. H. Nguyen, M. Cao, T.-M. Nguyen, and L. Xie, “Post-mission
(IROS). IEEE,2018,pp.1154–1161. autonomous return and precision landing of uav,” in 2018 15th
[10] Z.Zhang,R.Zhao,E.Liu,K.Yan,andY.Ma,“Scaleestimationand InternationalConferenceonControl,Automation,RoboticsandVision
correction of the monocular simultaneous localization and mapping (ICARCV). IEEE,2018,pp.1747–1752.
(slam) based on fusion of 1d laser range ﬁnder and vision data,” [28] T.-M.Nguyen,Z.Qiu,T.H.Nguyen,M.Cao,andL.Xie,“Distance-
Sensors,vol.18,no.6,p.1948,2018. basedcooperativerelativelocalizationforleader-followingcontrolof
[11] Q.Lv,J.Ma,G.Wang,andH.Lin,“Absolutescaleestimationoforb- mavs,”IEEERoboticsandAutomationLetters,vol.4,no.4,pp.3641–
slamalgorithmbasedonlaserranging,”in201635thChineseControl 3648,2019.
Conference(CCC). IEEE,2016,pp.10279–10283. [29] T.-M. Nguyen, Z. Qiu, M. Cao, T. H. Nguyen, and L. Xie, “An
[12] T. Caselitz, B. Steder, M. Ruhnke, and W. Burgard, “Monocular integrated localization-navigation scheme for distance-based docking
cameralocalizationin3dlidarmaps,”in2016IEEE/RSJInternational of uavs,” in 2018 IEEE/RSJ International Conference on Intelligent
ConferenceonIntelligentRobotsandSystems(IROS). IEEE,2016, RobotsandSystems(IROS). IEEE,2018,pp.5245–5250.
pp.1926–1931. [30] T. M. Nguyen, Z. Qiu, M. Cao, T. H. Nguyen, and L. Xie, “Single
[13] C.Kerl,J.Stuckler,andD.Cremers,“Densecontinuous-timetracking landmark distance-based navigation,” IEEE Transactions on Control
andmappingwithrollingshutterrgb-dcameras,”inProceedingsofthe SystemsTechnology,2019.
IEEE international conference on computer vision, 2015, pp. 2264– [31] A.Shariati,K.Mohta,andC.J.Taylor,“Recoveringrelativeorienta-
2272. tionandscalefromvisualodometryandrangingradiomeasurements,”
[14] R. Giubilato, S. Chiodini, M. Pertile, and S. Debei, “Scale correct in2016IEEE/RSJInternationalConferenceonIntelligentRobotsand
monocularvisualodometryusingalidaraltimeter,”in2018IEEE/RSJ Systems(IROS). IEEE,2016,pp.3627–3633.
International Conference on Intelligent Robots and Systems (IROS). [32] K. Levenberg, “A method for the solution of certain non-linear
IEEE,2018,pp.3694–3700. problemsinleastsquares,”Quarterlyofappliedmathematics,vol.2,
[15] T.Qin,P.Li,andS.Shen,“Vins-mono:Arobustandversatilemonoc- no.2,pp.164–168,1944.
ular visual-inertial state estimator,” IEEE Transactions on Robotics, [33] T.H.Nguyen,T.-M.Nguyen,M.Cao,andL.Xie,“Loosely-coupled
vol.34,no.4,pp.1004–1020,2018. ultra-wideband-aidedscalecorrectionformonocularvisualodometry,”
[16] S. Lynen, T. Sattler, M. Bosse, J. A. Hesch, M. Pollefeys, and UnmannedSystems,2020.
R.Siegwart,“Getoutofmylab:Large-scale,real-timevisual-inertial [34] M.Burri,J.Nikolic,P.Gohl,T.Schneider,J.Rehder,S.Omari,M.W.
localization.”inRobotics:ScienceandSystems,2015. Achtelik,andR.Siegwart,“Theeurocmicroaerialvehicledatasets,”
[17] S. Leutenegger, S. Lynen, M. Bosse, R. Siegwart, and P. Furgale, The International Journal of Robotics Research, vol. 35, no. 10, pp.
“Keyframe-based visual–inertial odometry using nonlinear optimiza- 1157–1163,2016.
tion,”TheInternationalJournalofRoboticsResearch,vol.34,no.3, [35] J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cremers,
pp.314–334,2015. “A benchmark for the evaluation of rgb-d slam systems,” in 2012
[18] J.Kaiser,A.Martinelli,F.Fontana,andD.Scaramuzza,“Simultaneous IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems.
state initialization and gyroscope bias calibration in visual inertial IEEE,2012,pp.573–580.
aidednavigation,”IEEERoboticsandAutomationLetters,vol.2,no.1, [36] J. Delmerico and D. Scaramuzza, “A benchmark comparison of
pp.18–25,2017. monocular visual-inertial odometry algorithms for ﬂying robots,” in
[19] K.Tateno,F.Tombari,I.Laina,andN.Navab,“Cnn-slam:Real-time 2018 IEEE International Conference on Robotics and Automation
densemonocularslamwithlearneddepthprediction,”inProceedings (ICRA). IEEE,2018,pp.2502–2509.
671
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 16:49:59 UTC from IEEE Xplore.  Restrictions apply. 