2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
Cooperative Multi-Robot Navigation in Dynamic Environment with Deep
Reinforcement Learning
Ruihua Han, Shengduo Chen and Qi Hao*
Abstract—The challenges of multi-robot navigation in dy-
namicenvironmentslieinuncertaintiesinobstaclecomplexities, Dynamic Environment
partially observation of robots, and policy implementation Target
from simulations to the real world. This paper presents a
cooperative approach to address the multi-robot navigation Target
problem (MRNP) under dynamic environments using a deep
reinforcement learning (DRL) framework, which can help
multiple robots jointly achieve optimal paths despite a certain
degree of obstacle complexities. The novelty of this work
includes threefold: (1) developing a cooperative architecture
that robots can exchange information with each other to select
the optimal target locations; (2) developing a DRL based
framework which can learn a navigation policy to generate Target
theoptimalpathsformultiplerobots;(3)developingatraining Moving 
Obstacles
mechanismbasedondynamicsrandomizationwhichcanmake
Target
the policy generalized and achieve the maximum performance
intherealworld.ThemethodistestedwithGazebosimulations
and4differentialdriverobots.Bothsimulationandexperiment
results validate the superior performance of the proposed
Fig.1. Multi-robotnavigationindynamicenvironment
methodintermsofsuccessrateandtraveltimewhencompared
with the other state-of-art technologies. target locations to robots will determine the travel time
of the group.
I. INTRODUCTION 2) Lack of Cooperation Scheme. For multiple robots,
Multi-robotsystemsareadvantageousinperformingcom- they could experience different situations and conditions;
plex tasks such as surveillance and rescue, formation and how to develop a cooperative architecture to take full
exploration, cooperative manipulation [1]. However, when advantage of the accumulative experiences to achieve an
the number of robots increases, more technical challenges optimal navigation policy.
arise in perception, navigation, and formation control. Many 3) Ineffective Transfer from Simulation to the Real
approaches have been developed to solve MRNP such as World. How to develop a mechanism that can constantly
simultaneous localization and mapping (SLAM) based plan- improve the system parameters in simulation to achieve
ning[2]–[4],velocityobstacles(VO)basedvelocityselection an optimal policy workable in the real world.
[5]–[7], based on assumptions such as fully environment Some approaches focus on learning the collision avoid-
modeling, perfect sensing, which are unfeasible in the dy- ance policy with pre-allocated goal locations from the raw
namic environment, as shown in Fig. 1. sensor measurements of individual robots using the DRL
Recently, reinforcement learning (RL) based approaches framework [9]–[11]. Other approaches learn the navigation
have been developed to solve the navigation problem in policy only for the single robot or in the static environment
dynamic environments [8]. Nevertheless, there are still tech- [12], [13]. Generally speaking, these approaches cannot ad-
nicalchallengesforthemtosolvethemulti-robotnavigation equately address the MRNP. RL methods are also developed
problem in complex environments, including: to achieve cooperative schemes among multiple robots such
1) Inefﬁcient Target Location Allocation. Usually, multi- asparametersharingpolicy[14],[15],buttheyareunableto
ple robots have several target locations; how to allocate allocate target locations optimally, leading to low navigation
efﬁciency. Besides, many expensive efforts have been made
*This work is partially supported by the National Natural tocollecthugeamountsofdataforthetransferlearningfrom
Science Foundation of China (No: 61773197); the Science
simulation to the real world [16], [17].
and Technology Innovation Committee of Shenzhen City (No:
CKFW2016041415372174,GJHZ20170314114424152); and the Nanshan Inthispaper,weproposeaDRLframeworktotheMRNP
DistrictScienceandTechnologyInnovationBureau(No:LHTD20170007); in dynamic environments, where a cooperative architecture
andtheIntelICRI-IACVGrant2019.
is developed to combine the target location allocation and
The authors are with Department of Computer Science and Engineer-
ing, Southern University of Science and Technology, SUSTech-Haylion collision avoidance into the training process to learn the
Center for Intelligent Transportation, Shenzhen Research Institute for navigation policy. We also develop a set of randomized
TrustworthyAutonomousSystems,Shenzhen,Guangdong,518055,China.
{ } dynamicsmodelsduringthetrainingprocessinthesimulator
hanruihuaff@gmail.com haoq@sustc.edu.cn *Corre-
spondingauthor to alleviate the mismatch between simulation and the real
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 448
Authorized licensed use limited to: University of New South Wales. Downloaded on September 19,2020 at 17:54:47 UTC from IEEE Xplore.  Restrictions apply. TABLEI
RELATEDWORK
Approach Algorithm Strength(s) Limitation(s)
RLbased Onlyconsidersinglerobot;
Q-learning Cantackleunknownenvironmentwithmovingobstaclesandtargets
navigation assumptionofperfectsensing
RLbasedPolicy Onlyconsidertherangesensor
SLCAP PPO Abletohandlelargerobotsystemandcomplexenvironment
whendetectstheobstacles
Policyupdateasynchronously
GA3C-CADRL A3C Abletotackleobstacleswithvariousnumberandvelocity
isinefﬁcient
DQN
IDRL Avoidthecollisionandselectthetargetsimultaneously Onlyavoidthestaticobstacles
&DDPG
Inverse Requirethecollection
- Abletotacklevariousdynamicsmodelwithsuperiorperformance
Sim-to-RealTransfer dynamicsmodel oftherealworlddata
Dynamics Requireaspeciﬁc
- Abletomakepolicygeneralizedtoadapttherealworld
randomization dynamicsmodel
world. The main contributions of this work include: AnotherRL-basedcollisionavoidance(CADRL)approach
has been developed by training the value network, which
1) Developing a cooperative architecture that can utilize the
can ﬁnd a collision-free velocity vector for multiple agents
accumulative experiences of all robots for target location
[10].ItsextensionGA3C-CADRLappliestheAsynchronous
allocation.
Advantage Actor-Critic (A3C) to learn multi-agent collision
2) Developing a training algorithm to learn the navigation
avoidance policy [11]. However, in these methods, all the
policy, including target location allocation and collision
robots update the policy with many threads asynchronously,
avoidance based on the DRL framework.
which is inefﬁcient during the training process for homo-
3) Developing a transfer mechanism from simulation to the
geneous robots. Besides, these methods only consider the
real world during the simulation training, which can
collisionavoidancepolicywithpre-allocatedtargetlocations,
help achieve a better-generalized navigation policy and
which is only a part of the multi-robot navigation problem.
maximize the performance of robots in the real world.
An interlaced DRL (IDRL) approach has been proposed to
Therestofpaperisorganizedasfollows.SectionIIdiscusses
combinethedynamictargetselectionandcollisionavoidance
the related approaches to solve the MRNP. Section III de-
into the navigation policy [19]. It uses a Deep Q-Network
scribes the system setup and the problem statement. Section
(DQN) to learn the target selection policy and uses the deep
IV describes the framework of DRL and its conﬁguration.
deterministic policy gradient (DDPG) to learn the collision
Section V presents the dynamics parameters applied in the
avoidance policy simultaneously. However, this approach
training process. Section VI presents the target location
onlydealswiththeenvironmentwithstaticobstacles.Mean-
allocation and the policy training algorithm. Section VII
while, a deep inverse dynamics model to decide what the
provides the simulation and experiment results. Section VIII
action should be in the real world based on the simulator
concludes the paper and outlines future work.
policyisproposedin[16].However,thisapproachneedsdata
collection from the real world to train the inverse dynamics
II. RELATEDWORK model constantly, which is inefﬁcient for the navigation
problem. A randomized dynamics model during the training
RL-basedapproacheshavedemonstratedmanyadvantages
process has been proposed to achieve a generalized policy
in solving the navigation problem under dynamic environ-
withoutusinganyreal-worlddata[20].Nevertheless,thisap-
ments [8]. For example, Q-learning based approaches are
proach requires a complicated dynamics model for training.
proposed to solve the mobile robot navigation problem
The summary of these approaches is listed in Table I.
in unknown dynamic environments [12]. However, those
In this work, we focus on tackling the MRNP in dynamic
approaches only deal with single robot navigation and as-
environments with multiple moving obstacles. We develop
sume perfect sensing, which limits their applications. A use
a cooperative DRL based framework to learn the navigation
of the actor-critic method referred to as multi-agent deep
policy based on the collective experiences of all robots. The
deterministicpolicygradient(MADDPG)hasbeenproposed,
policy is shared and updated by all robots simultaneously.
which allows the policies to use extra information to ease
Unlike SLCAP, this policy uses the poses and velocities of
training [18]. Deep neural networks have been proposed
robots as input, which can be implemented for various types
to learn a collision avoidance policy without perfect sens-
of sensors. Besides, a simple randomized dynamics model
ing assumptions, which can directly map the raw sensor
for navigation is developed during the simulation training
measurements to the robot’s actions [9]. This approach is
process to make the trained policy more adaptive in the real
extended to a decentralized sensor-level collision avoidance
world.
policy(SLCAP)forlarge-scalerobotsystemsusingProximal
Policy Optimization (PPO) [15]. However, those approaches
III. SYSTEMSETUPANDPROBLEMSTATEMENT
heavily rely on range sensors that cannot detect all types
A. System Framework
of obstacles; besides, it will incur high hardware cost and
energy consumption for their applications in large scale The system framework of our multi-robot navigation sys-
multi-robot systems. tem is shown in Fig. 2. The sensors for obstacle detection
449
Authorized licensed use limited to: University of New South Wales. Downloaded on September 19,2020 at 17:54:47 UTC from IEEE Xplore.  Restrictions apply. Individual Robot ROS Communication Network Policy Training Robots Control
Configuration  ROS Master Publish -Subscribe DRL  Navigation
Sensors Master Topic State Space Reward Action
Dynamics model Robot 1 Robot N Publish Subcribe Policy Action Space Dynamics 
Model
Sensors Robot 1…Robot N Information Exchange Target Allocation Action 1…Action N
Camera UWB Distance  Allocate 
Share policy Robots states
to Goal Goal Simulator Real World
Lidra etc.
Share network Goal positions Collision Avoidance Motion with 
Dynamics model Motion
Train Policy Noise
Motion model
Share information Obstacles states
Avoid  Dynamics
Action
Observation model Collision Randomization
Fig.2. Systemframeworkofmulti-robotnavigationpolicy
includeLidar,camera,Ultra-WideBand(UWB).Agroupof reachesthegoalpositiondependingonwhetheritislessthan
robotsinthesamenetworkwillusethesameROSmasterto the minimum distance d . The travel time T denotes the
min
guarantee the messages passing. Based on the accumulative moment when all the robots reach their own target position,
information from all robots, the target location allocation, respectively, without any collision. The goal of this paper
which can make navigation more efﬁcient and collision is to ﬁnd an optimal navigation policy shared and updated
avoidance are combined in the DRL framework to train by all robots in a dynamic environment by minimizing the
the navigation policy. The policy is shared and updated by expectation of travel time T.
all robots simultaneously. In the simulation, the randomized
IV. DEEPREINFORCEMENTLEARNINGFRAMEWORK
dynamics models are applied during the training process to
make policy more adaptive to the real world. The framework of our DRL with actor-critic model is
shown in Fig. 3. Firstly, the target locations are allocated
B. Problem Statement to each robot. Secondly, each robot receives a state from
S
a state space and select an action from an action space
The MRNP is actually a Partially Observable Markov
A
Decision Process (POMDP) that a group of robots navigate , following the stochastic policy πθ. Finally, the action
leads to a new environment state and reward. Speciﬁcally,
to the target location in dynamic environment with collision
avoidance and minimum time cost. The state vector of the the critic part computes the value V of taking that action at
that state to evaluate the action. The actor part updates its
single robot s at time t is composed of Cartesian coordi-
nates, orientation and speed, st = [pt ,pt ,θt,vt ,vt ]. policy parameters using the value V.
r rx ry r rx ry
Speciﬁcally, all robots have the same radius r. The state of A. State Space and Action Space
moving obstacles includes Cartesian coordinates, speed and
The state space can be divided into four parts: the current
radius, st = [pt ,pt ,vt ,vt ,r ]]. The target position of
o ox oy ox oy o robot’sstatest,theotherrobots’state˜st,theobstacles’state
robots can be denoted as s = [p ,p ]. In the condition r r
g gx gy st and the target position st as mentioned in Section III,
of these previous states, the navigation policy generates the o g
∼ | st =[st,˜st,st,st].Theobstaclesstatest isaggregatedfrom
actionsaateachtimestepforrobots,at π (at st,st,s ). r r o g o
θ r o g the observation of all robots.
Where, θ is the parameter of policy. The action is composed
The action space is a probability distribution of contin-
of transitional and rotational velocity of differential drive
uous actions including transitional and rotational velocity,
robots,at=[vt,vt].Thus,foragroupofN robotsnavigating ∼ S
in dynamic etnvirron(cid:2)ment with M moving ob(cid:3)stacles, the at=[vtt,vrt] . At each time step, in the condition of
state st, robots select the highest probability of action from
optimization objective should be:
an action space to navigate following the stochastic policy.
E |
argmin T π ,s ,s ,s , Consider the real world situation that the robot only moves
θ r,1:N o,1:M g,1:N
πθ ∀ ∈ ∈ forwardbecauseofthelimitedviewofobservation,therange
s.t. i,j [1,N],k [1,M]
(1) of transitional velocity is limited between 0 and 0.4 m/s.
d >2r
rr,i,j r Similarly, given the obstacles detection speed, the rotational
d >r +r
ro,i,k r o velocity is limited between -1 and 1 rad/s.
d <d
g,i min
B. Reward Design
where, d represents the distance between two arbitrary
rr,i,j
robots, d denotes the distance between robots and ThegoaloftheDRLalgorithmistomaximizethereward
ro,i,k
obstacles.Thesetwodistancesareusedtojudgethecollision. of a series of actions. Therefore, for robot i at time t, the
The distance d is used to judge whether the robot i reward rt is designed as rt = rt + rt , where, rt is
g,i i i g,i c,i g,i
450
Authorized licensed use limited to: University of New South Wales. Downloaded on September 19,2020 at 17:54:47 UTC from IEEE Xplore.  Restrictions apply. eachepisode,aseriesofdynamicsparametersλareselected
Policy
uniformly from a range γ and keep the value until the end
Environment AllocationTarget  State Space RRoobboott  N1 ActorCollision Avoidance  SpaceAction  AAccttiioonn  N1 odfri---vtheiNNNsrooooebiiipsssoeeeitssoiiinnndinetrtc.rhoaletuTnadhstpieieoot:isnodiatnyilaonvlnaemvl(eocilccoosioctyripdt,ayiξ,rna2aξmt1eest)erosfλrofboortsd,iξfferential
3
- Noise in position (coordinates) of obstacles, ξ
Reward Critic Neural Network 4
- Noise in the measurements of obstacles ξ
5
- Mass of robots m
r
Fig.3. Frameworkofdeepreinforcementlearningfornavigationpolicy
where the noises are all Gaussian noise with a zero mean
the award when the robot reaches or approaches its target andspeciﬁcvariance.Ateachtimestep,thenoiseissampled
location which is judged by distance dtg. Otherwise, the from the Gaussian distribution with a ﬁxed variance. ξ1 and
reward is related to the change of dtg: ξ2 is the action noise from the chassis motor. It will affect
the motion model of differential drive robots. ξ and ξ is
 3 4
5 ∗ ifdtg <0.1 the noise in external sensors like Ultra-Wide Band (UWB),
rt = 1+dt 0.5 if0.1<dt <0.4 . (2) whichcanaffordtheglobalpositionofrobotsandobstacles.
g,i ∗ g − − g
10 (dtg 1 dtg) otherwise ξ5 isthenoiseinobstacledetectionderivedfromtheonboard
sensors, including the range and relative bearing. The mass
rt isthepunishmentwhentherearecollisionamongrobots
ocr,iobstacles judged by the distance dt and dt : of robots mr can affect the dynamics model like inertia and
rr ro friction and remains constant during an episode.
 − ∗
10 if dt <2 r
rt = −10 if drtr <r +rr . (3) VI. NAVIGATIONPOLICYTRAINING
c,i ro r o
0 otherwise A. Target Location Allocation Algorithm
C. Actor and Critic Usually, multiple robots have several corresponding target
positions. Each robot has a ﬁxed target will be inefﬁcient
The actor part that mapping from the state to action
whentherobothastomovetothefurthertargetinsteadofthe
is designed as a 3-hidden-layer neural network. The input
closerone.Inthesamenetwork,theglobalcoordinatesofall
state is fed into the three fully-connected layers with 256,
robotsandtargetlocationscanbeacquiredateachtimestep.
128, and 64 rectiﬁer units separately. The output layer is
Thus, we develop a simple target location allocation algo-
also a fully-connected layer with four units and different
rithm depending on the distance between robots and targets
activationfordifferentpartsofactionspace.Thesigmoidand
described in Algorithm 1. The core principle is to calculate
Hyperbolictangentactivationsareappliedtooutputthemean
allthepermutationsoftheallocationschemebetweenrobots
oftransitionalandrotationalvelocityseparately.Speciﬁcally,
and targets to ﬁnd the permutation with the shortest total
theclipfunctionisalsousedtoconstrainthesetwovelocities
distance length. In consideration of computation cost, this
in range (0, 0.4) and (-1, 1). The ﬁnal activation softplus
policy is performed with a time step interval T decided by
is used to generate the standard deviation of these two a
the number of robots and computer performance.
velocities. The mean and standard deviation compose the
Gaussian distribution of actions. Robots select the action at Algorithm 1 Target Location Allocation Algorithm
sampled from the Gaussian distribution at each time step t.
The critic part is the state value function V(st) and is 1: ReceivethestateofN robotsandgoalpositions,str,1:N,
designed as a 2-hidden-layer neural network, including two st
g,1:N
fully-connected layers with 128 and 64 rectiﬁer units. The 2: for Each permutation in the permutations of N robots
output layer only has one unit to generate value with linear and N targets do
activation. 3: for robot i=1,...,N do
4: Calculate the distance dt between robot i and its
i
V. DYNAMICSRANDOMIZATION target
The policy trained in the simulator may have superior 5: end for
performance. However, the performance is hard to remain 6: Calculate the total distance length of all robots dt
total
in the real world because of the ubiquitous uncertainties 7: end for
in the process of robot control and obstacle detection. In 8: Select the permutation with shortest distance length
a simulator, the dynamics model of robots obeys differential
drive robot model; however, instead of the precise data
B. Training Algorithm
read from the simulator, there are numerous sensor noises
in the real world. Thus, inspired by [20], we apply the Thoserobotsinadynamicenvironmentarehomogeneous;
randomized dynamics model into the training process to theinformationofposesandvelocitiesisusedastheinputof
makethenavigationpolicyadaptivetotherealworld.During the training algorithm. All the robots share and update the
451
Authorized licensed use limited to: University of New South Wales. Downloaded on September 19,2020 at 17:54:47 UTC from IEEE Xplore.  Restrictions apply. navigation policy simultaneously. After the target location Algorithm 2 Policy Training with PPO
allocation, the robots should move to the goal position 1: Initialize neural network π
θ
followingthecollisionavoidancepolicy.Becauseofthecon- 2: for episode=1,2,... do
tinuousactionspaceandcomplexenvironment,weapplythe 3: Reset the environment with the initial state, s
init
PPO [21], [22] with actor-critic style to learn the navigation 4: Sample the dynamics parameters λ from a range γ
∼
policy. uniformly, λ γ
The summary of the training algorithm is described in 5: for robot i=1,2,... do
Algorithm 2. Firstly, at the start of the episode, the environ- 6: Receive state st, select the goal position st
∼i g,i
ment resets to an initial state. Dynamics parameters λ are 7: Add noise,ˆsti s∼ti+[ξ3,ξ|4,ξ5]
uniformlysampledfromarangeγ.Secondly,thestatesofthe 8: Sample action∼ati πθ(atiˆsti)
robotsˆstr arereceivedwithnoiseξ3,ξ4,ξ5.Thegoalposition 9: Add noise, ˆati ati+[ξ1,ξ2]
is allocated for each robot depending on target location 10: Publish ˆat to robot i
i
allocation policy. Thirdly, the action is sampled from policy 11: Collect stateˆst, reward rt and at for T time steps
i i i i
π withhighestprobabilityintheconditionofcurrentstates, 12: Compute advantage estimates Aˆ1,...,AˆT
θ ∼ | i i
at π (atˆst). Speciﬁcally, before the action is published 13: end for
i θ i i
to the robot, noise ξ ,ξ is applied. Simultaneously, the 14: Optimize surrogate loss LCLIP(θ) wrt θ, with Adam
1 2
action ati without noise, stateˆsti and reward rit are collected optim←izer and learning rate la for K epochs
to compute the estimated advantage Aˆi used by [23]. Fi- 15: θold θ
nally, according to the estimated advantage Aˆ and policy 16: Optimize value loss LV(ψ) wrt ψ, with Adam opti-
i
πθ, surrogate loss LCLIP(θ) and state value loss LV(ψ) mizer←and learning rate lv for L epochs
are constructed. LCLIP(θ) is a clipped surrogate objective 17: ψold ψ
proposed in [21]. It is optimized by Adam optimizer and 18: end for
learning rate l for K epochs. LV(ψ) is the loss of state
a graphicscardandthesixtimesacceleratedGazebosimulator;
valuefunctionV(st)constructedbysquareofadvantageAˆt.
i the whole training process takes about 36 hours.
It is also optimized by Adam optimizer and learning rate l
v
for L epoch.
B. Experiment Setup
VII. EXPERIMENTSANDRESULTS The hardware experiment that implements navigation pol-
icy on a differential drive robot (Turtlebot) in a dynamic
A. Simulation Setup
environment has been performed to demonstrate the perfor-
Numerous robotics simulators can be used to simulate the mance of our policy in the real world. The conﬁguration
navigation of multiple robots such as Matlab, ROS Stage, of Turtlebot and the obstacles is illustrated in Fig. 5. The
ARGoS. In this paper, we use Gym-Gazebo [24] to simulate dynamic obstacles are formulated with a kobubi base and
thedynamicobstaclesanddifferentialdriverobots,Turtlebot. block. To acquire the state of the environment, we use the
Gym-Gazebo integrates Gym API, Gazebo simulator, and UWB base station to build a global coordinate system for
ROS to build a 3D robotics platform for RL. Speciﬁcally, TABLEII
OpenAIGymisapopularkitforRLresearchandGazebois
PARAMETERSDURINGTRAININGPROCESS
a 3D robotics simulator with a physics engine and realistic
rendering. This platform reduces the difference between the Parameters Value(Range)
simulator, and the real world considerably. lainline14 0.0005
Kinline14 15
As shown in Fig. 4, the training scenario is composed lv inline16 0.001
Linline16 10
of several Turtlebot models and green circles with the
inLCLIP(θ) 0.2
corresponding number as goal positions for robots. Speciﬁ- ξ1,ξ2 variance[0.01,0.1]
cally, parts of Turtlebot models are formulated as dynamic ξ3 variance[0.005,0.05]
ξ4 variance[0.005,0.05]
obstacles with stochastic motion. Each stochastic trajectory ξ5 variance[0.01,0.1]forrange,[0.01,0.05]forbearing
is achieved by using the periodically changed velocity to mr [5,6.5]kg
generate as many as possible scenarios for robots to tackle.
The key hyper-parameter and dynamics parameters during TABLEIII
the training process are listed in Table II. To accelerate PERFORMANCECOMPARISONOFDIFFERENTPOLICIES
reward converge and compare the performance, the policy
Simulation RealWorld
is trained with three stages. We ﬁrst pre-train the policy Metrics Method
1 2 3 4 1 2 3 4
without dynamic parameters and target allocation, i.e., Pre- ORCA 1.0 0.97 0.96 0.88 0.91 0.85 0.76 0.69
Policy. Based on this Pre-Policy, we train the TA-Policy Success Pre-Policy 1.0 1.0 1.0 0.95 0.84 0.78 0.68 0.61
Rate TA-Policy 1.0 1.0 1.0 0.98 0.81 0.81 0.71 0.66
with the target location allocation algorithm. Finally, the DP-Policy 1.0 1.0 0.94 0.91 1.0 1.0 0.91 0.88
dynamic parameters are applied during the training process ORCA 4.54 6.84 10.56 15.65 6.46 10.69 15.64 20.64
Extra Pre-Policy 3.25 5.26 7.63 10.63 7.56 10.32 14.44 17.24
to achieve the DP-Policy. The navigation policy is trained Time TA-Policy 3.25 4.26 6.21 8.54 6.94 9.24 11.62 14.35
using Algorithm 2on a computer with anNvidia GTX 1070 DP-Policy 4.18 6.45 9.33 10.98 4.89 7.12 9.69 11.84
452
Authorized licensed use limited to: University of New South Wales. Downloaded on September 19,2020 at 17:54:47 UTC from IEEE Xplore.  Restrictions apply. Select Target 1 1.2
ORCA-Sim Pre-Policy-Sim TA-Policy-Sim DP-Policy-Sim
Target 1 Select Target 2 ORCA-Real Pre-Policy-Real TA-Policy-Real DP-Policy-Real
 Obstacle 1 Select Target 3 e 1
at
R
Target 2 Robot 3 ccess 0.8
u
S
0.6
 Obstacle 2 Robot 1
1 1.5 2 2.5 3 3.5 4
Robot 2 Number of Robots
Target 3
 Obstacle 3
Fig.6. Thecomparisonofvariouspoliciesintermsofsuccessrate
Fig.4. Simulationscenario
20 ORCA-Sim TA-Policy-Sim
UWB UWB OPrRe-CPAol-iRcyea-Slim TDAP--PPoolliiccyy--SRiemal
e[s]15 Pre-Policy-Real DP-Policy-Real
Kinect  m
v2 Block Ti10
a 
NUC xtr
Battery E 5
Battery
NUC
0
1 1.5 2 2.5 3 3.5 4
Kobubi Kobubi Number of Robots
(a) (b) Fig.7. Thecomparisonofvariouspoliciesintermsofextratime
Fig.5. Conﬁgurationofrealrobotsandobstacles the location readings from the UWB tag, leading to serious
collisions. The results demonstrate the superior performance
all robots and obstacles. The state of the dynamic obstacles
of our policy in both the simulator and real world.
is derived from the fusion of UWB and Kinect v2, which is
an RGB-D camera. The master control is a mini PC called
VIII. CONCLUSION
Intel NUC kit to run the policy in the ROS architecture.
In this paper, a DRL based navigation policy for multiple
C. Results and Discussions
robots in a dynamic environment is proposed. All the robots
Tovalidatetheperformance,weusetwoperformancemet- inthesamenetworkcanexchangeinformation,andthetarget
rics:successrateandextratime[15].Thesuccessrateisthe location allocation policy is proposed depending on the total
ratio of the number of robots arriving at their goal positions distance. The PPO algorithm for multi-robot is developed
without any collision over the total number of robots. Extra to train the navigation policy based on the accumulative
timemeasuresthedifferencebetweentheaveragetraveltime information collected by all robots in a simulator. The
over all robots and the lower bound of the travel time. randomizeddynamicsparametersareusedduringthetraining
We compare four policies, including ORCA [5], Pre-Policy, processtomakepolicyadaptivetotherealworldwithoutany
TA-Policy, and DP-Policy. ORCA is a popular VO-based requirement of real data collection. The experiment results
policyusedasabaselineforperformancecomparison.These show that the target location allocation algorithm can save
policies are tested in the simulator and the real world for extra travel time about 14.32 percent in comparison with
30 times with 3 moving obstacles and the robots with the other methods. With the randomized dynamics parameters,
number from 1 to 4 in each case. The paths of 3 obstacles the performance deterioration in real-world applications is
are randomized with a ﬁxed velocity. alleviated from a 24.91 percent decrease in the success rate
TheresultsarelistedinTableIII.Inthesimulationexper- and96.36percentincreaseintraveltimeto1.62percentand
iments, the TA-Policy has the best performance. Compared 9.76 percent, respectively. Future work includes addressing
to the Pre-Policy, its performance of extra time decreases by morecomplexdynamicenvironmentsandusingfewerexter-
about 14.32 percent on average. However, in the real world, nal sensors. Moreover, a distributed training framework will
its performance decreases heavily, about a 24.91 percent be developed to reduce the training time.
decrease in the success rate and a 96.39 percent increase
in extra time. The performance decrease occurs on other REFERENCES
policies except for the DP-Policy, which has a only 1.5
[1] R.N.DarmaninandM.K.Bugeja,“Areviewonmulti-robotsystems
percentdecreaseanda10percentincreaseasshowninFig.6 categorised by application domain,” in 2017 25th Mediterranean
andFig.7.AlthoughtheDP-Policytakesmoretraveltimein Conference on Control and Automation (MED). IEEE, 2017, pp.
701–706.
thesimulator,ithastheslightestperformancedecreaseinthe
[2] H. Durrant-Whyte and T. Bailey, “Simultaneous localization and
real world. Thus, it has the best performance in terms of the mapping:parti,”IEEErobotics&automationmagazine,vol.13,no.2,
success rate and extra time in the real world. Speciﬁcally, pp.99–110,2006.
[3] D.Fox,J.Ko,K.Konolige,B.Limketkai,D.Schulz,andB.Stewart,
the performance of ORCA in the real world decreases a
“Distributedmultirobotexplorationandmapping,”Proceedingsofthe
lot because this approach is sensitive to the uncertainty in IEEE,vol.94,no.7,pp.1325–1339,2006.
453
Authorized licensed use limited to: University of New South Wales. Downloaded on September 19,2020 at 17:54:47 UTC from IEEE Xplore.  Restrictions apply. [4] R.Reid,A.Cann,C.Meiklejohn,L.Poli,A.Boeing,andT.Braunl, [24] I.Zamora,N.G.Lopez,V.M.Vilches,andA.H.Cordero,“Extending
“Cooperativemulti-robotnavigation,exploration,mappingandobject theopenaigymforrobotics:atoolkitforreinforcementlearningusing
detectionwithros,”in2013IEEEIntelligentVehiclesSymposium(IV). rosandgazebo,”arXivpreprintarXiv:1608.05742,2016.
IEEE,2013,pp.1083–1088.
[5] J. Van Den Berg, S. J. Guy, M. Lin, and D. Manocha, “Reciprocal
n-body collision avoidance,” in Robotics research. Springer, 2011,
pp.3–19.
[6] D. Bareiss and J. van den Berg, “Generalized reciprocal collision
avoidance,”TheInternationalJournalofRoboticsResearch,vol.34,
no.12,pp.1501–1514,2015.
[7] J. E. Godoy, I. Karamouzas, S. J. Guy, and M. Gini, “Implicit
coordination in crowded multi-agent navigation,” in Thirtieth AAAI
ConferenceonArtiﬁcialIntelligence,2016.
[8] T.T.Nguyen,N.D.Nguyen,andS.Nahavandi,“Deepreinforcement
learning for multi-agent systems: A review of challenges, solutions
andapplications,”arXivpreprintarXiv:1812.11794,2018.
[9] P.Long,W.Liu,andJ.Pan,“Deep-learnedcollisionavoidancepolicy
fordistributedmultiagentnavigation,”IEEERoboticsandAutomation
Letters,vol.2,no.2,pp.656–663,2017.
[10] Y. F. Chen, M. Liu, M. Everett, and J. P. How, “Decentralized non-
communicating multiagent collision avoidance with deep reinforce-
ment learning,” in 2017 IEEE international conference on robotics
andautomation(ICRA). IEEE,2017,pp.285–292.
[11] M. Everett, Y. F. Chen, and J. P. How, “Motion planning among
dynamic, decision-making agents with deep reinforcement learning,”
in2018IEEE/RSJInternationalConferenceonIntelligentRobotsand
Systems(IROS). IEEE,2018,pp.3052–3059.
[12] M. A. K. Jaradat, M. Al-Rousan, and L. Quadan, “Reinforcement
based mobile robot navigation in dynamic environment,” Robotics
andComputer-IntegratedManufacturing,vol.27,no.1,pp.135–149,
2011.
[13] J. Lin, X. Yang, P. Zheng, and H. Cheng, “End-to-end decentralized
multi-robot navigation in unknown complex environments via deep
reinforcement learning,” in 2019 IEEE International Conference on
MechatronicsandAutomation(ICMA). IEEE,2019,pp.2493–2500.
[14] J. K. Gupta, M. Egorov, and M. Kochenderfer, “Cooperative multi-
agent control using deep reinforcement learning,” in International
ConferenceonAutonomousAgentsandMultiagentSystems. Springer,
2017,pp.66–83.
[15] P. Long, T. Fanl, X. Liao, W. Liu, H. Zhang, and J. Pan, “Towards
optimally decentralized multi-robot collision avoidance via deep re-
inforcement learning,” in 2018 IEEE International Conference on
RoboticsandAutomation(ICRA). IEEE,2018,pp.6252–6259.
[16] P. Christiano, Z. Shah, I. Mordatch, J. Schneider, T. Blackwell,
J.Tobin,P.Abbeel,andW.Zaremba,“Transferfromsimulationtoreal
worldthroughlearningdeepinversedynamicsmodel,”arXivpreprint
arXiv:1610.03518,2016.
[17] A. A. Rusu, M. Vecerik, T. Rotho¨rl, N. Heess, R. Pascanu, and
R. Hadsell, “Sim-to-real robot learning from pixels with progressive
nets,”arXivpreprintarXiv:1610.04286,2016.
[18] R. Lowe, Y. Wu, A. Tamar, J. Harb, O. P. Abbeel, and I. Mordatch,
“Multi-agent actor-critic for mixed cooperative-competitive environ-
ments,”inAdvancesinNeuralInformationProcessingSystems,2017,
pp.6379–6390.
[19] Y. Jin, Y. Zhang, J. Yuan, and X. Zhang, “Efﬁcient multi-agent
cooperativenavigationinunknownenvironmentswithinterlaceddeep
reinforcement learning,” in ICASSP 2019-2019 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP).
IEEE,2019,pp.2897–2901.
[20] X.B.Peng,M.Andrychowicz,W.Zaremba,andP.Abbeel,“Sim-to-
realtransferofroboticcontrolwithdynamicsrandomization,”in2018
IEEEInternationalConferenceonRoboticsandAutomation(ICRA).
IEEE,2018,pp.1–8.
[21] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,
“Proximal policy optimization algorithms,” arXiv preprint
arXiv:1707.06347,2017.
[22] N.Heess,S.Sriram,J.Lemmon,J.Merel,G.Wayne,Y.Tassa,T.Erez,
Z.Wang,S.Eslami,M.Riedmilleretal.,“Emergenceoflocomotion
behaviours in rich environments,” arXiv preprint arXiv:1707.02286,
2017.
[23] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,
D.Silver,andK.Kavukcuoglu,“Asynchronousmethodsfordeeprein-
forcementlearning,”inInternationalconferenceonmachinelearning,
2016,pp.1928–1937.
454
Authorized licensed use limited to: University of New South Wales. Downloaded on September 19,2020 at 17:54:47 UTC from IEEE Xplore.  Restrictions apply. 