2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
Urban Driving with Conditional Imitation Learning
Jeffrey Hawke, Richard Shen, Corina Gurau, Siddharth Sharma, Daniele Reda, Nikolay Nikolov
Przemysław Mazur, Sean Micklethwaite, Nicolas Grifﬁths, Amar Shah, Alex Kendall
Abstract—Hand-crafting generalised decision-making rules to generalise to the large state space and address variations
forreal-worldurbanautonomousdrivingishard.Alternatively, such as weather, time of day, lighting, surroundings, type of
learningbehaviourfromeasy-to-collecthumandrivingdemon-
roads and behaviour of external agents. The attraction here
strationsisappealing.Priorworkhasstudiedimitationlearning
isthatwehaveseenlearnedapproachestomanyvisiontasks
(IL) for autonomous driving with a number of limitations.
Examples include only performing lane-following rather than outperform hand-crafted approaches. We expect that it will
followingauser-deﬁnedroute,onlyusingasinglecameraview also be possible to do so for learned visual control.
or heavily cropped frames lacking state observability, only Prior works primarily explore the data-driven approach in
lateral (steering) control, but not longitudinal (speed) control
the context of imitation learning, but to date address only
andalackofinteractionwithtrafﬁc.Importantly,themajority
a subset of the state space [4], [5] or the action space [6].
ofsuchsystemshavebeenprimarilyevaluatedinsimulation-a
simpledomain,whichlacksreal-worldcomplexities.Motivated In more recent work, [7] learns both steering and speed,
bythesechallenges,wefocusonlearningrepresentationsofse- conditioned on a route command (e.g. turn left, go straight,
mantics,geometryandmotionwithcomputervisionforILfrom etc.),butevaluatesthemethodonasmall-sizedtoyvehiclein
human driving demonstrations. As our main contribution, we
structured scenarios with no trafﬁc. Similarly, [6] addresses
presentanend-to-endconditionalimitationlearningapproach,
controlconditionedonaroutewithGPSlocalisation,though
combiningbothlateralandlongitudinalcontrolonarealvehicle
for following urban routes with simple trafﬁc. We address learnsonlylateralcontrol(steering)fordrivingemptyroads.
inherentdatasetbiasbydatabalancing,trainingourﬁnalpolicy Ourmaincontributionisthedevelopmentofanend-to-end
onapproximately30hoursofdemonstrationsgatheredoversix conditional imitation learning approach, which, to the best
months.Weevaluateourmethodonanautonomousvehicleby
of our knowledge, is the ﬁrst fully learned demonstration
driving 35km of novel routes in European urban streets.
of complete control of a real vehicle, following a user-
I. INTRODUCTION prescribed route in complex urban scenarios. Additionally,
we include the ﬁrst examples of a learned driving policy
Driving in complex environments is hard, even for hu-
reacting to other trafﬁc participants. Our method requires
mans, with complex spatial reasoning capabilities. Urban
monocular camera images as input and can be conditioned
roads are frequently highly unstructured: unclear or missing
on a route command (e.g., taking a turn at an intersection
lane markings, double-parked cars, narrow spaces, unusual
[7]), in order to follow the speciﬁed route.
obstacles and other agents who follow the road rules to
Importantly, we utilise small amounts of driving data
widely varying degrees. Driving autonomously in these en-
such that our ﬁnal policy is trained on only 30 hours of
vironments is an even more difﬁcult robotics challenge.
demonstrations collected over six months, yet it generalised
The state space of the problem is large; coming up with
to routes it has not been trained to perform. We train
a driving policy that can drive reasonably well and safely
and evaluate1 on European urban streets: a challenging,
in a sufﬁciently wide variety of situations remains an open
unstructured domain in contrast to simulated environments,
challenge. Additionally, while the action space is small, a
or structured motorways and suburban streets.
good driving policy likely requires a combination of high-
The novel contributions of this work are:
level hierarchical reasoning and low-level control.
• the ﬁrst end-to-end learned control policy able to drive
There are two main established paradigms for solving the
an autonomous vehicle in dense urban environments,
problem of autonomous driving: a traditional engineering-
• a solution to the causal confusion problem, allowing
based approach and a data-driven, machine-learning ap-
motioninformationtobeprovidedtothemodelforboth
proach. The former performs well in structured driving
lateral and longitudinal control of the vehicle,
environments, such as highways or modern suburban devel-
• demonstration of data efﬁciency, showing that it is
opments, where explicitly addressing different scenarios is
possible to learn a model capable of decision making
tractable and sufﬁcient for human-like driving [1], [2], [3].
in urban trafﬁc with only 30 hours of training data,
This approach is more mature, and the focus of commercial
• comprehensive performance evaluation of end-to-end
efforts today. However, is still unclear if this can scale to
deep learning policies driving 35km on public roads.
deployingfullyautonomousvehiclesworld-wide,incomplex
urban scenarios with wider variation and visual diversity.
II. RELATEDWORK
In comparison, data driven methods avoid hand-crafted
rules and learns from human driving demonstrations by The ﬁrst application of imitation learning (IL) to au-
training a policy that maps sensor inputs, such as images, to tonomousdrivingwasALVINN[5],predictingsteeringfrom
controlcommandsorasimplemotionplan(e.g.,steeringand images and laser range data. Recently, IL for autonomous
speed). Importantly, such an approach has greater potential driving has received renewed interest due to advances in
The authors are with Wayve, in London, UK. Equal contribution. 1A video of the learned system driving a vehicle on urban streets is
research@wayve.ai availableathttps://wayve.ai/blog/learned-urban-driving
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 251
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:40:08 UTC from IEEE Xplore.  Restrictions apply. Leftcamera
||
Centrecamera || Steering,
|| Speed
||
||
Rightcamera
Perception SensorFusion Roucte Control
Command
(a) Model structure, showing a conceptual grouping based on (b) An example of the intermediate learned perception features
classicalrobotarchitectures.Weframethissinglefullydifferentiable from the three forward-facing cameras on our vehicle in a typical
architecture conceptually in terms of perception, sensor fusion and urbanscene(forward-left,forward,andforward-right).Fromtopto
control components, though the division is purely semantic. bottom: RGB input, segmentation, monocular depth, optical ﬂow.
Fig. 1: We demonstrate conditional route following using an end-to-end learned driving policy in dense urban driving
scenarios, including simple trafﬁc following behaviour. The model structure is outlined in Figure 1a. Each camera provides
animage(orimageswhereusingopticalﬂow)tothenetwork,generatingalearnedcomputervisionrepresentationfordriving.
We decode various perception outputs from this intermediate representation (semantics, geometry, and motion) shown in
||
Figure 1b. The output features are compressed by sensor fusion and concatenated ( denotes concatenation). Based on this
representation and a route command c, the network outputs a short motion plan in steering and speed.
deep learning. [4], [8] demonstrated an end-to-end network, resultsinoff-roadtrackdriving[20].Model-freeRLhasalso
controlling steering from single camera for lane following been studied for real-world rural lane following [21].
on empty roads. [9] adds conditional navigation to this Ourworkﬁtsintheend-to-endimitationlearningliterature
sameapproach.[7]learnslongitudinalandlateralcontrolvia and takes inspiration from the much of the work here. We
conditionalimitationlearning,followingroutecommandson extend the capabilities of a driving policy beyond what has
a remote control car in a static environment. [6] develops a been demonstrated to date: learning full lateral and longitu-
steering-only system that learns to navigate a road network dinal control on a vehicle with conditional route following
using multiple cameras and a 2D map for localisation, and behaviour with trafﬁc. We focus on novel urban routes,
however it uses heavily cropped images, and focuses on evaluated over 35km of driving.
localisation and route map generation.
III. METHOD
The CARLA simulator [10] has enabled signiﬁcant work
on learning to drive. One example is the work in [11], Consider a dataset of observation-action pairs
D{(cid:104) (cid:105)}
which established a new behaviour cloning benchmark for o ,a N , collected from expert demonstrations.
i i i=1 O → A
driving in simulation. However, simulation cannot capture The goal of IL is to learn a policy π (o ) : that
θ t
real-world complexities, and achieving high performance in maps observations o to actions a at every time step and
t t
simulation is signiﬁcantly simpler due to a state space with is able to imitate the expert. The parameters θ of the policy
L
less entropy, and the ability to generate near-inﬁnite data. are optimized by mini(cid:88)mizin(cid:0)g a distan(cid:1)ce between the
Severalapproacheshavetransferredpoliciesfromsimulation predicted action and the expert action:
to the real world. [12] uses semantic segmentation masks L
min π (o ),a (1)
as input and waypoints as output. [13] learns a control θ i i
θ
latent space that allows domain transfer between simulation i
and real world. Sim-to-real approaches are promising, but Conditional IL (CIL) [7] seeks to condition the policy on
policytransferisnotalwayseffectiveandrobustperformance some high-level command c that can convey the user intent
requires careful domain randomisation [14], [15], [16]. at test time. Such a command can serve to disambiguate
Rather than learning a driving policy from egocentric multi-modal behaviour: for example, when a car approaches
camera images, [17] and [18] present approaches to using an intersection, the camera input is not sufﬁcient to predict
imitation learning based on a bird’s eye view (BEV) of whether the car should turn left or right, or go straight.
a scene (using a fused scene representation and LiDAR Providingaroutecom(cid:88)mand(cid:0)chelpsresolv(cid:1)ingtheambiguity.
data respectively, neither being fully end-to-end due to the The CIL objective can be written as:
need for prior perception and/or mapping). However, both L
min π (o ,c ),a (2)
approaches require additional infrastructure in the form of θ i i i
θ
perception,sensorfusion,andhigh-deﬁnition(HD)mapping: i
inpreferencetothis,weadvocateforlearningfromrawsen- A. Model Architecture
sor(camera)inputs,ratherthanintermediaterepresentations. Our method learns directly from images and outputs
Recently, model-based reinforcement learning (RL) for a local motion plan for speed and steering. The driving
learning driving from simulated LiDAR data in [19], but policy is a fully end-to-end neural network, but we form
it has yet to be evaluated in real urban environments. Ap- three conceptual components: perception, sensor fusion and
proaches with low dimensional data have shown promising control. We use a combination of pretrained and trainable
252
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:40:08 UTC from IEEE Xplore.  Restrictions apply. layers to learn a robust driving policy. Figure 1 outlines the Instead, we propose a method, which cannot determine
architectural details discussed below. the causal structure, but can overcome the causal confusion
1) Perception: The perception component of our system problemandscaletolargenumberoffeatures.Givenaninput
consists of a deep encoder-decoder similar to multitask x, we add random noise, e.g. Gaussian, and apply dropout
segmentation and monocular depth [22], trained to recon- [31] on the noise with probability 0.5 for the full duration
struct RGB, depth and segmentation. For driving, we use of training. This allows the model to use the true value of x
theencodedfeatures,whichcontaincompressedinformation whenavailableandbreaksthecorrelationtherestofthetime,
abouttheappearance,thesemantics,anddistanceestimation. forcingthemodeltofocusonotherfeaturestodeterminethe
In principle, such representations could be learned simulta- correct output. Applying this approach on the ﬂow features
neouslywiththedrivingpolicy,forexample,throughdistilla- during training allowed the model to use explicit motion
tionorbyaddingperceptionlabels.However,toimprovedata information without learning the trivial identity solution.
efﬁciency and robustness when learning control, we pretrain 3) Control: The control module consists of several fully-
the perception network on several large, heterogeneous, connected layers that process the representation computed
research vision datasets [23], [24], [25], [26], [27]. by sensor fusion. At this stage, we also input a driving route
The perception architecture above does not have any command c , corresponding to one of go-straight, turn-left
t
temporalinformation aboutthe scene.One solutionisto use or turn-right, as a one-hot-encoded vector. We found that
concatenated representations of the past n frames, but this inputting the command multiple times at different stages of
did not perform well in our experiments. Instead, we use the network improves robustness of the model.
intermediate features from an optical ﬂow model similar to We selected this encoding to ground this work with that
[28],andconcatenatethistoformfeaturesrichinsemantics, demonstrated in CARLA [7], [11], [12], [19]. In practice,
geometry, and motion information. this limits real world testing to grid-like intersections. We
In some experiments we additionally use a left-facing and use this encoding for the approach outlined here, but we
a right-facing view. In this case, the perception component favour a richer learned route embedding similar to [6], [32].
treatseachoftheseframesindependently.Figure1bprovides The output of the control module consists of a simple
an example of the perception output on input images. parameterised motion plan for lateral and longitudinal con-
2) Sensor Fusion: The purpose of the sensor fusion trol. In particular, we assume that the vehicle motion is
component is to aggregate information from the different locally linear and we output a line, parameterised by the
sensors into a single representation of the scene for driving. current prediction y and a slope m , for both speed and
t t
Singleview. In the case of a single camera, the model is steering. During training, we minimise the mean squared
composed of a simple feedforward architecture of convolu- error between the expert actions, taken over N timesteps
tional and fully-connected layers. into the future, and the corresponding motion predicted by
(cid:88)
Multiview. Based on single-frame observations from 3 the network:
cameras:front,leftandright(Figure1).Drivingwithasingle −
view means the model must predict an action without full N 1γ∆n(y +m ∆ −a )2 (3)
state observability (e.g., losing sight of the adjacent kerb t t n t+n
n=0
at a junction). Enhancing observability with multiple views
should improve behaviour in these situations. However, we whereγ isafuturediscountfactor,∆n isthetimedifference
found that naively incorporating all camera features led to between steps t and t + n, yt and mt are the outputs
over-reliance on spurious information in the side views. of the network for ot, and at+n is the expert speed and
Thus,weoccasionallycorruptthesideviewencoderoutputs steering control at timestep t+n. Predicting such a trend
provides for smoother vehicle motion and demonstrated
during training, using the augmentation procedure described
betterperformancebothinclosedloopandopenlooptesting.
below for ﬂow. We also apply self-attention [29] on the
perception output of each camera, allowing the network to
B. Data
model dependencies across the image and focus on the parts
of the image that are important for driving. Drivingdataisinherentlyheavilyimbalanced,wheremost
Multiview and Flow. Additionally to the Multiview ar- of the captured data will be driving near-straight in the
chitecture above, optical ﬂow is computed only for the front middle of a lane (Figure 3). To mitigate this, during training
camera using the last two consecutive frames. The ﬂow we sample data uniformly across lateral and longitudinal
features are concatenated to the perception features for the control dimensions. We found that this avoids the need for
front camera. data augmentation [17] or synthesis [4].
≥
Access to more information, such as motion, can lead to We split the data into k 1 bins by steering, deﬁning
worse performance. As discussed by [11], [30], imitation x ,x astheminimalandmaximalvalues,andthebinedges
0 k
learning in particular can suffer from causal confusion: x ,...,x − . We deﬁne edges such that the product of the
1 k 1
unless an explicit causal model is maintained, spurious number of data in each bin and the width of that bin (i.e.
−
correlations cannot be distinguished from true causes in the x x − )areequalacrossallbins,ﬁndingtheseedgesusing
j j 1
demonstrations. For example, inputting the current speed to gradient descent. We assign sampling weights to data:
thepolicycausesittolearnatrivialidentitymapping,making −
x x − · W
the car unable to start from a static position. While [30] w = j j 1 − , (4)
j N x x
proposes a way to learn a causal graph, it assumes access j k 0
to a disentangled representation and its complexity grows where w denotes the weight of data in the j-th bin, N —
j j
exponentially with the number of features. the number of data in that bin and W — the total weight of
253
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:40:08 UTC from IEEE Xplore.  Restrictions apply. the dataset (equal to the number of data). The total weight Route annotations were added during postprocessing of the
of each bin is proportional to its width as opposed to the data, based on the route the driver took. Table I and Figure
number of data. The second factor is for normalisation: it 3 summarise the training and test datasets.
ensures that the total dataset weight is W. We recursively
apply this to balance each bin w.r.t. speed. Dataset Frames Distance(km) Time(hours) Drivers
Training 735K 613 27.2 8
IV. EXPERIMENTS Test 91K 89 3.4 1
The purpose of our experiments is to evaluate the impact TABLE I: Training and test driving datasets, collected over
of the following features on the driving performance of the a 6-month period in a typical European city.
policy:
• Usingcomputervisiontolearnexplicitvisualrepresen- EvaluationProcedureWeevaluatetheabilityofalearned
tations of the scene, in contrast to learning end-to-end drivingsystemtoperformtasksthatarefundamentaltourban
strictly from the control loss, driving: following lanes, taking turns at intersections and
• Improving the learned latent representation by provid- interacting with other road agents. We select two routes
ing wider state observability through multiple camera not present in the training data, each approximately 1km
views,andaddingtemporalinformationviaopticalﬂow, in length, and measure the intervention rates while au-
• Understanding inﬂuence of training data diversity on tonomously completing the manoeuvres required in order
driving performance. to drive the route. Each route was selected to have dense
To assess these questions, we train the following models: intersections, and be a largely Manhattan-like environment
• SV:Singleviewmodel.Basedonthenetworkdescribed analogousto [7]toavoidambiguitywiththerouteencoding.
in Section III-A, using only the forward facing camera. Figure 2 shows the locations of these routes. Completing a
Theparametersoftheperceptionmodulearepretrained routeinabusyurbanenvironmentimplicitlyentailsnavigat-
and frozen while training the sensor fusion and the ing around other dynamic agents such as vehicles, cyclists
control modules. A local linear motion plan for speed and pedestrians, to which the model has to respond safely
and steering is predicted over 1s in the future. This through actions such as stopping or giving way.
model serves as our baseline. Eachmodelwascommandedtodrivetherouteinbothdi-
• MV: Multiview model. As SV, but camera observations rections,eachofwhichcountsasanattempt.Shouldagiven
from forward, left, and right views are used, described model have ﬁve interventions within a single route attempt,
in III-A.2. the attempt was terminated early. Models with consistently
• MVF: As MV, but with optical ﬂow. poor performance were evaluated on fewer attempts.
• SV[75, 50, 25]: As SV, but using only the ﬁrst N% We additionally evaluate these models interacting with
of the training data. The latter portion of the data another vehicle, following a pace car which periodically
was discarded, degrading the diversity while retaining stopped throughout the driving route. During these tests, we
comparable temporal correlation. measure intervention rates for stopping behind a stationary
• SVE2E: As SV, but trained fully end-to-end with ran- vehicle or failing to follow the pace car.
domly initialised parameters. The interventions as well as their types are left to the
• SVE2EFT: E2E ﬁne-tuned perception. SV model with judgmentofthesafety-driver,whoisabletotakeovercontrol
a second E2E ﬁne-tuning stage. whenever the model exhibits poor behaviour. Evaluating a
• SVE2EPT: As SVE2E, but perception parameters pre- driving policy’s performance in terms of the number and
trained as in SV. type of interventions is preferable to computing low level
We evaluate all of the above models in closed loop metrics(e.g.,cross-trackerrortoapath)astherearemultiple
on real-world urban European streets using the procedure solutions to performing the task correctly.
described in Section IV-A and the metrics discussed in IV- We note that neither of the testing routes has been seen
C. We note that we also evaluated the baseline model SV in by the model during training, though the data has some
simulation with an in-house simulator: its performance was overlapwithsomesections.Whilestoppingforandfollowing
signiﬁcantly better compared to the real world, highlighting vehicles is a common occurrence in the data, the particular
the simpliﬁcation of the problem by simulation. test vehicle used was not seen during training.
Finally, the testing environment will inevitably contain
A. Procedure time-dependent factors beyond our control - this is expected
Data Collection. We collected data from human demon- for outdoor robot trials. Examples include the position and
strations over the span of 6 months for a total of 30 driving appearance of parked and dynamic vehicles, weather con-
hours in a densely populated, urban environment represen- ditions, presence and behaviour of other agents. We have
tative of most European cities. The drivers involved were controlled for these factors as much as possible by running
not given any explicit instructions and were free to choose trials with batches of models in succession. Fundamentally,
random routes. Collected data includes front, left and right this real-world constraint forces a driving system to gener-
cameras as well as measurements of the speed and steering alise: each attempt of the same route is novel in a different
controls from the driver. Images were captured from rolling way, e.g., weather, lighting, road users, static objects.
shutter cameras, synchronised within 5ms with a frame rate
B. Training Procedure
of 15Hz. Scalar values (e.g., speed) were received at 100Hz.
We used the front camera as the primary clock, associating The network described in Sections III-A is trained jointly
thiswiththelatestreceivedvaluesfortherestofthesensors. using stochastic gradient descent using batches of size 256
254
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:40:08 UTC from IEEE Xplore.  Restrictions apply. (a) Training collection area (b) Test route A (c) Test route B
Fig.2:WecollectexemplardrivingdataintheEuropeanurbanareain2a.Weevaluatethelearneddrivingpolicyperformance
intwourbanroutes,eachapproximately1kminlengthwithmultipleintersections.Bothroutesaredenseurbansceneswhich
broadly fall into a Manhattan-like grid structure, with a mix of road widths. Models tested on each route are evaluated in
both directions: note that route A takes a slightly different route on the return due to visibility at an intersection.
Finally, while these metrics provide quantitative results,
they fail to capture qualitative properties. We therefore pro-
vide some commentary on the models’ observed behaviour.
± D. Results
(a) Steering ( %) (b) Speed (km/h) (c) Time (hour)
Fig. 3: We collect training data driving across a European Table II outlines the experimental results. A total of
city(seetheheatmapin2a,showingdatadensity).Thedata, 34.4km of driving was performed. We consider the perfor-
asistypical,isimbalanced,withthemajoritydrivingstraight manceinastaticenvironment(drivingmanoeuvres)indepen-
(3a), and a signiﬁcant portion stationary (3b). dently of that while following a pace car (trafﬁc following).
1) Representation Learning with Computer Vision: Com-
paring the performance of SV to the E2E trained models
for 200k iterations. The initial learning rate is set to 0.01 (SVE2E, SVE2EFT, and SVE2EPT) suggests that learned
and decayed linearly during training, while momentum and
scene understanding is a critical component for a robust
weight decay are set to 0.9 and 0.0001 respectively. I−n total, representation for driving. Both fully E2E trained models
the network architecture used here consisted of 13 26M (SVE2E, SVE2EPT) perform very poorly. We attribute this
trainable parameters, varying depending on the perception
to the fact that our perception model is more robust to the
encoder used, and the number of cameras.
diversityofrealworldimages[35],inpartinﬂuencedbythe
additional data seen during the perception training phase.
C. Metrics
In robotics we must consider methods which prioritise data
We need a principled way of identifying which of the efﬁciency: we will never have the luxury of inﬁnite data.
models may perform best under closed-loop test conditions. Here we train on 30 hours of driving: not nearly sufﬁcient
For this purpose we use open-loop metrics of the control to cover the diversity in real world scenes. We observed
policy, computed on a validation dataset. As discussed by that the E2E trained models performed comparably to SV
[33], [34], the correlation between ofﬂine open-loop metrics in shaded environments, but poorly in bright sunlight. We
and online closed-loop performance is weak. We observed attributethistoappearancechangebetweenourtrainingdata
similar results, but we found some utility in using weighted and the test environment as the data was gathered in winter,
mean absolute error for speed and steering as a proxy for threemonthspriortotestinginspring.Additionally,theﬁne-
real world performance (referred to in Table II as Balanced tunedmodelperformssimilarlytothebaseline;onthisbasis
MAE). We used this metric on the test dataset in Table I to we recommend using pretrained perception due to the wider
select models for deployment. For the experimental results, beneﬁts of interpretability.
we adopt the following closed-loop metrics: 2) Improving the learned state: observability & temporal
• Intervention rate, computed as metres per intervention, information: Firstly we consider the performance of multi-
• Intervention rate only during lane following, view models compared to the singleview baseline. We see
• Success rate of turning manoeuvres (left, right), an improvement in driving performance when providing the
• Intervention rate while following a pace car, travelling modelwithamorecompleteviewofthescene.Thisisappar-
at up to 15 km/h, entwhenconsideringthedisobeyednavigationinterventions:
• Success rate of stopping behind a pace car. the SV model systematically fails to take certain turns. For
We also assess the following intervention categories: example, if the side road is narrow, it tends to drive straight
• Obstacle: e.g., prevention of crashing into an obstacle past ignoring the route. Equally, the model commonly cuts
such as a parked car or road works, the near-side kerb while turning. Both these failure modes
• Road position: e.g., wrong side of trafﬁc or dangerous areattributabletothefactthattheforwardfacingcamerahas
proximity to the kerb, novisibilityofthispartofthescene.Multiviewmodelstend
• Disobeyed navigation: e.g., drove straight instead of nottoexhibitthesefailuremodes.Qualitatively,wefoundthe
turning left, MV models provided a much larger buffer around the kerb,
• Failed manoeuvre: e.g., attempted the correct turn, but however both would exhibit the same failure mode of poor
failed to complete the manoeuvre, positioning behind parked cars around turns without space
• Dynamic vehicle: e.g., acted poorly with other trafﬁc, to correct. We attribute this to the low perception resolution
such as not stopping behind a car. andsimpleactionrepresentationused.Secondly,weconsider
255
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:40:08 UTC from IEEE Xplore.  Restrictions apply. Attempts Distance(m) Interventionrate(m/int) SuccessRate(%) OpenLoop
Model
A B Manoeuvres(LFonly) Trafﬁcfollow Manoeuvres(LFonly) TrafﬁcFollow Turn:Left Turn:Right Stop:Car MAE Bal-MAE
SV 14 12 5113(4094) 3158 222(372) 121 74%(N=31) 87%(N=31) 69%(N=36) 0.0715 0.0630
MV 14 12 5811(4613) 3440 306(659) 191 70%(N=23) 86%(N=36) 65%(N=40) 0.0744 0.0706
MVF 14 12 5313(4086) 2569 253(511) 161 78%(N=32) 82%(N=34) 81%(N=36) 0.0633 0.0612
SV75 2 2 2777(2258) - 154(376) - 57%(N=13) 80%(N=17) - 0.0985 0.0993
SV50 2 4 2624(2145) - 67(98) - 54%(N=19) 50%(N=16) - 0.0995 0.1015
SV25 2 - 926(746) - 44(50) - 33%(N=6) 71%(N=7) - 0.1081 0.1129
SVE2E 2 2 823(615) - 30(30) - 42%(N=12) 100%(N=5) - 0.1410 0.1365
SVE2EFT 2 2 1591(759) - 177(379) - 69%(N=16) 90%(N=21) - 0.0769 0.0801
SVE2EPT 2 2 260(177) - 26(59) - 33%(N=3) 0%(N=5) - 0.0966 0.0946
TABLE II: Driving performance metrics evaluated on routes A and B (see IV and IV-A). Trafﬁc Follow and Manoeuvres
present the metrics for following the routes with and without a pace car. The LF only metrics refer to lane following. In
Success Rate metrics, N refers to the number of manoeuvre attempts.
(a) Performance trend with increasing data (b) Interventions by type
Fig. 4: From the results in Table II we make a number of observations. In 4a we see a clear relationship between data and
model performance (showing mean, s.d.). Inspecting the interventions in 4b we observe that the addition of ﬂow improves
dynamic vehicle behaviour, but potentially suffers due to a lower signal-to-noise ratio with increased dimensionality.
the beneﬁt of augmenting the learned representation with a real vehicle. To the best of our knowledge, we are the
temporalstate,describingthescenemotionwithopticalﬂow. ﬁrst to learn a policy that can drive on a full-size vehicle
MVF performs slightly worse than MV in these metrics. with full vehicle actuation control in complex real-world
We attribute this to a lower signal-to-noise ratio given the urbanscenarioswithsimpletrafﬁc.Ourﬁndingsindicatethat
increased dimensionality, though we consider these to be learning intermediate representations using computer vision
largely comparable. Where this motion information clearly yields models which signiﬁcantly outperform fully end-to-
has beneﬁt is behaviour with other vehicles (stopping): the end trained models. Furthermore, we highlight the impor-
MVF model responds more robustly, demonstrated by a tance of data diversity and addressing dataset bias. Reduced
success rate increase from 65% to 81%. state observability models (i.e., single view) can perform
3) Inﬂuenceofdataquantity&diversityonperformance: some manoeuvres, but have systematic failure modes. In-
The learned driving policies presented here need signiﬁcant creasedobservabilitythroughtheadditionofmultiplecamera
furtherworktobecomparabletohumandriving.Weconsider views helps, but requires dealing with causal confusion.
the inﬂuence of data quantity on SV model performance, The method presented here has a number of limitations
observing a direct correlation in Figure 4a. Removing a whichwebelieveareessentialforachieving(andexceeding)
quarterofthedatanotablydegradesperformance,andmodels human-level driving. For example, it does not have access
trained with less data are almost undriveable. We attribute to long-term dependencies and cannot ‘reason’ about the
this to a reduction in data diversity. As with the E2E trained road scene. This method also lacks a predictive long-term
models, we found that the reduced data models were able to planningmodel,importantforsafeinteractionwithoccluded
perform manoeuvres in certain circumstances. Qualitatively, dynamic agents. There are many promising directions for
all models tested perform notably better during conditions future work. The time and safety measures required to run a
closertothecollectionperiod(concludedthreemonthsprior policy in closed loop remain a major constraint. Therefore,
to these experiments). We observed that, in the best case, being able to robustly evaluate a policy ofﬂine and quantify
these models drove 2km without intervention. While we do its performance is important area for research. Including the
not know the upper limit on performance for the effect of abilitytolearnfromcorrectiveinterventions(e.g.,[36],[21])
data, it is clear that more data than our training set — and isanothervitaldirectionforfuturework,andforgeneralising
more diversity — is likely to improve performance. to real-world complexities across different cities.
ACKNOWLEDGMENTS
V. CONCLUSIONS
We thank our colleagues for their support for this work:
We present a conditional imitation learning approach Andrew Jones, Nikhil Mohan, Alex Bewley, Edward Liu,
which combines both lateral and longitudinal control on John-Mark Allen, Julia Gomes, and Yani Ioannou.
256
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:40:08 UTC from IEEE Xplore.  Restrictions apply. REFERENCES reinforcementlearning,”inIEEEInternationalConferenceonRobotics
andAutomation(ICRA),2017,pp.1714–1721.
[1] S. Thrun, M. Montemerlo, H. Dahlkamp, D. Stavens, A. Aron, [21] A.Kendall,J.Hawke,D.Janz,P.Mazur,D.Reda,J.-M.Allen,V.-D.
J. Diebel, P. Fong, J. Gale, M. Halpenny, G. Hoffmann, K. Lau, Lam,A.Bewley,andA.Shah,“Learningtodriveinaday,”in2018
C.Oakley,M.Palatucci,V.Pratt,P.Stang,S.Strohband,C.Dupont, IEEEInternationalConferenceonRoboticsandAutomation(ICRA),
L.-E.Jendrossek,C.Koelen,C.Markey,C.Rummel,J.vanNiekerk, 2018.
E. Jensen, P. Alessandrini, G. Bradski, B. Davies, S. Ettinger, [22] A.Kendall,Y.Gal,andR.Cipolla,“Multi-tasklearningusinguncer-
A. Kaehler, A. Neﬁan, and P. Mahoney, “Stanley: The robot taintytoweighlossesforscenegeometryandsemantics,”inTheIEEE
that won the darpa grand challenge: Research articles,” J. Robot. ConferenceonComputerVisionandPatternRecognition(CVPR),June
Syst., vol. 23, no. 9, pp. 661–692, Sep. 2006. [Online]. Available: 2018.
http://dx.doi.org/10.1002/rob.v23:9 [23] F.Yu,W.Xian,Y.Chen,F.Liu,M.Liao,V.Madhavan,andT.Darrell,
[2] E. D. Dickmanns, “The development of machine vision for road “Bdd100k:Adiversedrivingvideodatabasewithscalableannotation
vehicles in the last decade,” in Intelligent Vehicle Symposium, 2002. tooling,”arXivpreprintarXiv:1805.04687,2018.
IEEE,vol.1. IEEE,2002,pp.268–281. [24] G. Neuhold, T. Ollmann, S. Rota Bulo`, and P. Kontschieder, “The
[3] J. Leonard, J. How, S. Teller, M. Berger, S. Campbell, G. Fiore, mapillaryvistasdatasetforsemanticunderstandingofstreetscenes,”
L.Fletcher,E.Frazzoli,A.Huang,S.Karamanetal.,“Aperception- in International Conference on Computer Vision (ICCV), 2017.
drivenautonomousurbanvehicle,”JournalofFieldRobotics,vol.25, [Online].Available:https://www.mapillary.com/dataset/vistas
no.10,pp.727–774,2008. [25] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Be-
[4] M. Bojarski, D. D. Testa, D. Dworakowski, B. Firner, B. Flepp, nenson, U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset
P. Goyal, L. D. Jackel, M. Monfort, U. Muller, J. Zhang, forsemanticurbansceneunderstanding,”inProceedingsoftheIEEE
X. Zhang, J. Zhao, and K. Zieba, “End to End Learning for conference on computer vision and pattern recognition, 2016, pp.
Self-Driving Cars,” CoRR, vol. abs/1604.07316, 2016. [Online]. 3213–3223.
Available:http://arxiv.org/abs/1604.07316 [26] L. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille,
“Deeplab:Semanticimagesegmentationwithdeepconvolutionalnets,
[5] D. A. Pomerleau, “Alvinn: An autonomous land vehicle in a neural
atrous convolution, and fully connected crfs,” IEEE Transactions on
network,”inAdvancesinneuralinformationprocessingsystems,1989,
PatternAnalysisandMachineIntelligence,vol.40,no.4,pp.834–848,
pp.305–313.
April2018.
[6] A.Amini,G.Rosman,S.Karaman,andD.Rus,“Variationalend-to-
[27] A.Geiger,P.Lenz,C.Stiller,andR.Urtasun,“Visionmeetsrobotics:
endnavigationandlocalization,”in2019IEEEInternationalConfer-
Thekittidataset,”InternationalJournalofRoboticsResearch(IJRR),
enceonRoboticsandAutomation(ICRA). IEEE,2019.
2013.
[7] F. Codevilla, M. Mueller, A. Lo´pez, V. Koltun, and A. Dosovitskiy,
[28] D.Sun,X.Yang,M.-Y.Liu,andJ.Kautz,“Pwc-net:Cnnsforoptical
“End-to-enddrivingviaconditionalimitationlearning,”in2018IEEE
ﬂowusingpyramid,warping,andcostvolume,”inProceedingsofthe
InternationalConferenceonRoboticsandAutomation(ICRA). IEEE,
IEEEConferenceonComputerVisionandPatternRecognition,2018,
2018,pp.1–9.
pp.8934–8943.
[8] M. Bojarski, P. Yeres, A. Choromanska, K. Choromanski, B. Firner,
[29] H.Zhang,I.Goodfellow,D.Metaxas,andA.Odena,“Self-attention
L. Jackel, and U. Muller, “Explaining how a deep neural net-
generative adversarial networks,” arXiv preprint arXiv:1805.08318,
work trained with end-to-end learning steers a car,” arXiv preprint
2018.
arXiv:1704.07911,2017.
[30] P. de Haan, D. Jayaraman, and S. Levine, “Causal confusion in
[9] C. Hubschneider, A. Bauer, M. Weber, and J. M. Zo¨llner, “Adding
imitation learning,” CoRR, vol. abs/1905.11979, 2019. [Online].
navigation to the equation: Turning decisions for end-to-end vehicle
Available:http://arxiv.org/abs/1905.11979
control,” in 2017 IEEE 20th International Conference on Intelligent
[31] N.Srivastava,G.Hinton,A.Krizhevsky,I.Sutskever,andR.Salakhut-
TransportationSystems(ITSC). IEEE,2017,pp.1–8.
dinov, “Dropout: A simple way to prevent neural networks from
[10] A.Dosovitskiy,G.Ros,F.Codevilla,A.Lopez,andV.Koltun,“Carla:
overﬁtting,”JournalofMachineLearningResearch,vol.15,pp.1929–
Anopenurbandrivingsimulator,”inConferenceonRobotLearning,
1958,2014.
2017,pp.1–16.
[32] S. Hecker, D. Dai, and L. Van Gool, “End-to-End Learning of
[11] F.Codevilla,E.Santana,A.M.Lo´pez,andA.Gaidon,“Exploringthe Driving Models with Surround-View Cameras and Route Planners,”
limitations of behavior cloning for autonomous driving,” CoRR, vol. inEuropeanConferenceonComputerVision(ECCV),2018.
abs/1904.08980,2019. [33] F. Codevilla, A. M. Lopez, V. Koltun, and A. Dosovitskiy, “On
[12] M. Mueller, A. Dosovitskiy, B. Ghanem, and V. Koltun, “Driving ofﬂine evaluation of vision-based driving models,” in The European
Policy Transfer via Modularity and Abstraction,” in Proceedings of ConferenceonComputerVision(ECCV),September2018.
The2ndConferenceonRobotLearning,vol.87,2018,pp.1–15. [34] X.Liang,T.Wang,L.Yang,andE.Xing,“Cirl:Controllableimitative
[13] A. Bewley, J. Rigley, Y. Liu, J. Hawke, R. Shen, V.-D. Lam, and reinforcementlearningforvision-basedself-driving,”inTheEuropean
A. Kendall, “Learning to drive from simulation without real world ConferenceonComputerVision(ECCV),September2018.
labels,”inIEEEInternationalConferenceonRoboticsandAutomation [35] W. Maddern, G. Pascoe, C. Linegar, and P. Newman, “1 Year,
(ICRA),2018. 1000km: The Oxford RobotCar Dataset,” The International Journal
[14] M. Andrychowicz, B. Baker, M. Chociej, R. Jozefowicz, B. Mc- ofRoboticsResearch(IJRR),vol.36,no.1,pp.3–15,2017.[Online].
Grew, J. Pachocki, A. Petron, M. Plappert, G. Powell, A. Ray Available:http://dx.doi.org/10.1177/0278364916679498
et al., “Learning dexterous in-hand manipulation,” arXiv preprint [36] S. Ross, G. Gordon, and D. Bagnell, “A reduction of imitation
arXiv:1808.00177,2018. learning and structured prediction to no-regret online learning,” in
[15] J.Tobin,L.Biewald,R.Duan,M.Andrychowicz,A.Handa,V.Kumar, Proceedings of the Fourteenth International Conference on Artiﬁcial
B. McGrew, A. Ray, J. Schneider, P. Welinder et al., “Domain Intelligence and Statistics, ser. Proceedings of Machine Learning
randomization and generative models for robotic grasping,” in 2018 Research,G.Gordon,D.Dunson,andM.Dud´ık,Eds.,vol.15. Fort
IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems Lauderdale,FL,USA:PMLR,11–13Apr2011,pp.627–635.
(IROS). IEEE,2018,pp.3482–3489.
[16] X.B.Peng,M.Andrychowicz,W.Zaremba,andP.Abbeel,“Sim-to-
realtransferofroboticcontrolwithdynamicsrandomization,”in2018
IEEEInternationalConferenceonRoboticsandAutomation(ICRA).
IEEE,2018,pp.1–8.
[17] M.Bansal,A.Krizhevsky,andA.Ogale,“Chauffeurnet:Learningto
drivebyimitatingthebestandsynthesizingtheworst,”inProceedings
ofRobotics:ScienceandSystems,FreiburgimBreisgau,Germany,June
2019.
[18] W.Zeng,W.Luo,S.Suo,A.Sadat,B.Yang,S.Casas,andR.Urtasun,
“End-to-end interpretable neural motion planner,” in The IEEE Con-
ference on Computer Vision and Pattern Recognition (CVPR), June
2019.
[19] N.Rhinehart,R.McAllister,andS.Levine,“Deepimitativemodelsfor
ﬂexibleinference,planning,andcontrol,”CoRR,vol.abs/1810.06544,
2018.
[20] G.Williams,N.Wagener,B.Goldfain,P.Drews,J.M.Rehg,B.Boots,
and E. A. Theodorou, “Information theoretic mpc for model-based
257
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:40:08 UTC from IEEE Xplore.  Restrictions apply. 