2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
Multi-Task Recurrent Neural Network for Surgical Gesture Recognition
and Progress Prediction
Beatrice van Amsterdam1, Matthew J. Clarkson1, Danail Stoyanov1
Abstract—Surgical gesture recognition is important for sur- focused on surgical workﬂow analysis [14, 15], where the
gical data science and computer-aided intervention. Even with aim is to recognise surgical phases representing high-level
robotickinematicinformation,automaticallysegmentingsurgi-
surgicalstates.Weadoptthisapproachwithhigh-granularity
calstepspresentsnumerouschallengesbecausesurgicaldemon-
gesture sequences and design a multi-task recurrent neural
strationsarecharacterizedbyhighvariabilityinstyle,duration
andorderofactions.Inordertoextractdiscriminativefeatures network for simultaneous gesture recognition and progress
from the kinematic signals and boost recognition accuracy, we estimation.Differentlyfrompreviouswork,however,thetask
proposeamulti-taskrecurrentneuralnetworkforsimultaneous progress is based on the underlying action sequence rather
recognition of surgical gestures and estimation of a novel
than on time. We hypothesize that action-based progress
formulationofsurgicaltaskprogress.Toshowtheeffectiveness
estimation could help to learn action sequentiality despite
of the presented approach, we evaluate its application on
the JIGSAWS dataset, that is currently the only publicly duration variability and the presence of adjustment gestures
availabledatasetforsurgicalgesturerecognitionfeaturingrobot and spurious motions, and thus reduce out-of-order predic-
kinematic data. We demonstrate that recognition performance tionsandover-segmentationerrors.Wealsoanalysedifferent
improves in multi-task frameworks with progress estimation
progress estimation strategies and highlight correlations be-
without any additional manual labelling and training.
tween gesture and progress predictions.
I. INTRODUCTION We validate our algorithm on the kinematic data of the
Automated surgical gesture recognition aims at automat- JIGSAWS dataset [16], featuring demonstrations of ele-
ically identifying meaningful action units within surgical mentary surgical tasks collected from eight surgeons with
tasks that constitute a surgical intervention. The process different skill level using the da Vinci Surgical System
forms a fundamental step in the development of systems for (dVSS, Intuitive Surgical Inc.) [17]. Our experiments show
surgical data science [1], objective skill evaluation [2, 3] that gesture recognition performance improves in multi-task
and surgical automation [4, 5, 6]. The problem is how- frameworks with progress estimation at no additional cost,
ever challenging because surgical gestures have high degree as the progress labels can be generated automatically from
of variability due to multiple parameters in the operating the data and available action labels.
surgeon’s style and the patients’ anatomy which alters the
A. Related Work
duration, kinematics and order of actions among different
demonstrations [7]. Gesture recognition from robot kinematics has been tack-
Much research in the ﬁeld, however, is based on the led through probabilistic graphical models such as Hid-
premise that many surgical tasks have well-deﬁned structure den Markov Models (HMMs) [9, 10, 18] and Conditional
andusespeciﬁcactionpatternstoprogresstowardsasurgical Random Fields (CRFs) [19, 20, 21]. These however rely
goal. Gesture ﬂow has then been described through task- on frame-to-frame and segment-to-segment transitions only,
speciﬁc probabilistic grammars [8], which have been mod- ignoring long-range temporal dependencies in the surgical
elledwithpowerfulstatisticaltoolssuchasgraphicalmodels demonstrations.Deeplearningtechniqueshavebeenrecently
[9, 10] and neural networks [11, 12]. This work investigates used to capture complex, long-distance patterns through
if the recognition performance improves when the progress hierarchies of temporal convolutional ﬁlters [11, 22], LSTM
of the surgical task is modelled explicitly and learnt jointly networks [12] or deep Reinforcement Learning (RL) [23].
with the action sequence, resulting in a more discriminative Besides, unsupervised [24, 25] and weakly-supervised [26]
feature extraction process. recognition have been shown through clustering, which re-
The effectiveness of multi-task learning [13] and surgical duces the dependency on annotations but at the expense of
progress modelling has been demonstrated in previous work performance.
Surgicalvideoratherthankinematicsalsoembedsgesture
This work was supported by the Wellcome/EPSRC Centre for In-
information which can be extracted with spatio-temporal
terventional and Surgical Sciences (WEISS) at UCL (203145Z/16/Z),
EPSRC (EP/P027938/1, EP/R004080/1,NS/A000027/1), the H2020 FET CNNs [27], 3D CNNs [28], multi-scale temporal convo-
(GA 863146) and Wellcome [WT101957]. Danail Stoyanov is supported lutions [29, 30] or hybrid encoder-decoder networks with
by a Royal Academy of Engineering Chair in Emerging Technolo-
temporal-convolutionalﬁltersforlocalmotionmodellingand
gies (CiET1819/2/36) and an EPSRC Early Career Research Fellowship
(EP/P012841/1). bidirectional LSTM for long-range dependency memoriza-
1B. van Amsterdam, M. J. Clarkson and D. Stoyanov are with tion [31].
the Wellcome/EPSRC Centre for Interventional and Surgical Sci-
Finally, a number of studies have approached surgical
ences (WEISS), University College London, London, United Kingdom.
beatrice.amsterdam.18@ucl.ac.uk workﬂow analysis through multi-task learning. Examples
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 1380
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 14:18:59 UTC from IEEE Xplore.  Restrictions apply. Fig. 1. Our deﬁnition of progress is dictated by the underlying action sequence. We identiﬁed ﬁve gestures that represent essential progressive stages
in any complete suturing demonstration. The other classes represent adjustment gestures that serve to prepare or help to complete the execution of the
essentialgestures.
includesystemsforjointtaskandgestureclassiﬁcation[32], other classes represent adjustment gestures that serve to
andmodelsforjointphaserecognitionandtooldetection[33] prepare or help to complete the execution of the essential
orprogressestimation[15].Phaserecognitionnetworkshave gesturesandthat generallyappearinvariableorder. Wethus
also been pre-trained on auxiliary tasks such as prediction grouped fundamental gestures (performed by any of the two
of the Remaining Surgery Duration (RSD) [14] or estima- arms, even if JIGSAWS only features right-handed suturing
tion of the frame temporal order [34], aiming to improve demonstrations)andtheircorrespondingadjustmentgestures
understanding of the temporal progression of the surgical into 5 progress stages (from 0 to 4), as detailed below:
workﬂow. Such approaches show that multi-task learning • Progress 0: G1 Reaching for needle with right + G5
and progress modelling are beneﬁcial for surgical workﬂow Moving to center of workspace
understanding and could support ﬁne-grained analysis that • Progress 1: G2 Positioning the tip of the needle + G4
requires discriminative feature extraction. Transferring the needle from left to right before G2 +
G8 Orienting the needle before G2
II. METHODS • Progress 2: G3 Pushing the needle through the tissue +
A. Dataset G4/G8 before G3
• Progress 3: G6 Pulling the suture with left + G9 Using
Wetrainedournetworkonthe39suturingdemonstrations
right hand to tighten suture + G10 Loosening more
of the JIGSAWS dataset, using the kinematic data (end-
suture + G4/G8 before G6/G9/G10
effector position, velocity, gripper angle) recorded at 30 Hz
• Progress 4: G11 Dropping suture and moving to end
fromthetwoPatientSideManipulators(PSMs)ofthedVSS.
points + G4/G8 before G11
The trajectories were ﬁrst smoothed with a low-pass ﬁlter
with cut-off frequency f = 1.5 Hz against measurement As the task evolution in time is affected by numerous
c
noise [35], and then normalized to zero mean and unit factors, such as surgical skill and surgical context, we
variance to compensate for different units of measure. Fi- believe that activity-based progress could be better than
nally, data were re-sampled from 30 Hz to 5 Hz for shorter time-based progress in reducing the kinematic feature
computation time. variation for equal progress values. Moreover, it could
help to learn action sequentiality despite the presence of
Inordertolearnthetaskprogress,newgroundtruthlabels
adjustment gestures which occur in variable frequency and
were automatically generated from the available data and
uncertain order.
action labels. As a preliminary step, however, we carefully
inspected the video recordings in order to identify possible
imprecisions in the available annotations, that would affect
B. Multi-task Recurrent Neural Network
the automatic generation of our progress labels. We iden-
tiﬁed and corrected 12 mistakes, affecting a total of 2356 Our multi-task architecture performs action recognition
data samples. Amendments to the original annotations are jointlywithprogressestimation.Astheprogressisquantized
reported in the Appendix. into 5 sequential steps, we estimate it using three different
As illustrated in Fig. 1, our deﬁnition of progress is strategies: regression, standard classiﬁcation and classiﬁca-
dictated by the underlying action sequence. Out of the 10 tion with ordered classes (or ordinal regression).
original action labels from JIGSAWS, we identiﬁed ﬁve Notation: vectors are represented in bold lowercase letters
gestures that constitute essential progressive stages in any (e.g. y), scalars in lowercase letters (e.g. y), parameters and
complete suturing demonstration (Reaching for the needle, losses in uppercase letters (e.g. C).
Positioningthetipoftheneedle,Pushingtheneedlethrough
the tissue, Pulling the suture, Dropping the suture), gener- Regression: As shown in Fig. 2, the kinematic features
ating a simpliﬁed probabilistic state machine that describes (K) are fed to a single-layer bidirectional LSTM with 1024
the commonly-observed workﬂow of the suturing task. The hidden units. Activations from the forward and backward
1381
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 14:18:59 UTC from IEEE Xplore.  Restrictions apply. Fig.2. Multi-taskarchitectureforjointactionrecognitionandprogressregression.Thekinematicfeatures(K)arefedtoabidirectionalLSTMcell,whose
hiddenunitsareconnectedtotheregressionnodebyafullyconnectedlayer.Thesamehiddenunitsareprojectedbyasecondfullyconnectedlayerinto
10logitswithsoftmaxactivationfunctionforactionclassiﬁcation.
a 5-logit fully connected layer with softmax activation
function and MCE loss, thus obtaining a multi-hierarchical
action recognition network (Fig. 2). After model training,
the logit with largest activation is considered for progress
prediction.
Ordered classiﬁcation: Standard classiﬁcation considers
independentcategoriesanddoesnotpenalizemajorordering
mistakes. In order to represent the succession of progress
classes, we thus encoded the target vectors with the ordinal
Fig.3. Targetencodingsforprogressregression,classiﬁcationandordered formulation of [36] as represented in Fig. 3, and substituted
classiﬁcation. the categorical MCE loss with the Mean Binary Cross
Entropy (MBCE) loss (i.e. Sigmoid activation function and
MCE loss). MBCE sets up an independent binary classiﬁer
streamsareconcatenatedintoa2048-dimensionalvectorand
for each class and, in combination with the ordinal target
then connected to the regression node by a Fully Connected
encoding, generates a larger loss the further the prediction
(FC) layer with linear activation function. The same 2048
is from its ground truth. After model training, progress
featuresarealsoprojectedbyasecondfullyconnectedlayer
predictions are obtained from the output yp of this classiﬁer
into 10 logits with softmax activation function for action
by ﬁnding the ﬁrst index k where yp<0.5.
classiﬁcation. k
At each training iteration, we compute the regression
In all the three cases, the ﬁnal multi-task loss (L) is a
loss using the Mean Absolute Error (MAE) over individual
(cid:88) weighted combination of the two single-task losses (L =
demonstrations: 1
MCE, L =MAE or MCE or MBCE):
2
1 T | − |
MAE = T ytp yˆtp , (1) L=w1L1+w2L2 (3)
t=1 However, multi-task networks are generally difﬁcult to
and the classiﬁcation loss(cid:18)using the Mean C(cid:19)ross Entropy train, as task imbalances may lead to the generation of
(cid:88) (cid:88)
(MCE) over individual demonstrations, as in [12]: shared features that are not useful across all tasks. In order
to automatically balance our model training, we used the
MCE = 1 T − C ya log(yˆa) , (2) GradNorm algorithm [37] for gradient normalization, that
T tc tc has been shown to improve accuracy and reduce overﬁtting
t=1 c=1
acrossmultipletaskswhencomparedtosingle-tasknetworks.
whereT isthedemonstrationlength(numberofsamples),C
GradNorm dynamically updates the single-task loss weights
is the number of action classes, yp and ya are the regression
and prediction nodes’ output at titmestamtp t, and yˆp and ˆya (w1, w2) during training by optimizing an additional loss
are the corresponding ground truths. t t (Lgrad), which aims at(cid:88)reg(cid:12)ularizing the train(cid:12)ing rate of the
individual tasks: (cid:12) (cid:12)
After model training, the regression output is rounded (cid:12) (cid:12)
to the nearest integer for progress prediction, and the logit 2 − ×
with largest activation is considered for action prediction. Lgrad= gw(i) g¯w (ri)α (4)
i=1
||(cid:53) ||
Classiﬁcation: To perform standard progress gw(i) = wwiLi is the L2-norm of the gradient of the
classiﬁcation, we substitute the regression layer with weighted single-task loss w L with respect to the network
i i
1382
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 14:18:59 UTC from IEEE Xplore.  Restrictions apply. Fig. 4. On the left: multi-task architecture for joint action recognition and progress classiﬁcation. The LSTM hidden units are connected to 5 logits
with softmax activation and MCE loss for progress classiﬁcation. On the right: multi-task architecture for joint action recognition and ordered progress
classiﬁcation.TheLSTMhiddenunitsareconnectedto5logitswithsigmoidactivationandMBCElossfororderedprogressclassiﬁcation.
weights w. batch size 5 and initial learning rate 0.1. The multi-task
g¯w is the average gradient norm across all tasks. architectures for standard progress classiﬁcation (APc) and
r =(L /L0)/L¯ istherelativeinversetrainingrateoftask ordered progress classiﬁcation (APoc) were instead trained
i i i
i, with L0 the single-task loss at the ﬁrst training iteration with GD, batch size 5 and initial learning rate 1.0. We
i
and L¯ the average loss across all tasks. always applied learning rate decay of 0.5 after 80 iterations
α is a balancing hyperparameter to be tuned. and stopped the training after 120 iterations. We used
gradient clipping to avoid exploding gradients and dropout
regularization with dropout rate of 0.5, as for the baseline
C. Evaluation Setup
(A). The single-task loss weights (w , w ) were updated
1 2
As in [22], we evaluated our network recognition per- at a learning rate of 0.025 using GD on the regularization
formance using accuracy, i.e. the percentage of correctly loss (Lgrad), with α set to 1.5. Testing was performed
labelled frames, normalized segmental Edit score, which after 100, 110 and 120 training iterations and results were
determines the precision of the predicted temporal ordering averaged. We trained all networks on the pre-processed
of actions, and segmental F1@10 score, which penalizes kinematic data with revised annotations.
over-segmentation errors but is not sensitive to minor tem-
poral shifts between predictions and ground truth. Progress
regressionwasevaluatedwithMAE,normalizedwithrespect Comparison between A, APr, APc and APoc is pre-
to the full range of progress values (M−AE ∗100). sented in Table I. Multi-task performance is evaluated with
We followed the standard JIGSAWS4L0eave One User Out (α=1.5)andwithout(w1=w2=1)GradNormregularization.
(LOUO) cross-validation setup [16]: for every fold, all the Scores are reported as mean values across the 8 valida-
trials performed by a single user are kept out as the test set tion folds and corresponding standard deviations, which are
and the other demonstrations are used to train our model. strongly representative of inter-surgeon style variability in
the LOUO setup. All three multi-task architectures outper-
III. EXPERIMENTSANDRESULTS form the single-task baseline on the segmental scores (Edit
We used the open source TensorFlow implementation of and F1@10), which seems to conﬁrm the hypothesis that
theBidirectionalLSTMpresentedin[12]asourbaseline(A). action-based progress estimation could help to learn action
We also relied on the provided training parameters, since sequentialityandtoreduceout-of-orderpredictionsandover-
they were carefully tuned on the same dataset. Given the segmentation errors. Even if none of the proposed archi-
stochasticnatureoftheoptimizationprocess,allexperiments tectures clearly stands out from the others, APoc generates
were performed three times and results were averaged. All slightly better results, which could be explained by stronger
runsweretrainedonNVIDIATeslaV100-DGXSGPU,with penalization of major ordering mistakes than standard clas-
training time of about 1 hour per run. siﬁcation, and easier optimization goal than regression of
Our multi-task network for joint action recognition and a discontinuous progress function. The architecture that
progress regression (APr) was learnt on the multi-task beneﬁts the most from multi-task gradient normalization
loss L using Gradient Descent (GD) with Momentum 0.9, is APr, as it is perhaps more challenging to balance two
1383
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 14:18:59 UTC from IEEE Xplore.  Restrictions apply. TABLEI
GESTURERECOGNITION(A)ANDPROGRESSESTIMATION(P)PERFORMANCE.SCORESAREREPRESENTEDASMEAN(STD).
Scores A APr APc APoc Pr Pc Poc
w1=w2=1 α=1.5 w1=w2=1 α=1.5 w1=w2=1 α=1.5
Accuracy 85.3(5.8) 85.1(6.6) 85.2(6.3) 85.7(5.7) 85.8(5.6) 85.5(5.8) 86.0(5.4) - - -
A Edit 83.1(7.4) 84.2(6.7) 85.9(6.4) 85.4(5.7) 85.7(5.7) 86.2(6.3) 86.1(6.2) - - -
F1@10 88.5(5.7) 89.0(5.5) 90.1(5.4) 90.1(4.9) 90.2(4.9) 90.5(5.0) 90.7(4.8) - - -
MAE - 5.3(1.5) 5.1(1.5) - - - - 6.2(1.1) - -
Accuracy - - - 89.0(2.7) 89.1(2.8) 87.9(3.5) 88.4(3.1) - 89.2(2.8) 87.0(4.0)
P
Edit - - - 89.3(6.7) 89.2(6.6) 83.7(4.8) 83.6(4.5) - 87.8(7.8) 87.3(5.7)
F1@10 - - - 93.2(4.4) 93.2(3.4) 90.0(3.1) 90.0(3.1) - 91.9(4.9) 91.9(3.6)
TABLEII
COMPARISONWITHRELATEDWORKONROBOTKINEMATICS.
Accuracy Edit
TCN[11] 79.6 85.8
TCN+RL[23] 82.1 87.9
BiLSTM[12] 83.3 81.1
APoc 85.3 84.5
APc 85.5 85.3
Fig. 6 illustrates an example of recognition output where
predictions generated by the multi-task network show re-
Fig.5. Recognitionaccuracy[%]ofindividualgestures.
duced over-segmentation with respect to the baseline, as
quantiﬁed by the segmental score improvement previously
different loss functions (MCE for classiﬁcation and MSE reported.Itisalsointerestingtovisualizetherelationshipbe-
for regression) than two similar or identical ones. However, tweengestureandprogresspredictions,asthesegmentations
balanced multi-task networks rely on a large number of boundariesarefrequentlyaligned(Fig.7),andpoorprogress
hyperparameters, including optimization parameters for the estimationoftencorrespondstopoorgesturerecognition,and
regularizationloss.Webelievethatresultscouldbeimproved vice versa (Fig. 8).
and differences between the three proposed architectures We also trained APc and APoc with the original anno-
could be emphasized with more extensive parameter tuning, tations of JIGSAWS, in order to compare our multi-task
as well as with larger datasets. modelstotheoriginalsingle-taskbaseline[12]andtorelated
Fig. 5 shows recognition accuracies of individual gestures work on robot kinematics. Our investigation, however, was
fromAandAPoc.APocconsistentlymatchesoroutperforms carried out on a simple LSTM architecture, and we suggest
A, even if improvement is only marginal. Results in Table I, the proposed multi-task approach could be applied on top of
however, showed that the advantage of the proposed method morecomplexarchitecturestoboostperformance.Resultsin
reliesintheregularizationofthepredictedsequences,which Table II highlight sensitivity of our models to action annota-
mainly affects the segmental scores and only marginally the tionnoise,whichpartiallyspoilstheautomaticgenerationof
framewiseevaluationmetrics.Somegestures,suchasG9and progresslabels.Thisresultsinperformancedegradationwith
G10, are extremely challenging to recognize in both cases, respect to the previous experiments, especially for APoc.
as they are under-represented in the dataset. Nonetheless,theproposednetworkssigniﬁcantlyoutperform
In addition to recognising surgical gestures, our multi- [12] both in accuracy and Edit score, and reach competitive
taskarchitecturessegmentthesurgicaldemonstrationsinto5 performance with respect to related work on robot kinemat-
fundamental progressive steps of the suturing task, reaching ics.
an average accuracy of 89.1% with standard classiﬁcation Finally, we substituted the Bidirectional LSTM cell in
(Table I). For APr and APc, but not for APoc, all evaluation APc with a Forward LSTM cell for online recognition.
scoresimprovedwithrespecttotheirsingle-taskcounterparts We reached accuracy and Edit score of 82.2 and 76.2
Pr and Pc1: not only higher-level progress understanding respectively,improvingupontheoriginalsingle-taskbaseline
can help gesture recognition, but gesture recognition can [12] (Table III).
reciprocally boost progress prediction.
Our results support the hypothesis that joint surgical
gesturerecognitionandprogressestimationcaninducemore
1Pr,PcandPocweretrainedoncewiththesamehyperparametersastheir
robust feature learning than gesture recognition alone, and
multi-taskcounterpart.Weightdecaywashoweveranticipatedandtraining
wasstoppedafter80iterations. boost performance in both online and ofﬂine applications.
1384
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 14:18:59 UTC from IEEE Xplore.  Restrictions apply. TABLEIII
ONLINEGESTURERECOGNITIONPERFORMANCE.
Accuracy Edit
LSTM[12] 80.5 75.3
APc 82.2 76.2
Fig.7. Comparisonbetweenactionandprogresspredictions(APrprogress
Fig.6. Exampleofrecognitionoutputwherepredictionsgeneratedbythe
predictioninred,progressgroundtruthinblue).Thepredictedsegmentation
multi-tasknetwork(APoc)showreducedover-segmentationwithrespectto
boundariesarefrequentlyaligned.
thebaseline (A),as quantiﬁedby improvedsegmental scores.The ground
truthsegmentation(GT)isshownatthetop.
IV. CONCLUSIONS
In this paper, we performed joint recognition of surgical
gestures and progress prediction from robot kinematic data.
Differentlyfrompriorwork,theprogresslabelsweredeﬁned
on the underlying action sequence rather than on time, in
ordertoreducekinematicfeaturevariationforequalprogress
values. Moreover, adjustment gestures did not contribute to Fig.8. Actionandprogresspredictionaccuracy[%]fromAPocforeach
cross-validationfold(B,C,D,E,F,G,H,I).Poorprogressestimationoften
the progress advancement. We assumed that action-based
correspondstopoorgesturerecognition,andviceversa.
progresspredictioncouldhelptorecognizesurgicalgestures
in well-structured tasks such as suturing and knot tying,
which are generally performed several times during surgi- progress in time of the individual gestures, which could
cal interventions. We analysed different progress estimation improve understanding of gesture evolution and duration.
strategies, and demonstrated on the suturing demonstrations Moreover, the integration of visual features extracted from
of the JIGSAWS dataset that the proposed multi-task net- surgical videos could boost both action recognition and
works outperform the single-task baseline in terms of Edit progress estimation, as video data encode complementary
score and F1@10 score, indicating a reduction in out-of- information about the surgical tools and the state of the
orderpredictionsandover-segmentationerrors.Sinceaction- environment.
based progress does not depend on time nor on adjustment Finally, evaluation of the proposed methodology was per-
gestures,weconjecturethisapproachcouldalsobeeffective formedontheJIGSAWSdataset,whichiscurrentlytheonly
beyond JIGSAWS in unconstrained environments, such as publicly available dataset for surgical gesture recognition
real surgical interventions or free surgical training sessions, featuringrobotkinematics.However,JIGSAWSissmalland
where demonstrations do not have standardized length, right contains a limited range of surgical motions. New surgical
andlefthandsareoftenusedinterchangeably,andadjustment data will be collected in the future, and extensive evaluation
gestures, pauses and undeﬁned motions are more frequent. will be carried out on larger datasets of robotic surgical
In this scenario, contextualization of surgical motion into demonstrations.
high-level progress stages could help to better recognize the
APPENDIX
surgicalactions.Thelimitationofthismethod,however,isin
therecognitionofunstructuredtaskssuchasbluntdissection, Amendments to the original annotations of JIGSAWS:
where action-based progress can not be clearly deﬁned. In
Demonstration Start End Label
the presence of frequent and scattered mid-task failures and
SuturingB004 2650 2860 G3
restarting, the ordered classiﬁcation method might also lose
SuturingC002 1596 1685 G4
its advantage to the standard classiﬁcation method. SuturingD003 1013 1250 G9
SuturingD003 1251 1339 G4
As suggested in [14], further investigation could be per-
SuturingD004 0099 0166 G5
formed on alternative multi-task integration modalities, such SuturingD004 0167 0275 G8
SuturingD004 0956 1020 G4
as pre-training on the auxiliary task for feature extraction
SuturingE003 1095 1267 G4
or ﬁne-tuning on the target task. This might potentially SuturingF001 2401 2498 G6
SuturingG001 1132 1353 G6
match or even improve upon multi-task training, at the cost
SuturingG001 7628 8181 G8
of additional training time. Another study could model the SuturingI003 0800 1250 G3
1385
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 14:18:59 UTC from IEEE Xplore.  Restrictions apply. REFERENCES tion,” Modeling and Monitoring of Computer Assisted Interventions
(M2CAI)-MICCAIWorkshop,pp.1–10,2015.
[1] L. Maier-Hein, S. S. Vedula, S. Speidel, N. Navab, R. Kikinis, [19] L. Tao, L. Zappella, G. D. Hager, and R. Vidal, “Surgical gesture
A. Park, M. Eisenmann, H. Feußner, G. Forestier, S. Giannarou, segmentation and recognition,” International Conference on Medical
M. Hashizume, D. Katic, H. Kenngott, M. Kranzfelder, A. Malpani, Image Computing and Computer-Assisted Intervention (MICCAI),
K.Ma¨rz,T.Neumuth,N.Padoy,C.M.Pugh,N.Schoch,D.Stoyanov, vol.8151LNCS,no.PART3,pp.339–346,2013.
R. H. Taylor, M. Wagner, G. D. Hager, and P. Jannin, “Surgical [20] C.Lea,G.D.Hager,andR.Vidal,“AnImprovedModelforSegmen-
data science for next-generation interventions,” Nature Biomedical tationandRecognitionofFine-GrainedActivitieswithApplicationto
Engineering,vol.1,pp.691–696,2017. SurgicalTrainingTasks,”IEEEWinterConferenceonApplicationsof
[2] S. S. Vedula, A. O. Malpani, L. Tao, G. Chen, Y. Gao, P. Poddar, ComputerVision(WACV),pp.1123–1129,2015.
N. Ahmidi, C. Paxton, R. Vidal, S. Khudanpur, G. D. Hager, and [21] E. Mavroudi, D. Bhaskara, S. Sefati, H. Ali, and R. Vidal, “End-to-
C. C. G. Chen, “Analysis of the Structure of Surgical Activity for a endﬁne-grainedactionsegmentationandrecognitionusingconditional
SuturingandKnot-TyingTask,”PLOSONE,vol.11,2016. randomﬁeldmodelsanddiscriminativesparsecoding,”IEEEWinter
[3] C. E. Reiley and G. D. Hager, “Task versus Subtask Surgical Skill Conference on Applications of Computer Vision (WACV), vol. 2018-
Evaluation of Robotic Minimally Invasive Surgery,” International Janua,pp.1558–1567,2018.
Conference on Medical Image Computing and Computer-Assisted [22] C.Lea,M.D.Flynn,R.Vidal,A.Reiter,andG.D.Hager,“Temporal
Intervention(MICCAI),pp.435–442,2009. convolutionalnetworksforactionsegmentationanddetection,”IEEE
[4] C. E. Reiley, E. Plaku, and G. D. Hager, “Motion generation of Conference on Computer Vision and Pattern Recognition (CVPR),
roboticsurgicaltasks:Learningfromexpertdemonstrations,”Annual vol.2017-Janua,pp.1003–1012,2017.
International Conference of the IEEE Engineering in Medicine and [23] D. Liu and T. Jiang, “Deep Reinforcement Learning for Surgical
BiologySociety(EMBC),pp.967–970,2010. Gesture Segmentation and Classiﬁcation,” International Conference
[5] A.Murali,S.Sen,B.Kehoe,A.Garg,S.McFarland,S.Patil,W.D. on Medical Image Computing and Computer-Assisted Intervention
Boyd,S.Lim,P.Abbeel,andK.Goldberg,“Learningbyobservation (MICCAI),vol.11073LNCS,pp.247–255,2018.
for surgical subtasks: Multilateral cutting of 3D viscoelastic and [24] S. Krishnan, A. Garg, S. Patil, C. Lea, G. D. Hager, P. Abbeel, and
2DOrthotropicTissuePhantoms,”IEEEInternationalConferenceon K. Y. Goldberg, “Transition state clustering: Unsupervised surgical
RoboticsandAutomation(ICRA),pp.1202–1209,2015. trajectorysegmentationforrobotlearning,”TheInternationalJournal
[6] D.Nagy,“ADVRK-basedFrameworkforSurgicalSubtaskAutoma- ofRoboticsResearch,vol.36,pp.1595–1618,2015.
tion,”ActaPolytechnicaHungarica,vol.16,no.8,pp.61–78,2019. [25] M.J.Fard,S.Ameri,R.B.Chinnam,andR.D.Ellis,“SoftBoundary
[7] C. G. Cao, C. L. MacKenzie, and S. Payandeh, “Task and motion ApproachforUnsupervisedGestureSegmentationinRobotic-Assisted
analyses in endoscopic surgery,” in American Society of Mechanical Surgery,” IEEE Robotics and Automation Letters, vol. 2, no. 1,
Engineers,DynamicSystemsandControlDivisionDSC,1996. pp.171–178,2017.
[8] N.Ahmidi,L.Tao,S.Sefati,Y.Gao,C.Lea,B.B.Haro,L.Zappella, [26] B. van Amsterdam, H. Nakawala, E. D. Momi, and D. Stoyanov,
S.Khudanpur,R.Vidal,andG.D.Hager,“ADatasetandBenchmarks “WeaklySupervisedRecognitionofSurgicalGestures,”inIEEEInter-
for Segmentation and Recognition of Gestures in Robotic Surgery,” nationalConferenceonRoboticsandAutomation(ICRA),pp.9565–
IEEETransactionsonBiomedicalEngineering,2017. 9571,IEEE,IEEE,2019.
[27] C.Lea,A.Reiter,R.Vidal,andG.D.Hager,“Segmentalspatiotempo-
[9] B. Varadarajan, C. Reiley, H. Lin, S. Khudanpur, and G. Hager,
ralCNNsforﬁne-grainedactionsegmentation,”EuropeanConference
“Data-DerivedModelsforSegmentationwithApplicationtoSurgical
Assessment and Training,” in International Conference on Medical onComputerVision(ECCV),vol.9907LNCS,pp.36–52,2016.
Image Computing and Computer-Assisted Intervention (MICCAI), [28] I. Funke, S. Bodenstedt, F. Oehme, F. von Bechtolsheim, J. Weitz,
and S. Speidel, “Using 3D Convolutional Neural Networks to Learn
pp.426–434,2009.
SpatiotemporalFeaturesforAutomaticSurgicalGestureRecognition
[10] L. Tao, E. Elhamifar, S. Khudanpur, G. D. Hager, and R. Vidal,
inVideo,”inInternationalConferenceonMedicalImageComputing
“SparsehiddenMarkovmodelsforsurgicalgestureclassiﬁcationand
andComputer-AssistedIntervention(MICCAI),2019.
skill evaluation,” International conference on information processing
[29] P. Lei and S. Todorovic, “Temporal Deformable Residual Networks
in computer-assisted interventions, vol. 7330 LNCS, pp. 167–177,
for Action Segmentation in Videos,” IEEE Conference on Computer
2012.
VisionandPatternRecognition(CVPR),pp.6742–6751,2018.
[11] C.Lea,R.Vidal,A.Reiter,andG.D.Hager,“Temporalconvolutional
[30] J.Wang,Z.Du,A.Li,andY.Wang,“Atroustemporalconvolutional
networks: A uniﬁed approach to action segmentation,” European
network for video action segmentation,” IEEE International Confer-
Conference on Computer Vision (ECCV), vol. 9915 LNCS, pp. 47–
enceonImageProcessing(ICIP),pp.1585–1589,2019.
54,2016.
[31] L.DingandC.Xu,“TricorNet:AHybridTemporalConvolutionaland
[12] R.DiPietro,C.Lea,A.Malpani,N.Ahmidi,S.S.Vedula,G.I.Lee,
Recurrent Network for Video Action Segmentation,” arXiv preprint
M. R. Lee, and G. D. Hager, “Recognizing Surgical Activities with
arXiv:1705.07818,2017.
RecurrentNeuralNetworks,”inInternationalConferenceonMedical
[32] D.Sarikaya,K.A.Guru,andJ.J.Corso,“JointSurgicalGestureand
Image Computing and Computer-Assisted Intervention (MICCAI),
TaskClassiﬁcationwithMulti-TaskandMultimodalLearning,”arXiv
pp.551–558,2016.
preprintarXiv:1805.00721,2018.
[13] S. Ruder, “An Overview of Multi-Task Learning in Deep Neural
[33] A.P.Twinanda,S.Shehata,D.Mutter,J.Marescaux,M.DeMathelin,
Networks,”arXivpreprintarXiv:1706.05098,2017.
andN.Padoy,“EndoNet:ADeepArchitectureforRecognitionTasks
[14] G. Yengera, D. Mutter, J. Marescaux, and N. Padoy, “Less is More:
on Laparoscopic Videos,” IEEE Transactions on Medical Imaging,
Surgical Phase Recognition with Less Annotations through Self-
vol.36,no.1,pp.86–97,2017.
Supervised Pre-training of CNN-LSTM Networks,” arXiv preprint
[34] S.Bodenstedt,M.Wagner,D.Katic´,P.Mietkowski,B.Mayer,H.Ken-
arXiv:1805.08569,2018.
ngott, B. Mu¨ller-Stich, R. Dillmann, and S. Speidel, “Unsupervised
[15] X. Li, R. S. Burd, Y. Zhang, J. Zhang, M. Zhou, S. Chen, Y. Gu,
temporal context learning using convolutional neural networks for
Y.Chen,I.Marsic,andR.A.Farneth,“ProgressEstimationandPhase
laparoscopic workﬂow analysis,” arXiv preprint arXiv:1702.03684,
Detection for Sequential Processes,” ACM on Interactive, Mobile,
vol.abs/1702.0,2017.
WearableandUbiquitousTechnologies,vol.1,no.3,pp.1–20,2017.
[35] F.Despinoy,D.Bouget,G.Forestier,C.Penet,N.Zemiti,P.Poignet,
[16] Y.Gao,S.S.Vedula,C.E.Reiley,N.Ahmidi,B.Varadarajan,H.C.
and P. Jannin, “Unsupervised Trajectory Segmentation for Surgical
Lin, L. Tao, L. Zappella, B. Be´jar, D. D. Yuh, C. C. G. Chen,
Gesture Recognition in Robotic Training,” IEEE Transactions on
R.Vidal,S.Khudanpur,andG.D.Hager,“JHU-ISIGestureandSkill
BiomedicalEngineering,vol.63,no.6,pp.1280–1291,2016.
AssessmentWorkingSet(JIGSAWS):ASurgicalActivityDatasetfor
[36] J.Cheng,Z.Wang,andG.Pollastri,“Aneuralnetworkapproachto
Human Motion Modeling,” Modeling and Monitoring of Computer
ordinalregression,”inIEEEInternationalJointConferenceonNeural
AssistedInterventions(M2CAI)-MICCAIWorkshop,2014.
Networks,pp.1279–1284,IEEE,2008.
[17] G.S.GuthartandJ.K.S.Jr,“TheIntuitiveTMtelesurgerysystem:
[37] Z. Chen, V. Badrinarayanan, C.-Y. Lee, and A. Rabinovich, “Grad-
overviewandapplication,”RoboticsandAutomation,vol.1,no.April,
Norm:GradientNormalizationforAdaptiveLossBalancinginDeep
pp.618–621,2000.
MultitaskNetworks,”inInternationalConferenceonMachineLearn-
[18] S. Sefati, N. J. Cowan, and R. Vidal, “Learning Shared, Discrimi- ing(ICML),2017.
nativeDictionariesforSurgicalGestureSegmentationandClassiﬁca-
1386
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 14:18:59 UTC from IEEE Xplore.  Restrictions apply. 