2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
FisheyeDistanceNet: Self-Supervised Scale-Aware Distance Estimation
using Monocular Fisheye Camera for Autonomous Driving
Varun Ravi Kumar1, Sandesh Athni Hiremath1, Markus Bach1, Stefan Milz1,
Christian Witt1, Cle´ment Pinard2, Senthil Yogamani3 and Patrick Ma¨der4
1Valeo DAR Kronach, Germany 2ENSTA ParisTech Palaiseau, France
3Valeo Vision Systems, Ireland 4Technische Universita¨t Ilmenau, Germany
Abstract—Fisheye cameras are commonly used in applica-
tions like autonomous driving and surveillance to provide a
◦
large ﬁeld of view (> 180 ). However, they come at the cost
of strong non-linear distortions which require more complex
algorithms. In this paper, we explore Euclidean distance es-
timation on ﬁsheye cameras for automotive scenes. Obtaining WoodScape KITTI
accurate and dense depth supervision is difﬁcult in practice,
butself-supervisedlearningapproachesshowpromisingresults
andcouldpotentiallyovercometheproblem.Wepresentanovel
self-supervised scale-aware framework for learning Euclidean
distance and ego-motion from raw monocular ﬁsheye videos
without applying rectiﬁcation. While it is possible to perform
piece-wise linear approximation of ﬁsheye projection surface
and apply standard rectilinear models, it has its own set Fig.1Distanceanddepthderivedfromasingleﬁsheyeimage(left)
of issues like re-sampling distortion and discontinuities in andsinglepinholeimage(right)respectively.Ourself-supervised
transition regions. To encourage further research in this area, model,FisheyeDistanceNet,producessharp,highqualitydistanceand
depthmaps.
we will release our dataset as part of the WoodScape project
[1].WefurtherevaluatedtheproposedalgorithmontheKITTI
dataset and obtained state-of-the-art results comparable to Depth estimation models may be learned in a super-
otherself-supervisedmonocularmethods.Qualitativeresultson vised fashion on LiDAR distance measurements, such as
an unseen ﬁsheye video demonstrate impressive performance1. KITTI[23].Inpreviouswork,wefollowedthisapproachand
demonstratedthepossibilitytoestimatehigh-qualitydistance
maps using LiDAR ground truth on ﬁsheye images [12].
I. INTRODUCTION However, setting up the entire rig for such recordings is
expensive and time consuming, and therefore limits the
There has been a signiﬁcant rise in the usage of ﬁsheye
amount of data on which a model can be trained. To
cameras in various automotive applications [2], [3], [4],
overcome this problem, we propose FisheyeDistanceNet,
surveillance [5] and robotics [6] due to their large Field
the ﬁrst end-to-end self-supervised monocular scale-aware
of View (FOV). Recently, several computer vision tasks
training framework. FisheyeDistanceNet uses convolutional
on ﬁsheye cameras have been explored including object
neural networks (CNN) on raw ﬁsheye image sequences to
detection [7], soiling detection [8], motion estimation [9],
regressaEuclideandistancemapandprovidesabaselinefor
image restoration [10] and SLAM [11]. Depth estimation is
single frame Euclidean distance estimation. We summarize
animportanttaskinautonomousdrivingasitisusedtoavoid
our contributions as follows:
obstacles and plan trajectories. While depth estimation has
been substantially studied for narrow FOV cameras, it has • A self-supervised training strategy that aims at infer-
barely been explored for ﬁsheye cameras [12], [13]. ring a distance map from a sequence of distorted and
Previous learning-based approaches [14], [15], [16], [17] unrectiﬁed raw ﬁsheye images.
have solely focused on traditional 2D content captured with • A solution to the scale factor uncertainty with the bol-
cameras following a typical pinhole projection model based ster from ego-motion velocity allows outputting metric
on rectiﬁed image sequences. With the surge of efﬁcient distance maps. This facilitates the map’s practical use
and cheap wide angle ﬁsheye cameras and their larger FOV for self-driving cars.
in contrast to pinhole cameras, there has been signiﬁcant • A novel combination of super resolution networks and
interest in the computer vision community to perform depth deformable convolution layers [24] to output high reso-
estimationfromomnidirectionalcontentsimilartotraditional lution distance maps with sharp boundaries from a low
2D content via omnidirectional stereo [18], [19], [20], [21] resolutioninput.Inspiredbythesuperresolutionofim-
and structure-from-motion (SfM) [22] approaches. agesapproach[25]thisapproachallowsustoaccurately
resolve distances replacing the deconvolution [26] and
1seeFig.1andhttps://youtu.be/Sgq1WzoOmXg naive nearest neighbor or bilinear upsampling.
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 574
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:56:08 UTC from IEEE Xplore.  Restrictions apply. perform image synthesis. Depth estimation is an ill-posed
ℳ𝑡 →𝑡−1 ℳ 𝑡→𝑡+1 problem as there could exist a large number of possible
incorrect depths per pixel, which can also recreate the novel
view, given the relative pose between I and I (cid:48).
t t
Using view-synthesis as the supervising technique we can
train the network using the viewpoint of I − and I to
t 1 t+1
estimate the appearance of a target image I on raw ﬁsheye
t
𝐷𝑡−1   𝐷𝑡 𝐷𝑡+1 images. A naive approach would be correcting raw ﬁsheye
DistanceNet PoseNet DistanceNet PoseNet DistanceNet images to piecewise or cylindrical projections and would
essentially render the problem equivalent to Zhou et al.’s
work [15]. In contrast, at the core of our approach there
is a simple yet efﬁcient technique for obtaining scale-aware
distance maps.
This section starts with discussing the geometry of the
C𝐼 𝑡o−n1c+a𝐼t𝑡  It C𝐼𝑡o+nc𝐼𝑡a+t 1 problemandhowitisusedtoobtaindifferentiablelosses.We
       𝐼𝑡−1  𝐼𝑡 𝐼𝑡+1  describe the scale-aware FisheyeDistanceNet and its effects
    V𝑡−1 V𝑡 V𝑡 V𝑡+1 on the output distance estimates. Additionally, we provide
𝑡−1 𝑡 𝑡 𝑡+1
an in-depth discussion of the various losses.
Fig.2Overviewofourmethod.Theﬁrstrowrepresentsouregomasks
M M
asdescribedinSectionII-D, →− , → indicatewhichpixel
t t 1 t t+1 A. Modeling of Fisheye Geometry
coordinatesarevalidwhenconstructingIˆt−1→t fromIt−1 andIˆt+1→t
fromIt+1 respectively.Thesecondrowindicatesthemaskingofstatic 1) Projection from camera coordinates to image coordi-
pixelscomputedafter2epochs,whereblackpixelsareﬁlteredfromthe (cid:55)→
photometricloss(i.e.ω=0).Itpreventsdynamicobjectsatsimilarspeed nates: The projection function Xc Π(Xc) = p of a 3D
astheegocarandlowtextureregionsfromcontaminatingtheloss.The point X = (x ,y ,z )T in camera coordinates to a pixel
c c c c
masksarecomputedforforwardandbackwardsequencesfromtheinput p = (u,v)T in the image coordinates is obtained via a 4th
sequenceS andreconstructedimagesusingEq.10asdescribedinSection
order polynomial in the following way:
II-D.Thethirdrowrepresentsthedistanceestimatescorrespondingto
theirinputframes.Finally,thevehicle’sodometrydataisusedtoresolve
thescalefactorissue. ϕ=arctan2(y ,x ) (1)
c c
π −
θ =(cid:18)2 (cid:19)arct(cid:18)an2(zc,rc) (cid:19) (2)
• We depict the importance of using backward sequences · · · ·
(θ)=k θ+k θ2+k θ3+k θ4 (3)
for training and construct a loss for these sequences. 1 2 · 3 · 4
Moreover, a combination of ﬁltering static pixels and p=(cid:112)u = (θ)·cosϕ·ax+cx (4)
an ego mask is employed. The incorporated bundle- v (θ) sinϕ ay+cy
adjustment framework [27] jointly optimizes distances
and camera poses within a sequence by increasing where rc = x2c +yc2, θ is the angle of incidence,
the baseline and providing additional consistency con- (θ) is the mapping of incident angle to image radius,
straints. (ax,ay)istheaspectratioand(cx,cy)istheprincipalpoint.
II. SELF-SUPERVISEDSCALE-AWARE 2) Unprojection from image coordinates to camera coor-
FISHEYEDISTANCENET dinates: The unprojection function (p,Dˆ) (cid:55)→ Π−1(p,Dˆ) =
(cid:0)
Zhou et al.’s [15] self-supervised monocular structure- X of an image pixel p=(u,v)T and it’s distance estimate
c
from-motion (SfM) framework aims at learning: Dˆ to th(cid:1)e 3D point X = (x ,y ,z )T is obtained via the
1) a monocular depth model gd : It → D predicting a following steps. Lettincg (xi,cyi)cT c= (u − cx)(cid:112)/ax,(v −
scale-ambiguous depth Dˆ = gd(It(p)) per pixel p in cy)/ay T,weobtaintheangleofincidenceθbynumerically
the target image It; and → calculating the 4th order polynomial roots of = x2i +yi2
2) an ego-motion predictor gx : (It,It(cid:48)) It→t(cid:48) pre- usingthedistortioncoefﬁcientsk1,k2,k3,k4 (seeEq.3).For
dicting a set of six degrees of freedom rigid transfor- training efﬁciency, we pre-calculate the roots and store them
∈
mations Tt→t(cid:48) SE(3), between the target image(cid:48) I∈t inalookuptableforallthepixelcoordinates.Now,θ isused
and the set of reference images I (cid:48). Typically, t to get
{ − } t
ast+ref1e,rtence1im, ia.eg.esth,ealftrhaomuegshIuts−i1nganadlaIrtg+e1rawreinudsoewd rc =Dˆ ·sinθ and(cid:112) zc =Dˆ ·cosθ (5)
is possible. where the distance estimate Dˆ from the network represents
(cid:107) (cid:107)
A limitation of this approach is that both depth and pose are theEuclideandistance X = x2+y2+z2 ofa3Dpoint
c c c c
estimated up to an unknown scale factor in the monocular X . The polar angle ϕ and the x , y components can be
c c c
SfM pipeline. obtained as follows:
The depth which acts as an intermediary variable is
· ·
obtained from the network by constraining the model to ϕ=arctan2(y ,x ), x =r cosϕ, y =r sinϕ.
i i c c c c
575
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:56:08 UTC from IEEE Xplore.  Restrictions apply. B. Photometric Loss chosentoconstrainD between0.1and100units[14].Fora
Letusconsidertheimagereconstructionerrorfromapair spherical image, we can only obtain angular disparities [31]
of images I (cid:48) and I , distance estimate Dˆ at time t, and by rectiﬁcation. To perform distance estimation on raw
t t t
the relative pose for I , with respect to the source image ﬁsheye images, we would require metric distance values to
t
It(cid:48)’s pose, as Tt→t(cid:48). Using the distance estimate Dˆt of the warp the source image It(cid:48) onto the target frame It. Due
network a point cloud P is obtained via: to the limitations of the monocular SfM objective, both the
t
Pt =Π−1(pt,Dˆt) (6) mscoanleo-caumlabrigdueoputshvgadlueasndwehgicoh-mwotoiounldpmreadkiectoirt igmxpopsresidbilcet
−
whereΠ 1representstheunprojectionfromimagetocamera to estimate distance maps on ﬁsheye images. To achieve
coordinatesasexplainedinSectionII-A.2,p thepixelsetof scale-awaredistancevalues,wenormalizetheposenetwork’s
t
image It. The pose estimate Tt→t(cid:48) from the pose network is estimate Tt→t(cid:48) and scale it with ∆x, the displacement
used to get an estimate Pˆt(cid:48) =Tt→t(cid:48)Pt for the point cloud of magnitude relative to target frame It which is calculated(cid:48)
theim(cid:48)ageIt(cid:48).Pˆt(cid:48) isthenprojectedontotheﬁsheyecameraat usingvehicle’sinstantaneousvelocityestimatesvt(cid:48) attimet
timet usingtheprojectionmodelΠdescribedinSectionII- andvt attimet.WealsoapplythistechniqueonKITTI[23]
A.1. Combining transformation and projection with Eq. 6 to obtain metric depth maps.
establishes a mapping from image coordinates p =(u,v)T
t (cid:48) T → (cid:48) ·
amtatpimpiengt taolliomw(cid:0)asgefocroothredinreacteosnspˆttr(cid:48)u(cid:1)=cti(ouˆn,vˆIˆ)(cid:48)T→at(cid:68)otifmteh(cid:69)et.taTrgheist Tt→t(cid:48) = (cid:107)Ttt→tt(cid:48)(cid:107) ∆x (9)
t t
frame I by backward warping the source frame I (cid:48). D. Masking Static Pixels and Ego Mask
t t
−
pˆ(cid:48) =Π T → (cid:48)Π 1(p ,Dˆ ) , Iˆu(cid:48)v→ = Iuˆ(cid:48)vˆ (7) Following [14], we incorporate a masking approach to
t t t t t t t t
ﬁlter out static pixels which do not change their appearance
Since the warped coordinates uˆ,vˆ are continuous, we apply from one frame to the other in the training sequence. The
the differentiable spatial transformer network introduced
(cid:10) (cid:11) approach would ﬁlter out objects which move at the same
by[28]tocomputeIˆt(cid:48)→tbyperformingbilinearinterpolation speed as the ego-car, and also ignore the static frame when
ofthefourpixelsfromIt(cid:48) whichlieclosetopˆt(cid:48).Thesymbol the ego-car stops moving. Similar to other approaches [14],
... denotes the corresponding sampling operator. [15], [32], [33] the per-pixel mask ω is applied to the loss
Following [16], [29] the image reconstruction error be-
by weighting the pixels selectively. Instead of being learned
tween the target image It and the reconstructed target im- from the object motion [34], the mask is computed in the
age Iˆt(cid:48)→t is calculated using the L1 pixel-wise loss term forward pass of the network, yielding a binary mask output
combined with Structural Similarity (SSIM) [30], as our ∈ { }
L where ω 0,1 . Wherever the photometric error of the
photometric loss given by Eq. 8 below.
p warped image Iˆ(cid:48)→ is not lower than that of the original
− (cid:13) M (cid:13) t t
L˜p(It,Iˆt(cid:48)→t)=α 1 SSIM(cid:13)(cid:13)(It,2Iˆt(cid:48)→t, t→t(cid:48)) (cid:13)(cid:13) utanrwgeatrpfreadmseoIu(cid:2)tr,cωe ifsrasmetetoIti(cid:48)gninoreeatchhelcoassseocfosmucpharp(cid:3)eidxetlos,it.hee.
− − (cid:12)M
L +(1 α) L(It Iˆt(cid:48)→t) t→t(cid:48) l1 ω = mti(cid:48)npe(It,Iˆt(cid:48)→t)<mti(cid:48)npe(It,It(cid:48)) (10)
p = (cid:48)∈{min− } ˜p(It,Iˆt(cid:48)→t) (8) where[]istheIversonbracket.Additionally,weaddabinary
t t+1,t 1 M
M ego mask → (cid:48) proposed in [35] that ignores computing
where α = 0.85, → (cid:48) is the binary mask as discussed t t
t t (cid:12) the photometric loss on the pixels that do not have a valid
in Section II-D and the symbol denotes element-wise
mapping i.e. some pixel coordinates of the target image I
multiplication. Following [14] instead of averaging the pho- t
may not be projected onto the source image I (cid:48) given the
tometric error over all source images, we adopt per-pixel t
estimated distance Dˆ .
minimum. This signiﬁcantly sharpens the occlusion bound- t
aries and reduces the artifacts resulting in higher accuracy.
E. Backward Sequence
The self-supervised framework assumes a static scene,
In the forward sequence, we synthesize the target frame
no occlusion and change of appearance (e.g. brightness
constancy). A large photometric cost is incurred, potentially It with the s(cid:48)ou∈rce{frames I−t−1 }and It+1 (i.e. as per above
worsening the performance, if there exist dynamic objects discussion t t + 1,t 1 ). Analogously, backward
and occluded regions. These areas are treated as outliers sequence is carried out by using It−1 and It+1 as target
similar to [27] and clip the photometric loss values to a 95th frames and It as source frame. We include warps Iˆt→t−1
percentile. Zero gradient is obtained for errors larger than and Iˆt→t+1, thereby inducing more constraints to avoid
overﬁttingandresolveunknowndistancesintheborderareas
95%. This improves the optimization process and provides a
at the test time, as also observed in previous works [14],
way to strengthen the photometric error.
[15], [36]. We construct the loss for the additional backward
C. Solving Scale Factor Ambiguity at Training Time sequence in a similar manner to the forward. This comes at
∝
For a pinhole projection model, depth 1/disparity. thecostofhighcomputationaleffortandlongertrainingtime
Henceforth, the network’s sigmoided output σ can be con- asweperformtwoforwardandbackwardwarpswhichyields
verted to depth with D = 1/(aσ+b), where a and b are superior results on the Fisheye and KITTI dataset compared
576
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:56:08 UTC from IEEE Xplore.  Restrictions apply. to the previous approaches [14], [15] which train only with H. Final Training Loss
one forward sequence and one backward sequence.
The overall self-supervised structure-from-motion (SfM)
L
from motion objective consists of a photometric loss
F. Edge-Aware Smoothness Loss p
imposed between the reconstructed target image Iˆ(cid:48)→ and
t t
In order to regularize distance and avoid divergent values the target image I , included once for the forward and once
t
inoccludedortexture-lesslow-imagegradientareas,weadd for the backward sequence, and a distance regularization
L
a geometric smoothing loss. We adopt the edge-aware term term ensuring edge-aware smoothing in the distance
s L
similarto[16],[35],[37].Theregularizationtermisimposed estimates.Finally, across-sequencedistanceconsistency
dc
ontheinversedistancemap.Unlikepreviousworks,theloss lossderivedfromthechainofframesinthetrainingsequence
isnotdecayedforeachpyramidlevelbyafactorof2dueto S is also included. To prevent the training objective getting
down-sampling, as we use a super resolution network (see stuck in the local minima due to the gradient locality of the
Section III-A) bilinear sampler [28], we adopt 4 scales to train the network
as followed in [15], [16]. The ﬁnal objective function is
L (Dˆ )=|∂ Dˆ∗|e−|∂uIt|+|∂ Dˆ∗|e−|∂vIt| (11) averaged over per(cid:88)-pixel, scale and image batch.
s t u t v t
L
To discourage shrinking of estimated distance [17], mean- L 4
no−rmalized inverse distance of It is conside−red, i.e. Dˆt∗ = = 2n−n1, (13)
Dˆ 1/D , where D denotes the mean of Dˆ 1 :=1/Dˆ . L n=L1 L L L
t t t t t =n f +n b +γ n +β n
n p p dc s
G. Cross-Sequence Distance Consistency Loss
{ The S·f·M· set}ting uses an N-frame training snippet S = III. NETWORKDETAILS
I ,I , ,I from a video as input. The FisheyeDis-
1 2 N A. Deformable Super-Resolution Distance and PoseNet
tanceNet can estimate the distance of each image in the
trainingsequence.Anotherconstraintcanbeenforcedamong The distance estimation network is mainly based on the
the frames in S, since the distances of a 3D point estimated U-net architecture [42], an encoder-decoder network with
from different frames should be consistent. skip connections. After testing different variants of ResNet
Let us assume Dˆ (cid:48) and Dˆ are the estimates of the family, such as ResNet50 with 25M parameters, we chose
t t ∈
images I (cid:48) and I respectively. For each pixel p I , a ResNet18 [43] as the encoder. The key aspect here is
t t t t
we can use Eq. 7 to obtain pˆ(cid:48). Since it’s coordinates are replacingnormalconvolutionswithdeformableconvolutions
t
real valued, we apply the differentiable spatial transformer sinceregularCNNsareinherentlylimitedinmodelinglarge,
network introduced by [28] and estimate the distance value unknown geometric distortions due to their ﬁxed structures,
ofpˆ(cid:48) byperformingbilinearinterpolationofthefourpixel’s such as ﬁxed ﬁlter kernels, ﬁxed receptive ﬁeld sizes, and
t
values in Dˆ (cid:48) which lie close to pˆ(cid:48). Let us denote the ﬁxed pooling kernels [44], [24].
t t
distance map obtained through this as Dˆ → (cid:48)(p ). Next, we In previous works [14], [15], [16], [17], [36], the decoded
t t t
can transform the point cloud in frame I to frame I (cid:48) by features were upsampled via a nearest neighbor interpola-
t t
ﬁrstobtainingP usingEq. 6.Wetransformthepointcloud tion or with learnable transposed convolutions. The main
t
P using the pose network’s estimate via Pˆ(cid:48) = T → (cid:48)P . drawback of this process is that it may lead to large errors
t (cid:107) (cid:107) t t t t
Now, D → (cid:48)(p ) := Pˆ(cid:48) denotes the distance generated at object boundaries in the upsampled distance map as the
t t t t
from point cloud Pˆ(cid:48). Ideally, D → (cid:48)(p ) and Dˆ → (cid:48)(p ) interpolationsimplycombinesdistancevaluesofbackground
t t t t t t t
should be equal. Therefore, we can deﬁne the following andforeground.Foreffectiveanddetailedpreservationofthe
cross-sequence distance consistency loss (CSDCL) for the decoded features, we leverage the concept of sub-pixel con-
training(cid:88)sequ(cid:88)ence(cid:18)S:(cid:88) (cid:12)(cid:12) (cid:12)(cid:12) volutions [25] to our super resolution network. We use pixel
(cid:12) (cid:12) shufﬂe convolutions and replace the convolutional feature
−
Ldc =N 1 N (cid:88)Mt→t(cid:48)(cid:12)(cid:12)Dt→t(cid:48)(pt)−Dˆt→t(cid:48)(pt) (cid:12)(cid:12)(cid:19) uoprsawmitphlinlega,rpnearbfloermtreadnsvpioaseadnecaornevsotlnuetiiognhsb.orTihneterrpeosulalttiionng
(cid:48) (cid:12) (cid:12)
t=1 t=t+1 pt distancemapsaresuper-resolved,havesharpboundariesand
+ M (cid:48)→ D (cid:48)→ (p (cid:48))−Dˆ (cid:48)→ (p (cid:48)) expose more details of the scene.
t t t t t t t t
The backbone of our pose estimation network is based
p(cid:48)
t (12) on[14]andpredictsrotationusingEulerangleparameteriza-
tion.TheoutputisasetofsixDOFtransformationsbetween
Eq. 12 contains one term for which pixe(cid:48)ls and point clouds It−1 andIt aswellasIt andIt+1.Wehavereplacednormal
are warped forwards in time (from t to t) and one term for convolutions with deformable convolutions for the encoder-
(cid:48)
which they are warped backwards in time (from t to t). decoder setting.
In prior works [32], [37], the consistency error is limited
B. Implementation Details
toonlytwoframes,whereasweapplyittotheentiretraining
sequence S. This induces more constraints and enlarges the We use Pytorch [45] and employ Adam [46] optimizer to
baseline, inherently improving the distance estimation [27]. minimize the training objective function (13) with β =0.9,
1
577
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:56:08 UTC from IEEE Xplore.  Restrictions apply. Abs Rel Sq Rel RMSE RMSE (log) δ <1.25 δ <1.252 δ <1.253
Approach lower is better higher is better
KITTI
Zhou [15]† 0.183 1.595 6.709 0.270 0.734 0.902 0.959
Yang [38] 0.182 1.481 6.501 0.267 0.725 0.906 0.963
Vid2depth [35] 0.163 1.240 6.220 0.250 0.762 0.916 0.968
GeoNet [36]† 0.149 1.060 5.567 0.226 0.796 0.935 0.975
DDVO [17] 0.151 1.257 5.583 0.228 0.810 0.936 0.974
DF-Net [37] 0.150 1.124 5.507 0.223 0.806 0.933 0.973
Ranjan [39] 0.148 1.149 5.464 0.226 0.815 0.935 0.973
EPC++ [33] 0.141 1.029 5.350 0.216 0.816 0.941 0.976
Struct2depth ‘(M)’ [34] 0.141 1.026 5.291 0.215 0.816 0.945 0.979
Zhou [27] 0.139 1.057 5.213 0.214 0.831 0.940 0.975
PackNet-SfM [40] 0.120 0.892 4.898 0.196 0.864 0.954 0.980
Monodepth2 [14] 0.115 0.903 4.863 0.193 0.877 0.959 0.981
FisheyeDistanceNet 0.117 0.867 4.739 0.190 0.869 0.960 0.982
×
FisheyeDistanceNet (1024 320) 0.109 0.788 4.669 0.185 0.889 0.964 0.982
WoodScape
FisheyeDistanceNet cap 80m 0.167 1.108 3.814 0.216 0.794 0.953 0.972
FisheyeDistanceNet cap 40m 0.152 0.768 2.723 0.210 0.812 0.954 0.974
FisheyeDistanceNet cap 30m 0.149 0.613 2.402 0.204 0.810 0.957 0.976
TABLEIQuantitativeresultsofleaderboardalgorithmsonKITTIdataset[23]andFisheyeDistanceNetonFisheyedatasetpartofWoodScape[1].
Single-viewdepthestimationresultsusingtheEigenSplit[41]fordepthsreportedlessthan80m,asindicatedin[41]forpinholemodel.Allthe
approachesareself-supervisedonmonocularvideosequences.Attest-time,allmonocularmethodsexcludingourFisheyeDistanceNet,scaletheestimated
depthsusingmedianground-truthLiDARdepth.Fortheﬁsheyedataset,weestimatedistanceratherthandepth.†marksnewerresultsreportedonGitHub.
β =0.999. We train the model for 25 epochs, with a batch distance maps using sub-pixel convolution [25], we initial-
2
size of 20 on 24GB Titan RTX with initial learning rate of ized the last convolutional layer in a speciﬁc way before the
− −
10 4 fortheﬁrst20epochs,thendropto10 5 forthelast5 pixel shufﬂe operation as described in [50].
epochs. The sigmoided output σ from the distance decoder
·
is converted to distance with D =a σ+b. For the pinhole IV. EXPERIMENTS
·
model, depth D = 1/(a σ+b), where a and b are chosen A. Datasets
toconstrainD between0.1and100units.Theoriginalinput
× 1) WoodScape – Fisheye Dataset: The dataset contains
resolutionoftheﬁsheyeimageis1280 800pixels,wecrop
× roughly 40,000 raw images obtained with a ﬁsheye camera
itto1024 512toremovethevehicle’sbumper,shadowand
and point clouds from a sparse Velodyne HDL-64E rotating
other artifacts of the vehicle. Finally the cropped image is
× 3D laser scanner as ground truth for the test set. The
downscaled to 512 256 before feeding to the network. For
× training set contains 39,038 images collected by driving
pinhole model on KITTI, we use 640 192 pixels as the
around various parts of Bavaria, Germany. The validation
network input.
and the test split contain 1,214 and 697 images respectively.
Weexperimentedwithbatchnormalization[47]andgroup
The dataset distribution is similar to the KITTI Eigen split
normalization[48]layersintheencoder-decodersetting.We
used in [14], [15] for the pinhole model. The training set
have found that group normalization with G = 32 signiﬁ- comprises three scene categories: city, residential and sub-
cantlyimprovestheresults[49].Thesmoothnessweightterm
urban.Whiletraining,thesecategoriesarerandomlyshufﬂed
β and cross-sequence distance consistency weight term γ and fed to the network. We ﬁlter static scenes based on the
have been set to 0.001. We applied deformable convolutions speed of the vehicle with a threshold of 2km/h to remove
tothe3x3convlayersinstagesconv3,conv4,andconv5in
imageframesthatonlyobserveminimalcameraego-motion,
ResNet18 and ResNet50, with 12 layers of deformable con-
since distance cannot be learned under these circumstances.
volutionintheencoderpartcomparedto3layersin[44],all
Comparable to previous experiments on pinhole SfM [15],
intheconv5stageforResNet50.Wereplacedthesubsequent
[14], we set the length of the training sequence to 3.
layers of the decoder with deformable convolutions for the
2) KITTI – Eigen Split: We use the KITTI dataset and
distanceandposenetwork.Forthepinholemodel,onKITTI
data split according to Eigen et al. [51] for the experiments
Eigen split in Section IV-A.2 we used normal convolutions
with pinhole image data. We ﬁlter static frames as proposed
instead of deformable convolutions.
byZhouetal.[15].Theresultingtrainingsetcontains39,810
Finally,toalleviatecheckerboardartifactsfromtheoutput images and the validation split comprises 4,424 images. We
578
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:56:08 UTC from IEEE Xplore.  Restrictions apply. ut
p
n
I
w
a
R
ut
p
n
I
d
e
p
p
o
r
C
Fig.3QualitativeresultsontheFisheyeWoodScapedataset.OurFisheyeDistanceNetproducessharpdistancemapsonrawﬁsheyeimages.
Method FS BS SR CSDCL DCN Abs Rel Sq Rel RMSE RMSE log δ<1.25 δ<1.252 δ<1.253
(cid:88) (cid:88) (cid:88) (cid:88) (cid:88)
Ours 0.152 0.768 2.723 0.210 0.812 0.954 0.974
(cid:88) (cid:88) (cid:88) (cid:88)
Ours 0.172 0.829 2.925 0.243 0.802 0.952 0.970
(cid:88) (cid:88) (cid:88)
Ours 0.181 0.913 3.180 0.250 0.823 0.938 0.963
(cid:88) (cid:88)
Ours 0.190 0.997 3.266 0.258 0.796 0.930 0.963
(cid:88)
Ours 0.201 1.282 3.589 0.276 0.590 0.898 0.949
TABLEIIAblationstudyondifferentvariantsofourFisheyeDistanceNetusingtheFisheyeWoodScapedataset[1].Distancesarecappedat40m.BS,
SR,CSDCLandDCNrepresentbackwardsequence,super-resolutionnetworkwithPixelShufﬂeorsub-pixelconvolutioninitializedtoconvolutionNN
×
resize(ICNR)[50],cross-sequencedistanceconsistencylossanddeformableconvolutionsrespectively.Theinputresolutionis512 256pixels.
use the standard test set of 697 images. The length of the is only trained for the forward sequence which consists
training sequence is set to 3. of two warps as explained in Section II-E; (ii) Addition-
ally remove Super Resolution using sub-pixel convolution:
B. Evaluation
Removal of sub-pixel convolution has a huge impact on
WeevaluateFisheyeDistanceNet’sdepthanddistanceesti-
Woodscape compared to KITTI. This is mainly attributed
mationresultsusingthemetricsproposedbyEigenetal.[41]
totheﬁsheyemodel,asfar-awayobjectsaretinyandcannot
to facilitate comparison. The quantitative results shown in
be resolved accurately with naive nearest neighbor inter-
the Table I illustrate that our scale-aware self-supervised
polation or transposed convolution [26]; (iii) Additionally
approach outperforms all the state-of-the-art monocular ap-
remove cross-sequence distance consistency loss: Removing
proaches. We could not leverage the Cityscapes dataset into
theCSDCLmainlydiminishesthebaseline;(iv)Additionally
our training regime to benchmark our scale-aware frame-
remove deformable convolutions: If we remove all the major
work, due to absence of odometry data.
components, especially deformable convolution layers [24],
Since the projection operators are different, previous SfM
our model will fail miserably as the distortion introduced
approaches will not be feasible on Fisheye Woodscape
by ﬁsheye model will not be learned correctly by normal
dataset without adaption of the network and projection
convolutional layers.
model. It is important to note that due to the geometry of
the ﬁsheye, it would not be a fair comparison to evaluate V. CONCLUSION
We propose a novel self-supervised training strategy to
the distance estimates up to 80m. Our ﬁsheye automotive
obtain metric distance maps on unrectiﬁed ﬁsheye im-
camerasalsoundergohighdatacompressionandourdataset
ages. Through extensive experiments, we show that our
contains images of inferior quality when compared with
FisheyeDistanceNetestablishesanewstateoftheartinself-
KITTI. Our ﬁsheye cameras can perform well up to a range
supervisedmonoculardistanceanddepthestimationonFish-
of 40m. Therefore, we also report results on a 30m and a
eye WoodScape and KITTI dataset respectively. We obtain
40m. range (see Table I).
promisingresultsdemonstratingthepotentialofusingaCNN
C. Fisheye Ablation Study
based approach for deployment in commercial automotive
Weconductanablationstudytoevaluatetheimportanceof systems, in particular for replacing current classical depth
different components. We ablate the following components estimation approaches. To encourage further research on
and report their impact on the distance evaluation metrics ﬁsheye distance estimation, we will release the dataset as
in Table II: (i) Remove Backward Sequence: The network a part of WoodScape [1] project.
579
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:56:08 UTC from IEEE Xplore.  Restrictions apply. REFERENCES [21] S. Li and K. Fukumori, “Spherical stereo for the construction of
immersive vr environment,” in IEEE Proceedings. VR 2005. Virtual
[1] S. Yogamani, C. Hughes, J. Horgan, G. Sistu, P. Varley, D. O’Dea, Reality,2005. IEEE,2005,pp.217–222.
M. Urica´r, S. Milz, M. Simon, K. Amende, et al., “Woodscape:
[22] J. Huang, Z. Chen, D. Ceylan, and H. Jin, “6-dof vr videos with a
A multi-task, multi-camera ﬁsheye dataset for autonomous driving,”
single360-camera,”in2017IEEEVirtualReality(VR). IEEE,2017,
in Proceedings of the IEEE International Conference on Computer
pp.37–44.
Vision,2019,pp.9308–9318.
[23] A.Geiger,P.Lenz,C.Stiller,andR.Urtasun,“Visionmeetsrobotics:
[2] M.Heimberger,J.Horgan,C.Hughes,J.McDonald,andS.Yogamani,
The kitti dataset,” The International Journal of Robotics Research,
“Computer vision in automated parking systems: Design, implemen-
vol.32,no.11,pp.1231–1237,2013.
tationandchallenges,”ImageandVisionComputing,vol.68,pp.88–
[24] X. Zhu, H. Hu, S. Lin, and J. Dai, “Deformable convnets v2: More
101,2017.
deformable,betterresults,”inProceedingsoftheIEEEConferenceon
[3] A.Dahal,J.Hossen,C.Sumanth,G.Sistu,K.Malhan,M.Amasha,
ComputerVisionandPatternRecognition,2019,pp.9308–9316.
and S. Yogamani, “Deeptrailerassist: Deep learning based trailer de-
[25] W. Shi, J. Caballero, F. Husza´r, J. Totz, A. P. Aitken, R. Bishop,
tection,trackingandarticulationangleestimationonautomotiverear-
D.Rueckert,andZ.Wang,“Real-timesingleimageandvideosuper-
view camera,” in Proceedings of the IEEE International Conference
resolutionusinganefﬁcientsub-pixelconvolutionalneuralnetwork,”
onComputerVisionWorkshops,2019,pp.0–0.
inProceedingsoftheIEEEconferenceoncomputervisionandpattern
[4] J.Horgan,C.Hughes,J.McDonald,andS.Yogamani,“Vision-based
recognition,2016,pp.1874–1883.
driver assistance systems: Survey, taxonomy and advances,” in 2015
IEEE 18th International Conference on Intelligent Transportation [26] A. Odena, V. Dumoulin, and C. Olah, “Deconvolution and checker-
Systems. IEEE,2015,pp.2032–2039. boardartifacts,”Distill,vol.1,no.10,p.e3,2016.
[5] M.Drulea,I.Szakats,A.Vatavu,andS.Nedevschi,“Omnidirectional [27] L. Zhou, J. Ye, M. Abello, S. Wang, and M. Kaess, “Unsupervised
stereo vision using ﬁsheye lenses,” in 2014 IEEE 10th International learningofmonoculardepthestimationwithbundleadjustment,super-
Conference on Intelligent Computer Communication and Processing resolutionandcliploss,”arXivpreprintarXiv:1812.03368,2018.
(ICCP). IEEE,2014,pp.251–258. [28] M.Jaderberg,K.Simonyan,A.Zisserman,etal.,“Spatialtransformer
[6] D. Caruso, J. Engel, and D. Cremers, “Large-scale direct slam for networks,” in Advances in neural information processing systems,
omnidirectionalcameras,”in2015IEEE/RSJInternationalConference 2015,pp.2017–2025.
onIntelligentRobotsandSystems(IROS). IEEE,2015,pp.141–148. [29] H.Zhao,O.Gallo,I.Frosio,andJ.Kautz,“Lossfunctionsforimage
[7] G.Sistu,I.Leang,andS.Yogamani,“Real-timejointobjectdetection restoration with neural networks,” IEEE Transactions on Computa-
and semantic segmentation network for automated driving,” arXiv tionalImaging,vol.3,no.1,pp.47–57,2016.
preprintarXiv:1901.03912,2019. [30] Z.Wang,A.C.Bovik,H.R.Sheikh,E.P.Simoncelli,etal.,“Image
[8] M.Uˇricˇa´ˇr,P.Kˇr´ızˇek,G.Sistu,andS.Yogamani,“Soilingnet:Soiling qualityassessment:fromerrorvisibilitytostructuralsimilarity,”IEEE
detectiononautomotivesurround-viewcameras,”in2019IEEEIntel- transactionsonimageprocessing,vol.13,no.4,pp.600–612,2004.
ligent Transportation Systems Conference (ITSC). IEEE, 2019, pp. [31] Z.ArıcanandP.Frossard,“Densedepthestimationfromomnidirec-
67–72. tionalimages,”2009.
[9] M.Yahiaoui,H.Rashed,L.Mariotti,G.Sistu,I.Clancy,L.Yahiaoui, [32] S. Vijayanarasimhan, S. Ricco, C. Schmid, R. Sukthankar, and
V. R. Kumar, and S. Yogamani, “Fisheyemodnet: Moving object K. Fragkiadaki, “Sfm-net: Learning of structure and motion from
detection on surround-view cameras for autonomous driving,” arXiv video,”arXivpreprintarXiv:1704.07804,2017.
preprintarXiv:1908.11789,2019. [33] C.Luo,Z.Yang,P.Wang,Y.Wang,W.Xu,R.Nevatia,andA.Yuille,
[10] M. Urica´r, J. Ulicny, G. Sistu, H. Rashed, P. Krizek, D. Hurych, “Every pixel counts++: Joint learning of geometry and motion with
A. Vobecky, and S. Yogamani, “Desoiling dataset: Restoring soiled 3dholisticunderstanding,”arXivpreprintarXiv:1810.06125,2018.
areas on automotive ﬁsheye cameras,” in Proceedings of the IEEE [34] V.Casser,S.Pirk,R.Mahjourian,andA.Angelova,“Depthprediction
International Conference on Computer Vision Workshops, 2019, pp. without the sensors: Leveraging structure for unsupervised learning
0–0. from monocular videos,” in Proceedings of the AAAI Conference on
[11] N. Tripathi, G. Sistu, and S. Yogamani, “Trained trajectory based ArtiﬁcialIntelligence,vol.33,2019,pp.8001–8008.
automated parking system using visual slam,” arXiv preprint
[35] R.Mahjourian,M.Wicke,andA.Angelova,“Unsupervisedlearning
arXiv:2001.02161,2020.
of depth and ego-motion from monocular video using 3d geometric
[12] V. R. Kumar, S. Milz, C. Witt, M. Simon, K. Amende, J. Petzold,
constraints,” in Proceedings of the IEEE Conference on Computer
S. Yogamani, and T. Pech, “Monocular ﬁsheye camera depth esti-
VisionandPatternRecognition,2018,pp.5667–5675.
mation using sparse lidar supervision,” in 2018 21st International
[36] Z. Yin and J. Shi, “Geonet: Unsupervised learning of dense depth,
Conference on Intelligent Transportation Systems (ITSC). IEEE,
opticalﬂowandcamerapose,”inProceedingsoftheIEEEConference
2018,pp.2853–2858.
onComputerVisionandPatternRecognition,2018,pp.1983–1992.
[13] N. Zioulis, A. Karakottas, D. Zarpalas, and P. Daras, “Omnidepth:
[37] Y.Zou,Z.Luo,andJ.-B.Huang,“Df-net:Unsupervisedjointlearning
Densedepthestimationforindoorssphericalpanoramas,”inProceed-
ofdepthandﬂowusingcross-taskconsistency,”inProceedingsofthe
ingsoftheEuropeanConferenceonComputerVision(ECCV),2018,
EuropeanConferenceonComputerVision(ECCV),2018,pp.36–53.
pp.448–465.
[38] Z. Yang, P. Wang, W. Xu, L. Zhao, and R. Nevatia, “Unsupervised
[14] C. Godard, O. Mac Aodha, M. Firman, and G. Brostow, “Dig-
learningofgeometryfromvideoswithedge-awaredepth-normalcon-
gingintoself-supervisedmonoculardepthestimation,”arXivpreprint
sistency,”inThirty-SecondAAAIConferenceonArtiﬁcialIntelligence,
arXiv:1806.01260,2018.
2018.
[15] T. Zhou, M. Brown, N. Snavely, and D. G. Lowe, “Unsupervised
learningofdepthandego-motionfromvideo,”inProceedingsofthe [39] A. Ranjan, V. Jampani, L. Balles, K. Kim, D. Sun, J. Wulff, and
IEEEConferenceonComputerVisionandPatternRecognition,2017, M. J. Black, “Competitive collaboration: Joint unsupervised learning
pp.1851–1858. of depth, camera motion, optical ﬂow and motion segmentation,” in
[16] C.Godard,O.MacAodha,andG.J.Brostow,“Unsupervisedmonoc- ProceedingsoftheIEEEConferenceonComputerVisionandPattern
ulardepthestimationwithleft-rightconsistency,”inCVPR,2017. Recognition,2019,pp.12240–12249.
[17] C. Wang, J. Miguel Buenaposada, R. Zhu, and S. Lucey, “Learning [40] V. Guizilini, R. Ambrus, S. Pillai, and A. Gaidon, “Packnet-sfm:
depth from monocular videos using direct methods,” in The IEEE 3d packing for self-supervised monocular depth estimation,” arXiv
ConferenceonComputerVisionandPatternRecognition(CVPR),June preprintarXiv:1905.02693,2019.
2018. [41] D. Eigen, C. Puhrsch, and R. Fergus, “Depth map prediction from
[18] S. Li, “Binocular spherical stereo,” IEEE Transactions on intelligent a single image using a multi-scale deep network,” CoRR, vol.
transportationsystems,vol.9,no.4,pp.589–600,2008. abs/1406.2283, 2014. [Online]. Available: http://arxiv.org/abs/1406.
[19] C.Ma,L.Shi,H.Huang,andM.Yan,“3dreconstructionfromfull- 2283
viewﬁsheyecamera,”arXivpreprintarXiv:1506.06273,2015. [42] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional
[20] S.Pathak,A.Moro,A.Yamashita,andH.Asama,“Dense3drecon- networksforbiomedicalimagesegmentation,”inInternationalConfer-
struction from two spherical images via optical ﬂow-based equirect- enceonMedicalimagecomputingandcomputer-assistedintervention.
angularepipolarrectiﬁcation,”in2016IEEEInternationalConference Springer,2015,pp.234–241.
onImagingSystemsandTechniques(IST). IEEE,2016,pp.140–145. [43] K.He,X.Zhang,S.Ren,andJ.Sun,“Deepresiduallearningforimage
580
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:56:08 UTC from IEEE Xplore.  Restrictions apply. recognition,” in Proceedings of the IEEE conference on computer
visionandpatternrecognition,2016,pp.770–778.
[44] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei,
“Deformable convolutional networks,” in Proceedings of the IEEE
internationalconferenceoncomputervision,2017,pp.764–773.
[45] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito,
Z.Lin,A.Desmaison,L.Antiga,andA.Lerer,“Automaticdifferen-
tiationinPyTorch,”inNIPSAutodiffWorkshop,2017.
[46] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimiza-
tion,”arXivpreprintarXiv:1412.6980,2014.
[47] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep
networktrainingbyreducinginternalcovariateshift,”inProceedings
of the 32Nd International Conference on International Conference
onMachineLearning-Volume37,ser.ICML’15. JMLR.org,2015,
pp. 448–456. [Online]. Available: http://dl.acm.org/citation.cfm?id=
3045118.3045167
[48] Y. Wu and K. He, “Group normalization,” in Proceedings of the
EuropeanConferenceonComputerVision(ECCV),2018,pp.3–19.
[49] K.He,R.Girshick,andP.Dolla´r,“Rethinkingimagenetpre-training,”
arXivpreprintarXiv:1811.08883,2018.
[50] A. Aitken, C. Ledig, L. Theis, J. Caballero, Z. Wang, and W. Shi,
“Checkerboardartifactfreesub-pixelconvolution:Anoteonsub-pixel
convolution,resizeconvolutionandconvolutionresize,”arXivpreprint
arXiv:1707.02937,2017.
[51] D.EigenandR.Fergus,“Predictingdepth,surfacenormalsandseman-
tic labels with a common multi-scale convolutional architecture,” in
ProceedingsoftheIEEEinternationalconferenceoncomputervision,
2015,pp.2650–2658.
581
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:56:08 UTC from IEEE Xplore.  Restrictions apply. 