2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
Learning to Drive Off Road on Smooth Terrain in Unstructured
Environments Using an On-Board Camera and Sparse Aerial Images
Travis Manderson, Stefan Wapnick, David Meger, Gregory Dudek
Abstract(cid:151)We present a method for learning to drive on
Rough
smoothterrainwhilesimultaneouslyavoidingcollisionsinchal-
lenging off-road and unstructured outdoor environments using
onlyvisualinputs.Ourapproachappliesahybridmodel-based
and model-free reinforcement learning method that is entirely
self-supervisedinlabelingterrainroughnessandcollisionsusing
on-board sensors. Notably, we provide both (cid:2)rst-person and Smooth
overhead aerial image inputs to our model. We (cid:2)nd that
the fusion of these complementary inputs improves planning
foresight and makes the model robust to visual obstructions. Fig. 1: Example of smooth and rough terrain in an unstruc-
Ourresultsshowtheabilitytogeneralizetoenvironmentswith tured, outdoor environment.
plentiful vegetation, various types of rock, and sandy trails.
During evaluation, our policy attained 90% smooth terrain Eventual large scale and long duration deployment of
traversal and reduced the proportion of rough terrain driven robots in remote, natural, unstructured, and complex en-
over by 6.1 times compared to a model using only (cid:2)rst-
vironments will require these robots to be robust and to
person imagery. Video and project details can be found at
reason about unforeseen situations that they will inevitably
www.cim.mcgill.ca/mrl/offroad driving/.
encounter. There has been extensive work on navigating
I. INTRODUCTION outdoors [1], [2], [3]. These methods generally rely on
Inthispaper,wepresentasystemforlearninganavigation complex geometric and sensor modeling combined with
policy that preferentially chooses smooth terrain (see Fig. 1) terrainclassi(cid:2)cationtrainedfromlabeleddata,ortheyrequire
for off-road driving in natural, unstructured environments human demonstrations.
using an on-board camera and sparse aerial images. The Reinforcement learning methods - especially model-free
emphasisofthepaper,however,isnotroadclassi(cid:2)cationper methods that learn from vision - are rarely used in real-
se,butrathertoproposeanapproachforonlineadaptiveself- world scenarios due to high dimensionality and sample
supervised learning for off-road driving in rough terrain and inef(cid:2)ciency [4]. Model-based approaches are more sample
to explore the synthesis of aerial and (cid:2)rst-person (ground) ef(cid:2)cient but may perform poorly due to cascading model
sensing in this context. errors and the fact that the learned dynamics is only a proxy
Scaled robot vehicles, such as remote-controlled vehicles, to determining a policy to maximize the agent’s reward.
are capable of covering large distances at high speeds when InspiredbytherecentworkbyKahnetal.[5],wecombine
onpavedroad,butcomparedtotheirlargercounterparts,they aspectsofmodel-freeandmodel-basedmethodsintoasingle
are less capable of traversing rough terrain when navigating computationgraphtoleveragethestrengthsofbothmethods
off road. However, their small size also makes them a good while offsetting their individual weaknesses. Our model
choice for operating in remote natural environments due to learns to navigate collision-free trajectories while maxi-
their limited disturbance and impact on the environment. mizing smooth terrain in a purely self-supervised fashion
Modern Micro Air Vehicles (MAVs) have become ubiq- withlimitedtrainingdata.Duringtraining,terrainroughness
uitous due their ease of deployment, continually increasing is estimated using an on-board Inertial Measurement Unit
enduranceandrange,andhighqualitycamerasatarelatively (IMU) while obstacles are measured using a short-range
low cost. Although lightweight MAVs lack the payload to Lidar.Toachievegreatervisualcontext,wefuseinputimages
carry heavy sensors, they are able to provide a top-down from an on-board (cid:2)rst-person camera and local aerial view.
view of the surface and an unrestricted Field of View (FOV) We validate our system both in simulation and on a real off-
by either taking pictures at higher altitude or in a mosaic road vehicle in a remote environment. We demonstrate the
sequence. In this work, we use a single image taken at 80 m vehicle’s ability to navigate smooth, collision-free trajecto-
above the surface, combined with a Real-Time Kinematic ries and show that the addition of aerial imagery improves
(RTK) Global Positioning System (GPS) to enable our off- predictive performance, especially in the presence of sharp
road vehicle to better reason about its surroundings. This turning trajectories or visual obstructions.
technique enhances the ability of a vehicle to do long range
II. RELATEDWORK
planning beyond its own ground-based FOV.
A. Imitation Learning for Visual Navigation
MobileRoboticsLaboratory,SchoolofComputerScience,McGillUni-
Learning vision-based controllers for autonomous driving
versity,Montreal,Canada
f g
travism,swapnick,dmeger,dudek @cim.mcgill.ca hasbeenstudiedforseveraldecades.Pomerleauetal.[6],[7]
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 1263
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 21,2020 at 05:49:00 UTC from IEEE Xplore.  Restrictions apply. used behavioral cloning to train a feed-forward neural net- C. Air-Assisted Ground Vehicle Navigation
work to predict steering angles for on-road driving. Training
Several researchers have presented methods using aerial
imagesandLidardatawerecollectedinsimulation,whilethe
imagery to assist ground navigation. Sofman et al. [20]
system was demonstrated to work on real roads in limited
trained a classi(cid:2)er on aerial images of urban environments
conditions. More recently, Bojarski et al. [8] trained a Deep
discretized into grid cells. The classi(cid:2)cation label of each
Neural Network (DNN) to predict steering angles from a
cell was translated to a cost and used for D* planning [21]
single forward-facing camera using human demonstration.
of a ground vehicle. Similarly, Delmerico et al. [22] [23]
Additional training images were collected from left-shifted
proposed a lightweight, patch-based terrain classi(cid:2)cation
and right-shifted cameras to depict recovery cases when the
network requiring only minimal training time from aerial
vehicle drifts off course. The labels of these images were
images. The classi(cid:2)er output forms a cost map for D*
adjusted to turn right and left, respectively.
planning for the purpose of guiding a ground vehicle in
Bearing some resemblance to our own work, several
disaster recovery situations. Wang et al. [24] used a team of
researchers have explored autonomous navigation in forests
air and ground vehicles for collaborative exploration to map
and unstructured environments by following natural or
an area more quickly. Garzon et al. [25] combined sensor
human-madepaths.Rasmussenetal.[9]usedanappearance
data from air and ground vehicles to accurately perform
and structural-based model from stereo cameras and Lidar
obstacle pose estimation and used a potential (cid:2)elds-based
to predict the most likely path. Some authors have treated
approach for collision-free navigation.
path navigation as an image classi(cid:2)cation problem. Giusti et
al. [10] and Smolyanskiy et al. [11] collected data using D. Sensor Fusion
threeoffsetcameras(left/center/right)whilewalkingthrough Sensorfusionencompassesvarioustechniquesforcombin-
forest trails. The images were labeled with desired yaw ingdifferentinputmodalitiestoimprovemodelperformance.
actions where the left-offset and right-offset cameras were One common approach is to train an ensemble of models
used to depict situations when the learner drifts from the on different sensory inputs and use voting or measures of
expert path (the forward-facing image labeled as zero yaw uncertaintytocombinepredictions.Leeetal.[26]trainedan
while the left and right images were labeled as yaw right ensembleofuncertainty-awarepolicynetworkscomposedof
and left, respectively). The DNN was then validated using a a left camera, right camera, and GPS input modalities. They
MAV to autonomously navigate forest trails. applied their ensemble to the task of autonomous driving
In our previous work [12], we used behavioral cloning where the network output possessing minimum uncertainty
and an approach similar to DAGGER [13] to train a DNN waschosen.Otherapproachesdirectlyinputmultiplesensory
to predict yaw and pitch angles for an underwater robot. streamsintothesamenetworkundertheassumptionthatthe
The behavioral objective was to navigate close to coral model itself will learn to automatically weigh each input
whilesimultaneouslyavoidingcollisionsandmovingtowards optimally [27] [28] [29] [30].
regionsofinterest.Weiterativelylabeled,trained,andevalu-
III. PRELIMINARIES
ated our image-based controller during several deployments
until the robot achieved acceptable performance. A. Reinforcement Learning
In reinforcement learning, the environment is represented
B. Semantic Segmentation for Terrain Traversability M S A R T
as a Markov Decision Process, = ( ; ; ; ), rep-
A common approach to visual navigation in the presence
resenting the set of states, actions, rewards, and transition
of diverse terrain is a two step process of visual seman-
dynamics, respectively. The agent seeks to learn a control
tic segmentation followed by geometric path planning. A (cid:24)
policy a (cid:25)(s ) that maXximizes the return G :
number of authors have addressed urban navigation using t t t
such an approach. Barnes et al. [14] and Tang et al. [15] T
trained a network to segment the agent’s view into drivable, G = (cid:13)kR (1)
t t+k+1
non-drivable,andunknownregionsusinglabeledtrajectories k=0
(cid:20) (cid:20)
generated from expert demonstrations. More recent work where 0 (cid:13) 1 is the discount factor and R is the
k
by Wellhausen et al. [16] applied semantic segmentation reward. The value function of a state V (s ) is de(cid:2)ned as
(cid:25) t
in a self-supervised fashion by using a quadruped robot’s the expected return by following policy (cid:25) from state s .
t
recorded force-torque signals to label traversed terrain. La- Thevaluefunctioncanbede(cid:2)nediterativelybytheBellman
beledterrainwasprojectedintotherobot’scameraframefor equation:
X X
generatingatrainingsetforsegmentation.Forpathplanning, E (2)
V (s )= [G ]
(cid:25) t t
the world was discretized into grid cells where cost was de- (cid:25)
j j
rived from the re-projected predicted segmented images and = (cid:25)(a s ) p(s s ;a )[r +(cid:13)V (s )]
t t t+1 t t t (cid:25) t+1
Dijkstra’s algorithm [17] was used to (cid:2)nd the optimal path. atj st+1
Forenvironmentsinvolvingmanyterrainclasses,Rothrocket wherep(s s ;a )representsthetransitiondynamicsofthe
t+1 t t
al.[18]appliedtheDeepLab[19]segmentationnetworkwith system.Inmodel-freereinforcementlearning,anagentlearns
(cid:2)rst-person to top-down image projection for the task of a control policy to directly maximize the return without
evaluating traversability for Mars rovers. explicitly learning the dynamics of the system. Conversely,
1264
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 21,2020 at 05:49:00 UTC from IEEE Xplore.  Restrictions apply. Extracted ReLU FC ReLU
Aerial Convolution dropout
Image
32x8x8  64x4x4  64x4x4  64x3x3 
OnIbmoaagrde s:4x4 s:2x2 s:2x2 s:1 256 128 LSTceMll dropout FC ReLU softmax
FC FC
ReLU ReLU FC ReLU
softmax
Fig. 2: Neural network model for predicting terrain types over a horizon H. The state s is composed of the current and
t
several previous ground and aerial aligned images to increase context. The action inputs a through a (cid:0) are steering
t t+H 1
angles. The image inputs travel through the convolutional block to form the initial hidden state of the recurrent network.
The softmax outputs p(y ) represent the probability of each terrain class. Dropout is optionally used to reduce over(cid:2)tting.
t+i
in model-based reinforcement learning, the agent (cid:2)rst seeks collision or no collision), instead of real-valued outputs,
to learn the system dynamics and then plans a policy by improved performance. Furthermore, they found that re-
simulating rollouts. Typically, the learned dynamics model moving the prediction on the bootstrapped value function
takestheformofastatepredictivefunctions =f(s ;a ). improved learning ef(cid:2)ciency and stability without hindering
t+1 t t
performance because this real-valued, recursive quantity can
B. ValuePredictionNetworksandGeneralizedComputation be more dif(cid:2)cult to predict and may not yield noticeable
Graphs performance gains for a short-horizon control task.
Recently, Oh et al. [31] proposed Value Prediction Net-
works (VPN), a hybrid model-based and model-free rein-
IV. MODELOVERVIEW
forcement learning architecture. Value Prediction Networks
learn an encoded state h optimized to predict immediate
t
rewards r and value functions on the abstract state V(h ). A. Representation
t t
Planning is done by simulating rollouts on abstract states
We propose a deep learning model for predicting the
h over a horizon of timesteps from which the action that
t terrain roughness and collision probabilities over a (cid:2)xed
maximizes the average expected return (calculated jointly
horizon of H timesteps from which planning can be done
from rewards r and value functions V(h )) is taken. The
t t to derive an action policy. We use an architecture similar
intuition behind this approach is that planning need not
to Khan et al. [5], which has previously been shown to
requirefullobservationpredictions(suchasimages)andcan
produce good performance in short-term control situations.
instead be done by only predicting the upcoming rewards
The model operates on an input image state s and action
and value functions on a learned model of encoded abstract h t i
sequence of steering commands a ;a ;:::a (cid:0) . The
states. Oh et al. [31] assert that Value Predictive Networks t t+1 t+H 1
image state is composed of the current image and recent
can be viewed as jointly model-based and model-free. It h i
visualhistoryofthelastMtimesteps(s = I (cid:0) ;:::;I )
is model-based as it implicitly learns a dynamics model t t M+1 t
to provide additional context. For this input sequence, the
for abstract states optimized for predicting future rewards
model predicts the probability each terrain classes over the
and value functions. It is also model-free as it maps these h i 2
planninghorizon p(y );p(y );:::;p(y ) wherey
encoded abstract states to rewards and value functions using t+1 t+2 t+H t
C andC isthesetofallterrainclasses.Wechooseincreasing
direct experience with the environment prior to the planning
labelvaluestoberougherterrain(0beingcompletelysmooth
phase. j j(cid:0)
and C 1 being an obstacle). Throttle is assumed to be
Oh et al. [31] found that this hybrid model possessed
constant and therefore is not present in the action space.
several advantages over conventional model-free and model-
This architecture can be seen as modeling the joint prob-
based learning methods. Speci(cid:2)cally, they found it to be h i
more sample ef(cid:2)cient and to generalize better than Deep Q- ability of terrain classes yt+1;yt+2:::yt+H over a horizon
Networks [32] while simultaneously outperforming a con- H whileassumingconditionalindependencebetweenlabels:
Y j
ventional observation predictive network in cases where p(y ;:::y a (cid:0) :::a ;s )=
t+H t+1 t+H 1 t t
planningontherawstatespaceisdif(cid:2)cultduetothespace’s H j (3)
complexity or the environment is highly stochastic. p(y a (cid:0) :::a ;s )
t+i t+i 1 t t
Khan et al. [5] developed a similar architecture, known i=1
as Generalized Computation Graphs (GCG), for the task of In the context of Value Prediction Networks, the predicted
collision avoidance over a short predictive horizon. Their terrainlabelsactasrewardvalues.Toenforcehigherrewards
model was deployed on a small, remote-controlled car in formoredesirableorsmootherterrainclasses,weremapthe
an indoor corridor environment. They found that predicting label to a reward value:
j j(cid:0) (cid:0)
discrete reward values as in a classi(cid:2)cation problem (e.g. r = C 1 y (4)
t t
1265
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 21,2020 at 05:49:00 UTC from IEEE Xplore.  Restrictions apply. B. Network Architecture and Objective Function
The network architecture modeling the mapping from an Front Left Front Right
Camera IMU Camera
image state and action sequence to predicted terrain class RTK
GPS
probabilitiesisshowninFig.2.Thecurrentimagestates is
t
putthroughaconvolutionalneuralnetworktoformtheinitial EmAbRedMd ed  SSteeerrviong 
hidden state of a Long Short-Term Memory (LSTM) [33] Microcontroller Brake Servo
recurrentneuralnetwork.Airandgroundimageinputsfollow Core i7 NUC Drive Motor
different convolutional branches as their learned features Jetson
Xavier
differ based on their unique perspectives. The feature map
outputs of these dual branches are concatenated. Steering
actions a and predicted terrain class probabilities p(y^ )
t t+1
form the input-output pair of the recurrent network. Fig. 3: Off-road vehicle used for validation experiments.
The cross-entropy loss objective for training is obtained Only one forward-facing camera is used for these experi-
by applying maximum log likelihood to Eq. 3 with L2 ments.
X X
regularization: and a u-blox ZED-F9P RTK GPS. The microcontroller
(cid:0) H j k k also controls the two servo motors and brushless motor
Lt = 2 1yt+i=cjlogp(y^t+i at+i(cid:0)1:::at;st)+(cid:21) w 22 controllerbyforwardingcommandseitherfromauserhand-
i=1cj C (5) held remote or from a connected Intel i7 NUC computer
when autonomously driven. The microcontroller receives
where y and y^ are the true and predicted labels.
t+i t+i RTK correction signals through a long-range wireless link
Toinstantiateournetwork,wechooseM =4asthevisual
h i and relays all sensor information to the NUC. Mounted on
history length to give s = I (cid:0) ;I (cid:0) ;I (cid:0) ;I and use
t t 3 t 2 t 1 t the front of the vehicle is a Hokuo UTM-30LX Lidar and
color image inputs of size of 72x128x3. The visual image
two forward-facing IDS Imaging UI-3251LE global-shutter
history inputs are concatenated as additional channels, mak-
cameras with 4mm lenses which are tightly coupled to a
ing the overall image input size 72x128x12 for both air and
VectorNav VN-100 IMU (although only one camera was
ground. Four convolutional layers consisting of (32, 64, 64,
used in our experiments).
64) kernel units of sizes (8x8, 4x4, 4x4, 3x3) and of strides
TheInteli7NUCminiPCrunsUbuntuandtheRobotOp-
(4, 2, 2, 1) were chosen, respectively. Additional parameters
erating System (ROS) software package. The NUC records
uniquetoeachexperimentationenvironment(simulationand
camera images, Lidar obstacles detections, and IMU read-
real-world) are detailed in section VI.
ings. An NVIDIA Jetson Xavier is also connected to the
C. Planning NUC via a shared gigabit network. The Xavier likewise
runs the ROS software package and hosts our terrain pre-
During the planning phase, we seek to solve for a trajec-
dictive neural network model (Fig. 2) implemented in Ten-
tory of actions that maximizes the expected reward over a
X X sor(cid:3)ow [35].
planning horizon of H (where rewards follow Eq. 4):
Before deploying the ground vehicle, sparse aerial images
(cid:0) H (cid:3) j are taken using a MAV at a high altitude above the deploy-
AH = argmax c p(y^ =c a (cid:0) :::a ;x )
t j t+i j t+i 1 t t ment region. In our case, a single image taken at 80 m
at;:::at+H(cid:0)1 i=1cj2C (6) provided a resolution of 0.01 meters-per-pixel. Using four
landmarksmeasuredwiththegroundvehicle,weshift,rotate,
To solve this optimization, a randomized K-shooting
and scale the image to align with the landmarks. Visual
method is applied [34]. K action rollouts are randomly
(cid:20) (cid:20) inspection showed this image to be accurate within 0.1 m.
generated and each k’th (1 k K) rollout AH =
(t;k) At each timestep during runtime, we use the GPS location
(a ;:::a (cid:0) ) is evaluated using our learned model to
t t+H 1 k and compass heading to extract a 12 m x 9 m aerial image
predicttheexpectedcumulativetrajectoryreward.Ourpolicy
patch that is oriented with the ground vehicle and centered
appliesthe(cid:2)rstactionfromthethetrajectorywiththehighest
1.5 m directly in front of the vehicle.
perceived return. Following the Model Predictive Control
Many authors have considered the use of an IMU to mea-
paradigm, planning is repeated at every time step.
sure road roughness [36], [37], [38]. Similarly, we measure
V. SYSTEMOVERVIEW the Root Mean Square (RMS) linear acceleration reported
by the robot’s IMU over short time windows to approximate
Our experimental off-road vehicle is based on a modi-
an instantaneous roughness score of the traversed terrain.
(cid:2)ed 1:5 scale, electric off-road, remote-controlled buggy as
showninFig.3.Itcontainstwoservomotorsforcontrolling
VI. EXPERIMENTS
the steering and mechanical brake, while a 260 kV sen- A. Simulation
sored brushless motor drives the four wheels. An embedded We evaluated our model in off-road environments devel-
ARM microcontroller maintains a pose estimate using an oped using an Unreal Engine simulator. Two environments
extended Kalman (cid:2)lter that fuses sensor information from weremade(showninFig.4)forthepurposesoftrainingand
redundant triple-axis accelerometers, magnetometers, gyros, testing generalization, respectively. The environments were
1266
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 21,2020 at 05:49:00 UTC from IEEE Xplore.  Restrictions apply. noise was applied to the action at each timestep during this
interval. This process was repeated iteratively.
Approximately75,000sampleswerecollected,amounting
(cid:25)
to 3.5 hours of driving. The ground-truth terrain and
collision labels were queried directly from the simulator.
2) Off-Policy Training: The network was trained on over
28,000 timesteps with a batch size of 32 and a learning rate
(cid:0)
of (cid:11) = 10 4. A training and validation set with a 3:1 split
was used. The loss function given by Eq. 5 was optimized
with Adam [39] and a L2 weight regularization factor of
(cid:0)
Fig. 4: Map of simulation environments with sample on- (cid:21) = 10 6. Training was repeated for ground-only, air-only,
and fusion visual input networks. The ground-only and air-
policy evaluation trajectories.
only networks are analogous to the fusion network shown
in Fig. 2 with the alternative camera input branches being
removed.
To gain further insight into the predictive breakdown of
our model, we plot the validation set accuracy divided into
short-term and long-term portions of the prediction horizon
H, as shown in Fig. 5.
When comparing ground versus air for short-term terrain
predictions, we found that the ground view attained higher
accuracy as it provided a closer, more detailed view of the
immediate terrain being navigated. The short-term view of
theaerialperspectivewasalsopronetocompleteobstruction
whentreecoverwaspresent,makingitinclinedtocollisions.
Fig. 5: Simulation validation set accuracy for different mod-
Conversely,forlongerhorizons,theaerialviewpointenabled
els over short-term and long-term planning horizons.
some performance gains on certain trajectories due to its
divided into sections of tall grass and tree cover to evaluate increased FOV, (apparent for turning trajectories) and its
our model’s ability to jointly exploit air and ground imagery ability to see over small barriers. However, the ground view
inputs depending on visual obstructions in either view. A couldgenerallyseefurtherintheforward-facingdirectionin
h
set of four terrain classes was used: C = 0 : smooth, 1 : the case of no obstacles or turning trajectories, making its
i
medium, 2:rough, 3:obstacle . long-termpredictionspreferableinothersituations.Nonethe-
A simulated off-road buggy with full drivetrain physics less, the fusion model outperformed both ground-only and
and suspension acted as the learner. Ground and aerial air-only architectures, as the model learned to leverage data
(cid:14)
cameras were con(cid:2)gured with 90 FOV. The aerial camera from both viewpoints to maximize its FOV and counteract
was placed 15 m above the vehicle. A control rate of 6 Hz visual obstructions in either view.
and a predictive horizon of H =12 was chosen. (cid:6)
TABLEI:Simulationaverageepisodeon-policyreturn( (cid:27)).
1) DataCollection: Datacollectionandtrainingwasdone
off-policy. Applying a random exploratory action at each Env. Fus(cid:6)ion Gro(cid:6)und A(cid:6)ir Ran(cid:6)dom
Train 1768.4 496.1 1527.8 644.7 618.5 460.4 178.5 189.6
timestep was found to generate trajectories with excessive (cid:6) (cid:6) (cid:6) (cid:6)
Test 2026.3 253.2 1621.4 531.2 576.8 441.6 155.7 127.7
stuttering and insuf(cid:2)cient variety. Instead, a random time
interval was drawn from a Gaussian distribution during
3) On-Policy Evaluation: Each trained model was eval-
which a random, uniformly sampled steering action was
uated on-policy for over 30 trajectories. Trajectories were
applied and held constant. Additional but small Gaussian
terminated at the (cid:2)rst collision or once a maximum of two
minuteshadelapsed.Theaveragereturn(orsumofrewards)
ofeachmodelisrecordedinTableI.Fig.6plotstheaverage
terrain percentage traversed by each model. Fig. 4 shows
sample trajectories generated by the fusion model.
During on-policy evaluation, the fusion model was again
found to perform best, as it learned to incorporate data from
both viewpoints, thus making it robust to visual obstructions
and allowing it to maximize its FOV. The performance
gains were more notable in the test environment, as the test
environment had reduced tree cover (decreasing the ground
model’s relative performance). The fusion model attained a
Fig. 6: Percentage terrain traversed during simulation on-
90% smooth terrain traversal rate in the test environment
policy evaluation.
1267
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 21,2020 at 05:49:00 UTC from IEEE Xplore.  Restrictions apply. 7
]2 Rough Terrain
/ms6 Smooth Terrain
n [5
o
ati4
er3
el
cc2
a
S 1
M
R0 4 km/h 5 km/h 6 km/h 7 km/h 8 km/h
Fig.9:RMSlinearaccelerationversusspeedandterraintype
for rough and smooth classes.
Fig. 7: An example path executed during training overlayed
on the aerial image. Detected smooth terrain is shown in scores varied with speed. Fig. 9 shows that the RMS metric
green, while rough terrain is shown in red. became more noticeable at higher speeds, but actions were
more uncertain due to violent vibrations and impacts. In our
and a rough terrain traversal rate of only 1.8% compared to
experiments, we selected 6 km/h speed, which still provided
11.2% of the ground-only model (6.1 times reduction).
a good distinction between terrain classes.
B. Real-world Field Trials The collected trajectories amounted to approximately
(cid:25)
We validated our model in a real-world, rugged, outdoor 5.25 km or 15;000 data samples.
environmentspanningroughly0.25km2 usingtherobotsys- 2) Training and Evaluation: Despite a relativel(cid:25)y small
tem described in Sec. V. Fig. 7 shows an example path that dataset, we obtained reasonable performance after 8,000
was executed while training our system. Three terrain types training steps. The prediction accuracy using the forward
f g
were used: C = 0:smooth, 1:rough, 2:obstacle . Fig. 8 ground camera alone ranged from 78% to 60% over horizon
shows terrain samples that were measured both qualitatively intervals of 1 to 16, respectively. For actions that were gen-
and quantitatively (visible small and large bushes and trees erally forward in direction, there was minimal improvement
were also treated as obstacles). using the aerial image. However, we found a consistent
(cid:25)
1) Data Collection: We collected joint ground and air improvementinaccuracyof 10%whenincorporatingaerial
(cid:14)
imagery in this environment using the left camera of the imagery for trajectories possessing a change in angle of 45
groundvehicleandaerialviewfromaDJIMavicProdrone. ormore.Fig.10showssamplepathpredictionsofthetrained
TheMAVwasusedtoconstructamosaicimageoftheexper- model navigating towards smooth terrain.
imental region, taken at an altitude of 80 m. We aligned this
VII. CONCLUSIONS
aerialimageusingfourlandmarksmeasuredwithRTKGPS.
This paper has described the (cid:2)rst application of a deep
Training trajectories were generated consisting of tuples
terrain predictive model using ground-air image fusion and
made from the vehicle’s left camera image, corresponding
self-supervised training to learn a smooth, collision-free
aerial image patch, labeled terrain class, and steering action.
navigation policy in rugged outdoor environments. Through
Data was sampled every 0.35 m travelled instead of at (cid:2)xed
(cid:2)elddeploymentsandextensivesimulations,wehaveshown
timeinterval.Thissamplingapproachwasusedinanattempt
thatourmodelplanscollision-freetrajectoriesthatmaximize
to mitigate unintentional speed changes due to factors such
smooth terrain with limited training data requirements. Our
as draining battery voltage. A planning horizon of H = 16
key contributions include the use of IMU self-supervision
was selected.
as a proxy for driveability and the combined use of aligned
Terrain labeling followed a purely self-supervised ap-
aerial images and (cid:2)rst-person on-board images. This multi-
proach. We measured the RMS linear acceleration values
modal input strengthens the predictive capabilities of the
in the direction normal to the ground plane reported by the
model by increasing the available (cid:2)eld of view and making
robot’s on-board IMU over a short window of 20 60 Hz
it robust to visual obstructions.
samples. A frequency representation of this short window
signalwasobtainedbytakingtheFouriertransformandthen
Move towards smooth terrain
the magnitude component was put into a 15 bin histogram
divided by frequency. The histogram was used to make a
feature vector processed by K-Means clustering to generate
labels. The on-board Lidar was used to label obstacles and
pre-emptively stop the vehicle prior to physical collision.
Stay on smooth terrain
We also experimented with how our assigned roughness
Rough Smooth Smooth Obstacle Fig. 10: Sample action predictions for moving towards and
Fig. 8: Example of terrain that was covered during our real- stayingonsmoothterrain.Highrewardpredictionsareshown
world validation experiments. in green while low reward predictions are shown in red.
1268
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 21,2020 at 05:49:00 UTC from IEEE Xplore.  Restrictions apply. REFERENCES ImagesViaSelf-SupervisedLearning,(cid:148)IEEERoboticsandAutomation
Letters,vol.4,no.2,pp.1509(cid:150)1516,April2019.
[1] S. Thrun, M. Montemerlo, H. Dahlkamp, D. Stavens, A. Aron, [17] E.W.Dijkstra,(cid:147)Anoteontwoproblemsinconnexionwithgraphs,(cid:148)
J. Diebel, P. Fong, J. Gale, M. Halpenny, G. Hoffmann, K. Lau, Numerischemathematik,vol.1,no.1,pp.269(cid:150)271,1959.
C.Oakley,M.Palatucci,V.Pratt,P.Stang,S.Strohband,C.Dupont, [18] B.Rothrock,R.Kennedy,C.Cunningham,J.Papon,M.Heverly,and
L.-E. Jendrossek, C. Koelen, C. Markey, C. Rummel, J. van Niek- M.Ono,SPOC:DeepLearning-basedTerrainClassi(cid:2)cationforMars
erk, E. Jensen, P. Alessandrini, G. Bradski, B. Davies, S. Ettinger, RoverMissions.
A.Kaehler,A.Ne(cid:2)an,andP.Mahoney,(cid:147)Stanley:Therobotthatwon [19] L. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille,
thedarpagrandchallenge,(cid:148)JournalofFieldRobotics,vol.23,no.9, (cid:147)Deeplab: Semantic image segmentation with deep convolutional
pp.661(cid:150)692,2006. nets, atrous convolution, and fully connected crfs,(cid:148) CoRR, vol.
[2] C. Urmson, J. Anhalt, H. Bae, J. A. D. Bagnell, C. R. Baker, R. E. abs/1606.00915,2016.
Bittner,T.Brown,M.N.Clark,M.Darms,D.Demitrish,J.M.Dolan, [20] B. Sofman, J. A. Bagnell, A. Stentz, and N. Vandapel, (cid:147)Terrain
D. Duggins, D. Ferguson, T. Galatali, C. M. Geyer, M. Gittleman, classi(cid:2)cation from aerial data to support ground vehicle navigation,(cid:148)
S.Harbaugh,M.Hebert,T.Howard,S.Kolski,M.Likhachev,B.Litk- 2006.
ouhi,A.Kelly,M.McNaughton,N.Miller,J.Nickolaou,K.Peterson, [21] A.Stentz,(cid:147)TheFocussedD*AlgorithmforReal-timeReplanning,(cid:148)in
B.Pilnick,R.Rajkumar,P.Rybski,V.Sadekar,B.Salesky,Y.-W.Seo, Proceedings of the 14th International Joint Conference on Arti(cid:2)cial
S.Singh,J.M.Snider,J.C.Struble,A.T.Stentz,M.Taylor,W.R.L. Intelligence - Volume 2, ser. IJCAI’95. San Francisco, CA, USA:
Whittaker, Z. Wolkowicki, W. Zhang, and J. Ziglar, (cid:147)Autonomous MorganKaufmannPublishersInc.,1995,pp.1652(cid:150)1659.
drivinginurbanenvironments:Bossandtheurbanchallenge,(cid:148)Journal [22] J. Delmerico, E. Mueggler, J. Nitsch, and D. Scaramuzza, (cid:147)Active
ofFieldRoboticsSpecialIssueonthe2007DARPAUrbanChallenge, autonomousaerialexplorationforgroundrobotpathplanning,(cid:148)IEEE
PartI,vol.25,no.8,pp.425(cid:150)466,June2008. RoboticsandAutomationLetters,vol.2,no.2,pp.664(cid:150)671,2017.
[3] B. Suger, B. Steder, and W. Burgard, (cid:147)Traversability analysis for [23] J.Delmerico,A.Giusti,E.Mueggler,L.Gambardella,andD.Scara-
mobile robots in outdoor environments: A semi-supervised learning muzza,(cid:147)(cid:147)on-the-spottraining(cid:148)forterrainclassi(cid:2)cationinautonomous
approachbasedon3D-lidardata,(cid:148)inProceedings-IEEEInternational air-groundcollaborativeteams,(cid:148)032017,pp.574(cid:150)585.
Conference on Robotics and Automation, vol. 2015-June, no. June. [24] L.Wang,D.Cheng,F.Gao,F.Cai,J.Guo,M.Lin,andS.Shen,(cid:147)A
IEEE,may2015,pp.3941(cid:150)3946. collaborativeaerial-groundroboticsystemforfastexploration,(cid:148)2018.
[4] G.Dulac-Arnold,D.Mankowitz,andT.Hester,(cid:147)ChallengesofReal- [25] M. Garzon, J. Valente, D. Zapata, and A. Barrientos, (cid:147)An
WorldReinforcementLearning,(cid:148)apr2019. aerial(cid:150)groundroboticsystemfornavigationandobstaclemappingin
[5] G. Kahn, A. Villa(cid:3)or, B. Ding, P. Abbeel, and S. Levine, (cid:147)Self- largeoutdoorareas,(cid:148)Sensors(Basel,Switzerland),vol.13,pp.1247(cid:150)
Supervised Deep Reinforcement Learning with Generalized Compu- 67,012013.
tationGraphsforRobotNavigation,(cid:148)inProceedings-IEEEInterna- [26] K. Lee, Z. Wang, B. I. Vlahov, H. K. Brar, and E. A. Theodorou,
tionalConferenceonRoboticsandAutomation,2018,pp.5129(cid:150)5136. (cid:147)Ensemblebayesiandecisionmakingwithredundantdeepperceptual
[6] D. A. Pomerleau, (cid:147)Alvinn: An autonomous land vehicle in a neural controlpolicies,(cid:148)2018.
network,(cid:148)inAdvancesinneuralinformationprocessingsystems,1989, [27] X. Chen, H. Ma, J. Wan, B. Li, and T. Xia, (cid:147)Multi-view 3d object
pp.305(cid:150)313. detectionnetworkforautonomousdriving,(cid:148)2017IEEEConferenceon
[7] T.M.Jochem,D.A.Pomerleau,andC.E.Thorpe,(cid:147)Maniac:Anext ComputerVisionandPatternRecognition(CVPR),Jul2017.
generationneurallybasedautonomousroadfollower,(cid:148)inProceedings [28] J.Ku,M.Mozi(cid:2)an,J.Lee,A.Harakeh,andS.L.Waslander,(cid:147)Joint
of the International Conference on Intelligent Autonomous Systems. 3d proposal generation and object detection from view aggregation,(cid:148)
IOSPublishers,1993,pp.15(cid:150)18. 2018 IEEE/RSJ International Conference on Intelligent Robots and
Systems(IROS),Oct2018.
[8] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp,
[29] M. Engelcke, D. Rao, D. Z. Wang, C. H. Tong, and I. Posner,
P. Goyal, L. D. Jackel, M. Monfort, U. Muller, J. Zhang, and
(cid:147)Vote3deep: Fast object detection in 3d point clouds using ef(cid:2)cient
Others, (cid:147)End to end learning for self-driving cars,(cid:148) arXiv preprint
convolutionalneuralnetworks,(cid:148)2017IEEEInternationalConference
arXiv:1604.07316,2016.
onRoboticsandAutomation(ICRA),May2017.
[9] C.Rasmussen,Y.Lu,andM.Kocamaz,(cid:147)Atrail-followingrobotwhich
[30] D. Xu, D. Anguelov, and A. Jain, (cid:147)Pointfusion: Deep sensor fusion
usesappearanceandstructuralcues,(cid:148)inSpringerTractsinAdvanced
for 3d bounding box estimation,(cid:148) 2018 IEEE/CVF Conference on
Robotics,vol.92. Springer,Berlin,Heidelberg,2014,pp.265(cid:150)279.
ComputerVisionandPatternRecognition,Jun2018.
[10] A.Giusti,J.Guzzi,D.C.Cires‚an,F.L.He,J.P.Rodr·(cid:17)guez,F.Fontana,
[31] J. Oh, S. Singh, and H. Lee, (cid:147)Value prediction network,(cid:148) in Pro-
M.Faessler,C.Forster,J.Schmidhuber,G.D.Caro,D.Scaramuzza,
ceedingsofthe31stInternationalConferenceonNeuralInformation
and L. M. Gambardella, (cid:147)A Machine Learning Approach to Visual
Processing Systems, ser. NIPS’17. USA: Curran Associates Inc.,
Perception of Forest Trails for Mobile Robots,(cid:148) IEEE Robotics and
2017,pp.6120(cid:150)6130.
AutomationLetters,vol.1,no.2,pp.661(cid:150)667,jul2016.
[32] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou,
[11] N. Smolyanskiy, A. Kamenev, J. Smith, and S. Birch(cid:2)eld, (cid:147)Toward
D. Wierstra, and M. A. Riedmiller, (cid:147)Playing Atari with Deep Re-
low-(cid:3)ying autonomous MAV trail navigation using deep neural net-
inforcementLearning,(cid:148)CoRR,vol.abs/1312.5602,2013.
worksforenvironmentalawareness,(cid:148)in2017IEEE/RSJInternational
[33] S.HochreiterandJ.Schmidhuber,(cid:147)Longshort-termmemory,(cid:148)Neural
Conference on Intelligent Robots and Systems (IROS), sep 2017, pp.
Comput.,vol.9,no.8,pp.1735(cid:150)1780,Nov.1997.
4241(cid:150)4247.
[34] R.Tedrake,(cid:147)Underactuatedrobotics:Learning,planning,andcontrol
[12] T. Manderson, J. C. G. Higuera, R. Cheng, and G. Dudek, (cid:147)Vision-
foref(cid:2)cientandagilemachines:Coursenotesformit6.832,(cid:148)Working
basedautonomousunderwaterswimmingindensecoralforcombined
draftedition,vol.3,2009.
collision avoidance and target selection,(cid:148) in 2018 IEEE/RSJ Interna-
[35] M.Abadi,P.Barham,J.Chen,Z.Chen,A.Davis,J.Dean,M.Devin,
tionalConferenceonIntelligentRobotsandSystems(IROS). IEEE,
S. Ghemawat, G. Irving, M. Isard et al., (cid:147)Tensor(cid:3)ow: A system
2018,pp.1885(cid:150)1891. f g
for large-scale machine learning,(cid:148) in 12th USENIX Symposium on
[13] S.Ross,G.Gordon,andD.Bagnell,(cid:147)Areductionofimitationlearning f g
Operating Systems Design and Implementation ( OSDI 16), 2016,
andstructuredpredictiontono-regretonlinelearning,(cid:148)inProceedings
pp.265(cid:150)283.
ofthefourteenthinternationalconferenceonarti(cid:2)cialintelligenceand
[36] I. Kertesz, T. Lovas, and A. Barsi, (cid:147)Measurement of road rough-
statistics,2011,pp.627(cid:150)635.
nessbylow-costphotogrammetricsystem,(cid:148)InternationalArchivesof
[14] D.Barnes,W.Maddern,andI.Posner,(cid:147)Findyourownway:Weakly-
Photogrammetry, Remote Sensing and Spatial Information Sciences,
supervised segmentation of path proposals for urban autonomy,(cid:148)
vol.36,012007.
in Proceedings - IEEE International Conference on Robotics and
[37] W.Wen,(cid:147)Roadroughnessdetectionbyanalysingimudata,(cid:148)Master’s
Automation,2017,pp.203(cid:150)210.
thesis,RoyalInstituteofTechnology(KTH),2008.
[15] L.Tang,X.Ding,H.Yin,Y.Wang,andR.Xiong,(cid:147)Fromonetomany:
[38] A.Barsi,T.Lovas,I.Kerte·szetal.,(cid:147)Thepotentialoflowendimusfor
Unsupervisedtraversableareasegmentationinoff-roadenvironment,(cid:148)
mobilemappingsystems,(cid:148)InternationalArchivesofPhotogrammetry
in2017IEEEInternationalConferenceonRoboticsandBiomimetics
andRemoteSensing,vol.36,no.Part1,p.4,2006.
(ROBIO),Dec2017,pp.787(cid:150)792.
[39] D.KingmaandJ.Ba,(cid:147)Adam:Amethodforstochasticoptimization,(cid:148)
[16] L.Wellhausen,A.Dosovitskiy,R.Ranftl,K.Walas,C.Cadena,and InternationalConferenceonLearningRepresentations,122014.
M.Hutter,(cid:147)WhereShouldIWalk?PredictingTerrainPropertiesFrom
1269
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 21,2020 at 05:49:00 UTC from IEEE Xplore.  Restrictions apply. 