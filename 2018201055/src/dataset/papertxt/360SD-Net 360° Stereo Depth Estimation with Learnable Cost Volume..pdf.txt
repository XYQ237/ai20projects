2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
Omnidirectional Depth Extension Networks
†
Xinjing Cheng, Peng Wang , Yanqi Zhou, Chenye Guan and Ruigang Yang
◦
Abstract—Omnidirectional 360 camera proliferates rapidly
for autonomous robots since it signiﬁcantly enhances the per-
ception ability by widening the ﬁeld of view (FoV). However,
◦
corresponding 360 depth sensors, which are also critical for
theperceptionsystem,arestilldifﬁcultorexpensivetohave.In
this paper, we propose a low-cost 3D sensing system that com-
bines an omnidirectional camera with a calibrated projective
depth camera, where the depth from the limited FoV can be
automatically extended to the rest of recorded omnidirectional
image. To accurately recover the missing depths, we design an
omnidirectional depth extension convolutional neural network
(ODE-CNN), in which a spherical feature transform layer
(SFTL) is embedded at the end of feature encoding layers,
and a deformable convolutional spatial propagation network
(D-CSPN) is appended at the end of feature decoding layers.
The former re-samples the neighborhood of each pixel in the
omnidirectional coordination to the projective coordination, Fig. 1: Different from other setups which adopt LiDAR
which reduce the difﬁculty of feature learning, and the later or omnidirectional stereo, we combine one projective depth
automaticallyﬁndsapropercontexttowellalignthestructures
sensor with omnidirectional camera and propose a new low-
in the estimated depths via CNN w.r.t. the reference image,
cost solution for omnidirectional depth. The last row shows
which signiﬁcantly improves the visual quality. Finally, we
demonstrate the effectiveness of proposed ODE-CNN over the thatourODE-CNNoutperformSoTARGBonlyOmniDepth
popular 360D dataset, and show that ODE-CNN signiﬁcantly network (RectNet) [7] signiﬁcantly both in depth accuracy
outperforms (relatively 33% reduction in depth error) other and structural details.
state-of-the-art (SoTA) methods.
I. INTRODUCTION Inthispaper,ratherthanworkingonthesideofhardware,
Ominidirection camera (OmniCamera) has been popular motivated by the high quality single view depth estimation
in robotics due to its 360 perception capability, which from deep fully convolutional networks [8], [9], we propose
brings richer information to help with decisions. In real asoftware-basedsolutiontoalleviatetheissue.Nevertheless,
applications, besides RGB images, the per-pixel depth is directlyapplyingdepthlearnedfromasingleomnidirectional
also critical for many tasks, e.g. obstacle avoidance [1], image (OmniImage) may suffer from the scale confusion
3D reconstruction [2] and self-localization [3]. While the when switching environments as illustrated in Fig. 1(a).
cost for OmniCamera has dropped considerably, the cost Therefore, in this work, we present a hybrid setup that
for building omnidirectional depth sensors remains to be combines a regular omnidirectional video camera with a
prohibitively high. For example, the popular depth sensors regular FoV depth senor, e.g. a Kinect Camera. The depth
dependent on structure light, e.g. Kinect V1 [4], or time camera provides metric depth values for part of the scene,
of ﬂight (ToF), e.g. Kinect V2 [5] are having limited FoV and the depth values for the remaining regions are extended
aroud 70◦. Therefore, options of putting multiple Kinects based on both the known depths and the image content.
together would require signiﬁcant large bandwidth, yielding Compared to single-image based method [7], our hybrid
the difﬁculties of real-time synchronizing and calibration, depth sensor is much accurate as shown in Fig. 1(b).
andinducingsigniﬁcantlatencyinrealapplications.Another Practically, building a depth CNN performing over Omni-
solutionisinstallingaspinningLiDAR,e.g.Velodyne,while Image is different from that over regular projective images,
the cost is much higher compared with Kinect, and usually since the warped image texture under spherical coordinate
theframerateisalsosigniﬁcantlylower[6]withonlysparse does not suitable to use the feature learned under projective
depth points, as illustrated at the top in Fig. 1. coordinates.Therefore,researchersmainlyadopttwowaysto
tackle this. The ﬁrst one is transforming an omnidirectional
Xinjing Cheng, Chenye Guan are with Robot{ics and Auto-driving image (OmniImage) to six cube maps, and estimating depth
Lab (RAL), Baidu Research, Baidu Inc., chengxinjing,
} for each cube map [10]. However, the discontinuities/non-
guanchenye @baidu.com. Peng Wang is currently working
on Bytedance US AI Lab peng.wang@bytedance.com. Ruigang smoothnessofdepthscanbesigniﬁcantattheboundarywhen
YangiswithKentuckyuniversityryang@cs.uky.edu. stitchingdifferentdepthmaps.Thesecondoptionisadopting
This work is done when Peng Wang, Yanqi Zhou, Ruigang Yang are
the spherical convolution [11], [7] over the OmniImage
employedatBaiduResearch,BaiduInc.
† denotesthecorrespondingauthor. directly,wheretheconvolutionalneighborhoodsofeachpixel
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 589
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 14:55:59 UTC from IEEE Xplore.  Restrictions apply. arere-sampleddynamicallyw.r.t.equirectangularcoordinate. from CNNs has largely outperform the traditional ones [18].
However, the practical latency of the modiﬁed convolution In this section, we majorly review relevant works about
as proposed in [11] is signiﬁcantly slower than a normal the dense prediction with OmniImages, and elaborate their
convolution, which is not practice when performed at every relationship with ODE-CNN proposed in this paper.
layer of CNN over the OmniImage with high resolution.
A. Single view depth estimation and completion
Therefore, we chose to allocate the modiﬁed convolution
at the end of encoding, where the spatial resolution of the The idea of dense depth estimation with CNNs is ﬁrst
convolutionalfeaturehasbeenreducedsigniﬁcantly(usually introduced in [19], and then the quality of estimation is
× ×
16 or 32 after encoding). We call this operation as vastlyimprovedbyjointlyadoptingconditionalrandomﬁeld
spherical feature transform layer (SFTL), which reduces (CRF) [20], combining with auxiliary tasks, such as seman-
the difﬁculty of feature learning yielding better accuracy. tic[9],[21],normal[22],edges[23],[24]andespeciallywith
Besides, this also inspires us to propose a more generalized the development of fully convolutional networks (FCN) [25]
transformation by allowing its convolutional neighborhoods with stronger CNN backbones such as ResNet [15].
of a pixel to be automatically learned w.r.t. the dataset. In Although the estimated depth map from a single image
practice,weinducedeformableconv[12]toactinsideSFTL, achieves impressive quality, it still suffers from instability
yielding even better results. of domain transfer, i.e. performing the learned model on
Moreover, depths obtained directly from a CNN have other unseen places. To alleviate the issue, most recently,
less focuses on image details and structures, yielding non- someresearchersproposetocombinesparsedepthscaptured
satisﬁed estimation. Therefore, we borrow an efﬁcient depth through devices like LiDAR, and perform depth comple-
reﬁnement strategies, i.e. convolutional spatial propagation tion [13], while others propose to combine some low cost
network(CSPN)[13],tohandletheissue.Here,similarly,we depthsensors,wheredensedepthsarepartiallycaptured,and
modify the sampled neighborhood of CSPN w.r.t. equirect- perform depth enhancement [26] or in-painting [27]. In both
angular coordinates using the inverse gnomonic projection setting, depth estimation accuracy and generalization ability
(IG) [14], namely IG-CSPN, to enhance its performance. are signiﬁcant improved.
In addition, we also choose to deform its convolutional In this work, since we are dealing with the OmniImages
neighborhoods similiar as SFTL, namely Deformable CSPN whose FoV is much larger than that of a single view image,
(D-CSPN).WeﬁndtheD-CSPNadaptswellwithbothimage we here propose a novel task called omnidirectional depth
contextanddistortion,yieldingsigniﬁcantperformanceboost extension which extends the dense depths captured at front
over CSPN and IG-CSPN. Finally, after properly embed the view to the rest of corresponding OmniImage. As discussed
two proposed modules, we illustrate our whole framework inSec.I,itisamorepracticalsettingthandepthcompletion
for ominidirectional depth extension (ODE) in Fig. 2, which or in-painting.
is named as ODE-CNN.
B. Deep network over OmniImage
To validate our proposed depth extension setup, we
perform various study over the recently proposed dataset To deal with the projection distortion on OmniImages
with omnidirectional images [7], where we show ﬁrstly by when processing with CNNs, researchers propose various
adopting additional one depth senor with limited FoV can technologies either modifying over the input image or over
signiﬁcantly reduce overall depth error from the estimation the convolutional kernels. For the former strategy, early
from a single RGB image, and jointly adopting our ODE- works [28], [29] propose to process repeatedly over each
CNNachieverelatively33%errorreductionvs.theprevious projected image around the sphere, and merge the processed
SoTA networks [7]. results,yieldinghighcomputationalcostinrealapplications.
In summary, our paper has following contributions: Later, works such as estimating saliency [30] or artistic
style transfer with OmniImages [31] propose to project
1) We propose a low-cost omnidirectional depth sensing
the OmniImage to faces of a cube (cubemap), and then
system by combining an OmniCamera with a regular
processing each cubemap independently with regular CNN.
depthsensor(limitedFoV),whichachievesstrongdepth
However, such a solution is not recommended for depth
sensing efﬁciency (50ms) and accuracy (4% relative
estimation, since there would be signiﬁcant depth artifacts
depth error).
along the boundary connecting different estimated results.
2) For building the depth network, we propose to use a
In addition, for depth extension in our case, processing each
sphericalfeaturetransformlayerattheendofencoderto
viewsindependentlywillignorethecriticalinformationfrom
reduce the difﬁculty of feature learning, and extend the
the depth sensor.
CSPN [13] to inverse gnomonic projection CSPN (IG-
CSPN) and deformable CSPN (D-CSPN), which better Therefore, we choose the methodology of processing the
recover the structural details of estimated depths. OmniImage as a whole by modifying the convolutional
kernels. In this direction, Su et al. [32] ﬁrst propose to
II. RELATEDWORK learnconvolutionweightsforequirectangularprojectedOm-
AsdiscussedinSec.I,withtherapiddevelopmentofdeep niImages by transferring them from an pre-trained network
neural networks [15], [8] and growth accessibility for large from 2D images with a pinhole camera. Cohen et al. [33],
scale depth estimation dataset [16], [17]. Depth estimation [34] propose spherical CNNs that are based on a rotation
590
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 14:55:59 UTC from IEEE Xplore.  Restrictions apply. Fig.2:NetworkarchitectureofourODE-CNN(bestviewincolor).Weintroducesphericalfeaturetransformlayer(SFTL)to
transform the features from pinhole model to equirectangular model Sec. III-B. At the end of the network, we generate the
afﬁnity matrix and convolutional offsets for deformable convolutional spatial propagation network (D-CSPN), which reﬁne
the predicted depth with more detailed structures.
×
equivalent deﬁnition of spherical cross-correlation, which is resolutionofthefeaturemap16 w.r.t.theimageresolution.
specially designed for classiﬁcation tasks. Later works [35], On the other side, the decoder has the reverse structure,
[11], [36] adopt the intrinsic relationship between sphere where transposed convolution layers are adopted here to
and equirectangular coordinates, i.e. inverse gnomonic upsamplethefeaturemaptotheoriginalresolutionfordense
projection, to re-sample the neighborhoods of convolution depthestimation.Inthenetwork,allconvolutionallayersare
according to a spherical tangent plane. Most recently, in followed by batch normalization [42] and ReLU [43].
mapped convolution [37], Eder et al. indicate that tan- To alleviate the distortion problem of OmniImage, at the
gent plane is not the optimal model for OmniImage, and end of encoder, we adopt a SFTL to obtain spherical neigh-
they induce an inverse equirectangular projection for the borhoodintheequirectangularimagewithinversegnomonic
convolutional kernels to better compensate the distortion. projectionordeformableconvolution.Inaddition,torecover
Finally, in our perspective, the spherical convolution can be the structure detail of the estimated depth map, at the end of
seen as a special case of free-form deformable convolution decoder, we adopt the module of CSPN [13], and modify its
as introduced in some recent works [38], [12] for object propagation neighborhood to be dynamically changing w.r.t.
recognition. However, it is only adopted in segmentation of to the pixel location. In the following sections, we elaborate
ﬁsh-eye images for now [39]. the difference of both modules.
InourproposedODE-CNNforhandlingdepthestimation,
we consider both the underlining formula of equirectangu- B. Spherical feature transform layer (SFTL)
lar projection, and free-form deformation jointly inside the
In this section, we elaborate the concept of spatial fea-
architecture.ThankstotherecentlyreleasedlargescaleOm-
ture transform (SFT) for convolution, and then introduce
niDepth dataset [7], we are able to learn inspiring deformed
our strategy of learning per-pixel transform for OmniDepth
kernel shapes and features from scratch, yielding signiﬁcant ∈ × ×
improvement. Last but not the least, we would like to note estimation. Formally, suppose a feature map Hl Rc h w
is from the image represented at the target coordinate, i.e.an
that there are other options of acquiring omnidirectional
depth map such as spherical stereo estimation by setting OmniImage. Here, h,w,c are height, width and number of
up multiple OmniCameras [40], which could be served as feature channels respectively, and l is the layer id. Here,
complementary components in our framework. given coordination transform gxt() at a target location xt
from a source coordinate, e.g. a tangent plane, we may
III. APPROACH map any points at source coordinate x to target by gxt(x).
WhenperformingconvolutionovertheOmniImage,wewant
Given the setup described in Sec. I, we here elaborate our
to sample neighborhoods from the source planar surface
designedencoder-decoderarchitecture,andthetwoproposed
without distortion. Therefore, the convolution needs to use
modules, i.e. SFTL and D-CSPN, for enhancing the perfor-
thetransformedspatialneighborhoods,whichcanbewritten
mance over the depth extension task. (cid:88)
as,
A. Network Architecture ∗
Hl+1(x )= κ(∆x ) Hl(g (x +∆x )) (1)
As displayed in Fig. 2, our network architecture follows t s xt s s
∈N
an encoder-decoder paradigm proposed by [41], which is a ∆xs (xs)
improvedResNet-34[15]byaddingmirrorskipconnections where κ(∆ ) represents the convolutional kernel weights
x × ∗
at corresponding layers with same spatial resolution from of shape c c, and is matrix multiplication. Here,
o
encoder to decoder. we set g (x ) = x for making x as the convolutional
xt Ns t t
The input OmniImage and the partial dense depth map, center, and (x ) is the set of locations around x at
s s ∈
when available, are separately processed by their initial source coordination for convolution. For example, ∆x
− − ··· × s
convolutions. The convolved outputs are concatenated into a [( 1, 1), ,(1,1)]representsa3 3convolutionalkernel
singletensor,whichactsasinputtotheencoder.Theencoder that is popularly adopted in many SoTA networks [15].
consistsofﬁveresidualblockswhichdownsamplethespatial H (x ) indicates the feature representation at x , and a
t t t
591
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 14:55:59 UTC from IEEE Xplore.  Restrictions apply. bilinear interpolation is usually performed for fractional
locations after tranformation g () .
xt
In spherical transformation [11], [36], [34], the source
to target transformation g (x) is called inverse gnomonic
xt
projection[14].Here,thetargetcoordinationisoverasphere,
(cid:2) (cid:3)
andgivenapointx =(φ,θ)inspheresurface,thepixelsin
t
OmniImage is uniformly sampled w.r.t. the latitude coordi-
∈ − ∈ −
nate φ π,π and the longitude coordinate θ [ π,π].
2 2
For each point x , the source coordinate is over the local
t
tangent plane x = (x,y) centered at x . Then, the inverse
s t
gnomonic projection function g (x ) can be formulated as Fig.3:(a)Convolutionalspatialpropagtionnetwork(CSPN).
follow, xt s (b) Inverse gnomonic CSPN (IG-CSPN) with neighborhood
sampled with predeﬁned formula in Eq. (3). (c) Deformable
gxt(xs)=(φ(xs),θ(xs)) · · CSPN with neighborhood automatically learned based on
φ(x )=sin−1(cosϕ·sinτ + y sinϕ cosτφ) (2) inverse gnomonic projection.
s φ ρ
·
θ(x )=τ +tan−1((cid:112)· · x −sinϕ· · ) differences are ﬁrst it adopts a per-pixel transformation
s θ ρ cosτφ cosϕ y sinτφ sinϕ kerneltosubstitutethekernelweights,andsecondarecurrent
(cid:107) (cid:107) − operation is performed inside for propagation. Here, we also
where ρ = x = x2+y2, ϕ = tan 1ρ. Here, to
s 2 embed the spatial transform as induced in Eq. (1) when
performconvolution,wefollow[11]tosampleneighborhood
performing the convolution.
∆xs on the tangent plane. ∈ × ×
However, in real cases, it might be not optimal to model Formally, given the feature blob Hτ Rc h w at time
thetransformlayerasaﬁxedgeometrictransformation[12], step τ, one step CSPN [44] at location xt in our case can
a feature can be much stronger when image context is also be written as,
(cid:88)
embedded for the transformation. Therefore, in this work,
(cid:12)
we propose to learn a deform offset inside the tangent plane H (x )=κ (0) H (g (x )+ (3)
τ+1 t xt 0 xt s
centered at x , i.e. setting ∆x in Eq. (1) as a trainable (cid:12)(cid:88)
t s κ (∆x ) H (g (x +∆x ))
variable rather than ﬁxed neighborhood. Speciﬁcally, at the ∈N xt s τ xt s s
ean3d ×of3enccoondveorl,utthioenanletlwayoerkr,owuhtpicuhtsiasdtdhietinoneaml b∆exddseudsifnogr where, κxt(∆∆xxss)k=(xκˆs(cid:88))xt(∆xs)/ ∈N |κˆxt(∆xs)|,
deformationinthesourcecoordinateoftangentplanarbefore ∆xs k
−
the transformation. κ (0)=1 κ (∆x )
xt ∆xs∈N xt s
Complexityanalysis.WeadopttheCUDAimplementation
(cid:12)
where is the element-wise production between the trans-
of deformable convolution [12] for SFTL, where the key
componentisdeform-im2colfunction.Comparedwitha formation kernel and image fea×ture×. The kNernel κxt is
regularconvolutionimplementedin [15],wheretheim2col locationdependentwithshapeofc ×k k,and k(xt)isthe
function inside can be easily implemented by shifting the neighborhoodpixels of xt witha k k kernel.The afﬁnities
image features. SFTL has the extra cost of predicting per- output from a network κˆxt() are properly normalized which
guaranteesthestabilityofCSPN,andthewholeprocesswill
pixel spatial transform offset, dynamically indexing image
features and bilinear interpolation. Therefore, our proposed iterate N times to obtain the ﬁnal results. Here, k,N are
predeﬁned hyper-parameters.
SFTL has extra memory cost of O(hwk2), where k is the
size of kernel, and speed latency of O(k2). In practice, Similar with SFTL, as illustrated in Fig. 3, we also adopt
when performing over a Nvidia P40 GPU, SFTL is 30% the inverse gnomonic (IG) projection (as stated in Eq. (3))
∗ ×
slower and requires (1+2 k2/c) memory comparing to a to re-samplethe neighborhood forCSPN, which wecall IG-
regularconvolution,hinderingthedeploymentofthemodule CSPN. Then, we allow the offset ∆x to be a trainable
s
to every convolutional layers. Therefore, to minimize the variable predicted from a shared network as illustrated in
extracostinthenetwork,weputSFTLattheendofencoder Fig. 2, which we call deformable CSPN (D-CSPN). In our
×
where spatial resolution of image feature is 16 reduced. experiments (Sec. IV), we show IG-CSPN is more effective
thanthevanillaCSPN[44]withthesameconﬁguration,and
C. Deformable convolutional spatial propagation network D-CSPNachievesevenbetterperformance,whichisadopted
As illustrated in Fig. 2, the estimated OmniDepth di- as our ﬁnal module for depth reﬁnement.
rectly after the process of decoder is still blurry due to Finally, to preserve the accurate depths from the partial
it has less focus on edges and detailed structures in the dense depth map obtained by the depth sensor, same with
OmniImage. Therefore, we adopt CSPN to reﬁne the ﬁnal CSPN for depth completion, we also induce a replacement
output. Speciﬁcally, as illustrated in Fig. 3, CSPN has a step. Speciﬁcally, let Hp to be a hidden representation for
{ }
similar computational formula with convolution, where the thepartialdensedepthmapDp = dp ,andthereplacement
x
592
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 14:55:59 UTC from IEEE Xplore.  Restrictions apply. TABLE I: Quantitative results for panoramic depth estimation experiments on 360D [7] datasets.
Lower the Better Higher the Better
Network SFT CSPN PD
Abs Rel Sq Rel RMSE RMSLog δ δ δ
1.25 1.252 1.253
UResNet [7] - - - 0.0835 0.0416 0.3374 0.1204 93.19 98.89 99.68
RectNet [7] - - - 0.0702 0.0297 0.2911 0.1017 95.74 99.33 99.79
- - - 0.0642 0.0174 0.2086 0.0964 96.86 99.59 99.88
- - front 0.0624 0.0173 0.2090 0.0943 97.03 99.61 99.87
IGT - front 0.0521 0.0129 0.1794 0.0828 97.91 99.68 99.90
DIGT - front 0.0494 0.0126 0.1780 0.0808 98.06 99.66 99.89
ODE-CNN
- CSPN [13] front 0.0539 0.0140 0.1849 0.0850 97.57 99.59 99.88
- IG-CSPN front 0.0523 0.0139 0.1812 0.0857 97.59 99.64 99.89
- D-CSPN front 0.0511 0.0139 0.1778 0.0850 97.61 99.65 99.89
DIGT D-CSPN front 0.0467 0.0124 0.1728 0.0793 98.14 99.67 99.89
can be written as, refer readers to the original paper for the detailed formula
− of these metrics due to space limits.
H (x )=(1 m )H (x )+m Hp(x ) (4)
τ+1 t xt τ+1 t xt t Implementation details. To train ODE-CNN, we use batch
where m =I(dp >0) is an indicator for the availability size of 8, and train it from scratch, i.e.random initialized
of partialxtdepth mxtap. Thanks to D-CSPN, the generated weights, with 20 epochs for all experiment. We adopt Adam
depths produce much better details align with structures in optimizer with β = 0.9, β = 0.999, and the a step-wise
1 2
the OmniImage, and the transition between the depths from learning rate decay policy, which starts at 0.0002 and is
the depth sensor and the estimated depths from ODE-CNN reduced by 1/2 every 3 epochs. We used L depth loss,
| − ∗| 1
is smooth and unnoticeable. i.e.D D ,tosupervisethetraining.ForD-CSPN,weset
1
IV. EXPERIMENTS thetransformationkernelas3anditerationnumbertobe12.
Finally, we train and test ODE-CNN using single NVIDIA
Inthissection,wevalidateODE-CNNexhaustively,andin
P40 GPU, where the training cost around 2 days and testing
thefollowing,wedescribeourexperimentalsetting,datasets,
cost around 50ms for each image.
metrics and implementation details. Then, results from a
detailedablationstudyofeachmoduleweproposedinODE- B. Ablation Study
CNNarepresented.Finally,wecompareagainstotherSoTA
InTab.I,wepresenttheperformancegainofeachmodule
methods,andqualitativelyillustratetheimprovementsinour
we proposed in ODE-CNN over our test set. Here, ”SFT” is
estimated OmniDepth maps.
short for type of spherical feature transform layer at the end
of encoder, ”CSPN” stands for the type of CSPN module
A. Experimental setting
at the end of decoder, and ”PD” refers to the view of
DataSet. The 360D [7] is a large-scale indoor spherical partial dense depth map we obtained from a depth sensor.
RGB-D dataset, which aggregates two realistic datasets, Speciﬁcally,wesimulatethefrontviewdepthsinOmniImage
i.e., Standford 2D-3D [45] and Matterport 3D [46], and asinputinallourexperiments.Inthefollowing,wemajorly
two computer generated (CG) dataset, i.e., SunCG [47] and compare the error metric of ‘Abs Rel‘ since results from
SceneNet [48]. Here, we adopt the same data split strategy others metrics follows consistently. At the row ‘UResNet‘
as [7]. Speciﬁcally, the train and test split are set as, and ‘RectNet‘, we present two baseline methods introduced
1) Training: remove scenes which contain regions with inOminiDepth[7],whichproducetheSoTAresultsoverthe
very large and small depth value in the remaining data datasets. Speciﬁcally, ‘RectNet‘ uses rectangular ﬁlter-banks
in the Standford 2D-3D, Matterport 3D and SunCG, toaddressthehorizontaldistortionoccursinequirectangular
containing 34679 samples. projection.Atthethirdrow,weshowtheperformanceofour
trained ODE-CNN without the proposed modules. It already
2) Testing: remove one complete area from Standford 2D-
signiﬁcantlyoutperformsthe‘RectNet‘proposedin[7],Abs
3D, and three complete buildings from Matterport 3D
Rel reduced by 8.5%, which indicates the effectiveness of
and three CAD scenes from SunCGcontaining 1298
our adopted backbone [41]. At the fourth row, we add in
samples totally.
the partial dense depth map at the front view, and the
Metrics. We adopt the same metrics as proposed in resultsareimprovedfurther,AbsRelreducedby2.5%.Here,
OmniDepth [7], including absolute/square relative error although the error does not improved much, we believe the
(Abs/Sq Rel), rooted mean square error (RMSE), RMSE generalization ability should be much better when testing
in log space(RMSLog) and δ , where δ means % of over images out of the dataset.
∈ t t
correct predicted depth d D under different criterion, Attherowswith‘IGT‘,weshowtheperformanceofODE-
∗ ∈ { }
i.e. max(d , d∗) < t, where t 1.25,1.252,1.253 . We CNN with SFTL via inverse gnomonic (IG) projection, and
d d
593
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 14:55:59 UTC from IEEE Xplore.  Restrictions apply. Fig. 4: Qualitative comparison with RectNet. (a) Image, (b) learned deformable offset at particular point, (c) results from
RectNet [7], (d) results from ODE-CNN, (e) ground truth.
CSPN‘, together and adding the partial dense depth map at
thefrontview,yieldingthebestresultsforODE-CNN,where
the Abs Rel is reduced from 0.0642 to 0.0467, or 27.3%
relative error reduction w.r.t. our baseline, and outperforms
previous SoTA [7] network by 33.4%.
C. Qualitative Results
In Fig. 5, we visualize the learned deformation with 3
propagtion steps inside D-CSPN at three different locations
in an image, i.e.at planar surface(Fig. 5(a)), within detailed
structure(Fig. 5(b)) and at intersection between different
surfaces(Fig. 5(c)). As can be seen, to achieve high esti-
mation quality, on one hand, the deformation follows the
pattern from inverse gnomonic projection at the planar sur-
face to compensate the image distortion. On the other hand,
it jointly considers the image content, where the context
neighborhoods are more concentrated at detailed structures
around the boundaries.
Fig. 5: Learned contexts though our deformable convolu- In Fig. 4, we illustrate two examples of estimated results
tional spatial propagation network, we zoom in three differ- fromourODE-CNN,whicharecomparedagainsttheresults
ent regions, where show that our method will consider both from ‘RectNet‘. We highlight the most improved regions in
distortion and context during propagation. the dashed rectangle, where our estimated depths not only
moreaccurateintheestimatedabsolutescales,butalsoreveal
compared with the results from our backbone, adding IG- the detailed scene structure.
SFTL reduce the Abs Rel error largely by 16.5%, which
V. CONCLUSION
demonstrates that re-sampling of neighborhood is one of the
In this paper, we introduce a novel low-cost omnidirec-
key factor for depth estimation over OmniImage. At the 6th
tional depth sensing system, which combines one omnidi-
row, ‘DIGT‘ indicates adding learnable deformation to IG
rectioanl camera with a regular pinhole depth sensor. To
transform in SFTL (Sec. III-B), which further improves Abs
our best knowledge, it provides the SoTA trade-off between
Rel by 5%.
dense depth sensing accuracy and efﬁciency comparing to
Startingat7throw,westudytheperformanceoftheCSPN
other sensing options such as multi-sensor and LiDAR.
module for depth reﬁnement, as proposed in Sec. III-C.
To successfully extend the depth map with limited FoV
Here, at the row ‘CSPN [13]‘, we ﬁrst evaluate its vanilla
to omnidirectional, we design ODE-CNN, where two crit-
version with our backbone, which is also shown to be very
ical convolutional modules are introduced, i.e.SFTL and D-
effective,reducingtheAbsRelerrorabout15%from0.0624
CSPN. Both modules embeds inverse gnomonic projection
to 0.0539. Then, at the row with ‘IG-CSPN‘, we adopt
for handling the image distortion, and a learnable deforma-
the formula of IG projection as the kernel offset in CSPN,
tion for jointly consider image context. Our results achieve
and the results are slightly improved by 1% from 0.0539
the SoTA performance (33% depth error reduction vs. the
to 0.0523. Next, at the row ‘D-CSPN‘, similar with the
baseline) over the large scale OmniDepth [7] dataset and in
‘DIGT‘, we adding learnable deformation in IG-CSPN to
the future, we will put it on device by further improve its
allow dynamic offset in its convolutional kernel, the error is
efﬁciency.
further reduce another 2% from 0.0523 to 0.0511. Finally,
we combine the most effective modules, ‘DIGT‘ and ‘D-
594
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 14:55:59 UTC from IEEE Xplore.  Restrictions apply. REFERENCES [24] Z.Yang,P.Wang,Y.Wang,W.Xu,andR.Nevatia,“Lego:Learning
edgewithgeometryallatoncebywatchingvideos,”inCVPR,2018,
[1] T.Fan,X.Cheng,J.Pan,P.Long,W.Liu,R.Yang,andD.Manocha, pp.225–234.
“Gettingrobotsunfrozenandunlostindensepedestriancrowds,”IEEE [25] J.Long,E.Shelhamer,andT.Darrell,“Fullyconvolutionalnetworks
RoboticsandAutomationLetters,vol.4,no.2,pp.1178–1185,2019. forsemanticsegmentation,”inCVPR,2015.
[2] R.A.Newcombe,S.Izadi,O.Hilliges,D.Molyneaux,D.Kim,A.J. [26] K. Matsuo and Y. Aoki, “Depth image enhancement using local
Davison, P. Kohli, J. Shotton, S. Hodges, and A. W. Fitzgibbon, tangentplaneapproximations,”inProceedingsoftheIEEEConference
“Kinectfusion: Real-time dense surface mapping and tracking.” in onComputerVisionandPatternRecognition,2015,pp.3574–3583.
ISMAR,vol.11,no.2011,2011,pp.127–136. [27] D.Miao,J.Fu,Y.Lu,S.Li,andC.W.Chen,“Texture-assistedkinect
[3] J.ZhangandS.Singh,“Loam:Lidarodometryandmappinginreal- depthinpainting,”in2012IEEEInternationalSymposiumonCircuits
time.”inRobotics:ScienceandSystems,vol.2,2014,p.9. andSystems. IEEE,2012,pp.604–607.
[4] Z.Zhang,“Microsoftkinectsensoranditseffect,”IEEEmultimedia, [28] J.Xiao,K.A.Ehinger,A.Oliva,andA.Torralba,“Recognizingscene
vol.19,no.2,pp.4–10,2012. viewpoint using panoramic place representation,” in CVPR. IEEE,
[5] E. Lachat, H. Macher, M. Mittet, T. Landes, and P. Grussenmeyer, 2012,pp.2695–2702.
“Firstexperienceswithkinectv2sensorforcloserange3dmodelling,” [29] Y.-C. Su and K. Grauman, “Making 360 video watchable in 2d:
The International Archives of Photogrammetry, Remote Sensing and Learningvideographyforclickfreeviewing,”inCVPR. IEEE,2017,
SpatialInformationSciences,vol.40,no.5,p.93,2015. pp.1368–1376.
[6] VelodyneLidar,“HDL-64E,”http://velodynelidar.com/,2018,[Online; [30] R.Monroy,S.Lutz,T.Chalasani,andA.Smolic,“Salnet360:Saliency
accessed01-March-2018]. mapsforomni-directionalimageswithcnn,”SignalProcessing:Image
[7] N. Zioulis, A. Karakottas, D. Zarpalas, and P. Daras, “Omnidepth: Communication,vol.69,pp.26–34,2018.
Densedepthestimationforindoorssphericalpanoramas,”inProceed- [31] M. Ruder, A. Dosovitskiy, and T. Brox, “Artistic style transfer for
ingsoftheEuropeanConferenceonComputerVision(ECCV),2018, videos and spherical images,” International Journal of Computer
pp.448–465. Vision,vol.126,no.11,pp.1199–1219,2018.
[8] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, [32] Y.-C. Su and K. Grauman, “Learning spherical convolution for fast
“Densely connected convolutional networks,” in Proceedings of the features from 360 imagery,” in Advances in Neural Information
IEEE conference on computer vision and pattern recognition, 2017, ProcessingSystems,2017,pp.529–539.
pp.4700–4708. [33] T. Cohen, M. Geiger, J. Ko¨hler, and M. Welling, “Convolutional
[9] P. Wang, X. Shen, B. Russell, S. Cohen, B. Price, and A. L. Yuille, networks for spherical signals,” arXiv preprint arXiv:1709.04893,
“Surge:Surfaceregularizedgeometryestimationfromasingleimage,” 2017.
inAdvancesinNeuralInformationProcessingSystems,2016,pp.172– [34] T.S.Cohen,M.Geiger,J.Ko¨hler,andM.Welling,“Sphericalcnns,”
180. ICLR,2018.
[10] F. Liu, C. Shen, G. Lin, and I. Reid, “Learning depth from single [35] R.KhasanovaandP.Frossard,“Graph-basedclassiﬁcationofomnidi-
monocular images using deep convolutional neural ﬁelds,” IEEE rectionalimages,”inICCVW,2017,pp.869–878.
transactions on pattern analysis and machine intelligence, vol. 38, [36] K.Tateno,N.Navab,andF.Tombari,“Distortion-awareconvolutional
no.10,pp.2024–2039,2015. ﬁlters for dense prediction in panoramic images,” in Proceedings of
[11] B.Coors,A.PaulCondurache,andA.Geiger,“Spherenet:Learning theEuropeanConferenceonComputerVision(ECCV),2018,pp.707–
sphericalrepresentationsfordetectionandclassiﬁcationinomnidirec- 722.
tionalimages,”inECCV,2018,pp.518–533. [37] M.Eder,T.Price,T.Vu,A.Bapat,andJ.Frahm,“Mappedconvolu-
[12] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei, tions,”CoRR,vol.abs/1906.11096,2019.
“Deformable convolutional networks,” in Proceedings of the IEEE [38] Y. Jeon and J. Kim, “Active convolution: Learning the shape of
internationalconferenceoncomputervision,2017,pp.764–773. convolutionforimageclassiﬁcation,”inCVPR,2017,pp.4201–4209.
[39] L. Deng, M. Yang, H. Li, T. Li, B. Hu, and C. Wang, “Restricted
[13] X.Cheng,P.Wang,andR.Yang,“Depthestimationviaafﬁnitylearned
deformableconvolutionbasedroadscenesemanticsegmentationusing
withconvolutionalspatialpropagationnetwork,”inProceedingsofthe
surroundviewcameras,”arXivpreprintarXiv:1801.00708,2018.
European Conference on Computer Vision (ECCV), 2018, pp. 103–
[40] K.LinandT.P.Breckon,“Real-timelow-costomni-directionalstereo
119.
vision via bi-polar spherical cameras,” in International Conference
[14] I.FrederickPearson,MapProjectionsTheoryandApplications. Rout-
ImageAnalysisandRecognition. Springer,2018,pp.315–325.
ledge,2018.
[41] F. Ma, G. V. Cavalheiro, and S. Karaman, “Self-supervised sparse-
[15] K.He,X.Zhang,S.Ren,andJ.Sun,“Deepresiduallearningforimage
to-dense:Self-superviseddepthcompletionfromlidarandmonocular
recognition,” in Proceedings of the IEEE conference on computer
camera,”2019.
visionandpatternrecognition,2016,pp.770–778.
[42] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep
[16] N.Silberman,D.Hoiem,P.Kohli,andR.Fergus,“Indoorsegmenta-
networktrainingbyreducinginternalcovariateshift,”inInternational
tionandsupportinferencefromrgbdimages,”inEuropeanConference
ConferenceonMachineLearning,2015,pp.448–456.
onComputerVision. Springer,2012,pp.746–760.
[43] V. Nair and G. E. Hinton, “Rectiﬁed linear units improve restricted
[17] A.Geiger,P.Lenz,C.Stiller,andR.Urtasun,“Visionmeetsrobotics:
boltzmann machines,” in Proceedings of the 27th international con-
The kitti dataset,” The International Journal of Robotics Research,
ferenceonmachinelearning(ICML-10),2010,pp.807–814.
vol.32,no.11,pp.1231–1237,2013.
[44] X.Cheng,P.Wang,andR.Yang,“Learningdepthwithconvolutional
[18] H. Hirschmu¨ller, “Accurate and efﬁcient stereo processing by semi-
spatialpropagationnetwork,”arXivpreprintarXiv:1810.02695,2018.
global matching and mutual information,” in null. IEEE, 2005, pp.
[45] I. Armeni, S. Sax, A. R. Zamir, and S. Savarese, “Joint 2d-
807–814.
3d-semantic data for indoor scene understanding,” arXiv preprint
[19] D.Eigen,C.Puhrsch,andR.Fergus,“Depthmappredictionfroma
arXiv:1702.01105,2017.
singleimageusingamulti-scaledeepnetwork,”inAdvancesinneural
[46] A.Chang,A.Dai,T.Funkhouser,M.Halber,M.Niessner,M.Savva,
informationprocessingsystems,2014,pp.2366–2374.
S.Song,A.Zeng,andY.Zhang,“Matterport3d:Learningfromrgb-d
[20] F. Liu, C. Shen, and G. Lin, “Deep convolutional neural ﬁelds for
datainindoorenvironments,”arXivpreprintarXiv:1709.06158,2017.
depth estimation from a single image,” in Proceedings of the IEEE
[47] S.Song,F.Yu,A.Zeng,A.X.Chang,M.Savva,andT.Funkhouser,
Conference on Computer Vision and Pattern Recognition, 2015, pp.
“Semantic scene completion from a single depth image,” in Pro-
5162–5170.
ceedings of the IEEE Conference on Computer Vision and Pattern
[21] P. Wang, X. Shen, Z. Lin, S. Cohen, B. L. Price, and A. L. Yuille,
Recognition,2017,pp.1746–1754.
“Towardsuniﬁeddepthandsemanticpredictionfromasingleimage,”
[48] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and
inCVPR,2015.
M.Nießner,“Scannet:Richly-annotated3dreconstructionsofindoor
[22] Z. Zhang, Z. Cui, C. Xu, Y. Yan, N. Sebe, and J. Yang, “Pattern-
scenes,”inProceedingsoftheIEEEConferenceonComputerVision
afﬁnitive propagation across depth, surface normal and semantic
andPatternRecognition,2017,pp.5828–5839.
segmentation,”inCVPR,2019,pp.4106–4115.
[23] X. Song, X. Zhao, H. Hu, and L. Fang, “Edgestereo: A context
integrated residual pyramid network for stereo matching,” in Asian
ConferenceonComputerVision. Springer,2018,pp.20–35.
595
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 14:55:59 UTC from IEEE Xplore.  Restrictions apply. 