2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
Learning Generalizable Locomotion Skills with
Hierarchical Reinforcement Learning
Tianyu Li1, Nathan Lambert2, Roberto Calandra1, Franziska Meier1, Akshara Rai1
Abstract—Learning to locomote to arbitrary goals on hard-
wareremainsachallengingproblemforreinforcementlearning.
In this paper, we present a hierarchical framework that
improves sample-efﬁciency and generalizability of learned lo-
comotionskillsonreal-worldrobots.Ourapproachdividesthe
problem of goal-oriented locomotion into two sub-problems:
learning diverse primitives skills, and using model-based plan-
ning to sequence these skills. We parametrize our primitives
as cyclic movements, improving sample-efﬁciency of learning Fig. 1: The hexapod Daisy used in the experiments. Using our
fromscratchona18degreesoffreedomrobot.Then,welearn hierarchical control framework, Daisy learns to reach goals as far
coarse dynamics models over primitive cycles and use them as 12 meters in 2 hours of training from scratch.
in a model predictive control framework. This allows us to
time or with minimum energy. There is a surprising lack
learn to walk to arbitrary goals up to 12m away, after about
two hours of training from scratch on hardware. Our results of learning literature that address the problem of reaching
on a Daisy hexapod hardware and simulation demonstrate the arbitrary goals, while there are multiple optimal control
efﬁcacyofourapproachatreachingdistanttargets,indifferent papersthataddressthis[9],[10].Thisisbecausegeneralizing
environments, and with sensory noise.
to unseen, arbitrary goals often requires a dynamics model.
However,manyoptimalcontrolalgorithmsarealsosensitive
I. INTRODUCTION
to modeling inaccuracies in dynamics and their performance
Reinforcement Learning (RL) can help robots general-
can suffer with poor models [9].
ize to unseen scenarios, and achieve novel tasks. In loco-
Model-free learning methods like [11] can generalize to
motion, there has been success in using RL to learn to
goals in the space explored during learning, but do not
walk in simulation [1], [2], [3], [4], but examples of RL
generalize well to arbitrary goals. Model-based RL holds
on locomotion hardware are rare. This is due to multiple
the promise of generalizing to new goals, but it is largely
reasons, such as sample inefﬁciency of RL methods, lack
validated in simulation [12]. [13] point out that learning
of robust locomotion platforms, challenging dynamics, and
dynamicsmodelsinthemodel-basedRLloopischallenging
high-dimensional robots. However, locomotion skills are
and might need specialized exploration. As a result, there is
importantforautonomousagentstoaccomplishtasksoutside
little to no evidence of learning to reach arbitrary goals in
of their workspace, such as clean a room, pick a far-away
locomotion literature on hardware.
object. For navigating uneven terrains, stairs, etc. legged
In this work, we improve sample-efﬁciency of RL on
platforms become important.
locomotion by using a cyclic parametrization of walking
Inthiswork,weaddresstwoofthemainchallengesfacing
policies, similar to [14], [15], [12]. We learn the parameters
learning for legged locomotion research - sample efﬁciency
of these policies using a model-free RL algorithm, Soft
and generalization. Typical examples of RL on locomotion
Actor Critic [16], from scratch on a 18 degree of freedom
platformsinvolvelearningconservativepoliciesinsimulation
hexapod robot. This cyclic structure is capable of achieving
and deploying them on hardware [5], [6], [7]. However,
manydifferentlocomotionbehaviorsandgaitswithoutexpert
the learned policy might not be efﬁcient on hardware, and
intervention, as demonstrated in our experiments.
might frequently fail [6]. This motivates learning directly on
Further, we improve generalization to multiple goals by
hardware. [8] demonstrate learning to walk on a Minitaur
proposing a hierarchical structure. We divide the problem
robot from scratch, but training on a higher degree of
of goal-oriented locomotion into two sub-problems: ﬁrst we
freedom robot can be very expensive, and most locomotion
learn temporally extended action primitives that can achieve
platformscannotwithstandsuchextendeduse.Moreover,[8]
simple goals such as turning and walking straight, using
do not generalize to targets other than walking forward.
model-free RL. Next, we build ‘coarse’ dynamics models
In fact, most works on RL for locomotion try to learn
of these primitives and use them for planning using model
to walk forward, but, realistic tasks for locomotion would
predictive control. Coarse dynamics models are ﬁt over
involve reaching a particular goal, in the shortest amount of
transitions over one cycle of primitive actions. This allows
1Facebook, Menlo Park, CA, USA, 2Work done during an internship us to build dynamics models with very small amount of
at Facebook AI Research, Department of Electrical Engineering hardware data, and plan efﬁciently in primitive space. An
and Computer Sciences, University of California, Berkeley, USA
{ } overview of our algorithm is shown in Figure 2. Note that
tianyul,rcalandra,fmeier,akshararai @fb.com,
nol@berkeley.edu the different levels of our hierarchy are not learned together,
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 413
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:42:55 UTC from IEEE Xplore.  Restrictions apply. and interact only in test time. As a result, either of the two
tiers could be replaced, such as with model-based low-level
controllers.
Our main contribution is a hierarchical framework that
combines model-free learning with model-based planning to
improves generalization of locomotion skills to new goals.
Our approach is easy to train, and robust to hardware noise.
We demonstrate our results on a Daisy hexapod (Figure 1)
over multiple targets up to 12m away, starting with training
on very short episodes. To the best of our knowledge, this is
the ﬁrst demonstration of such a hybrid model-free learning
withmodel-basedplanningframeworkonalocomotionrobot
hardware. Our results show that such a combination of the
two approaches can greatly improve the sample-efﬁciency
and generalization abilities of RL methods for locomotion.
Fig. 2: Hierarchical Control Flow Chart. Our low-level control
II. BACKGROUNDANDRELATEDWORK contains learned primitives. High-level control switches between
different primitives applied to the robot based on the goal and
Here, we present a brief overview of model-based and currentglobalstate.WeusetheViveTrackingsystemformeasuring
model-free optimization methods from literature and previ- the global state.
ous works that are closely related to our work.
MPC has been widely used for control of dynamical
A. Model-based and model-free optimization systems,[19],[20],[21].[10],[22]useMPCforcontrollinga
humanoid robot’s center of mass dynamics. However, these
We consider a Markov Decision Process with actions a,
works assume a known dynamics model and are sensitive
state s and dynamics governed by transition function f .
φ to dynamics modeling errors. As a result, they are hard to
Startingfromaninitialstates ,andsamplinganactiona at
0 t generalize to new tasks or robots.
statest accordingtopolicyπ,theagentgetsarewardr(st,at) 3) Hierarchical RL (HRL) with primitives: Using a hi-
and transitions to{the next state··s·t}+1= fφ(st,at), generating erarchical structure that decomposes complex task control
a trajectory τπ = s0,a0,s1,a1, . into easier sub-tasks control can speed up learning [23].
In planning and control, the objective is to maximize the
Previous works studied learning the different levels of the
cumulativerewardJπ=∑tT=0Eτπ[r(st,at)],where,τπ denotes hierarchytogether[24],[25],[26].Analternativeistodivide
thetrajectorydistributiongeneratedbypolicyπ.Thiscanbe
the task into learning primitives, followed by planning in
doneinamodel-freemanner[17]orusinginformationfrom
primitivespace,whileﬁne-tuningprimitives[26],[27],[28],
the dynamics f in a model-based way.
θ [2], [29]. However, most HRL literature is model-free and
1) Model-freereinforcementlearning: Model-freeRLop-
hence sample inefﬁcient. For example, [1] needs over a
timizes a policy π by directly maximizing the long-term
million samples to learn high-level control.
reward, without reasoning about the dynamics. Model-free
We combine model-based planning and model-free learn-
methods are typically less sample-efﬁcient that model-based
ing, by using model-free RL for learning action primitives,
but achieve better asymptotic performance. Our model-free
andsequencingthemusingmodel-basedplanning.Byincor-
learningalgorithmofchoiceisSoftActorCritic(SAC)[16],
porating dynamics models in HRL, we can improve sample-
which is a maximum entropy RL algorithm that maximizes
efﬁciency as well as generalization.
both the long-term reward and the entropy of the policy.
For a ﬁnite horizon MDP, the SAC objective function is: B. Learning for locomotion
Jπ =∑tT=0Eτπ[r(st,at)−αtlogπ(at|st)] where, α is the tem- Using Deep RL in locomotion system has been wildly
perature that trades off between reward and entropy. studied in simulation. [1], [30], [?] used hierarchical RL
SAC is an off-policy algorithm that allows re-using past to achieve challenging locomotion tasks in simulation such
data to update current policy, improving sample-efﬁciency. as moving a soccer ball and carrying an object to a goal.
It has been demonstrated to work on locomotion hardware [4] used deep RL to train locomotion systems in different
from scratch in [8], and hence we decided to use it. training environments, and found new emergent gaits. [31]
2) Model Predictive Control (MPC): An alternative to showthatrobustlocomotioncanevenbeachievedfromhigh-
model-free RL is to utilize the dynamics (if known) to dimensional inputs, such as images. However, since these
maximize the long term reward of a trajectory. One such methods take millions of samples, they are not usable on
popular approach is MPC, also known as receding horizon hardware without modiﬁcations.
control. MPC solves for an action sequence a that max- [12], [32], [33], [34] use a cyclic controller structure
0:T
imizes the long-term reward J=∑tT=0E[r(st,at)] subject to similartoours,andusemodel-freepolicysearchapproaches
the dynamics st+1= fφ(st,at) at each instant [18]. The ﬁrst tolearnlocomotionskills.Thesemethodseffectivelytransfer
action a is applied on the system, and the process repeats. information between different tasks, or from simulation and
0
414
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:42:55 UTC from IEEE Xplore.  Restrictions apply. hardware. However, they are limited to a relatively low- Here q are the current joint angles; q˙ is a feedforward
ff
dimensional parametric controller, and can be hard to gen- velocity. k and k are hand-tuned feedback gains.
p d
eralize to new robots. On the other hand, [5], [6] used Deep
B. Action Primitive Representations
RL to train unstructured neural network policies in simula-
We take inspiration from biological gaits in locomotion
tion and transfer them to hardware successfully. However,
andusetwocyclicparametrizationsforouractionprimitives
policies that perform well in simulation do not perform well ∈
on hardware due to differences between simulation and the πi. Our primitives take as input a phase variable t (0,1]
and predict the next desired joint conﬁguration as an action,
real world, such as contact models.
Instead of training in simulation and transferring policies qdes = πi(t). At the beginning of every cycle, the phase
variableisinitializedto0,andthengrowslinearlyto1,with
to the real-world, [8], [35] directly trained policies in the
the length of the cycle designed by the expert. This also
real world on a Minitaur robot. [8] used SAC, and [35] used
allows us to change the speed of our primitive, for example
model-basedRLwithtrajectorygeneratorstotrainaMinitaur
whentrainingweuseaslowerprimitiveforthesafetyofour
robottowalkforward.Minitaurhas8motorsthatcontrolits
robot, but when testing, we increase the frequency for better
longitudinal motion, and no control for lateral movements.
performance.
In comparison, our hexapod (Daisy) has omni-directional
This idea of periodic gaits was also used in [36], [37],
movements and 18 motors. This makes the problem of
[15], but these works designed the primitives manually.
controlling Daisy especially challenging, and would require
Instead, here we consider parametric policies, and learn the
signiﬁcantly longer training. Moreover, previous work only
parametersusingamodiﬁedSAC,describedinSectionIII-C.
learns to walk forward, and needs additional training to
We consider 2 types of parametrizations for our primitives:
achieve new goals. Our approach can learn to control Daisy
1) Neural Network Policy: An unstructured neural net-
and achieve arbitrary goals, with 2 hour of training from
work controller. The input to this network is the 1-
scratch on hardware. Such sample-efﬁciency is important as
dimensional phase t and the output are the 18 desired joint
most locomotion robots get damaged from wear and tear
angles. The neural network consists of 2 hidden layers with
when operated for long. For example, in the course of our
64 nodes and a Relu activation function. We also add tanh
experiments, we had to replace two motors.
to the output layer to saturate the outputs.
2) Sinusoidal Policy: A structured parametric controller,
III. LEARNINGGENERALIZABLELOCOMOTIONSKILLS
which consists of sine waves in each joint: qides(t) =
Wenowdescribeourproposedapproachindetail.Figure2 Aisin(2πt+Bi)+Ci.Eachmotor jhasanindependentphase
−
shows an overview of the hierarchical control structure B ,offsetC andamplitudeA forthei thprimitive,lead-
ij ij ij
proposed in this work. In a nutshell, our approach builds a ing to a total 54 dimensional controller π. The parameters
i
library of primitives L =(π0,π1,π2,π3) that encode low- of this controller are also learned using modiﬁed SAC.
level controllers for 4 micro-actions turn left, turn right,
C. Soft Actor-Critic with KL Constraint
move forward, stand still. These primitives are learned via
While maximum entropy in SAC makes the learning on
model-free reinforcement learning. On a higher level, our
approachdependsonamodel f thatpredictsthedynamicsof hardware robust and sample-efﬁcient, sometimes it leads to
aggressive policy updates that might harm our robot. Hence,
applyingacycleoftheprimitive.Amodel-predictiveplanner
we add an additional practical constraint. We introduce a
utilizes this model to optimize for the next optimal action
KL divergence constraint from the previous policy, similar
sequence to achieve a goal. In the following we start by
to Trust region policy optimization (TRPO) [38]. Now the
introducing notation and our experimental platform, we then
objective function for updating policy π is expressed as:
ptorolpeoarsnettwheomreapnredseﬁnntaaltliyondsefsocrritbheetahcetiohnigphr-ilmevietilvepslaannnderh.ow Jπi=∑tT=0Eτπi[rt−αtlogπi(t)]+ε DKL(πi(it)||πi,old(t)).This
costencouragesentropy-basedexploration,whilekeepingthe
A. Daisy - Hexapod updated policy close to the last policy, leading to more con-
servative policy updates that still explore. We use the SAC
Our test platform in this paper is the Daisy Hexapod
implementation from [39], and modify it for our purposes.
(Figure 1). Daisy is a six-legged robot with three motors
on each leg - base, shoulder, and elbow. The robot is omni- D. High-Level Control: Model-based Control
directional,andthecenterofmasscanmoveinanydirection, We use a model-based high-level planner that plans the
but the mass of the motors limits the leg velocity. A Vive best primitive sequence u1:H for our horizon H using MPC.
tracking system is used to measure robot’s position in the The dynamics used in this planning are learned over the
global frame. whole primitive cycle, rather than the instantaneous dynam-
The robot has 18 motors that we control by sending ics,i.e,sth+T = fφ(st,πuh)isthenextstateafterexecutingthe
desiredmotorvelocitiesaslow-levelactions.Thestatesused primitive π for T time steps, starting from s. This leads
uh t
in the high-level planner is the center of mass position and to a ‘coarse’ transition model learned over extended action
orientation. The low-level policies output 18 desired joint sequences, rather than per time step transitions. Moreover,
angles q , which are then converted into desired motor the planning is in a much reduced space of primitive actions
des
− −
velocitiesinafeedbackloop:q˙des=kp(qdes q) kdq˙+q˙ff. instead of the whole action space.
415
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:42:55 UTC from IEEE Xplore.  Restrictions apply. w w w
Algorithm 1: Hierarchical Reinforcement Learning 1 2 3
sim-forward [-1,5,-0.1] [0.1,0.1,0.1] [0.01]
Deﬁne primitives π and rewards r sim-turn [0.1,0.1,0.1] [0.1,0.1,20] [0.002]
1:K 1:K
for each primitive do hw-forward [-50,300,-10] [1,1,1] [0.01]
hw-turn [1,1,1] [0.1,0.1,40] [0.002]
for each environment step do
at ∼πi(at|st) TABLE I: Low-level reward function weights
Apply action a, measure s
← ∪{ t }t+1
D D (st,at,st+1,rt1:K) We train the move forward and turn right primitives in
for each gradient step do
sequence, starting with move forward. The parameters of
Update Q, π
Given primitive liibrairy L ={π1,···πK}, cycle time T the move forward policy are initialized randomly, and the
for each dynamics learning step do trainingdataisusedtoinitializetrainingofturnrightpolicy.
for each primitive π do In simulation both primitives are trained for 50 iterations
at ∼πi(at|st) i using the algorithm described in Section III-C.
Apply action a, measure s For training the move forward policy, we used the reward
(cid:48)← (cid:48)∪{ t } t+1
D D (s0,πi,sT) function − | |− | |
Le←arn dynamics model sT = fφ(s0,πi) r=www1δxxxcom www2θθθcom www3qqq˙joint , (1)
s s
0 T
Given reward r , primitive library L, horizon H, s where the ﬁrst term gives reward for moving forward and
hl 0
for each planning step do penalty for lateral and backward movements, the second
u1:H =argminrhl(s0,u1:H) term tries to minimize deviation in orientation, and the third
Apply primitive π , measure s penalizes for high joint velocities.
← u1 T
s s After training the move forward policy, we switch to
0 T
training the turn right policy. We reuse the data collected
in the ﬁrst training phase to initialize the parameters of the
turnrightpolicy.SinceSACisanoff-policymethod,wecan
Starting from the current center of mass position and
orientation st =(xcom,θcom), our high level planner does an re-evaluate the reward of each transition on the turn right
reward function and restart training. The reward function to
exhaustive search over the possible sequences of actions to
train the turn policy was
ﬁnd the globally optimal sequence for our horizon H =3.
− | |− | − |− | |
Moreover, to further simplify the dynamics, we learn a r= www1 δxxxcom www2θθθcom θθθdes www3qqq˙joint . (2)
delta dynamics model δs = fθ(πi), which reasons about This reward function penalizes the movement of the center
the change in the state after the execution of the primitive.
ofmassinanydirection.Foreachprimitivecycle,weassign
This makes the dynamics learning much more efﬁcient, and
a desired orientation for the robot. Lastly, we penalize high
generalize to unseen states.
joint velocity for the safety of our robots. Intuitively, this
IV. EXPERIMENTALRESULTS reward functions encodes that the optimal turning behavior
In the following we present evaluations on the Daisy istoturnonthespotataconstantspeed.Theparametersfor
hexapod both in simulation and hardware. Our experiments reward functions for training are shown in Table I.
are structured as follows: Our simulation training results for the neural network
• Learning of primitives: We train two primitive actions and sinusoidal controller are shown in Figure 3a, 3b. In
simulation, for the forward task, the neural network learns
on Daisy : walk forward, and turn. During training, the
faster than the sinusoidal controller, and the reward is also
total steps in a cycle is 100, and we sample 10 cycles
higher than the sinusoidal controller. This is because the
for each iteration, hence 1000 samples per iteration.
• High-level control: For experiments with the high-level groundcontactmodelsinthesimulationareveryinaccurate,
and with the neural network controller, the optimization
control, we use MPC for planning in the space of
quickly learns to exploit it by sliding. Since turning is a
trained primitives. We set targets far away from the
more controlled task with a target orientation, it is harder
training region of the primitives, and reach them using
to exploit the simulation and both controllers learn at a
the hierarchy.
comparablerate,withthesinusoidalcontrollerhavingamore
A. Simulation Experiments stable learning curve.
WesimulatetheDaisyrobotinPyBullet[40].Westartby 2) High-level control: Once the primitive actions are
describing our experimental setup for learning the low-level trained, we can move to the high-level control. We start
primitives in simulation. by training a dynamics model for each primitive by simply
1) Learning Primitives: We decompose locomotion be- building a look-up table for δs= f(πi). The look-up table
haviors into four elementary motions: move forward, turn is trained by sampling 50 cycles of randomly selected prim-
left, turn right, and stand-still. Since turning right can be itives and averaging the resultant displacement, as described
achieved by mirroring the control of turning left, we do not in Algorithm 1.
needtotrainanewpolicy;forstandingstillthedesiredjoint Once the look up table has been created, we utilize the
state is the current joint state. model within MPC to optimize the sequence of actions that
416
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:42:55 UTC from IEEE Xplore.  Restrictions apply. (a) Simulation: Forward (b) Simulation: Turning (c) Different Goals (d) Waypoint Goals
Fig.3:(a,b)Simulationtrainingplotforforwardturningcontrollers.Wecollect1000samplesperiteration.(c,d)Simulationexperimental
results of Daisy reaching different goals. (c) Comparison of our approach vs. PETS for achieving different goals starting from (0,0). (d)
Daisy moving to the corners of a square starting from (0,0).
minimizesthecostoverahorizonofH=3.Weapplytheﬁrst targets, despite slipping in simulation. In comparison, while
actionfromthisoptimizedsequenceontherobot,andreplan. PETSisabletoreachthegoalthatthedynamicswerelearned
The reward for the high-level control r is episodic, the on efﬁciently, it does not generalize to other goals in the
hlc,sim
ﬁnal distance between the robot and goal at the end of the environment. Since PETS with SAC data is only trained on
−| − |
horizon rhlc,sim= xgoal xcom . very short episodes, it is also unable to achieve far away
In simulation, we compare the high-level control against goals. Hence, the hierarchy helps improve generalization to
PETS [3], a state-of-the-art model-based RL baseline. We new goals, when trained with the same amount of data as
compare against two versions of PETS: PETS, a model-based RL approach.
• PETS : We train the full dynamics model of the robot
while trying to achieve a goal, in the standard PETS B. Hardware Experiments
loop. Then we do MPC with cross-entropy method
Simulation experiments allowed us to test the validity of
(CEM) using the trained dynamics to achieve other
our approach, but did not have an accurate contact model
goals, far away from the goal for which the dynamics
with the ground. The neural network controllers trained
was trained.
• PETSwithSACdata:Wetrainthefulldynamicsmodel in simulation performed very poorly on hardware because
of this mismatch, making it essential to train directly on
on data that was used for training the forward and
hardware.
turning controllers. This dynamics includes turning and
walking data, but for a very small part of the robot’s 1) Learning Primitives: For hardware experiments, we
space.Thegoalsaresetquitefarawayfromthetraining used the same formulation of reward as in simulation but
set, and MPC+CEM is used to optimize the action. with slightly different weights in rewards, as summarized
in Table I. The parameters of the move forward policy
We note that the dynamics trained for PETS comparison
are initialized randomly, and the training data is used to
are on the full state of the robot (18 joint angles), and
initialize training of turn right policy. We trained forward
the action is an optimized sequence of 18 desired joint
and turning policies on hardware, and their learning curves
velocities. As compared to our hierarchical framework, this
are shown in Figure 4. We used 20000 samples to train the
is a much higher dimensional optimization problem. We
forward controller which took approximately an hour and
do not compare our method against classical model-based
15000 samples to train the turning controller which took
approaches because we assume that we do not have access
about45minutes.Althoughinsimulationtheneuralnetwork
to the true dynamics of the robot.
trains faster than the sinusoidal controller, we were not
In simulation, we test two experimental settings:
successfulintraininganeuralnetworkpolicyfromscratchon
1) Different goals: The goals are at (5,0), (5,5), (0,5),
− − hardware, possibly due to noise in reward generation. Since
( 5,5), ( 5,0) starting from (0,0) (Figure 3c). Both
the sinusoidal controller is restricted in space of controllers,
the neural network and sinusoidal controllers can
in our experience, it was more robust to noise in reward
achievealltargetsusingourapproach.BaselinesPETS
signals, as compared to the neural network controller. The
and PETS trained on SAC data fail to achieve goals
trained sinusoidal forward controller can walk straight and
other than the one they were trained on.
the turning controller can turn left with a small turning
2) Waypoint goals: The robot has to achieve targets in
− − radius. In the future, we would like to study if the neural
a square at (0,4),( 4,4),( 4,0) sequentially, starting
network policy can be trained in simulation using domain
from (0,0) (Figure 3d). Both the neural network and
randomization, and then transferred to the robot.
sinusoidal controllers can achieve all targets using our
2) High-Level Controller: On hardware, we add an ori-
hierarchical control structure. This setting is similar to
entation term to the high level reward, because the position
waypoint goals, where the robot sequentially moves
sensing tends to drift over time, and the robot fails to reach
between targets.
the global goal without orientation guidance.
Inboththeseexperiments(Figure3c,3d),thehierarchical
−| − |−| |
control performs well, and the robot is able to reach the rhlc,hw= xgoal xcom <θgoal,θcom> (3)
417
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:42:55 UTC from IEEE Xplore.  Restrictions apply. (a) Training: Forward (b) Training: Turning (c) Different Goals (d) Waypoint Goals
Fig. 4: (a,b) Hardware learning curves for forward and turning controller (collecting 500 samples per iteration). (c,d) Hardware
experimental results of Daisy reaching different goals. (c) Daisy achieving different goals starting from (0,0). (d) Daisy moving to
the corners of a square starting from (0,0). Dotted lines indicate poor tracking.
Fig. 5: A time lapse of Daisy walking towards different goals.
Here, the ﬁrst term is the distance term, same as in simu- reaching far away goals. However, the performance of the
lation, and the second term measures the deviation of the action primitives can be improved on hardware, for example
center of mass orientation from the goal angle. We start by the forward primitive moves at about 0.15m/s. An online
building the dynamics models of the each primitive in the updating scheme that ﬁne-tunes the primitives and their
primitivelibraryL.Eachdynamicsistrainedfor50samples dynamicsmodelsforanewsettingcanimproveperformance
on hardware, leading to a total of 200 samples. on new ﬂoors.
FortestingouralgorithmontheDaisyrobot,wedesigned
V. CONCLUSION
a similar setup as simulation, where Daisy was commanded
Inthiswork,weproposedahierarchicalstructureforcon-
to go to goals up to 12m away from its start point. While
trolling locomotion robots. We decomposed the problem of
our method can generalize to arbitrarily far away locations,
learning locomotion skills into two sub-problems – learning
currently our hardware setup is limited by the sensing of
low-level locomotion skills, followed by sequencing them
Vive tracking system for global position of Daisy; our goals
in a model-based way. Our experiments on the Daisy robot
are limited to be in the region covered by the base stations.
show that such a decomposition can lead to very sample-
Despite this, sometimes the robot loses tracking during the
efﬁcient learning of generalizable locomotion skills. Using
experiments,andthehigh-levelactionishard-codedtostand
our approach, Daisy can reach goals up to 12m away from
stilluntilthetrackingisrecovered.Wetesttwoexperimental
its start location with only 2 hours of training from scratch
settings on hardware:
onhardware.Inthefuture,thesewaypointscanbegenerated
1) Different goals: The robot has to move to goals
by a separate controller that takes the environment state as
−
( 1.5,3),(0,3),(2,3.5) starting from (0,0) (Figure input, for example with an image.
4c). The sinusoidal controller can reliably achieve all Ourworkisasteptowardsbuildinggeneralizablelocomo-
targets despite slipping on the ground and measure- tion skills that can reach arbitrary goals in unknown envi-
ment noise. ronments.However,therearemanyavenuesforimprovement
2) Waypoint goals: The robot is sequentially asked to overourcurrentperformance.Thelow-levelprimitives,when
move to a series of goals. Similar to simulation, the trained and tested on different environments can have very
robot has to reach corners in a square, starting from different performance. For example, they might slip on a
(0,0) (Figure 4d). Our approach easily generalizes to slippery ﬂoor, or walk too conservatively. While the high-
this setting. In the future, these hand-designed goals levelcontrolhelpsachievetargetsdespitethesedisturbances,
can be replaced by a waypoint controller. performance can be improved by updating the primitives
Our hardware experiments show that our proposed hier- locally for different environments. Additionally, there might
archical controller can achieve far away goals using very be a need to discover new primitives for new settings. For
small amount of training data on hardware. It generalizes to example, if a leg breaks, or in the presence of stairs, the
different scenarios, as well as different experimental settings current library of primitives might not be enough to achieve
like different ﬂooring, sensing noise, etc. Though we could a goal. In such cases, one could try to incrementally learn
nottraintheneuralnetworkpolicysuccessfullyonhardware, new primitives for achieving new targets, and store them in
we achieved reliable success with the sinusoidal policy at thelibraryforfuturereusability.Weleavethistofuturework.
418
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:42:55 UTC from IEEE Xplore.  Restrictions apply. REFERENCES [23] R.S.Sutton,D.Precup,andS.Singh,“Betweenmdpsandsemi-mdps:
A framework for temporal abstraction in reinforcement learning,”
[1] X. B. Peng, G. Berseth, K. Yin, and M. Van De Panne, “Deeploco: Artiﬁcialintelligence,vol.112,no.1-2,pp.181–211,1999.
Dynamic locomotion skills using hierarchical deep reinforcement [24] C. Daniel, H. Van Hoof, J. Peters, and G. Neumann, “Probabilistic
learning,”ACMTransactionsonGraphics(TOG),vol.36,no.4,p.41, inferencefordeterminingoptionsinreinforcementlearning,”Machine
2017. Learning,vol.104,no.2-3,pp.337–357,2016.
[2] K.Frans,J.Ho,X.Chen,P.Abbeel,andJ.Schulman,“Metalearning [25] P.-L.Bacon,J.Harb,andD.Precup,“Theoption-criticarchitecture,”
sharedhierarchies,”arXivpreprintarXiv:1710.09767,2017. 2016.
[3] K. Chua, R. Calandra, R. McAllister, and S. Levine, “Deep rein- [26] F. Stulp and S. Schaal, “Hierarchical reinforcement learning with
forcementlearninginahandfuloftrialsusingprobabilisticdynamics movementprimitives,”in201111thIEEE-RASInternationalConfer-
models,”inAdvancesinNeuralInformationProcessingSystems,2018, enceonHumanoidRobots. IEEE,2011,pp.231–238.
pp.4754–4765. [27] C.Daniel,G.Neumann,O.Kroemer,andJ.Peters,“Learningsequen-
[4] N.Heess,S.Sriram,J.Lemmon,J.Merel,G.Wayne,Y.Tassa,T.Erez, tialmotortasks,”in2013IEEEInternationalConferenceonRobotics
Z.Wang,S.Eslami,M.Riedmilleretal.,“Emergenceoflocomotion andAutomation. IEEE,2013,pp.2626–2632.
behaviours in rich environments,” arXiv preprint arXiv:1707.02286, [28] X. B. Peng, G. Berseth, K. Yin, and M. van de Panne, “Deeploco:
2017. Dynamic locomotion skills using hierarchical deep reinforcement
[5] J. Tan, T. Zhang, E. Coumans, A. Iscen, Y. Bai, D. Hafner, S. Bo- learning,” ACM Transactions on Graphics (Proc. SIGGRAPH 2017),
hez, and V. Vanhoucke, “Sim-to-real: Learning agile locomotion for vol.36,no.4,2017.
quadrupedrobots,”arXivpreprintarXiv:1804.10332,2018. [29] J. Merel, A. Ahuja, V. Pham, S. Tunyasuvunakool, S. Liu, D. Tiru-
[6] T.Li,H.Geyer,C.G.Atkeson,andA.Rai,“Usingdeepreinforcement mala, N. Heess, and G. Wayne, “Hierarchical visuomotor control of
learning to learn high-level policies on the atrias biped,” in 2019 humanoids,”arXivpreprintarXiv:1811.09656,2018.
InternationalConferenceonRoboticsandAutomation(ICRA). IEEE, [30] X. B. Peng, M. Chang, G. Zhang, P. Abbeel, and S. Levine,
2019,pp.263–269. “Mcp: Learning composable hierarchical control with multiplicative
[7] J. Hwangbo, J. Lee, A. Dosovitskiy, D. Bellicoso, V. Tsounis, compositional policies,” CoRR, vol. abs/1905.09808, 2019. [Online].
V. Koltun, and M. Hutter, “Learning agile and dynamic motor skills Available:http://arxiv.org/abs/1905.09808
forleggedrobots,”ScienceRobotics,vol.4,no.26,p.eaau5872,2019. [31] X. B. Peng, G. Berseth, and M. van de Panne, “Terrain-adaptive
[8] T. Haarnoja, A. Zhou, S. Ha, J. Tan, G. Tucker, and S. Levine, locomotion skills using deep reinforcement learning,” ACM Trans.
“Learning to walk via deep reinforcement learning,” arXiv preprint Graph.,vol.35,no.4,pp.81:1–81:12,Jul.2016.[Online].Available:
arXiv:1812.11103,2018. http://doi.acm.org/10.1145/2897824.2925881
[9] S. Feng, “Online hierarchical optimization for humanoid control,” [32] R.Antonova,A.Rai,T.Li,andD.Kragic,“Bayesianoptimizationin
2016. variational latent spaces with dynamic compression,” arXiv preprint
[10] S. Mason, N. Rotella, S. Schaal, and L. Righetti, “An mpc walking arXiv:1907.04796,2019.
frameworkwithexternalcontactforces,”in2018IEEEInternational [33] J. Andre´, C. Teixeira, C. P. Santos, and L. Costa, “Adapting biped
Conference on Robotics and Automation (ICRA). IEEE, 2018, pp. locomotiontoslopedenvironments,”JournalofIntelligent&Robotic
1785–1790. Systems,vol.80,no.3-4,pp.625–640,2015.
[11] M.Andrychowicz,F.Wolski,A.Ray,J.Schneider,R.Fong,P.Welin- [34] A. Cully, J. Clune, D. Tarapore, and J.-B. Mouret, “Robots that can
der,B.McGrew,J.Tobin,O.P.Abbeel,andW.Zaremba,“Hindsight adaptlikeanimals,”Nature,vol.521,no.7553,pp.503–507,2015.
experience replay,” in Advances in Neural Information Processing [35] Y. Yang, K. Caluwaerts, A. Iscen, T. Zhang, J. Tan, and V. Sind-
Systems,2017,pp.5048–5058. hwani,“Dataefﬁcientreinforcementlearningforleggedrobots,”arXiv
preprintarXiv:1907.03613,2019.
[12] B.Yang,G.Wang,R.Calandra,D.Contreras,S.Levine,andK.Pister,
[36] Y.Fukuoka, Y. Habu,and T.Fukui, “Analysisof thegait generation
“Learning ﬂexible and reusable locomotion primitives for a micro-
principle by a simulated quadruped model with a cpg incorporating
robot,” IEEE Robotics and Automation Letters (RA-L), vol. 3, no. 3,
vestibular modulation,” Biological cybernetics, vol. 107, no. 6, pp.
pp.1904–1911,2018.
695–710,2013.
[13] S. Bechtle, A. Rai, Y. Lin, L. Righetti, and F. Meier, “Curi-
[37] H. Kimura, Y. Fukuoka, and A. H. Cohen, “Biologically inspired
ous ilqr: Resolving uncertainty in model-based rl,” arXiv preprint
adaptive walking of a quadruped robot,” Philosophical Transactions
arXiv:1904.06786,2019.
of the Royal Society A: Mathematical, Physical and Engineering
[14] A. Crespi, D. Lachat, A. Pasquier, and A. J. Ijspeert, “Controlling
Sciences,vol.365,no.1850,pp.153–170,2006.
swimming and crawling in a ﬁsh robot using a central pattern
[38] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust
generator,”AutonomousRobots,vol.25,no.1-2,pp.3–13,2008.
region policy optimization,” in International conference on machine
[15] D.OwakiandA.Ishiguro,“Aquadrupedrobotexhibitingspontaneous
learning,2015,pp.1889–1897.
gait transitions from walking to trotting to galloping,” Scientiﬁc
[39] D.Yarats,A.Zhang,I.Kostrikov,B.Amos,J.Pineau,andR.Fergus,
reports,vol.7,no.1,p.277,2017.
“Improving sample efﬁciency in model-free reinforcement learning
[16] T. Haarnoja, A. Zhou, K. Hartikainen, G. Tucker, S. Ha, J. Tan,
fromimages,”arXivpreprintarXiv:1910.01741,2019.
V. Kumar, H. Zhu, A. Gupta, P. Abbeel et al., “Soft actor-critic
[40] “Pybullet simulator,” https://github.com/bulletphysics/bullet3, ac-
algorithmsandapplications,”arXivpreprintarXiv:1812.05905,2018.
cessed:2019-09.
[17] R.S.SuttonandA.G.Barto,Reinforcementlearning:Anintroduction.
MITpress,2018.
[18] D. Q. Mayne, J. B. Rawlings, C. V. Rao, and P. O. Scokaert,
“Constrained model predictive control: Stability and optimality,” Au-
tomatica,vol.36,no.6,pp.789–814,2000.
[19] J.DiCarlo,P.M.Wensing,B.Katz,G.Bledt,andS.Kim,“Dynamic
locomotion in the mit cheetah 3 through convex model-predictive
control,” in 2018 IEEE/RSJ International Conference on Intelligent
RobotsandSystems(IROS). IEEE,2018,pp.1–9.
[20] H.-W. Park, P. M. Wensing, S. Kim et al., “Online planning for
autonomousrunningjumpsoverobstaclesinhigh-speedquadrupeds,”
2015.
[21] A. Herdt, H. Diedam, P.-B. Wieber, D. Dimitrov, K. Mombaur, and
M.Diehl,“Onlinewalkingmotiongenerationwithautomaticfootstep
placement,”AdvancedRobotics,vol.24,no.5-6,pp.719–737,2010.
[22] J. Koenemann, A. Del Prete, Y. Tassa, E. Todorov, O. Stasse,
M.Bennewitz,andN.Mansard,“Whole-bodymodel-predictivecon-
trolappliedtothehrp-2humanoid,”in2015IEEE/RSJInternational
ConferenceonIntelligentRobotsandSystems(IROS). IEEE,2015,
pp.3346–3351.
419
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:42:55 UTC from IEEE Xplore.  Restrictions apply. 