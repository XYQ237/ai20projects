2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
Keyﬁlter-Aware Real-Time UAV Object Tracking
∗
Yiming Li1, Changhong Fu1, , Ziyuan Huang2, Yinqiang Zhang3, and Jia Pan4
Abstract—Correlation ﬁlter-based tracking has been widely 𝟏−𝝆 … Training Response map
𝝆 𝝆
applied in unmanned aerial vehicle (UAV) with high efﬁciency.
#285
However, it has two imperfections, i.e., boundary effect and
Filter
ﬁltercorruption.Severalmethodsenlargingthesearchareacan
mitigateboundaryeffect,yetintroducingundesiredbackground
Correlation
distraction.Existingframe-by-framecontextlearningstrategies #286
for repressing background distraction nevertheless lower the Keyfilter 𝑛−1 Feature of 
tracking speed. Inspired by keyframe-based simultaneous lo- Restriction #286
calizationandmapping,keyﬁlterisproposedinvisualtracking
Contextual learning
fortheﬁrsttime,inordertohandletheaboveissuesefﬁciently Keyfilter 𝑛 Correlation
and effectively. Keyﬁlters generated by periodically selected
#285
keyframes learn the context intermittently and are used to
Filter
restrain the learning of ﬁlters, so that 1) context awareness
canbetransmittedtoalltheﬁltersviakeyﬁlterrestriction,and
2)ﬁltercorruptioncanberepressed.Comparedtothestate-of- 𝝆 𝝆 … Training
𝟏−𝝆
the-art results, our tracker performs better on twochallenging
benchmarks,withenoughspeedforUAVreal-timeapplications. Fig. 1. Comparison between response maps of our tracker and baseline.
Red frames are served as keyframes generating keyﬁlters. Keyﬁlters carry
out context learning intermittently and inﬂuence the current ﬁlter training
I. INTRODUCTION formitigatingﬁltercorruption.Featureofcurrentframeiscorrelatedwith
theﬁltertrainedinthelastframe,producingaresponsemap.Redandblack
Combined with extensibility, autonomy, and maneuver- rectanglesdenoterespectivelytheresultsfromKAOTandbaseline.
ability of unmanned aerial vehicle (UAV), visual object
tracking has considerable applications in UAV, e.g., person
tracing [1], autonomous landing [2], aerial photography [3], has introduced more context noise, distracting the detection
and aircraft tracking [4]. Notwithstanding some progress, phase especially in situations of similar objects around.
UAV tracking remains onerous because of the complex Inliterature,thecontext-awareframework[14]isproposed
background, frequent appearance variation caused by UAV toreducethecontextdistractionthroughresponserepression
motion,full/partialocclusion,deformation,aswellasillumi- of the context patches. However, the frame-by-frame con-
nationchanges.Besides,computationallyintractabletrackers text learning is extremely redundant, because the capture
are not deployable onboard UAVs because of the harsh frequency of drone camera is generally smaller than the
calculation resources and limited power capacity. frequencyofcontextvariation,e.g.,theintervaltimebetween
Recently,theframeworkofdiscriminativecorrelationﬁlter twoconsecutivetimeina30framepersecond(FPS)videois
(DCF) [5], aiming to discriminate the foreground from the 0.03 second, but generally the context appearance in aerial
background via a correlation ﬁlter (CF), is widely adopted view remains unchanged for a certain time far more than
in UAV object tracking. The speed is hugely raised because 0.03 second. In addition, the learned single ﬁlter without
of its utilization of the circulant matrices’ property to carry restriction is prone to corruption due to the omnipresent
out the otherwise cumbersome calculation in the frequency appearance variations in the aerial scenarios.
domain rather than spatial one. Yet the circulant artiﬁcial In this work, inspired by keyframe-based simultaneous
samples used to train the ﬁlter hamper the ﬁlter’s discrimi- localization and mapping (SLAM) [15], the keyframe tech-
nativeability.Thisproblemiscalledboundaryeffectbecause nique is used to raise the tracking performance efﬁciently
the artiﬁcial non-real samples have periodical splicing at the and effectively. The contributions of this work are two-fold:
boundary. Several approaches [6]–[13] expand the search • AnovelapplicationofthekeyﬁlterinUAVvisualobject
area for alleviating boundary effects, but the enlargement tracking is presented. Keyﬁlters generated at a certain
frequency learn the context intermittently and enforce
1Yiming Li and Changhong Fu are with the School of Me- temporal restriction. Through the restriction, the ﬁlter
chanical Engineering, Tongji University, 201804 Shanghai, China.
corruption in the time span is alleviated and context
changhongfu@tongji.edu.cn
noise is efﬁciently suppressed.
2ZiyuanHuangiswiththeAdvancedRoboticsCentre,NationalUniver-
sityofSingapore,Singapore.ziyuan.huang@u.nus.edu • Extensive experiments on 193 challenging UAV im-
3Yinqiang Zhang is with the Department of Mechanical age sequences have shown that the keyﬁlter-aware
Engineering, Technical University of Munich, Munich, Germany. object tracker, i.e., KAOT, has competent performance
yinqiang.zhang@tum.de
compared with the state-of-the-art tracking approaches
4Jia Pan is with the Computer Science Department, The University of
HongKong,HongKong,China.panjia1983@gmail.com based on DCF and deep neural network (DNN).
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 193
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 15:48:27 UTC from IEEE Xplore.  Restrictions apply. II. RELATEDWORKS III. REVIEWOFBACKGROUND-AWARE
CORRELATIONFILTER
A. Discriminative correlation ﬁlter
The objective function of background-aware correlation
In recent years, the framework of discriminative correla-
ﬁlters (BACF) [12] is(cid:88)as follows: (cid:88)
tion ﬁlter (DCF) [5] has broadly aroused research interest
due to its remarkable efﬁciency. Yet classic CF-based track- E 1(cid:107) − D (cid:107) λ D (cid:107) (cid:107)
ers [16]–[18] have limited performance due to the lack of (w)= y Bxd(cid:63)wd 2+ wd 2 , (1)
2 0 2 2 2
negative samples, i.e., the circulant artiﬁcial samples created d=1 d=1
∈R ∈R ∈R
to train the CF hugely reduce its discriminative power. One where y M, xd N and wd M denote the desired
solution to this problem is spatial penalization to punish response, the dth one of D feature channels and correlation
E
the ﬁlter value at the boundary [6]–[10]. Another solution ﬁlter respectively. λ is a regularization parameter and (w)
is cropping both the background and target to use negative refers to an error between the desired response y and the
samples in the real word instead of synthetic samples [11]– actual one. (cid:63) is the spatial correlation operator. The main
∈ R ×
[13]. However, the aforementioned approaches are prone to idea of BACF is to utilize a cropping matrix B M N
introduce context distraction because of enlarging search to extract real negative samples. However, more background
area, especially in the scenarios of similar object around. distraction is introduced because of the enlargement.
B. Prior work to context noise and ﬁlter corruption IV. KEYFILTER-AWAREOBJECTTRACKER
Inliterature,M.Muelleretal.[14]proposedtorepressthe Inspired by the keyframe technique used in SLAM, the
response of context patches, i.e., the features extracted from keyﬁlter is ﬁrstly proposed in visual tracking to boost ac-
surrounding context are directly fed into classic DCF frame- curacy and efﬁciency, as illustrated in Fig. 2. The objective
(cid:88) (cid:88)
workandtheirdesiredresponsesaresuppressedaszero.The function of KAOT tracker is written as follows:
contextdistractionisthuseffectivelyrepressed,consequently E 1(cid:107) − D (cid:107) λ D (cid:107) (cid:107)
the discriminative ability of the ﬁlter is enhanced. Neverthe- (w)= 2 y(cid:88) (cid:88)Bxd0(cid:63)wd 22+ 2 (cid:88)wd 22
less, the frame-by-frame context learning is effective but not d=1 d=1 , (2)
efﬁcient, and its redundancy can be signiﬁcantly reduced. S P (cid:107) D (cid:107) γ D (cid:107) − (cid:107)
+ p Bxd(cid:63)wd 2+ wd w˜d 2
Another problem of classic DCF trackers is that the learned 2 p 2 2 2
p=1 d=1 d=1
single ﬁlter is commonly subjected to corruption because of
wherethethirdtermisresponserepressionofcontextpatches
the frequent appearance variation. Online passive-aggressive (their desired responses are zero), and S is the score of pth
p
learning is incorporated into the DCF framework [19] to patch to measure the necessity of penalization (introduced
∈ R ∈ R
mitigate the corruption. Compared to [19], the presented in IV-B). wd M and w˜d M are the current ﬁlter
keyﬁlter performs better in both precision and speed. and keyﬁlter, respectively. γ is the penalty parameter of the
gap between wd and w˜d. To improve the calculation speed,
C. Tracking by deep neural network Eq. (2) is calculated in the frequency domain:
Recently, deep neural network has contributed a lot to E 1(cid:107) − (cid:107) λ(cid:107) (cid:107) γ(cid:107) − (cid:107)
(w,gˆ)= Xˆgˆ Yˆ 2+ w 2+ w w˜ 2
the development of computer vision. For visual tracking, 2 2√ 2 2 2 2 , (3)
⊗ (cid:62)
some deep trackers [20]–[22] ﬁne-tuning the deep network s.t. gˆ= N(I FB )w
D
onlineforhighprecisionyetruntooslow(around1fpsona ⊗ (cid:2) ∈ R × (cid:3)
high-end GPU) to use in practice. Other methods like deep where(cid:2) is the Kron(cid:3)ecker product and ID D D is an
reinforcementlearning[23],unsupervisedlearning[24],con- identity matrix.ˆdenotes the discrete Fourier t·ra·n·sform with
trienpureesseonpteartiaotonr[[286]],heanvde-taol-seondinlceraeransiendgt[h2e5t]raacnkdindgeeapccfeuaratucrye. oYˆrt∈h=oCgonyˆa,l×0m,·a·t·ri,x0F∈., aXRˆndT Xˆ=×p ∈ CXˆN0,×SD1ˆNX(1p,= 0,,S1p,X.ˆ..P,P),,
Among them, incorporating lightweight deep features into gˆ DN 1 and w DM 1 are respectively deﬁned as
(cid:62) ··· (cid:62) (cid:62) ··· (cid:62) (cid:62)
online learned DCF framework has exhibited competitive Xˆ = [diag(xˆ1) , ,diag(xˆD) ], gˆ = [gˆ1 , ,gˆD ] ,
(cid:62) ··· (cid:62) (cid:62) (cid:62) ··· (cid:62) (cid:62)
performance both in precision and efﬁciency. w˜ =[w˜1 , ,w˜D ] and w=[w1 , ,wD ] .
A. Optimization algorithm
D. Tracking for unmanned aerial vehicle
Equation (3) can be optimized via alternating direction
Mechanical vibration, motion blur, limited computation
method of multipliers (ADMM) [30]. The Augmented La-
capacity and rapid movement have made UAV tracking an
grangian form of Eq. (3) is:
extremely demanding task. In literature, the presented UAV-
(cid:0) (cid:1)
tailored tracking methods generally have lower robustness L 1(cid:107) − (cid:107) λ(cid:107) (cid:107) γ(cid:107) − (cid:107)
(w,gˆ,ζˆ)= Xˆgˆ Yˆ 2+ w 2+ w w˜ 2
and accuracy [4], [27]–[29]. In light of ofﬂine training 2 √2 2 2 2 2
on the large-scale image datasets, deep feature for robust +ζˆ(cid:62) gˆ−√ N(ID⊗FB(cid:62))w , (4)
representationcanimproveperformancesigniﬁcantly,yetthe + µ(cid:107)gˆ− N(I ⊗FB(cid:62))w(cid:107)2
speedofexistingdeep-featurebasedtrackersmostlyrunslow 2 D 2
even on a high-end GPU [9]. This work aims to improve whereζˆ∈CDN×1 istheLagrangianvectorinthefrequency
∗
the speed and accuracy for the deep feature-based DCF domain and µ is a penalty parameter. Two subproblems gˆ
∗
framework for real-time UAV applications. and w are solved alternatively.
194
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 15:48:27 UTC from IEEE Xplore.  Restrictions apply. variation Corrupted BKaAseOliTne 
Filter 
#281 #282 #283 #284 #285 #286
Ours
…
… #281 #282 #283 #284 #285 #286
Baseline 
𝑤෥ 𝑤෥ 200 Without keyfilter 
𝑛−1 𝑛 Without intermittent context learning 
Keyfilter restriction Contextual Learning T=1 T=5
E 20 T=2 T=6
CL T=3 T=7
10 T=4 T=8
0 Frame 500
Fig.2. IllustrationofadvantagesofKAOT.Withthekeyﬁlterrestriction,theﬁltercorruptionismitigated,asshownonthetopright.Withthecontext
learning, the distraction is reduced, as shown in the response maps from frame 281 to 286. Set the keyﬁlter update period T as 1-8 frames (learns the
contextevery2-16frames),andtheobjectistrackedsuccessfullyinalleighttrackers,whileFPS(framepersecond)israisedto15.2from9.8,lowering
theredundancyofcontextlearningsigniﬁcantly.Inaddition,trackerslackingofthekeyﬁlterrestrictionorthecontextlearningbothlosethetarget.
• ∗ (cid:26) ∗ (cid:16) (cid:17)
Subproblem w (ﬁlter in the spatial domain): and wˆ is obtained through the following formula:
j+1
∗ (cid:0) λ(cid:107) (cid:107) γ(cid:107) − (cid:107)(cid:1) ∗ ⊗ (cid:62) ∗
w =argmin w 2+ w w˜ 2 wˆ = I FB w , (10)
w 2 √ 2 2 2(cid:111) j+1 D j+1
+ζˆ(cid:62) gˆ−√ N(ID⊗FB(cid:62))w subscriptjdenotesthethevalueatlastiterationandsubscript
(cid:18)µ(cid:107) − (cid:19) ⊗ (cid:62) (cid:107) . (5) j+1 denotes the value at current iteration.
+ gˆ N(I FB )w 2
2 D 2
− B. Context patches scoring scheme
λ+γ 1 γ
= µ+ (µg+ζ+ w˜) This work adopts a simple but effective scheme for
N N
• ∗ (cid:26) measuring the score of context patches through Euclidean
Subproblem gˆ (ﬁlter in the frequency domain): distance. Speciﬁcally, the size of omni-directional patches
∗ (cid:0) 1(cid:107) − (cid:107) (cid:1) located around the object is the same as that of the object.
gˆ =argmin Xˆgˆ Yˆ 2
gˆ 2 √ 2 (cid:111) The score of patch p is calculated as follows:
+ζˆ(cid:62) gˆ−√ N(ID⊗FB(cid:62))w . (6) min{w,h}
+µ(cid:107)gˆ− N(I(cid:2) ⊗FB(cid:62))w(cid:107)2 (cid:3) Sp = |OO | s , (11)
(cid:2) 2(cid:0) (cid:1) D (cid:0) 2(cid:1)(cid:3) p
| |
(cid:62) where OO denotes the Euclidean distance between the
yˆ(n)onlydependsonxˆ(n)= xˆ1(n),xˆ2(n),...,xˆD(n) p
(cid:62) object and context patch p (p=1,2,...,P) (between center
and gˆ(n) = conj gˆ1∗(n) ,...,conj gˆD(n) . Hence, points) and s is the base score which is a constant number.
solving equation(cid:26)for gˆ can be identically written as N w, h are respectively the width and height of the object
separate functions gˆ(n) (n=[1,...,N]): rectangle. Through Eq. (11) , the patch which is closer to
∗ 1(cid:107) − (cid:62) (cid:107) object, obtains a higher score for stronger penalization.
gˆ(n) =ar(cid:88)gmin yˆ(n) xˆ (n) gˆ(n) 2
gˆ(n) 2 0 2 C. Keyﬁlter updating strategy
+ 1 P (cid:107)S xˆ (n)(cid:62)gˆ(cid:111)(n)(cid:107)2+ζˆ(n)(cid:62)(gˆ(n)−wˆ(n)), (7) Startingfromtheﬁrstframe,thekeyﬁlterisgeneratedata
2 p p 2
certainfrequencyusingkeyframesandcurrentkeyﬁlterrefers
p=1 (cid:2) (cid:3)
+µ(cid:107)gˆ(n)−wˆ(n)(cid:107)2 to the latest trained keyﬁlter, as shown in Fig. 2. Current
2 2 ﬁlterisrestrictedbycurrentkeyﬁlterthroughthepunishment
w√here wˆ(n) = wˆ1(n),...,wˆD(n) and wˆd = introduced by the gap between current ﬁlter and keyﬁlter. In
DFP(cid:62)wd. The(cid:88)solution to each sub-subproblem is: otherwords,currentkeyﬁlterisupdatedeverycframes(c=
8 in this work). When the (n+1)th keyframe arrives (frame
∗ P (cid:62) − ×
gˆ(n) =( S2xˆ (n)xˆ (n) +µI ) 1 k =c n+1), the ﬁlter of current frame (keyﬁlter (n+1))
p=0 p p p D . (8) is trained under inﬂuence from the keyﬁlter n. As for the
−
(yˆ(n)xˆ(n) ζˆ(n)+µwˆ(n)) non-keyframes after keyﬁlter (n+1), the ﬁlters of them are
0
(cid:0) (cid:1) learnedwiththerestrictionofcurrentkeyﬁlter(keyﬁlter(n+
Lagrangian parameter is updated as follows:
1)).Thedetailedwork-ﬂowofKAOTtrackerispresentedin
∗ − ∗
ζˆ =ζˆ +µ gˆ wˆ , (9) Algorithm 1.
j+1 j j+1 j+1
195
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 15:48:27 UTC from IEEE Xplore.  Restrictions apply. Precision plots on DTB70 Success plots on DTB70
KAOT [0.757] KAOT [0.503]
0.8 AECROC F[0 [.07.2629]4] 0.8 AECROC F[0 [.05.0427]2]
UDT+ [0.658] UDT+ [0.462]
STRCF [0.649] ECO_HC [0.453]
CSRDCF [0.646] CSRDCF [0.438]
0.6 ECO_HC [0.643] e0.6 STRCF [0.437]
n CF2 [0.616] at UDT [0.422]
Precisio0.4 fBUMDADCSCTCS FTT[ 0 _[[.0H06..05 5[2930]04.6]]04] uccess r0.4 fBCMDAFCS2CCS [FTT0  ._[[40H01..45 3[0]5027.4]]05]
SAMF [0.519] S Staple_CA [0.351]
Staple_CA [0.504] SAMF [0.340]
0.2 CFNet [0.475] 0.2 CFNet [0.322]
KCF [0.468] KCC [0.291]
DCF [0.467] KCF [0.280]
KCC [0.440] DCF [0.280]
0 Staple [0.365] 0 Staple [0.265]
0 10 20 30 40 50 0 0.2 0.4 0.6 0.8 1
Location error threshold Overlap threshold
Precision plots on UAV123@10fps Success plots on UAV123@10fps
0.8 0.8
ECO [0.711] ECO [0.520]
KAOT [0.686] KAOT [0.479]
UDT+ [0.675] UDT+ [0.478]
ARCF [0.666] ARCF [0.473]
0.6 CSRDCF [0.643] 0.6 ECO_HC [0.462]
ECO_HC [0.634] STRCF [0.457]
STRCF [0.627] e CSRDCF [0.450]
n CF2 [0.601] at MCCT_H [0.433]
Precisio0.4 BUSMtADaCpCTCl FeT[_ 0_[C.0H5A.75 [5 7[0]20.5].59867]] uccess r0.4 BCUStAFDa2pCT l[ Fe[0_ 0.[C4.042A.3450 1][]30].420]
KCC [0.531] S fDSST [0.379]
0.2 fCDFSNSeTt  [[00..552156]] 0.2 CKFCNCe [t0 [.03.7347]3]
SAMF [0.466] Staple [0.342]
Staple [0.456] SAMF [0.326]
DCF [0.408] DCF [0.266]
0 KCF[0.406] 0 KCF [0.265]
0 10 20 30 40 50 0 0.2 0.4 0.6 0.8 1
Location error threshold Overlap threshold
Fig.3. Precisionandsuccessplotsbasedonone-pass-evaluation[31]ofKAOTandotherreal-timetrackersonDTB70[32]andUAV123@10fps[33].
  Algorithm 1: KAOT tracker V. EXPERIMENTS
−
Input: Location of tracked object on frame k 1,
            In this section, the presented KAOT tracker is rig-
Current keyﬁlter w˜,
         orously evaluated on two difﬁcult UAV datasets, i.e.,
Keyﬁlter updating Stepsize.
         DTB70 [32] and UAV123@10ps [33], with overall 193
Output: Location and scale of object on frame k
         image sequences captured by drone camera. The tracking
1 for i=2 to end do
         results are compared with the state-of-the-art trackers in-
2 Extract features from the region of interest (ROI)
         cluding both real-time (>=12 FPS) and non-real-time (<
3 Convolute gˆ − with xˆi on different scales to
          k 1 detect 12FPS) ones, i.e., ARCF [13], UDT [24], UDT+ [24],
generate response maps
         MCCT [34], MCCT-H [34], CSR-DCF [10], STRCF [19],
4 Find the peak position of map and output
          DeepSRTCF [19], ECO [8], ECO-HC [8], BACF [12], Sta-
5 Update object model
           × ple [16], Staple-CA [14], CF2 [26], DCF [14], DSST [35],
6 if k mod Stepsize 2==0 then
           KCF [5], KCC [36], SAMF [17], ADNet [23], CFNet [25],
7 Calculate S (p=1,2,...,8) by Eq. (11)
          p MCPF [37], IBCCF [38]. This work evaluates the trackers
8 Learn CF w by Eq. (5), Eq. (8) and Eq. (9)
          k based on protocol in two datasets respectively [32], [33].
9 w˜ =w
          k Noted that the real-time trackers are trackers with enough
else
10
     speed for UAV real-time applications.
11 if k mod Stepsize ==0 then
12 S =0 (p=1,2,...,8)
p
   13 Learn w by Eq. (5), (8) and Eq. (9) A. Implementation details
k
           14 w˜ =wk KAOT adopts both the hand-crafted and deep features,
else
         15 i.e., histogram oriented gradient (HOG) [39], color name
           16 Sp =0 (p=1,2,...,8) (CN) [40] and conv3 layer from VGG-M network [41]. The
                17 Learn wk by Eq. (5), Eq. (8) and Eq. (9) value of γ is set as 10, and the base score s is set as
end
            18 0.28. ADMM iteration is set to 2 for raising efﬁciency. All
end
          19 trackers are implemented in MATLAB R2018a and all the
           20 Start detection of next frame experimentsareconductedonthesamecomputerwithani7-
end
         21 8700K processor (3.7GHz), 48GB RAM and NVIDIA GTX
            2080 GPU. It is noted that the original codes without any
         modiﬁcation are employed in this work for fair comparison.
196
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 15:48:27 UTC from IEEE Xplore.  Restrictions apply. Fast camera motion (41) Background clutter (13) Motion blur (27)
KAOT [0.780] KAOT [0.736] KAOT [0.738]
0.8 EACROC F[0 [.07.4724]2] 0.8 SETCROC [F0 .[607.56]11] 0.8 SETCROC [F0 .[701.86]89]
STRCF [0.713] CSRDCF [0.610] ARCF [0.675]
CSRDCF [0.710] CF2 [0.603] ECO_HC [0.640]
0.6 UECDOT_+H [0C. 6[808.6]80] 0.6 AECROC_FH [0C. 5[805.5]67] 0.6 UBADCT+F  [[00..663398]]
Precision0.4 fCUBMDFADCS2CTCS [ FTT[0 0 ._[[6.0H061..263 5[03]80]67.6]]21] Precision0.4 SUBUMAADDCMCTTC +FFT[  0 _[[[.000H4...3553 [34650]548.4]]]84] Precision0.4 fCUCMDFSDCS2RTCS D[ TT[0C0 ._[5.FH055 .79 [4[90]40].4.65]3062]]
Staple_CA [0.535] fDSST [0.356] SAMF [0.403]
SAMF [0.526] Staple_CA [0.336] Staple_CA [0.387]
0.2 KCF [0.470] 0.2 KCC [0.293] 0.2 DCF [0.345]
DCF [0.469] CFNet [0.286] KCF [0.345]
KCC [0.456] DCF [0.280] CFNet [0.299]
CFNet [0.455] KCF [0.280] KCC [0.298]
0 Staple [0.373] 0 Staple [0.260] 0 Staple [0.248]
0 10 20 30 40 50 0 10 20 30 40 50 0 10 20 30 40 50
Location error threshold Location error threshold Location error threshold
In-plane rotation (47) Deformation (18) Similar objects around (27)
KAOT [0.702] KAOT [0.728] ECO [0.803]
0.8 AECROC F[0 [.06.3673]6] 0.8 UADRCT F[ 0[.06.0695]4] 0.8 AKRACOFT [ 0[0.7.73905]]
CSRDCF [0.602] ECO [0.592] UDT+ [0.691]
STRCF [0.586] ECO_HC [0.584] STRCF [0.677]
0.6 UECDOT_+H [0C. 5[803.5]68] 0.6 SCTSRRCDFC F[0 [.505.546]1] 0.6 CECF2O [_0H.6C7 5[0].667]
Precision0.4 fBUCMDAFDCS2CTCS [ FTT[0 0 ._[[5.0H055..557 4[74]80]79.5]]51] Precision0.4 SCCUMAFFDCN2MTC e[+FT0t   ._[[[5000H0...5449 [576]0184.5]]]50] Precision0.4 fUCBMDSADCSRCTCSD FTT[C 0 _[[.0FH05. .76 [5[70260].42.66]]1046]]
SAMF [0.450] BACF [0.448] SAMF [0.552]
Staple_CA [0.439] fDSST [0.390] Staple_CA [0.538]
0.2 DCF [0.416] 0.2 Staple_CA [0.379] 0.2 KCF [0.497]
KCF [0.416] KCC [0.335] DCF [0.496]
CFNet [0.411] KCF [0.302] CFNet [0.490]
KCC [0.397] DCF [0.301] KCC [0.470]
0 Staple [0.309] 0 Staple [0.281] 0 Staple [0.416]
0 10 20 30 40 50 0 10 20 30 40 50 0 10 20 30 40 50
Location error threshold Location error threshold Location error threshold
Fig.4. Attributebasedevaluationonprecision.KAOTranksﬁrstplaceonﬁveoutofsixchallengingattributes.
TABLEI
AVERAGEPRECISION(THRESHOLDAT20PIXELS)ANDSPEED((FPS,*MEANSGPUSPEED,OTHERWISECPUSPEED))OFTOPTENREAL-TIME
TRACKERS.RED,GREEN,ANDBLUEFONTSRESPECTIVELYINDICATESTHEBEST,SECOND,ANDTHIRDPLACEINTENTRACKERS.
KAOT ECO[8] ARCF[13] UDT+[24] STRCF[19] CSRDCF[10] ECO_HC[8] CF2[26] MCCT_H[34] UDT[24]
Avg.precision 72.2 71.7 68.0 66.5 63.8 63.5 64.3 62.5 60.3 58.9
Speed(FPS) 14.7* 11.6* 15.3 43.4* 26.3 11.8 62.19 14.4* 59.0 57.5*
B. Comparison with real-time trackers # 000001 # 000155 # 000212
1) Overall performance: Figure 3 demonstrates the over-
all performance of KAOT with other state-of-the-art real-
time trackers on DTB70 and UAV123@10fps. On DTB70
# 000001 # 000016 # 000135
dataset, KAOT (0.757) has an advantage of 4.4% and 9.1%
over the second and third best tracker ECO (0.722), ARCF
(0.694) respectively in precision, along with a gain of 0.2%
and6.6%overthesecond(ECO,0.502)andthirdbesttracker
# 000001 # 000065 # 000115
(ARCF, 0.472) respectively in AUC. On UAV123@10fps
dataset, KAOT (0.686, 0.479) ranks second place followed
by the third place UDT+ (0.675, 0.478). ECO is the only
trackerperformingbetterthanKAOT.Nevertheless,itutilizes
# 000001 # 000065 # 000116
continuous operator to fuse the feature maps elaborately,
while KAOT just uses the simple BACF as baseline. Notice
that ECO can further enhance its performance with our
framework.Averageprecisiononthetwodatasetsandspeed # 000001 # 000135 # 000204
(evaluatedonDTB70)arereportedinTableI.KAOTis27%
faster than ECO when achieving higher precision.
Discussions: DTB70 [32] dataset is recorded on a drone
with more frequent and drastic displacements compared KAOT ECO ECO-HC ARCF UDT+
to UAV123@10fps [33], thus increasing the tracking dif- Fig. 5. Qualitative evaluation. From the top to bottom is re-
ﬁculties. Our method exhibits relatively big advantages on spectively the sequence ChasingDrones, RcCar6, SnowBoarding2,
DTB70, proving the robustness of our method in the scenar- Gull1 and wakeboard2. Code and UAV tracking video are: https:
//github.com/vision4robotics/KAOT-tracker and https:
ios of strong motion.
//youtu.be/jMfmHVRqv3Y.
2) Attribute-based performance: Precision plots of six
challenging attributes are demonstrated in Figure 4. In the
cases of background clutter, KAOT improves the ECO by suppress the background distraction effectively. In situations
9.0% in light of the intermittent context learning which can ofin-planerotationanddeformation,KAOThasasuperiority
197
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 15:48:27 UTC from IEEE Xplore.  Restrictions apply. of 10.2% and 23.0% respectively compared to ECO. This is 400
ECO
attributed to the keyﬁlter restriction, which can prevent the CLE  200 EAUCRDOCT-+FHC
ﬁlter from aberrant variation. In addition, KAOT exhibits KAOT
0
excellent performance in the scenario of fast camera motion 0 20 40 60 80 100 120 140 160 180 200
Frame
and motion blur, which is desirable in aerial tracking.
3) Qualitative evaluation: Qualitative tracking results on 1000
ECO
ﬁve difﬁcult UAV image sequences are shown in Figure 5. LE 500 EACROC-FHC
C  UDT+
Besides,therespectivecenterlocationerror(CLE)variations KAOT
0
of ﬁve sequences are visualized in Figure 6. Speciﬁcally, in 0 20 40 60 80 100 120 140 160 180 200
Frame
ChasingDrones sequence where tracking is bothered by
strong UAV motion, KAOT has effectively repressed the
20 ECO
distraction of the context, so it can perform well despite CLE  10 EAUCRDOCT-+FHC
the large movement in a certain complex context. Only the   KAOT
0
pre-trained UDT+ tracks successfully in addition to KAOT. 0 10 20 30 40 50 60 70 80 90 100 110
Frame
MotionbluroccursinsequencesRcCar6andGull1(severe
example is shown at frame 16 in RcCar6). In this situation, 200 ECO
ECO-HC
KAOThaskepttrackingowingtothemitigatedﬁltercorrup- CLE  100 AURDCT+F
tion. As for the last two sequences, keyﬁlter restriction and KAOT
0
intermittentcontextlearninghavecollaborativelycontributed 0 20 40 60 80 100 120
Frame
to successful tracking.
1000
C. Comparison with non-real-time trackers ECO
LE 500 EACROC-FHC
KAOT is also compared with ﬁve non-real-time trackers C  KUDATO+T
usingdeepneuralnetwork,asshowninTableII.Tosumup, 00 50 100 150 200
Frame
KAOT has the best performance in terms of both precision
and speed on two benchmarks. In addition, compared to Fig.6. IllustrationofCLEvariations.Fromtoptobottomistheresult
DeepSTRCF(usingthesamefeaturesasKAOT),ourtracker from sequence ChasingDrones, RcCar6, SnowBoarding2, Gull1
andwakeboard2,respectively.
has more robust performance in precision on both two
datasets and is around 2.4 times faster than it. Therefore,
the efﬁciency and accuracy of KAOT tracker can be proven.
deformation,etc.,itisstilllimitedwhentheobjectdisappear
for a long time. Also, KAOT can not handle the rotation
D. Limitations and future works situations. Thus the re-detection and rotation-aware modules
can be added to raise the performance.
Keyframe selection: This work only adopts a simple pe-
Speed: The speed of KAOT is around 15 fps with a GPU
riodic keyframe selection mechanism, which is possible to
and can be used in real-time applications. However, KAOT
introduce distraction when the tracking on the keyframes
tracker is implemented on MATLAB platform and the code
is not reliable. More elaborated strategy can be employed
is not optimized, so the speed can be further improved.
to adaptively choose the keyframe and further enhance the
robustness.
VI. CONCLUSIONS
Re-detection and rotation: Though KAOT performs favor-
ably in the situations of drastic appearance change like blur, This work proposes keyﬁlter-aware object tracker to
repress the ﬁlter corruption and lower the redundancy of
TABLEII context learning. Extensive experiments on two authoritative
PRECISION,SUCCESSRATE(THEAREAUNDERTHECURVE),ANDFPS datasets have validated our tracker performs favorably in
OFKAOTASWELLASFIVENON-REAL-TIMETRACKERS.RED,GREEN, precision,withenoughspeedforreal-timeapplications.This
ANDBLUEFONTSRESPECTIVELYINDICATESTHEBEST,SECOND,AND keyﬁlter-aware framework and intermittent context learning
THIRDPERFORMANCE. strategy can also be used in other trackers like C-COT [7]
and STRCF [19] to further boost their performance. We
DTB70 UAV123@10fps
strongly believe that our method can be used in practice
FPS
Trackers Prec. AUC Prec. AUC and promote the development of UAV tracking applications.
MCPF[37] 66.4 43.3 66.5 44.5 0.57*
MCCT[34] 72.5 48.4 68.4 49.2 8.49*
ACKNOWLEDGMENT
DeepSTRCF[19] 73.4 50.6 68.2 49.9 6.18*
IBCCF[38] 66.9 46.0 65.1 48.1 2.28* This work is supported by the National Natural Sci-
ADNet[23] 63.7 42.2 62.5 43.9 6.87* ence Foundation of China (No. 61806148) and the Fun-
KAOT 75.7 50.3 68.6 47.9 14.69* damental Research Funds for the Central Universities (No.
22120180009).
198
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 15:48:27 UTC from IEEE Xplore.  Restrictions apply. REFERENCES [20] N. Wang, S. Li, A. Gupta, and D.-Y. Yeung, “Transferring
rich feature hierarchies for robust visual tracking,” arXiv preprint
arXiv:1501.04587,2015.
[1] H. Cheng, L. Lin, Z. Zheng, Y. Guan, and Z. Liu, “An autonomous
[21] H. Nam and B. Han, “Learning multi-domain convolutional neural
vision-based target tracking system for rotorcraft unmanned aerial
networksforvisualtracking,”inProceedingsoftheIEEEconference
vehicles,” in 2017 IEEE/RSJ International Conference on Intelligent
oncomputervisionandpatternrecognition,2016,pp.4293–4302.
RobotsandSystems(IROS). IEEE,2017,pp.1732–1738.
[22] L.Wang,W.Ouyang,X.Wang,andH.Lu,“Visualtrackingwithfully
[2] C. Fu, A. Carrio, M. A. Olivares-Mendez, and P. Campoy, “Online
convolutional networks,” in Proceedings of the IEEE international
learning-basedrobustvisualtrackingforautonomouslandingofUn-
conferenceoncomputervision,2015,pp.3119–3127.
mannedAerialVehicles,”inProceedingsofInternationalConference
[23] S. Yun, J. Choi, Y. Yoo, K. Yun, and J. Y. Choi, “Action-Decision
onUnmannedAircraftSystems(ICUAS),2014,pp.649–655.
Networks for Visual Tracking with Deep Reinforcement Learning,”
[3] M. Gschwindt, E. Camci, R. Bonatti, W. Wang, E. Kayacan, and
in IEEE Conference on Computer Vision and Pattern Recognition
S. Scherer, “Can a Robot Become a Movie Director? Learning
(CVPR),July2017,pp.1349–1358.
Artistic Principles for Aerial Cinematography,” in 2019 IEEE/RSJ
[24] N.Wang,Y.Song,C.Ma,W.Zhou,W.Liu,andH.Li,“Unsupervised
International Conference on Intelligent Robots and Systems (IROS).
Deep Tracking,” in The IEEE Conference on Computer Vision and
IEEE,2019.
PatternRecognition(CVPR),2019.
[4] C.Fu,A.Carrio,M.A.Olivares-Méndez,R.Suarez-Fernandez,and
[25] J. Valmadre, L. Bertinetto, J. Henriques, A. Vedaldi, and P. H.
P.C.Cervera,“RobustReal-timeVision-basedAircraftTrackingFrom
Torr, “End-to-end representation learning for correlation ﬁlter based
Unmanned Aerial Vehicles,” in Proceedings of IEEE International
tracking,”inProceedingsoftheIEEEConferenceonComputerVision
Conference on Robotics and Automation (ICRA), 2014, pp. 5441–
andPatternRecognition,2017,pp.2805–2813.
5446.
[26] C.Ma,J.Huang,X.Yang,andM.Yang,“HierarchicalConvolutional
[5] J. F. Henriques, R. Caseiro, P. Martins, and J. Batista, “High-Speed FeaturesforVisualTracking,”in2015IEEEInternationalConference
Tracking with Kernelized Correlation Filters,” IEEE Transactions on onComputerVision(ICCV),2015,pp.3074–3082.
PatternAnalysisandMachineIntelligence,vol.37,pp.583–596,2015. [27] Y. Yin, X. Wang, D. Xu, F. Liu, Y. Wang, and W. Wu, “Ro-
[6] M. Danelljan, G. Häger, F. S. Khan, and M. Felsberg, “Learning bustVisualDetection-Learning-TrackingFrameworkforAutonomous
Spatially Regularized Correlation Filters for Visual Tracking,” in AerialRefuelingofUAVs,”IEEETransactionsonInstrumentationand
Proceedings of IEEE International Conference on Computer Vision Measurement,vol.65,pp.510–521,2016.
(ICCV),2015,pp.4310–4318. [28] C.Yuan,Z.Liu,andY.Zhang,“UAV-basedforestﬁredetectionand
[7] M. Danelljan, A. Robinson, F. S. Khan, and M. Felsberg, “Beyond tracking using image processing techniques,” in 2015 International
Correlation Filters: Learning Continuous Convolution Operators for ConferenceonUnmannedAircraftSystems(ICUAS),2015,pp.639–
VisualTracking,”inEuropeanConferenceonComputerVision,2016, 643.
pp.472–488. [29] C. Martinez, I. F. Mondragon, P. C. Cervera, J. L. Sanchez-Lopez,
[8] M.Danelljan,G.Bhat,F.S.Khan,andM.Felsberg,“ECO:Efﬁcient and M. A. Olivares-Mendez, “A Hierarchical Tracking Strategy for
Convolution Operators for Tracking,” in 2017 IEEE Conference on Vision-Based Applications On-Board UAVs,” Journal of Intelligent
Computer Vision and Pattern Recognition (CVPR), 2017, pp. 6931– andRoboticSystems,vol.72,pp.517–539,2013.
6939. [30] S.Boyd,N.Parikh,E.Chu,B.Peleato,andJ.Eckstein,“Distributed
[9] C.Fu,Z.Huang,Y.Li,R.Duan,andP.Lu,“BoundaryEffect-Aware Optimization and Statistical Learning via the Alternating Direction
VisualTrackingforUAVwithOnlineEnhancedBackgroundLearning MethodofMultipliers,”inFoundationsandTrendsinMachineLearn-
andMulti-FrameConsensusVeriﬁcation,”inProceedingsofIEEE/RSJ ing,vol.3,2010,pp.1–122.
International Conference on Intelligent Robots and Systems (IROS), [31] Y.Wu,J.Lim,andM.-H.Yang,“ObjectTrackingBenchmark,”IEEE
2019. Transactions on Pattern Analysis and Machine Intelligence, vol. 37,
[10] A. Lukežic, T. Vojír, L. C. Zajc, J. Matas, and M. Kristan, “Dis- pp.1834–1848,2015.
criminative correlation ﬁlter with channel and spatial reliability,” in [32] S. Li and D.-Y. Yeung, “Visual object tracking for unmanned aerial
2017IEEEConferenceonComputerVisionandPatternRecognition vehicles:Abenchmarkandnewmotionmodels,”inAAAI,2017.
(CVPR),July2017,pp.4847–4856. [33] M.Mueller,N.Smith,andB.Ghanem,“ABenchmarkandSimulator
[11] H. Kiani Galoogahi, T. Sim, and S. Lucey, “Correlation Filters for UAV Tracking,” in Proceedings of European Conference on
With Limited Boundaries,” in Proceedings of IEEE Conference on ComputerVision(ECCV),2016,pp.445–461.
ComputerVisionandPatternRecognition(CVPR),2015,pp.1–9. [34] N.Wang,W.Zhou,Q.Tian,R.Hong,M.Wang,andH.Li,“Multi-cue
[12] H. K. Galoogahi, A. Fagg, and S. Lucey, “Learning Background- Correlation Filters for Robust Visual Tracking,” in 2018 IEEE/CVF
AwareCorrelationFiltersforVisualTracking,”inProceedingsofIEEE Conference on Computer Vision and Pattern Recognition, 2018, pp.
InternationalConferenceonComputerVision(ICCV),2017,pp.1144– 4844–4853.
1152. [35] M.Danelljan,G.Häger,F.S.Khan,andM.Felsberg,“Discriminative
[13] Z. Huang, C. Fu, Y. Li, F. Lin, and P. Lu, “Learning Aberrance Scale Space Tracking,” IEEE Transactions on Pattern Analysis and
RepressedCorrelationFiltersforReal-TimeUAVTracking,”in2019 MachineIntelligence,vol.39,no.8,pp.1561–1575,2017.
IEEEInternationalConferenceonComputerVision(ICCV),2019. [36] C.Wang,L.Zhang,L.Xie,andJ.Yuan,“Kernelcross-correlator,”in
[14] M. Mueller, N. Smith, and B. Ghanem, “Context-Aware Correlation Thirty-SecondAAAIConferenceonArtiﬁcialIntelligence,2018.
Filter Tracking,” in Proceedings of IEEE Conference on Computer [37] T.Zhang,C.Xu,andM.-H.Yang,“Multi-taskcorrelationparticleﬁlter
VisionandPatternRecognition(CVPR),2017,pp.1387–1395. forrobustobjecttracking,”inProceedingsoftheIEEEConferenceon
ComputerVisionandPatternRecognition,2017,pp.4335–4343.
[15] R. Mur-Artal, J. M. M. Montiel, and J. D. Tardós, “ORB-SLAM: A
[38] F. Li, Y. Yao, P. Li, D. Zhang, W. Zuo, and M. Yang, “Integrating
VersatileandAccurateMonocularSLAMSystem,”IEEETransactions
Boundary and Center Correlation Filters for Visual Tracking with
onRobotics,vol.31,pp.1147–1163,2015.
Aspect Ratio Variation,” in 2017 IEEE International Conference on
[16] L. Bertinetto, J. Valmadre, S. Golodetz, O. Miksik, and P. H. Torr,
ComputerVisionWorkshops(ICCVW),Oct2017,pp.2001–2009.
“Staple: Complementary Learners for Real-Time Tracking,” in Pro-
[39] N.DalalandB.Triggs,“HistogramsofOrientedGradientsforHuman
ceedings of the IEEE conference on Computer Vision and Pattern
Detection,”inProceedingsofIEEEComputerSocietyConferenceon
Recognition(CVPR),2016,pp.1401–1409.
Computer Vision and Pattern Recognition (CVPR’05), vol. 1, 2005,
[17] Y.LiandJ.Zhu,“AScaleAdaptiveKernelCorrelationFilterTracker
pp.886–893.
withFeatureIntegration,”inProceedingsofEuropeanConferenceon
[40] M.Danelljan,F.S.Khan,M.Felsberg,andJ.vandeWeijer,“Adaptive
ComputerVisionWorkshops,2015,pp.254–265.
ColorAttributesforReal-TimeVisualTracking,”2014IEEEConfer-
[18] C.Ma,X.Yang,C.Zhang,andM.-H.Yang,“Long-TermCorrelation
ence on Computer Vision and Pattern Recognition, pp. 1090–1097,
Tracking,” in Proceedings of IEEE Conference on Computer Vision
2014.
andPatternRecognition(CVPR),2015,pp.5388–5396.
[41] K.SimonyanandA.Zisserman,“VeryDeepConvolutionalNetworks
[19] F. Li, C. Tian, W. Zuo, L. Zhang, and M. Yang, “Learning Spatial- for Large-Scale Image Recognition,” in Proceedings of International
TemporalRegularizedCorrelationFiltersforVisualTracking,”in2018 ConferenceonRepresentationLearning,2015,pp.1–14.
IEEE/CVFConferenceonComputerVisionandPatternRecognition,
2018,pp.4904–4913.
199
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 15:48:27 UTC from IEEE Xplore.  Restrictions apply. 