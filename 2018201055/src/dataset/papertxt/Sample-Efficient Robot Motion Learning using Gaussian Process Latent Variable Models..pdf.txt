2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
Zero-shot Imitation Learning from Demonstrations for Legged Robot
Visual Navigation
Xinlei Pan1,2, Tingnan Zhang2, Brian Ichter2, Aleksandra Faust2, Jie Tan2 and Sehoon Ha2
Abstract—Imitation learning is a popular approach for
trainingeffectivevisualnavigationpolicies.However,collecting
expert demonstrations for legged robots is challenging as these
robotscanbehardtocontrol,moveslowly,andcannotoperate
continuouslyforlongperiodsoftime.Inthiswork,wepropose
a zero-shot imitation learning framework for training a goal-
driven visual navigation policy on a legged robot from human
demonstrations (third-person perspective), allowing for high-
quality navigation and cost-effective data collection. However,
imitation learning from third-person demonstrations raises
unique challenges. First, these demonstrations are captured
fromdifferentcameraperspectives,whichweaddressviaafea-
ture disentanglement network (FDN) that extracts perspective-
Fig. 1. We develop a learning framework that trains a visual navigation
invariant state features. Second, as transition dynamics vary
policyforaleggedrobot(Left)fromhumandemonstrationsmountedwith
betweensystems,wereconstructmissingactionlabelsbyeither
threecameras(Right).Theredcirclesindicatecameras.
building an inverse model of the robot’s dynamics in the
for problems as complex as visual navigation, the amount
feature space and applying it to the human demonstrations
or developing a Graphic User Interface (GUI) to label human of data required is outside of the scope of what is available
demonstrations. To train a navigation policy we use a model- for legged robots. Especially that the possible compositions
basedimitationlearningapproachwithFDNandaction-labeled of initial and goal states can be inﬁnite while we can only
humandemonstrations.Weshowthatourframeworkcanlearn
collect limited data. In this work, our key insight is to
an effective policy for a legged robot, Laikago, from human
mitigate the data collection issue by building a learning sys-
demonstrationsinbothsimulatedandreal-worldenvironments.
Ourapproachiszero-shotastherobotnevernavigatesthesame tem that can learn to navigate from heterogeneous experts–
paths during training as those at testing time. We justify our i.e., expert demonstrators that have different perspectives
framework by performing a comparative study. and potentially different dynamics. Our assumption is that
these agents have better navigation capabilities than legged
I. INTRODUCTION
robots and are more readily available, thus alleviating the
Legged robots have a great potential as universal mobility datacollectionbottleneck.Speciﬁcally,wefocusonlearning
platformsformanyreal-worldapplications,suchaslast-mile visual navigation from human agents.
deliveryorsearch-and-rescue.However,visualnavigationof Theideaoflearningvisualnavigationfromheterogeneous
legged robots can be considerably more challenging than agentsimposesnewchallenges.Oneofthemainissuesisthe
wheeled robots due to the limited availability of legged perspective shift between different agents’ vision, because
robot navigation data: compared to over 1,000 hours for a robot may have different camera positions and viewing
self-driving cars [34], several kilometers [8] and 32 years in angles from other robots or humans. Directly transferring
simulation[3]forindoormobilerobotnavigation.Thereason policies learned on human perspective demonstrations can
ofthedifﬁcultyforleggedrobotlargescaledatacollectionis resultindomainshiftproblems.Additionally,insomecases,
that they are hard to control and operate continuously for a the demonstrations only contain raw state sequence, and do
long time due to the hardware limitations. This lack of data notcontainactionlabels.Wethusneedaneffectiveplanning
makes it difﬁcult to deploy deep learning methods, such as module that ﬁnds the optimal actions solely based on raw
reinforcementlearningorimitationlearningtotherealworld, images, without any additional information about the robot
especially when the robot are required to navigate towards a and surroundings.
new goal unseen during training in the setting of zero-shot Inthiswork,weproposeanovelimitationlearningframe-
visual navigation. work that trains a goal driven visual navigation policy for
We choose an imitation learning approach that obtains a a legged robot from human demonstrations. A human pro-
navigation policy by mimicking human demonstrations, due vides navigation demonstrations as videos that are recorded
to its data efﬁciency and data collection safety. However, by multiple body-mounted cameras. We extract relevant
state features from the temporally-aligned multi-perspective
1 UniversityofCalifornia,Berkeley,CA,94720,USA videosbytrainingafeaturedisentanglementnetwork(FDN),
2 RoboticsatGoogle,MountainView,CA,94043,USA which disentangles state related features from perspective
The research was conducted during Xinlei’s internship at Google
{ related features. FDN achieves such disentanglement by
Brain. xinleipan@berkeley.edu, tingnan, ichter,
}
faust, jietan, sehoonha @google.com training with our proposed cycle-loss, that composing dis-
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 679
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 15:56:51 UTC from IEEE Xplore.  Restrictions apply. entangled features should be able to generate images with with on-robot data is effective, it is very labor intensive
correspondence to the features. We consider two approaches to collect large scale datasets for many robots, and some
for labeling demonstrations with robot-compatible actions, of them may require special training to use. Learning from
either via an efﬁcient human labelling GUI or through a human demonstrations of different perspectives is natural to
learnedinversedynamicsmodel.Wethentakeamodel-based mimicthewayhumans(e.g.children)learntoperformmany
imitation learning approach for training a visual navigation control tasks by watching others (experts) performing the
policy in the learned latent feature space. Our proposed same tasks [20]. However, the perspective shift between a
approach is zero-shot in that the robot training data does humanandrobotsisnon-trivial.Inthisapproach,wepropose
not include the testing tasks and thus has to infer the policy a novel feature extraction framework to solve this problem.
from the human demonstrations. In addition, our work is related with model-based reinforce-
We demonstrate our proposed framework in both sim- ment learning [21] and model-based imitation learning [30].
ulated and real environments. Our framework can train Our imitation learning framework is similar to that of the
effectivenavigationpoliciestoguidearobotfromthecurrent universal planning network [30] (UPN), but differs in that
position to the goal position described by a goal image. we perform the model learning and planning in our learned
We also validate the feature disentanglement network by feature space, rather than in the raw pixel space. Imitation
comparing the prediction of the perspective-changed images learning on visual navigation from human video has been
to the ground truth images. In addition, we analyze the explored in [16], where they propose to train an inverse
performance of the proposed framework by conducting a dynamics model to learn an action mapping function from
comparative study and comparing with some baseline al- robot dynamics to human video. While their work focuses
gorithms for feature learning and imitation learning. We on learning subroutines for navigation, our work focuses on
observethatourapproachachievessimilarperformancewith learning a perspective-invariant feature space that is suitable
perspective changes to that of the oracle imitation learning for path planning and model-based imitation learning. Our
method without perspective change. work could be combined with their contributions to improve
the performance of visual navigation.
II. RELATEDWORK
Feature Disentanglement. General feature disentangle-
Robot Visual Navigation. Robot visual navigation is a mentinvolvesabroadspectrumofrelatedworks.Mostworks
fundamental task for mobile robots, such as legged robots. are done in image-to-image translation tasks such as [11]
Traditionalapproachessuchassimultaneouslocalizationand and [17], where they apply a similar cycle-consistency loss
mapping (SLAM) ﬁrst constructs a map of the environment to achieve image translations across different domains. We
and then plan paths [2], [15], [13]. However, these methods propose a similar model for feature disentanglement that
sometimesrequirearobottonavigateandgraduallymapthe only uses the temporally aligned videos for feature learning,
environment. Though these methods may work in normal without additional supervision.
navigation case, they may struggle in our case where the III. PROBLEM
robot has to learn from human demonstrations of different
We consider learning goal-driven visual navigation policy
perspectives and transition dynamics. Other approaches use
on a legged robot from human demonstrations. In our work,
learning to enable visual navigation through either imitation
a human expert mounts N cameras on the body and walks
learning (next paragraph) or reinforcement learning (RL).
in the training environment. Each demonstration yields a
RL based approaches learn to navigate given a reward ··· ∈ I
sequence of images I1···N with the perspective index
function, either learned from demonstration [38], [12] or 1 T
(superscript) and time index (subscript). We assume that the
deﬁned manually by human expert [33]. Most existing work
images with the same time indices are captured at the same
on visual navigation with reinforcement learning is done in
human state (their position in 2D and orientation).
simulation [7], [37]; a few are done on real robots [23], [9].
The robot’s observation space at time t is deﬁned by an
These approaches are limited in the legged robot case by ∈ I
image from the robot’s perspective Irobot . The action
requiring actual trial-and-error on real robots, which can be t ∈A
space consists of ﬁve discrete actions a : going forward,
time intensive and dangerous as collisions on legged robots
going backward, turning left, turning right and staying in
can easily damage themselves and the environment.
place. Each action only provides high-level control over the
Learning from demonstrations. Imitation learning [22],
robotwhilelow-levelmotortorquesonlegsarecomputedby
[19], [27] learns a policy given labeled expert trajectories,
atraditionalRaibertcontroller[24].AgoalimageIhuman is
such as imitating a goal driven navigation policy [36], and g
speciﬁedfromthehuman’sperspective.Therefore,thepolicy
conditional imitation learning [4]. As mentioned previously, →
π : (Irobot,Ihuman) a maps the robot’s observation
imitation learning requires large quantities of labeled data t g
Irobot and the speciﬁed goal Ihuman to the action a.
that are not practical for legged robots. The data could t g
come from either on-robot demonstration such as imitating IV. METHOD
autonomous driving policy [26] or from human observation This section introduces a zero-shot imitation learning
such as third-person imitation learning [31], learning by framework for visual navigation of a legged robot. We
translating context [19] and using time-contrastive network ﬁrst introduce our feature disentanglement network (FDN)
(TCN) to learn a reward function [28]. Though learning that extracts perspective-invariant features from a set of
680
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 15:56:51 UTC from IEEE Xplore.  Restrictions apply. Proprietary + Confidential
HuDmeamno Mnsutrltai-tVioinew  Training Time a0*:h= a0:h−∇a0:hLplan Limitation= ∥a0*:h−a0h:uhman∥ Imitation loss
Testing Time
Iiq Iip I0robot f0 fi a*  a0  a1  ah  Lplan= ∥fh+1−fg∥ Plan loss
Iiq+1 Iip+1 EFxSetartaatutcerteo  r fi+1ai:i+h−1 LImeMaitoradntieionlng   0:h−1  f0  f1  f2  fh+1  fg
F M * State  State 
Iiq+h Iip+h Ighuman fg fi+h M*=argmImiintaMtin∥ga Ei*:xi+phe−rt1:−ai:ai+ih:ihu+−mh1a−n1∥ EFxetraatuctroer  EFxetraatuctroer 
Start Image  I0robot Goal Image  Ighuman
Fig. 2. Overview of the proposed framework. The feature extractor
takesasinputshumandemonstrationtrajectoriesfrommultipleperspectives Fig.4. Diagramofthemodel-basedimitationlearning.Themodeltakes
(indicated by p and q) and learns a perspective-invariant state feature in randomly initialized action sequence a0:h andSource:  pLorerme ipsdum idoclor tsits amet, cfonusecttetuur ardipeiscing eslit. tDuais ntone erat sem
extractor. The imitation learning model takes in the extracted feature of latent representations. It optimizes the action sequence to minimize the
humandemonstrationdataandlearntoimitatethehumanactionsequence differencebetweenthepredictedﬁnalstatef andthegoalstatefeature
h+1
ahi:ui+mha−n1. During testing (indicated by red dash line), start image I0robot representationfg (gradientﬂowindicatedbyblueline).Modelparameters
and goal image Ihuman are fed into the state feature extractor and the areupdatedbyminimizingtheimitationloss.
g
imitationlearningmodeltakesinboththestartfeaturef0 andgoalfeature gp and reconstructs an image corresponding to the same
fﬁgnaalnsdtatoepftiematiuzreesaanndagcotiaolnstsaetqeufeenactuereto. minimize the difference between state speciﬁed by fi and the same perspective speciﬁed
by gp: Ip = R (F(Ip)),P(Ip)), where the subscript r
i,r ψ i i
denotes reconstructed image. For any two images Ip, Iq
State  fj thatcorrespondtodifferentstatefeaturesf ,f anddiffierenjt
Ip Feature  i j
i ExtrFactor fi Iiq,r perspective features gp,gq, we deﬁne the cycle-loss function
Decoder of training the feature extractor as: L (Ip,Iq,θ,φ,ψ)=
(cid:107) − (cid:107) (cid:107) −cycle i j (cid:107)
Ijq PeFrespaetucrteiv e  gq R Ijp,r Iiq Rψ(Fθ(Iip),Pφ(Ijq)) + Ijp Rψ(Fθ(Ijq),Pφ(Iip)) .
Extractor Assumingaccesstotemporallyalignedimagesfrommultiple
P gp Lcycle=∥Iiq,r−Iiq∥+∥Ijp,r−Ijp∥ perspectives, the feature extractor will learn to extract state
relatedinformationonlyinF andlearntoextractperspective
(cid:80)
Fig. 3. Diagram of the feature disentanglement network (FDN). FDN information only in P. The total loss function for train-
is composed of three sub-networks, the state feature extractor F, the ing FDN can be summarized by the following equation:
perspectivefeatureextractorP andthedecoderR.Givenimagesofdifferent
states (indicated by i,j) and of different perspectives (indicated by p,q), Ltotal(θ,φ,ψ)= ∀i,j,p,qLcycle(Iip,Ijq,θ,φ,ψ).
the network ﬁrst extracts and separates state/perspective information, then WetrainFDNbyrandomlysamplingtwoimagesfromthe
composes them together to generate another image that corresponds to
multi-perspective data. We use the CycleGAN [35] encoder
the input state and perspective features. The blue lines indicate the feed-
forwardingpathtogenerateIp andtheyellowlinesforIq . as the backbone of the feature extractor and convert the
j,r i,r
temporally-aligned video demonstrations. Then we present last layer output as a ﬂattened d dimensional vector. The
decoder or the image generator is inherited from CycleGAN
the imitation learning algorithm that trains a navigation
decoder.TheSwishactivationfunction [25]isusedthrough
policyinthelearnedfeaturespacedeﬁnedbyFDN.Figure2
the network when necessary.
gives an overview of the framework.
A. Feature Disentanglement Network B. Imitation Learning from Human Demonstrations
We design a feature disentanglement network (FDN, Fig- Inspired by the Universal Planning Network [UPN], we
ure3)toperformfeaturedisentanglementfromvisualinputs. train the model-based imitation learning network (Figure 4)
F
More speciﬁcally, the FDN tries to separate state feature in the latent feature space . We process the given hu-
D
from perspective feature, which is necessary for imitation man demonstration data into a sequence of the features
{ ··· }
learning between the heterogeneous agents. The network f ,f , ,f by applying the trained FDN to the data.
0 1 n { ··· }
is composed of two parts: the state feature extractor F We label the robot-compatible actions a ,a , ,a −
θ 0 1 n 1
with parameters θ, which extracts state-only feature from by training an inverse dynamics model or using a developed
the input; and the perspective feature extractor P with GUI to manually label actions. The inverse dynamics model
φ
parameters φ, which extracts perspective-only feature from (IDM)takesinFDNstatefeatureextractorprocessedimages
the input. For simplicity, we drop the parameters of the that is temporally consecutive and predicts the robot action
functions unless necessary. that completes the transition. To get one episode of robot
D
Denote the entire human demonstration dataset as = datafortrainingIDM,werandomlystarttherobotandwalk
{ }
Ip p=1:N where T is the total length and N is the total therobotintheenvironmentuntilcollisionorthenumberof
i i=1:T
number of perspectives. For a given image input Ip, both steps exceeds 30. We collect multiple episodes of such data.
i
networks extract one part of information from the visual We deﬁne a model M that takes in the current obser-
∈ F
input: f = F(Ip),gp = P(Ip), where f and vation’s feature encoding f , and a randomly initialized
∈ G i i i i { 0 ··· }
gp are the corresponding state features and perspective action sequence a = a ,a , ,a , where h + 1 is
0:h 0 1 h
features, respectively. For training FDN, we learn an image the prediction horizon of the model, and predicts future
···
reconstructor R with parameters ψ that takes in f and states’ feature representation f , ,f . Then we update
ψ i 1 h+1
681
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 15:56:51 UTC from IEEE Xplore.  Restrictions apply. NavWorld OfﬁceWorld As a result, a robot can move for about 0.2m, which is
near one-third of its body length, and can turn left or right
for about 30 degrees in the NavWorld environment and 90
degrees in the OfﬁceWorld environment. We choose a small
×
room of size 4 4m2 for training and testing purposes for
realrobots.Forsimplicity,weassumekinematicmovements.
Fig.5. Simulationenvironments.Left:NavWorld;Right:OfﬁceWorld.
For all environments, we consider the task as given a goal
the action sequence by performing gradient descent on the
image from another perspective, the robot needs to navigate
following plan loss:
towards the goal within limited number of steps (slightly
∗
a =argminL (M(F(Irobot),a ),F(Ihuman)), (1) longer than the optimal path). An episode terminates when
0:h H 0 0:h g
a0:h the robot reaches the goal or when the maximum allowed
which minimizes the difference between the predicted ﬁnal steps are taken.
future state feature M(F(Irobot),a ) and the given goal In our experiments, each human demonstration is a ﬁrst-
0 0:h
state feature F(Ihuman). Here we use the superscript robot personviewnavigationvideo(humanperspective).Inthereal
g
toexplicitlypointoutthatIrobot isfromtherobot’sperspec- world case, we collect temporally-aligned multi-perspective
0
tive while the superscript human means Ihuman is from data by mounting three Go-Pro cameras on the person
g
the human’s perspective. We use the Huber loss [10] (L ) (Figure1right)andletthepersonnavigatecertainpaths.The
H
to measure the difference between the predicted feature and reason to mount the three cameras in these positions is to
goal feature as it was used in [30]. Then given the human get perspectives of different heights and viewing angles that
demonstrator’s expert action sequence ahuman, we optimize caninterpolatetherobot’sperspective.Thevideosaredown-
0:h ×
the model parameters so as to imitate the expert behavior: sampled to 128 128 pixels. We obtain multiple video clips
∗ (cid:107) ∗ − (cid:107) with the same state sequence but different perspectives and
M =argmin a0:h ah0:uhman , (2) extract perspective-invariant features. In total we collected
M
25 demonstration trajectories, each of length 20 steps in the
the loss function above could be a cross entropy loss when
real world environment for 10 minutes. In simulation, we
theactionspaceisdiscrete.OncewetrainthemodelM,Eq. automaticallygeneratedemonstrationsusingapath-planning
(1) implicitly deﬁnes the policy π. At each time step, the algorithm [6] on randomly sampled start and goal locations.
policy replans the entire action sequence and only executes
We collect 500 demonstration trajectories, each of length
the ﬁrst action, which is similar to the way model predic-
20 steps. To improve the data efﬁciency, we perform data
tive control (MPC) [18] does. When training the imitation
augmentation by replaying the video and reversing the time
learning model, the prediction horizon can change, and it
order both for the simulated and the real data. We add
dependsonthenumberofexpertstepsbetweenthestartand
in augmented stay in place demonstration sequences by
goal state, a mask is applied on Equation 2 to only imitate
repeating randomly sampled observations for 20 steps.
thecorrespondingactionsequence.Thisissimilartotheway
Inourframework,weneedtoobtainrobot-compatibleac-
UPN[30]trainsthepolicy.Moredetailscanbefoundin[30].
tionlabelsofhumandemonstrationssincetheyhavedifferent
dynamics. In simulation, we trained an inverse dynamics
V. EXPERIMENTSANDANALYSIS model that takes in two consecutive images processed by
We design our experiments to investigate the following FDN and predicts the robot action that completes the tran-
questions. 1) Is the proposed feature disentanglement net- sition. Then we use the trained inverse dynamics model to
work able to disentangle features? 2) Can the proposed labeltheexpertdemonstration.Intherealworldexperiment,
model-based imitation learning ﬁnd an effective action plan since the robot trajectory data especially the actions contain
in the learned feature space? 3) How does our approach signiﬁcant noise, we develop a GUI that allows us to label
compare to other imitation learning methods? humanactionsmanuallywithinashortamountoftime.Note
that this work’s focus is not on action labeling. In addition,
A. Experiments and Results
the manually labeled action is only a rough estimation of
a) Environment Setup and Data Collection: We select where the robot is going, and it may still contain noise: for
LaikagofromUnitree[32]astherealworldroboticplatform example,whentherobotisstayinginplace,itmaystillmove
to evaluate the proposed framework. For simulation, we around a little bit due to drifting.
develop two simulation environments using PyBullet [5] b) Training: WetraintheFDNandtheimitationlearn-
(Figure 5), one is called Navworld, the other OfﬁceWorld. ingmodelbothinthesimulatedenvironmentsandonthereal
The latter one has more complex texture than the previous robot. Additionally, we train the inverse dynamics model in
one, and the space is larger. We put a simulated Laikago in the simulation for automatic human action labeling. For all
both simulation environments. The robot is 60cm tall and experiments,weusetheAdamoptimizer[14]withalearning
we mount the camera 30cm above the body (See Figure 1 rate of 0.0035 and batch size of 32, and we set the feature
×
left).Theframesaredown-sampledto128 128pixels.The dimension d = 256. We evaluate the success (reaching the
discrete actions are deﬁned as running a walking controller goal)rateofourexperimentsinsimulationbycomparingthe
with a constant linear and angular velocity for 1 second. robot’s state (location and orientation) to the goal state, and
682
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 15:56:51 UTC from IEEE Xplore.  Restrictions apply. TABLEI TABLEIII
RESULTSWITHDIFFERENTTASKDIFFICULTIES. COMPARISONOFDIFFERENTFEATURELOSSONSUCCESSRATE.
Env/Distance 2 5 10 SuccessRate/Loss Ours Cycle-triplet Triplet
NavWorld 79.66% 56.52% 21.81% NavWorld,4Steps 54.55% 52.63% 8.77%
OfﬁceWorld 78.71% 54.00% 26.67% NavWorld,10Steps 21.81% 20.72% 2.70%
OfﬁceWorld,10Steps 26.67% 27.30% 25.08%
TABLEII difﬁculty. Table II shows that with more demonstrations the
RESULTSWITHDIFFERENTAMOUNTOFHUMANDEMONSTRATIONS. success rate indeed increases.
e) Hardware Results: We provide results on the real
Env/#ofHumanDemos 100 300 500 legged robot, Laikago. We test the robot on three sets of
NavWorld 21.81% 23.42% 25.23%
goal-driven navigation tasks. We consider three targets and
OfﬁceWorld 26.67% 31.00% 32.00%
start locations to evaluate the robustness and consistency
the success rate on real robot by human visual evaluation.
of the policy. The distance from the target location to the
c) Validation of Feature Disentanglement Network:
start location of the robot is around two meters. For each
We present in Figure 6 the results of image generation by
testingstartandgoallocationpair,wetestforthreetimesand
composing state and perspective features using FDN. As
evaluate the success rate of the three trials. On these testing
illustrated in the generated image, the feature extraction net-
tasks,weobtainasuccessrateofaround60%.Weshowone
work can compose state and perspective features to generate
ofthesuccessfulgoal-drivenvisualnavigationtrajectoriesin
animagethathasthesamecorrespondenceastheinputstate
Figure 7. In our experience, a robot shows a better accuracy
and perspective features. In particular, the difference in the
when the goal image is visually salient, such as a brown
perspectivesliesinthecameraverticalpositioninsimulation
chair; it struggles to reach an object of colors similar to the
and camera vertical and horizontal location in real robot
background, e.g. a white desk in front of a white wall.
data. The results show that the network is able to learn such
B. Analysis
perspective information from training FDN.
a) Comparison with Different Loss Functions: To in-
Ip Iq Iq (target) Iq (pred.)
i j i i,r vestigate whether our proposed cycle-loss is suitable for
training feature disentanglement, we compare with other
baseline loss functions. Speciﬁcally, we experiment with
several combinations of the proposed cycle-loss and triplet
loss [28]. In the ﬁrst scenario, we train the feature extractor
with our cycle-loss. In the second case, we combine cycle-
losswithtripletloss.GiventhreeimagesIp,Iq,Ip,whereIp
i i j i
and Iq share their states and Ip,Ip share their perspective,
i i j
the triplet loss can be deﬁned as, L (θ,Ip,Iq,Ip) =
(cid:107) − (cid:107)−(cid:107) − trip(cid:107)let i i j
F (Ip) F (Iq) F (Ip) F (Ip) +α. which min-
θ i θ i θ i θ j
imizes state feature difference for the same state but from
different perspectives, and maximize state feature difference
Fig. 6. FDN image generation results. The ﬁrst two columns are the
for different states but from the same perspective. Here α is
inputs:ﬁrstoneprovidesthestateinformationandthesecondprovidesthe
perspectiveinformation.Thethirdcolumnisthegroundtruthtargetimage theenforcedmargintypicallyusedintriplet-lossandusually
andthelastcolumnisthepredictionfromFDN. thelossiscuttozerowhenitisnegative.Inthethirdcase,we
d) Simulation Results: First, we validate our frame- use triplet loss only to learn the state feature representation.
work in the simulated environment. Our framework shows The results are presented in Table III. It is clear that the
a learned zero-shot robot visual navigation behavior from Triplet loss alone has consistently worse results than our
human demonstrations with a success rate ranging from proposed Cycle loss. By combining Cycle-loss with Triplet
20% to 80% depending on the task difﬁculty (see Table I loss the performance improved a bit compared to the Triplet
for more details). We observe that a robot is able to ﬁnd loss. The triplet loss’s poor performance may be a result
the goal position, speciﬁed from human’s perspective, even of sensitivity to the data sampling process. Our proposed
when it is out of sight, indicating that the trained imitation Cycle-losstrainingismorestableandisnotsensitivetodata
learning model already models the environment with human sampling. Besides, triplet loss only learns the state feature
demonstrations. (perspective-invariant feature) and our network learns both
We evaluate the effect of task difﬁculty (by varying state and perspective feature and our decoder helps to verify
the number of minimum steps between the start and goal the learned feature has correct correspondence in terms of
location) and the number of human demonstrations on the states and perspectives.
success rate. For the latter, we ﬁx the task difﬁculty to be b) Comparison with baselines: We compare the pro-
10 steps. Table I shows that with more distance between the posed framework with a baseline algorithm, Universal Plan-
start and goal location, ﬁnding the correct path towards the ningNetworks(UPN)[30].Inparticular,wetestUPNintwo
goal becomes harder. In a larger OfﬁceWorld environment, scenarios: without and with perspective changes between a
weobservesuchdecreaseinsuccessratewithincreasingtask learner and a demonstrator. The former serves as the upper
683
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 15:56:51 UTC from IEEE Xplore.  Restrictions apply. Irobot Irobot Irobot Irobot Irobot Irobot Irobot Irobot Irobot Irobot Irobot Ihuman
0 1 2 3 4 5 6 7 8 9 10 g
Fig. 7. Robot successful visual navigation trajectories. In each row, the ﬁnal image is the goal image and the ﬁrst to the last second image show the
robotnavigationtrajectoryfromthestarttothegoallocation.Thoughcapturedfromdifferentperspectives,thegoalspeciﬁedbythehumanisthesame
asthelaststateoftherobot.
TABLEIV
such that the behavior of the robot is more controllable. In
COMPARISONOFSUCCESSRATEWITHUPNANDUPN-PERSPCHANGE.
addition, since we don’t need to have very accurate human
Env/Methods Ours UPN UPN-PerspChange action labels (only in an abstract way indicating in which
NavWorld 54.55% 58.00% 7.88% directionthehumanisgoing),opticalﬂow[29]methodscan
OfﬁceWorld 26.67% 27.00% 22.00%
be used to automatically label human data.
bound of our method’s performance. We will call the ﬁrst
VI. CONCLUSIONANDREMARKS
method UPN and the second method UPN-PerspChange. In
We propose a novel imitation learning framework for
the ﬁrst method UPN, we train and test UPN under the
learning a visual navigation policy on a legged robot from
same perspective. In the second case, UPN is trained with
human demonstrations. Since it is hard to collect human
multiple perspective data while the training perspective does
data from the robot’s perspective, one major challenge is
not include the testing perspective.
to interpret the human demonstrations from different per-
We perform a comparative study on NavWorld with 4
spectives. To this end, we develop a feature disentanglement
steps of start-goal distance and on OfﬁceWorld with 10
network (FDN) that extracts perspective-invariant features
steps of start-goal distance. We choose these numbers of
and a model-based imitation learning algorithm that trains a
steps since one step in the environment can be a huge
policy in the learned feature space. We demonstrate that the
distance and the robot can move in a few steps to a state
proposed framework can ﬁnd effective navigation policies
that is not visible from the starting state. The results are
in two simulated worlds and one real environment. We
presented in Table IV. The success rate of our method is
further validate the framework by conducting ablation and
approaching UPN, the empirical upper bound, indicating
comparative studies.
that FDN effectively handles the perspective shift. When
The bottleneck for deploying the current framework to
there is perspective change, UPN-PerspChange trained with
real-worldscenariosisthemanualactionlabelingprocessof
some perspective data can’t generalize to another unseen
human demonstrations. However, automated action labeling
perspective, and the result is worse than our method. This
isnotstraightforwardattherequiredhighaccuracy(>90%).
indicates that the perspective shift is nontrivial and direct
One possible approach is to collect a small amount of the
transfer does not work.
robot’s navigation data to build an inverse dynamics model
WeobservethattheUPN-PerspChangeworksbetterinthe thattakesintwoconsecutiveimagesandpredictstherobot’s
OfﬁceWorld environment than in NavWorld, this is because action. In our experience, this approach works in simulation
that in the OfﬁceWorld environment, the turning angle is 90 but not on the real robot because a legged robot’s gait blurs
degrees while in NavWorld the turning angle is 30 degrees. the camera images. In addition, the robot’s discrete actions
Therefore, to reach a state in the OfﬁceWorld, most of the are often not well-matched with real human demonstrations.
actions are either moving forward or back. Even though the In the future, we want to investigate more stable gaits with
texture in the OfﬁceWorld is more complicated, the task continuous control commands.
difﬁculty with the same number of step between start and Although we tested the framework for learning a legged
goal location is smaller. robot policy from human demonstrations, the framework is
c) Scalability of the Proposed Approach: Though we designed to support general imitation learning between any
rely on a GUI to label human data as an alternative of heterogeneous agents. In the future, we hope to build a
learningtherobotinversedynamicsmodel,therealchallenge general system that can learn navigation policies for data-
in the Laikago robot is the inaccurate robot action labels, expensive robots, such as legged robots or aerial vehicles,
which results in inaccurate inverse dynamics model and from easy-to-operate robots, such as mobile robots, au-
thus making it less feasible to use the inverse dynamics tonomous cars or humans. If we can fully exploit a large
modeltolabelhumandata.Toimprovethescalabilityofthe navigation data sets, such as Google Streetview [1], there is
current framework and the success rate of goal driven visual great potential to signiﬁcantly improve the performance on
navigation,wecanimprovethelow-levelcontroloftherobot real robots.
684
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 15:56:51 UTC from IEEE Xplore.  Restrictions apply. REFERENCES translation,”in2018IEEEInternationalConferenceonRoboticsand
Automation(ICRA). IEEE,2018,pp.1118–1125.
[1] D. Anguelov, C. Dulong, D. Filip, C. Frueh, S. Lafon, R. Lyon,
[20] A.N.Meltzoff,“Borntolearn:Whatinfantslearnfromwatchingus,”
A.Ogale,L.Vincent,andJ.Weaver,“Googlestreetview:Capturing
Theroleofearlyexperienceininfantdevelopment,pp.145–164,1999.
theworldatstreetlevel,”Computer,vol.43,no.6,pp.32–38,2010.
[2] F.Bonin-Font,A.Ortiz,andG.Oliver,“Visualnavigationformobile [21] X. Pan, X. Chen, Q. Cai, J. Canny, and F. Yu, “Semantic predictive
robots:Asurvey,”Journalofintelligentandroboticsystems,vol.53, controlforexplainableandefﬁcientpolicylearning,”in2019Interna-
no.3,pp.263–296,2008. tionalConferenceonRoboticsandAutomation(ICRA). IEEE,2019,
[3] H.-T. L. Chiang, A. Faust, M. Fiser, and A. Francis, “Learning pp.3203–3209.
navigation behaviors end-to-end with autorl,” IEEE Robotics and [22] Y. Pan, C.-A. Cheng, K. Saigol, K. Lee, X. Yan, E. Theodorou, and
AutomationLetters,vol.4,no.2,pp.2007–2014,2019. B.Boots,“Agileautonomousdrivingusingend-to-enddeepimitation
[4] F. Codevilla, M. Miiller, A. Lo´pez, V. Koltun, and A. Dosovitskiy, learning,”inRobotics:scienceandsystems,2018.
“End-to-enddrivingviaconditionalimitationlearning,”in2018IEEE [24] M.H.Raibert,Leggedrobotsthatbalance. MITpress,1986.
InternationalConferenceonRoboticsandAutomation(ICRA). IEEE, [25] P. Ramachandran, B. Zoph, and Q. V. Le, “Swish: a self-gated
2018,pp.1–9. activationfunction,”arXivpreprintarXiv:1710.05941,vol.7,2017.
[5] E.CoumansandY.Bai,“Pybullet,apythonmoduleforphysicssim-
[26] N. Rhinehart, R. McAllister, and S. Levine, “Deep imitative mod-
ulationforgames,roboticsandmachinelearning,”GitHubrepository,
els for ﬂexible inference, planning, and control,” arXiv preprint
2016.
arXiv:1810.06544,2018.
[6] E.W.Dijkstra,“Anoteontwoproblemsinconnexionwithgraphs,”
[27] S.Ross,G.Gordon,andD.Bagnell,“Areductionofimitationlearning
Numerischemathematik,vol.1,no.1,pp.269–271,1959.
andstructuredpredictiontono-regretonlinelearning,”inProceedings
[7] K.Fang,A.Toshev,L.Fei-Fei,andS.Savarese,“Scenememorytrans-
ofthefourteenthinternationalconferenceonartiﬁcialintelligenceand
formerforembodiedagentsinlong-horizontasks,”inProceedingsof
statistics,2011,pp.627–635.
the IEEE Conference on Computer Vision and Pattern Recognition,
2019,pp.538–547. [28] P. Sermanet, C. Lynch, Y. Chebotar, J. Hsu, E. Jang, S. Schaal,
[8] A.Francis,A.Faust,H.L.Chiang,J.Hsu,J.C.Kew,M.Fiser,and S.Levine,andG.Brain,“Time-contrastivenetworks:Self-supervised
T.E.Lee,“Long-rangeindoornavigationwithPRM-RL,”CoRR,vol. learning from video,” in 2018 IEEE International Conference on
abs/1902.09458,2019. RoboticsandAutomation(ICRA). IEEE,2018,pp.1134–1141.
[9] S.Gupta,J.Davidson,S.Levine,R.Sukthankar,andJ.Malik,“Cog- [29] S. P. Singh, P. J. Csonka, and K. J. Waldron, “Optical ﬂow aided
nitivemappingandplanningforvisualnavigation,”inProceedingsof motionestimationforleggedlocomotion,”in2006IEEE/RSJInterna-
the IEEE Conference on Computer Vision and Pattern Recognition, tional Conference on Intelligent Robots and Systems. IEEE, 2006,
2017,pp.2616–2625. pp.1738–1743.
[10] P. J. Huber, “Robust estimation of a location parameter,” in Break- [30] A. Srinivas, A. Jabri, P. Abbeel, S. Levine, and C. Finn, “Universal
throughsinstatistics. Springer,1992,pp.492–518. planning networks: Learning generalizable representations for visuo-
[11] A.H.Jha,S.Anand,M.Singh,andV.Veeravasarapu,“Disentangling motor control,” in Proceedings of the 35th International Conference
factors of variation with cycle-consistent variational auto-encoders,” onMachineLearning,ICML,vol.80,2018,pp.4739–4748.
in European Conference on Computer Vision. Springer, 2018, pp.
[31] B. C. Stadie, P. Abbeel, and I. Sutskever, “Third person imitation
829–845.
learning,” in 5th International Conference on Learning Representa-
[12] F. Justin, K. Anoop, L. Sergey, and G. Sergio, “From language to
tions,ICLR,2017.
goals:Inversereinforcementlearningforvision-basedinstructionfol-
[32] Unitree.(2019)Laikago.[Online].Available:http://www.unitree.cc/e/
lowing,”in7thInternationalConferenceonLearningRepresentations,
action/ShowInfo.php?classid=6&id=1
ICLR,2019.
[13] C. Kerl, J. Sturm, and D. Cremers, “Dense visual slam for rgb-d [33] X. Wang, Q. Huang, A. Celikyilmaz, J. Gao, D. Shen, Y.-F. Wang,
cameras,” in 2013 IEEE/RSJ International Conference on Intelligent W. Y. Wang, and L. Zhang, “Reinforced cross-modal matching and
RobotsandSystems. IEEE,2013,pp.2100–2106. self-supervised imitation learning for vision-language navigation,” in
[14] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimiza- ProceedingsoftheIEEEConferenceonComputerVisionandPattern
tion,” in 3rd International Conference on Learning Representations, Recognition,2019,pp.6629–6638.
ICLR2015,Y.BengioandY.LeCun,Eds.,2015. [34] F.Yu,W.Xian,Y.Chen,F.Liu,M.Liao,V.Madhavan,andT.Darrell,
[15] K.Konolige,J.Bowman,J.Chen,P.Mihelich,M.Calonder,V.Lepetit, “Bdd100k:Adiversedrivingvideodatabasewithscalableannotation
andP.Fua,“View-basedmaps,”TheInternationalJournalofRobotics tooling,”arXivpreprintarXiv:1805.04687,2018.
Research,vol.29,no.8,pp.941–957,2010. [35] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-
[16] A. Kumar, S. Gupta, and J. Malik, “Learning navigation subroutines to-image translation using cycle-consistent adversarial networks,” in
bywatchingvideos,”inConferenceonRobotLearning,2019. ProceedingsoftheIEEEinternationalconferenceoncomputervision,
[17] H.-Y.Lee,H.-Y.Tseng,J.-B.Huang,M.Singh,andM.-H.Yang,“Di- 2017,pp.2223–2232.
verseimage-to-imagetranslationviadisentangledrepresentations,”in
[36] Y.Zhu,D.Gordon,E.Kolve,D.Fox,L.Fei-Fei,A.Gupta,R.Mot-
ProceedingsoftheEuropeanconferenceoncomputervision(ECCV),
taghi,andA.Farhadi,“Visualsemanticplanningusingdeepsuccessor
2018,pp.35–51.
representations,”inProceedingsoftheIEEEInternationalConference
[18] I. Lenz, R. A. Knepper, and A. Saxena, “Deepmpc: Learning deep
onComputerVision,2017,pp.483–492.
latentfeaturesformodelpredictivecontrol.”inRobotics:Scienceand
[37] Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei, and
Systems. Rome,Italy,2015.
A. Farhadi, “Target-driven visual navigation in indoor scenes using
[19] Y. Liu, A. Gupta, P. Abbeel, and S. Levine, “Imitation from ob-
deepreinforcementlearning,”in2017IEEEinternationalconference
servation: Learning to imitate behaviors from raw video via context
onroboticsandautomation(ICRA). IEEE,2017,pp.3357–3364.
[23] D.Pathak,P.Mahmoudieh,G.Luo,P.Agrawal,D.Chen,Y.Shentu,
E.Shelhamer,J.Malik,A.A.Efros,andT.Darrell,“Zero-shotvisual [38] B.D.Ziebart,A.L.Maas,J.A.Bagnell,andA.K.Dey,“Maximum
imitation,”inProceedingsoftheIEEEConferenceonComputerVision entropy inverse reinforcement learning.” in AAAI, vol. 8. Chicago,
andPatternRecognitionWorkshops,2018,pp.2050–2053. IL,USA,2008,pp.1433–1438.
685
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 15:56:51 UTC from IEEE Xplore.  Restrictions apply. 