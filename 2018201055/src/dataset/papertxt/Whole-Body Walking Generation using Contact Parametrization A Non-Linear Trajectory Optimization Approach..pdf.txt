2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
Joint Pedestrian Detection and Risk-level Prediction
with Motion-Representation-by-Detection
Hirokatsu Kataoka1, Teppei Suzuki1, Kodai Nakashima1, Yutaka Satoh1 and Yoshimitsu Aoki2
Abstract—The paper presents a pedestrian near-miss de-
tector with temporal analysis that provides both pedestrian
detection and risk-level predictions which are demonstrated
on a self-collected database. Our work makes three primary
contributions: (i) The framework of pedestrian near-miss de-
tectionisproposedbyprovidingbothapedestriandetectionand
risk-levelassignment.Speciﬁcally,wehavecreatedaPedestrian
Near-Miss (PNM) dataset that categorizes trafﬁc near-miss
incidents based on their risk levels (high-, low-, and no-risk).
Unlike existing databases, our dataset also includes manually
localizedpedestrianlabelsaswellasalargenumberofincident- Fig.1. Normal(left)andnear-missincident(right)scenesonpublicroads.
related videos. (ii) Single-Shot MultiBox Detector with Motion We believe that conventional pedestrian detection methods are unsuitable
fordetectingnear-missincidentsbecausethetwoscenesincludedifferences
Representation(SSD-MR)isimplementedtoeffectivelyextract
suchasmovementandpedestrianposture.Therelatedwork[3],[4]actually
motion-based features in a detected pedestrian. (iii) Using the
separatesvariousdrivingscenesintonear-missesandbackgroundstodetect
self-collectedPNMdatasetandSSD-MR,ourproposedmethod
dangerous situations. Therefore, it was necessary to collect a database
achieved +19.38% (on risk-level prediction) and +13.00% (on and construct an approach that would allow joint analysis of motion
joint pedestrian detection and risk-level prediction) higher representationandpedestriandetection.
scores than that of the baseline SSD and LSTM. Additionally,
the running time of our system is over 50 fps on a graphics collection of pedestrian near-miss incidents is inevitably a
processing unit (GPU). very difﬁcult task due to (i) the rarity in trafﬁc scenes,
and (ii) the region of a video capture involving a potential
I. INTRODUCTION
pedestrian incident being relatively small. The challenging
To improve self-driving performance with a driving problem must be conducted in order to avoid such a trafﬁc
recorder, appropriate spatiotemporal understanding is re- near-miss incident.
quired in addition to object detection. Herein, we focus on In this paper, we propose a novel framework called the
pedestriandetectionandmovementanalysisforanadvanced Single-Shot MultiBox Detector with Motion Representation
safety system. (SSD-MR) for joint pedestrian detection and risk-level pre-
Representative databases for trafﬁc system and au- diction in trafﬁc safety systems, including self-driving cars.
tonomous driving such as the KITTI [1] dataset and Our goal is to predict pedestrian risk levels (high-/low-/no-
CityScapes[2],haveincreasedinscaleoverthepastdecade. risk) as well as perform localization with a vehicle-mounted
Unfortunately,theydonotcontainanytrafﬁcnear-missinci- driving recorder.
dents (A trafﬁc near-miss incident is deﬁned as an event in We summarize our contributions as follows:
which an accident is avoided through evasive driving action Conceptual contribution:Sincewebelievethattheanal-
such as braking and steering [3]). The analysis of near-miss ysis of various near-miss incidents is useful for avoiding
incident videos is still challenging because most existing risks, we have collected a new trafﬁc database consisting
pedestrian detection methods cannot determine whether or ofpedestriannear-missincidents.ThisPedestrianNear-Miss
not a situation is dangerous. For instance, Figure 1 shows dataset (PNM dataset) is used to complete the joint task of
both normal (without any danger) and near-miss incident pedestrianrisk-levelprediction(high-,low-,andno-risk)and
scenes,inwhichthevehicle-mounteddriverecorderscapture pedestrian detection enables the simultaneous output of a
pedestrians.However,recentdetectionapproachesoftenhave pedestrian bounding box (bbox) and risk level.
difﬁculty identifying pedestrians in rapid motion because Technical contribution: We also propose SSD [5] with
they have visual features, such as movement and posture, motion representation (SSD-MR) as a concept of motion-
thataredifferentfromthoseofnormalpedestrians.Moreover, representation-by-detection. SSD-MR processes the spa-
recentobjectdetectorscannotdirectlyunderstandimpending tiotemporal vectors of each detected pedestrian with dilated
dangers in incident/accident scenes. Therefore, a pedestrian convolution [6] and then uses that information to predict a
dataset that contains near-miss incidents and pedestrian lo- risk level. Though the self-collected PNM dataset contains a
cations would provide us with increased opportunities to lot of blurry images and cluttered backgrounds by vehicle-
recognize dangerous scenes in safety systems. However, the mounteddrivingrecorders,SSD-MRovercomesthedifﬁcul-
ties through the joint training with pedestrian detection and
1National Institute of Advanced Industrial Science and Technology
risk-level prediction.
(AIST)
2KeioUniversity Experimental contribution: Our results clearly show the
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 1021
Authorized licensed use limited to: Heriot-Watt University. Downloaded on September 21,2020 at 05:03:51 UTC from IEEE Xplore.  Restrictions apply. effectiveness of our approach when used in conjunction
with SSD-MR and our self-collected PNM dataset. The
data collection and sophisticated modeling of our proposed
approach enable us to decisively detect the most at-risk
pedestrian and the incident risk level.
II. RELATEDWORK
Pedestrian detection. Since the deep neural networks
(DNN) burst into public view in 2012 when AlexNet with f
Fig.2. Annotationexample:Weannotaterisk-level high-,low-,andno-
ImageNet [7], [8], [9], various approaches have been pro- g f g
risk andbboxwith x,y,w,h (height(h)andwidth(w))ateachframe.
posed in the computer vision ﬁeld and the region-based Toimproveannotationquality,thelabelsarecheckedbyvalidators.
convolutional neural networks (R-CNN) [10] has become
a critical algorithm in the object recognition ﬁeld, while improvements in 3D-kernels, the data sought after in our
the search for a de-facto-standard object detection process study does not involve action recognition from a video shar-
has witnessed numerous advancements. The improvements ingservice.Torobustlypredictrisklevels,weassignanSSD-
made to sophisticated algorithms such as the Fast/Faster R- MR that extracts temporal activations based on pedestrian
CNN[11],[12]havefacilitatedprogresstowardreal-timeob- detections. This allows our SSD-MR to effectively analyze
ject detection and researchers have proposed adjustments to sequentialconvolutionalactivationswithinthedetectedbbox
make them more suitable for pedestrian detection (e.g.[13]. without considering optical ﬂows.
As a result, one-shot algorithms such as SSD [5] and you Trafﬁc datasets. Efforts aimed at putting self-driving
only look once (YOLO) [14], [15] now provide faster ways cars into practical use have included the application of the
to detect many types of object. KITTI [1] and CityScapes datasets [2] as full-task bench-
The use of representative object detection algorithms has marks. Undoubtedly, KITTI is a well-organized benchmark
facilitated pedestrian detection [16], [17]. However, since andtherearesigniﬁcantongoingeffortstoupdatethevarious
pedestrian detection algorithms currently lack the motion algorithmssuchasstereovision,trafﬁcobjectdetection,and
representation required to understand more of the detailed visual odometry to a level sufﬁcient to permit their use in
informationavailableindrivingrecordervideos,wehaveex- self-driving cars. On the other hand, CityScapes provides
tendedourdetectionalgorithmtoallowrisk-levelpredictions well-deﬁned semantic labels that can be used to train trafﬁc
using a space-time feature. sceneparsingmodels.Themostimportantissueinadvanced
Temporalanalysis.Densetrajectories(DT)andimproved driverassistancesystems(ADASs)andself-drivingcarsmust
DT (IDT) models [18], [19] have been employed in video always be trafﬁc accident avoidance. However, the above-
analysis. To represent a sophisticated motion vector, the mentioned vision-based databases are missing a perspective
DT model densely captures optical ﬂows and combines of accident/incident database collection. Along this line, the
HOG [20], HOF [21] and MBH [22] feature vectors. In NTSEL [30], [31] and the Near-miss Incident DataBase
the DNN era, several three-dimensional (3D) convolutional (NIDB) [3], [4] have issued a highly motivated challenge
networks have been proposed by Ji et al. [23] and Tran aimed at achieving a more direct and active understanding
et al. [24]. First, Ji et al. proposed a 3D-CNN model of trafﬁc accidents. Especially, the NIDB contains a large
created from the input of multiple frames that could process number of trafﬁc videos captured in real-world situations.
spatiotemporal maps from various channels like gray, gradi- As a result, the database uses task-speciﬁc ﬁne-tuning and
ent, and optical ﬂow. Meanwhile, Tran et al. presented the semantic information to help us effectively understand inci-
concept of 3D convolutional networks (C3D) as a means to dent scenes.
directly capture a spatiotemporal representation in an image However, even using the NIDB, pedestrian locations in
sequence. The C3D model employs a 3D convolutional ker- trafﬁcincidentscenescannotbespeciﬁedatasufﬁcientlevel
nel on xyt space obtained using only red-green-blue (RGB) of detail. In contrast, using the PNM dataset, we provide
sequences.Recently,atwo-dimensional(2D)-basedtemporal a large number of bbox annotations in addition to risk-
model with a CNN known as a two-stream CNN [25] has level labels, which allows us to simultaneously train trafﬁc
provedtobeasuccessfulapproachtoactionrecognition.This risk levels (high-, low-, and no-risk) as well as pedestrian
model utilizes two CNN models, one of which is trained locations.
with stacked ﬂow images while the other is trained with
III. PEDESTRIANNEAR-MISSDATASET(PNMDATASET)
sequential RGB images. In the stacked ﬂow images, dense
(cid:0)
A. Dataset summary
op(cid:0)tical ﬂows are projected into a 2D image at each x and
y direction.Surprisingly,thetwo-streamCNNsoutperforms In this section, we show the details of the PNM dataset,
theC3Dmodelonrepresentativehumanactiondatasetssuch which is based on the NIDB [3] (Examples are shown in
as UCF101 [26] and HMDB51 [27] thereby indicating that Figure 1). Although the collection of such data is very
2D-kernel performance and two-stream models are suitable difﬁcultduetotherarityofincidentsinactualtrafﬁcscenes,
for action recognition on practical datasets. Although more we have managed to collect a total of 2,880 videos for the
recent action recognition studies [28], [29] show signiﬁcant currentPNMdataset.Eachvideoconsistsof10-15seconds
1022
Authorized licensed use limited to: Heriot-Watt University. Downloaded on September 21,2020 at 05:03:51 UTC from IEEE Xplore.  Restrictions apply. of footage taken at 30 fps before/after a near-miss incident
(cid:2)
at 640 480 [pixel], and each video segment was annotated
f g
with a pedestrian’s risk level high-, low-, or no-risk based
on the low/high risk division outlined in III-B. Next, we
applied 2,208 of the videos for training and used the other
(a) Overallframes (b) First10frames (c) Last10frames
672 videos for testing. The near-miss incident videos were
Fig. 3. Averaged images with movement of bounding boxes. The pixel
obtained by vehicle-mounted driving recorders installed in valuesarenormalizedforbestviewincolor.
more than 100 taxis.
B. Deﬁnition of trafﬁc near-miss incident
A trafﬁc near-miss incident is an event in which an
accident is avoided by driving operations such as braking
andsteering.Near-misssituationsoccurmorefrequentlythan
collisions.
We evaluated risk levels as low or high based on the
potential for a collision if drivers did not take appropriate
(a) Weather (b) Timezone
actions such as emergency braking and/or evasive steering
Fig.4. Statisticsinweatherconditionandtimezone.
maneuvers. The high- and low-level risk categories corre-
spond to the time-to-collision (TTC) [32]. In the case of a Note that the PNM dataset videos were collected from
f
high-level risk, a driver must react in less than 0.5 s (TTC various vehicles, places on intersection, city areas, and
g
< 0.5 s) to avoid a collision. For low-level risks, the TTC major roads .
is more than 2.0(cid:20)s (TTC(cid:20)> 2.0 s). Videos containing mid-
D. Dataset statistics
level risk (0.5s TTC 2.0s), which appear to show a
mixture of high- and low-level risks, were excluded from We list dataset statistics in Figure 3 and 4. Figure 3
the database. To train a deep CNN, a clear visual distinction illustrates the averaged images with movement of bounding
should be made by the PNM dataset. This paper focuses on boxes. Moreover, Figure 3(a), 3(b) and 3(c) show overall,
high- and low-level risks (without mid-level risk) in order to ﬁrst 10 frames and last 10 frames, respectively. Figure 4
clearly divide the degree of risk. Especially in the risk-level denotesthestatisticsofweatherandtimezone.Weseparated
prediction, we automatically divide risk into high or low. the weather attribute into sunny, cloudy and rainy (see
Therefore, human validator only checks the automatically Figure4(a)).Similarly,wedividedthetimezoneintodaytime,
annotated labels. dawn-dusk and night (see Figure 4(b)).
C. Collection, annotation, and cross-validation for the IV. PEDESTRIANRISKANALYSIS
database
The process ﬂow of our pedestrian risk analysis system is
Although near-miss videos are difﬁcult to collect, they shown in Figure 5. Referring to the SSD-MR, we jointly
areconsiderednecessaryfordevelopingautonomoussystems execute pedestrian detection (section IV-A) and risk-level
capable of driving safely in trafﬁc. These video recording prediction (section IV-B). Our strategy aims at predicting a
systemsweretriggerediftherewassuddenbrakingresulting pedestrian risk-level by analyzing temporal activations from
indecelerationofmorethan0.5G.Eachvideowasannotated a detected pedestrian.
f g
accordingtoitsrisklevel high-,low-,orno-risk ,andeach
f g
A. Pedestrian detection
framewasanannotatedbboxwiththe x,y,w,h (height(h)
andweight(w))ofthepedestrianinrelationtothenear-miss Toextractatemporalactivationfromapedestrianarea,we
situation. See Figure 2 for examples. When no pedestrian ﬁrstmustdetectapedestrianinadrivingrecordervideo.Our
is in the frame, the bbox does not appear. Of the training baselinedetectorisdesignedbasedontheSSD[5],whichis
samples utilized, 1,030 videos were annotated as no-risk, a de-facto-standard detection framework. The detailed SSD-
337 as high-risk, and 841 as low-risk. Test samples included based architecture is shown in Figure 6. The different point
523 no-risk videos, 49 high-risk videos, and 100 low-risk from the original SSD is to output a temporal activation x
T
videos. The number of validation set is following the near- with 256-dim vector per frame.
miss dataset [3]. The other videos are used as training set. Multi-scale voting with different layers. Multi-scale
To avoid ambiguity and prevent strong bias in our data voting is processed with four different layers (conv4, 6-8) in
annotations,threeexpertannotators(whohaveknowledgefor order to output a region proposal at each layer and reliably
computer vision) trimmed and categorized each video to 30 detectapedestrianineachdrivingrecordervideo.Themulti-
frames, after which they were assigned to a single category. scalevotingscoresareobtainedfromConv4,Conv6,Conv7,
(cid:2)
Totally, PNM dataset contains 66,240 (2,208 [video] 30 and Conv8 based on the SSD. As shown in Figure 7(a),
[frame]) video frames. The dataset was then cross-validated votingscoresareprojectedintotheregionaroundpedestrian
by the 2 annotators and 2 extra validators. The annotator as a distribution. The ﬁnal detection result is decided using
and validators thoroughly checked the videos at least once. conﬁdence scores from the anchor boxes (IoU = 0:5). The
1023
Authorized licensed use limited to: Heriot-Watt University. Downloaded on September 21,2020 at 05:03:51 UTC from IEEE Xplore.  Restrictions apply. Fig.5. UsingourSSD-MR,wedetectapedestrianateachframeinordertoextractatemporalac(cid:2)tivationN-dimforrisklevelprediction.Afterobtaining
temporalactivationsfromtheT frames,stackedtemporalactivation(STA)werestackedwithT N-dimtoproducerisk-levelpredictions.Thedetailed
modiﬁedSSDarchitectureisshowninthenextﬁgure.
Fig.6. DetailedmodiﬁedSSDarchitecture.Thekernelparametersusedin
(cid:2) (cid:2) (cid:2) (cid:2)
theSSDareshownaschannels width heightlike256 3 3.Theﬁrst
ﬁveconvolutionallayersfollowtheVisualGeometryGroup(VGG)-16[33]
architecture. The detailed kernel parameters that result after the 6th layer
(Conv6) are shown in the ﬁgure. In Conv9, a temporal activation xT can
beobtainedforeachframe.
(a) Visualization of multi-scale voting. The conﬁdence
scoresobtainedfromtheleftinputimageswithfourdif-
maximum anchor box with the conﬁdence score is the ﬁnal ferentconvolutionallayersareshownintherightﬁgures.
detection result. The multi-scale vote detection process is
shown in Figure 7(b).
Featurelearningtowardriskanalysis.Inordertoobtain
sophisticatedtemporalactivationasafeaturevector,wetrain
our SSD-MR by assigning pedestrian risk-level labels and
obtaining a temporal activation (x ; the vector corresponds
T
to the 256-dim vector in Figure 6) based on the output of
Conv9.
Since the SSD-MR is trained with risk-level labels and
(b) The ﬁnal detection results are decided based on conﬁdence scores.
bboxes, we can evaluate risk-speciﬁed features from the
(Left ﬁgure) Votes are cast for four different region proposals. (Right
detected pedestrian. The SSD-MR is trained with a multi- ﬁgure) The maximum conﬁdence score is used as the ﬁnal detection
task loss function as shown below: result.
1 Fig.7. PedestriandetectionwithmodiﬁedSSD.
L(x;c;l;g;r)= (L (x;c)+L (x;l;g))+L (r) (cid:2)
N conf loc risk t 256-dimmatrixcalledstackedtemporalactivation(STA),
(1)
which consists of the stacked feature vectors given in the
where the ﬁrst and second terms (L (x;c) + output of SSD-MR Conv9 (256-dim vector in Figure 6)
conf
L (x;l;g))=N make up the loss function based on in temporal order. We analyze the STA with a temporal
loc
the SSD, L (r) is the softmax cross-entropy loss for risk convolution based on dilated convolution [6], which also
risk
prediction, and r is the set of risk-level annotations and the allows easy expansion of the receptive ﬁeld. Our temporal
prediction. convolution is shown in Figure 8. If we were to use general
convolution with a kernel size of two, 31 convolution mod-
B. Risk prediction
ules would be required to obtain a receptive ﬁeld sufﬁcient
Pedestrian risk levels are predicted by using tempora2l to cover the T (=30) frames input. Lrisk(r) with temporal
afctivation in SSD-MR.gWe deﬁne y afs the risk labegl y 2 convolution is trained using softmax cross-entropy.
high-, low-, or no-risk and v = v ;v ;:::;v (i Next, we consider an evaluation between our SSD-MR
i i1 i2 it
1;2;:::;N) as temporajl activations. To calculate the condi- and SSD with Long Short-Term Memory (LSTM) [34] (see
tionalprobabilityP(y v),wecanusethetemporallystacked Table III). In the pure comparison, the temporal convolution
1024
Authorized licensed use limited to: Heriot-Watt University. Downloaded on September 21,2020 at 05:03:51 UTC from IEEE Xplore.  Restrictions apply. TABLEI
PEDESTRIANDETECTION:EVALUATIONOFPEDESTRIANDETECTION
WITHPRECISIONANDRECALL
Approach Precision Recall F-measure
3-layerSSD-MR w/ovote .9915 .6782 .8055
w/vote .9916 .6830 .8089
4-layerSSD-MR w/ovote .9910 .6865 .8111
w/vote .9911 .7036 .8230
TABLEII
RISK-LEVELPREDICTION:COMPARISONOFBLOCKNUMBERS
Approach Ave.Recall(AR) Ave.Precision(AP) Ave.F-score
Block1 .7417 .7419 .7305
Fig.8. Temporalconvolutionwithstackedblocks.Thekernelsizeofthe Block2 .8301 .6774 .6939
dilatedconvolutionissetto2. Block3 .7970 .7212 .7437
Block4 .7985 .6448 .6661
Block5 .7422 .7464 .7381
because our dataset is different from conventional datasets
like KITTI [1].
A. Pedestrian detection
We evaluated the following properties in the pedestrian
detection experiment:
(cid:15) Which is better? A model with three convolutional
layers (3-layer) or a model with four convolutional
Fig. 9. Example of detection results. The blue rectangle is the detection layers (4-layer)? (3-layer and 4-layer in Table I. The
resultandtheredrectangleisthegroundtruthonthedetectionfailureframe.
4-layer model is better.)
Next, we compared three convolutional layers (3-layer;
has fewer parameters than the LSTM(cid:2). Wh(cid:2)en the LSTM is Conv4,Conv6,andConv7)withfourconvolutionallayers(4-
used, the number of parameters is N N 8 (input, input layer; Conv4, Conv6, Conv7, and Conv8) in SSD-MR. The
gate,outputgate,andforgetgatetotallyhaveweightsforthe 3- and 4-layer models were trained using 70,000 iterations
current input and previous output). In co(cid:2)ntr(cid:2)ast, our temporal on the PNM dataset. The pedestrian detection methods are
c(cid:2)onvolutionhasf(cid:2)ewerparameterswith2 5 B (#kernelsize validated with average precision (AP) and average recall
#convolution #block). Further results and discussions (AR) in Table I. By using the plain setting (without multi-
are provided in the experimental section. scale voting), the 4-layer model is +1.41% better than the
3-layer model with F-measure.
C. Implementation detail
(cid:15) With or without multi-scale voting in pedestrian detec-
In the detection part of SSD-MR, we use risk level labels
tion? (w/ multi-scale voting and w/o multi-scale voting
if the annotations on the PNM dataset are high or low. In
in Table I. With multi-scale voting is better.)
the test set, we have 149 videos with annotated risk levels.
Inthe temporal convolutioninSSD-MR, we train with all We compared pedestrian detection models both with and
training samples and evaluate all test samples. without multi-scale voting. From the results of our com-
The both parts were separately optimized, namely we parison, we can see that multi-scale voting is an effective
initially trained the detection part with bbox ground truths method, especially in the recall scores (+0.48% for 3-layer
then the temporal representation part (STA) is trained with and +1.71% for 4-layer). Both models perform better with
the ground truths of risk-level score. multi-scale voting (+0.34%@3-layer and +1.19%@4-layer
We set the learning rate to 0.001, the momentum to with F-measure).
0.9, and use a weight decay of 0.0005 for the detection Finally, we improved +1.75%@F-measure and
part/0.001 for temporal convolution, and use a stochastic +2.54%@recall with SSD-MR with four-layer, multi-
gradient descent (SGD) optimizer. scale voting. The visual results are shown in Figure 9. Here,
it can be seen that our SSD-MR effectively detected various
V. EXPERIMENT scales even though nighttime scenes are included in the
In the section, we mainly show the recognition and test sample. Note that pedestrians cannot be detected when
detection performance for risk-level prediction (high-, low- the outline is not clear because of halation and background
and no-risk level) and joint task with pedestrian detection. darkness (the bottom-left in Figure 9). Pedestrian detection
However, at the beginning, we conﬁrm how to construct is obviously very challenging when halation is present.
our SSD model with multiple layers and voting mechanism Although pedestrian detection does not work well at every
1025
Authorized licensed use limited to: Heriot-Watt University. Downloaded on September 21,2020 at 05:03:51 UTC from IEEE Xplore.  Restrictions apply. (a) IDT (b) SSDwithLSTM (c) SSD-MR(ours)
Fig.10. Confusionmatricesforjointpedestriandetectionandrisk-levelprediction.OurSSD-MRachievedaveraged79.33score.
TABLEIII
recorded under a variety of adverse conditions, e.g. night-
RISK-LEVELPREDICTION:COMPARISONOFOURAPPROACHTO
time,rain,clutteredbackgrounds,andviewpointdifferences.
VARIOUSPARAMETERSANDRELATEDWORKS.AR,APANDAF
The motion models including the IDT are disadvantageous
INDICATEAVERAGERECALL,AVERAGEPRECISIONANDAVERAGE
to the conditions.
F-MEASURE.
Interestingly, our temporal convolution with three stacked
Approach AR AP AF blocks outperformed the SSD with LSTM by +19.38%
IDT[19] .6167 .5531 .5828 (.7437 vs. .5499 with F-measure). Against an LSTM, which
(cid:2) (cid:2) (cid:2) (cid:2)
TemporalStream[25] .3983 .3670 .3644
has 256 256 4 2 parameters (256 256 fully
SpatialStream[25] .3515 .3436 .3178
TwoStream[25] .3299 .2899 .2990 connected layer, input and two gates have weights for the
SSDwithLSTM .6647 .5667 .5499 currentinputandpreviousoutput),ourtemporalconvolution
(cid:2) (cid:2)
SSD-MR(ours) .7970 .7212 .7437
hasfewerparameters:2 4 3(kernelsize,4convolutions
and 3 blocks). Fewer parameters, faster training speeds, and
frame, our goal is to predict a risk-level (including no-risk amoreaccuratemodelarerealizedwithdilatedconvolution.
labels) from some of the frames in which pedestrians are
Finally, we show the confusion matrices of our model,
detected in temporal order.
SSD with LSTM, and IDT in Figure 10. Our SSD-MR
records 79.33% with averaged F-score which is +13.00%
B. Risk prediction
better than SSD with LSTM. Our SSD-MR model achieved
f g f
In the temporal convolution process, we tune the number the most balanced rates 80, 79, 79 at each zero-, low-
g
ofblocksasshowninFigure8.TableIIliststherelationship , high- risk level in the joint pedestrian detection and risk
between the stacked block(s) and the performance with prediction tasks. Other methods are slightly biased to zero-
AR, AP, and F-score. The mean performance rate in the risk or high-risk from their prediction results. Please view
joint pedestrian detection and risk-level prediction task is the visual results in the supplementary video. Additionally,
evaluatedinthetable.Thebestmodelisthreestackingblocks tomeasuretherunningtimeofoursystem,weranSSD-MR
with the proposed model. on an NVIDIA GeForce Titan X with Pascal architecture.
We compared our SSD-MR with representative mod- Fromtheresultsobtained,weconﬁrmedthatoursystemruns
els, speciﬁcally IDT, two-stream CNN, and SSD with at over 50 fps.
LSTM. Basically, we employ the original tunings from
IDT (HOG/HOF/MBH, codeword vector and support vector
machine), two-stream CNN, and SSD with LSTM [34]. The VI. CONCLUSION
two-stream CNN is trained on the PNM dataset in addition
to the UCF101 pre-trained spatial- and temporal-stream. We proposed a trafﬁc near-miss detection architecture
Additionally,theLSTMisassignedonbehalfofthetemporal with temporal representations that jointly solves pedestrian
convolution. Table III lists the results obtained on the PNM detection and risk-level prediction. We also presented our
dataset. Pedestrian Near-Miss dataset (PNM dataset), which pro-
Our approach achieves better results than the IDT and vides pedestrian annotations of location and risk level. We
two-stream CNN (ours .7437 vs. IDT .5828 and two-stream demonstrated the effectiveness of our SSD with motion
CNN .2990 with F-score). Although a two-stream CNN representation (SSD-MR). The proposal is superior to the
is known as a representative motion model, it does not othermodelincludingIDTandSSDwithLSTM.Webelieve
work well on the PNM dataset. We consider it likely that thatthecombinationofpedestriandetector,stackedtemporal
stacked ﬂow images constructed by displacements of dense activation (STA), and temporal convolution performs effec-
optical ﬂow would include a large amount of noise. Moving tivelyintermsoffewerparameters,fastertrainingspeed,and
vehicle-mountedcamerascapturerelativelylargeamountsof increasedaccuracybasedontheresultsofapurecomparison
ego-motion compared with the movements of a pedestrian. with SSD+LSTM. In future, we will continue to extend the
Moreover, the videos included in the PNM dataset were dataset to improve overall performance.
1026
Authorized licensed use limited to: Heriot-Watt University. Downloaded on September 21,2020 at 05:03:51 UTC from IEEE Xplore.  Restrictions apply. REFERENCES [24] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri,
“Learningspatiotemporalfeatureswith3dconvolutionalnetworks,”in
[1] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous
ProceedingsoftheIEEEInternationalConferenceonComuterVision
driving?thekittivisionbenchmarksuite,”inConferenceonComputer
(ICCV),2015,pp.4489–4497.
VisionandPatternRecognition(CVPR),2012.
[25] K.SimonyanandA.Zisserman,“Two-streamconvolutionalnetworks
[2] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Be-
for action recognition in videos,” in Advances in neural information
nenson, U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset
processingsystems,2014,pp.568–576.
forsemanticurbansceneunderstanding,”inProceedingsoftheIEEE
[26] K. Soomro, A. R. Zamir, and M. Shah, “Ucf101: A dataset of 101
Conference on Computer Vision and Pattern Recognition, 2016, pp.
human action classes from videos in the wild.” CRCV-TR-12-01,
3213–3223.
2012.
[3] H. Kataoka, T. Suzuki, S. Oikawa, Y. Matsui, and Y. Satoh, “Drive
[27] H.Kuehne,H.Jhuang,E.Garrote,T.Poggio,andT.Serre,“HMDB:
videoanalysisforthedetectionoftrafﬁcnear-missincidents.” ICRA,
alargevideodatabaseforhumanmotionrecognition.” International
2018.
ConferenceonComputerVision(ICCV),2011.
[4] T. Suzuki, H. Kataoka, Y. Aoki, and Y. Satoh, “Anticipating trafﬁc
[28] A. Carreira and A. Zisserman, “Quo vadis, action recognition? a
accidents with adaptive loss and large-scale incident DB.” IEEE
new model and the kinetics dataset,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR),
Conference on Computer Vision and Pattern Recognition (CVPR),
2018.
2017,pp.6299–6308.
[5] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu,
[29] K. Hara, H. Kataoka, and Y. Satoh, “Can spatiotemporal 3d cnns
and A. C. Berg, “Ssd: Single shot multibox detector,” in European
retrace the history of 2d cnns and imagenet?” in Proceedings of
ConferenceonComputerVision. Springer,2016,pp.21–37.
the IEEE Conference on Computer Vision and Pattern Recognition
[6] F. Yu and V. Koltun, “Multi-scale context aggregation by dilated
(CVPR),2018.
convolutions,”arXivpreprintarXiv:1511.07122,2015.
[30] H. Kataoka, Y. Aoki, Y. Satoh, S. Oikawa, and Y. Matsui, “Fine-
[7] A.Krizhevsky,I.Sutskever,andG.E.Hinton,“Imagenetclassiﬁcation
grained walking activity recognition via driving recorder dataset.”
with deep convolutional neural networks,” in Advances in neural
IEEEInternationalConferenceonIntelligentTransportationSystems
informationprocessingsystems,2012,pp.1097–1105.
(ITSC),2015.
[8] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei,
[31] H.Kataoka,Y.Aoki,Y.Satoh,S.Oikawa,andY.Matsui,“Temporal
“Imagenet: A large-scale hierarchical image database,” in Computer
and ﬁne-grained pedestrian action recognition on driving recorder
VisionandPatternRecognition,2009.CVPR2009.IEEEConference
database,”Sensors,2018.
on. IEEE,2009,pp.248–255.
[32] Y.Matsui,M.Hitosugi,K.Takahashi,andT.Doi,“Situationsofcar-
[9] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
to-pedestriancontact,”TrafﬁcInjuryPrevention,vol.14,2013.
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg,
[33] K. Simonyan and A. Zisserman, “Very deep convolutional networks
and L. Fei-Fei, “Imagenet large scale visual recognition challenge,”
for large-scale image recognition,” arXiv preprint arXiv:1409.1556,
InternationalJournalofComputerVision(IJCV),vol.115,no.3,2015.
2014.
[10] R. Girshick, J. Ddonahue, T. Darrell, and J. Malik, “Rich feature
[34] F. A. Gers, J. Schmidhuber, and F. Cummins, “Learning to forget:
hierarchiesforaccurateobjectdetectionandsemanticsegmentation,”
Continualpredictionwithlstm,”Neuralcomputation,vol.12,no.10,
in Computer Vision and Pattern Recognition (CVPR), 2014 IEEE
pp.2451–2471,2000.
Conferenceon,2015,pp.580–587.
[11] R. Girshick, “Fast r-cnn,” in International Conference on Computer
Vision(ICCV),2015,pp.1440–1448.
[12] S.Ren,K.He,R.Girshick,andJ.Sun,“Fasterr-cnn:Towardsreal-
timeobjectdetectionwithregionproposalnetworks,”inAdvancesin
neuralinformationprocessingsystems,2015,pp.91–99.
[13] S.Zhang,R.Benenson,M.Omran,J.Hosang,andB.Schiele,“How
fararewefromsolvingpedestriandetection?” IEEEConferenceon
ComputerVisionandPatternRecognition(CVPR),2016.
[14] J.Redmon,S.Divvala,R.Girshick,andA.Farhadi,“Youonlylook
once: Uniﬁed, real-time object detection,” in Computer Vision and
Pattern Recognition (CVPR), IEEE Conference on, 2016, pp. 779–
788.
[15] J. Redmon and A. Farhadi, “Yolo9000: Better, faster, stronger,” in
ComputerVisionandPatternRecognition(CVPR),IEEEConference
on,2017,pp.7263–7271.
[16] L.Zhang,L.Lin,X.Liang,andK.He,“Isfasterr-cnndoingwellfor
pedestrian detection?” in European Conference on Computer Vision
(ECCV). Springer,2016,pp.443–457.
[17] X. Du, M. El-Khamy, J. Lee, and L. S. Davis, “Fused dnn: A
deep neural network fusion approach to fast and robust pedestrian
detection,”inarXivpreprintarXiv:1610.03466,2016.
[18] H. Wang, A. Kla¨ser, C. Schmid, and C.-L. Liu, “Action recognition
by dense trajectories,” in Computer Vision and Pattern Recognition
(CVPR),2011IEEEConferenceon. IEEE,2011,pp.3169–3176.
[19] H.WangandC.Schmid,“Actionrecognitionwithimprovedtrajecto-
ries,”inProceedingsoftheIEEEInternationalConferenceonComuter
Vision,2013,pp.3551–3558.
[20] N.DalalandB.Triggs,“Histogramsoforientedgradientsforhuman
detection,”inComputerVisionandPatternRecognition(CVPR),IEEE
Conferenceon. IEEE,2005,pp.886–893.
[21] I. Laptev, M. Marszalek, C. Schmid, and B. Rozenfeld, “Learning
realistichumanactionsfrommovies,”inComputerVisionandPattern
Recognition(CVPR),IEEEConferenceon. IEEE,2008,pp.1–8.
[22] N.Dalal,B.Triggs,andC.Schmid,“Humandetectionusingoriented
histograms of ﬂow and appearance,” in European Conference on
ComputerVision(ECCV). Springer,2006,pp.428–441.
[23] S.Ji,W.Xu,M.Yang,andK.Yu,“3dconvolutionalneuralnetworks
forhumanactionrecognition,”IEEETransactionsonPatternAnalysis
andMachineIntelligence(TPAMI),vol.35,no.1,pp.221–231,2013.
1027
Authorized licensed use limited to: Heriot-Watt University. Downloaded on September 21,2020 at 05:03:51 UTC from IEEE Xplore.  Restrictions apply. 