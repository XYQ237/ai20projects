2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
Hierarchical Interest-Driven Goal Babbling for Efﬁcient Bootstrapping
of Sensorimotor skills
Rania Rayyes, Heiko Donat and Jochen Steil
Abstract—We propose a novel hierarchical online learning
scheme for fast and efﬁcient bootstrapping of sensorimotor
skills. Our scheme permits rapid data-driven robot model
learning in a “learning while behaving” fashion. It is updated
continuouslytoadapttotime-dependentchangesanddrivenby
an intrinsic motivation signal. It utilizes an online associative
radial basis function network, which is the ﬁrst associative
dynamicnetworktobeconstructedfromscratchwithhighsta-
bility. Moreover, we propose a parameter-sharing technique to
increaseefﬁciency,stabilizetheonlinescheme,avoidexhaustive
parametertuning,andspeedupthelearningprocess.Weapply
ourproposedalgorithmsona7-DoFphysicalrobotmanipulator
and demonstrate their performance and efﬁciency.
I. INTRODUCTION
Fig.1. OurproposedschemepermitsefﬁcientlearningofIKmodelswhich
It is widely accepted since the 1990’s that the human canbeappliedforexampletoenablereachingtaskswithaBaxterrobot
motor control is organized on the basis of forward and Themainchallengeofthesemethodsisefﬁciency,i.e.,the
inversemodels(i.e.,therelationbetweenactions/motorcom- number of required training samples. Most online learning
mands and outcomes) [1], these also play a main role in methodsinroboticstendtorequireahighnumberoftraining
motorcontrolarchitecturesinroboticsandobtainingthemis samples, which increases exponentially with the number of
essentialforanyembodiedagenttomasteritsbody.Learning DoF [3], [13], while sampling with real robot applications
inversemodelsofrobotmanipulators,i.e.,estimatethemotor is costly and intractable in terms of time, tear and wear.
command required to achieve a desired outcome, has been Therefore, the majority of the related works on intrinsic
considered as a promising alternative solution to analytical motivation have been demonstrated only in simulation as a
methods since obtaining an accurate analytical model for proof of concept and only a few works in real robot experi-
dexterous high degrees of freedom (DoF) robots and soft ments (e.g., [14], [15]). In this paper, we focus on tackling
robots requires a lot of engineering knowledge and can be the online data-driven learning challenges, i.e., efﬁciency,
challenging if no accurate parameters are available [2], [3]. stability,andentirelyonlinewithoutanyintermediateofﬂine
Learning robot models and skills has also been a core methods. Therefore, we propose a novel online interest-
researchtopicofthedevelopmentalandcognitiverobots[4]. driven exploration scheme which is updated continuously
The developmental robots autonomously develop and adapt with Online Episodic Mental Replay (OEMR) method in
in open-ended environments through lifelong learning [5]– ordertospeedupthelearningprocessanddrasticallyreduce
[7]. In contrast to the industrial robots, which are used to the number of required sample. We apply our scheme on
accomplish predeﬁned tasks, the developmental robots have a physical 7-DoF manipulator (Baxter robot by Rethink
to solve unforeseen and unpredictable challenges, acquire Robotics cf. Fig. 1).
repertoiresofskillsandshouldbeversatile,ﬂexible,andable
Most of the recent intrinsic motivation methods are
to adapt to time-dependent changes (e.g., friction [3] or tool
competence-based approaches [8], [10]–[12], [16], [17],
usage[8],[9]).Thatmakesonlinelearningandonlineadapta-
which derives the intrinsic motivation signal based on the
tionessentialrequirements.Therefore,intrinsicallymotivated
robot’s learning progress. In contrast to the previously pro-
goal-directed methods, which rely on online data-driven
posed competence-based schemes which rely on storing the
learning, have gained a lot of attention recently. Interest-
complete data set, which is not feasible through lifelong
driven or curiosity-driven behaviors have been observed in
learning, we propose an interest measurement which is
growing children. They get bored by already known things
updated on the ﬂy based on the current robot performance
and try to discover new ones and learn new skills [10]. This
withouttheneedofstoringcompletedatasets.Moreover,our
inspires the idea to implement intrinsic motivation schemes
schemeutilizesonlytheinterestmeasurementtointrinsically
for robots to explore their environments actively [10]–[12].
drive the system in contrast to other schemes, e.g., [12],
which is combined with random goal selection.
The authors are with Technische Universita¨t Braunschweig, Insti-
Ourschemeconsistsofahigh-levelgoalselectionutilizing
tut fu¨r Robotik und Prozessinformatik, 38106 Braunschweig, Germany
{ }
rrayyes,hdo,jsteil @rob.cs.tu-bs.de interest measurement and a low-level exploration, which
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 1336
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 21,2020 at 11:46:26 UTC from IEEE Xplore.  Restrictions apply. relies on Goal Babbling (GB) [18] (cf. Fig. 2) for direct
inverse model learning through exploration. GB has been
proposed as a promising goal-directed method for online
bootstrapping sensorimotor skills. It is inspired by infants’
motor learning skills ”Learning while Behaving” [19], e.g.,
learning how to reach by trying to reach and improving
the acquired skills by iterating the trials. Its ﬂexibility and
adaptability have been veriﬁed (see [8], [12], [20]–[23]).
For redundant high DoF robots, GB by design learns only
one preferred redundancy resolution [18]. However, humans
demonstrate more ﬂexibility to solve the required tasks with
different redundant solutions. To gain more ﬂexibility and
versatility in solving the required tasks, it is preferable to
learnmultiplesolutions.Associativedynamicnetworks[24],
[25]havebeenproposedtotacklethischallengeandprovide
a suitable representation for multiple redundant solutions.
However,theseapproachesworkonlyofﬂine,whichrequires Fig. 2. Hierarchical interest-driven associative GB scheme. (A) Goal
selectionmechanism,(B)GBexplorationscheme,(C):Associativememory
thefulldatasettobestored.Besides,thenetworkcomplexity
(a)OARBFtraining,(b)establishafeedbackforOARBFexploitation.
(e.g.,ahiddenlayersize)needstobesetinadvancewithout
and an associative memory to consolidate multiple solutions
anyonlineadaptationpossibility,whileanychange(e.g.,new
(C). We demonstrate our scheme for learning kinematics,
tasks or environment changes) will require to recollect the
where the task is to learn how to reach some desired
data and retrain the network from scratch.
spatial positions (goals). In the following, we will explain
We devise a fully online associative network to learn
in detail each component of the scheme. Note that each
multiple models through exploration. The different solutions
componentisratherageneralindependentalgorithmandcan
are biased by different default “home postures” to start the
be implemented in other learning schemes.
exploration from. The complexity of the network is au-
A. Interest measurement and goal selection
tonomouslyadaptedtothelearningproblem.Itturnsoutthat
themainchallengeforonlinedynamicassociativenetworkis Theinterestmeasurementdetermineswhichgoaltherobot
stability. In real applications, the exploration while behaving will try to attain. At the beginning, all goals are interesting
produces very noisy data, which makes online incremen- as the robot does not have any knowledge about them.
tal associative dynamics potentially unstable. To mediate The interest measurement is updated continuously on the
this effect, we propose a parameter-sharing technique that ﬂy based on the robot’s performance. There are two main
assures stability by combining incremental regression with criteria to determine the interest measurement: the relative
associative dynamics to leverage both advantages: stability, error(RE)andtheforgettingfactor(FF).TheRE(cf.Eq.(1))
accuracy, and multi-model representations. This increases measures the performance error on each goal relative to the
efﬁciency and avoids exhaustive hyperparameter tuning. other goals’ errors. A high performance error indicates that
Tosummarizeourcontributionsinthispaper:Wedevisea the attained task/goal has not been learned well yet. The
fully online, efﬁcient, applicable, ﬂexible, and stable hierar- better the performance is, (i.e., the error decreases), the less
chicallearningschemewhereitcombinesfournewmethods: interesting the goal becomes.
• A new intrinsic motivation signal based on the interest −
E(g ) E
measurement to enable efﬁcient online exploration. RE(g )= i − min (1)
i E E
• An online incremental associative radial basis function max min
network (OARBF), which is the ﬁrst online associative where E(g ) is the current performance error on the goal
∈ G i
dynamic network to be constructed totally from scratch g , E and E are the current minimum and
i min max
with high stability. maximum performance errors over all goals respectively.
• A new online episodic mental replay (OEMR) to accel- As the inverse model is updated locally and continuously,
erate the learning process. therobotperformancemightgetenhanced(therobotbeneﬁts
• A parameter-sharing technique which increases the ef- fromotherexperiences)orgetdeteriorated(therobotforgets
ﬁciency and stabilizes the full learning scheme in the potentially about the previously learned scenarios). Hence,
presence of highly noisy data. we propose a forgetting factor (FF) which indicates the
goals that the robot starts to forget about. The higher the
The video illustrating the work ia available in [26].
factor gets, the more interesting the goal becomes. FF can
II. HIERARCHICALINTEREST-DRIVENASSOCIATIVE be estimated using the following criteria:
GOALBABBLING 1) CurrentProgress: ThecurrentprogressfactorProg is
Fig.2illustratesourproposedHierarchicalInterest-Driven measuredbasedontherobotperformanceonagoalg overa
i
Associative GB scheme. It consists of a high-level layer: slidingtimewindowwithngoaltrialsasillustratedinFig.3.
goal selection (A) and two low-level layers: exploration (B) When the error increases over the time window, the goal
1337
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 21,2020 at 11:46:26 UTC from IEEE Xplore.  Restrictions apply. the lower dimensional task space, which represents the ob-
servationspace,toq inthehigherdimensionalconﬁguration
space,whichrepresentstheactionspace.Hence,GBpermits
efﬁcient exploration of the task space [27].
The robot starts exploring from its default (home) posture
qhome corresponding to the starting (home) position xhome
∈G ⊆P
bytryingtoreachsomepredeﬁnedtargetsg which
i
are selected based on the interest measurement mechanism
(cf. Sec. II-A). A linear path of intermediate targets is
Fig.3. Performanceprogressonag goalthroughnsamples generatedbyinterpolatingbetweenthecurrenttargetandthe
becomes more interesting as the robot starts to forget about next selected one. The robot tries to reach each generated
it.Progisnorma(cid:80)lizedrelativetot(cid:80)heminimumProgmin and and the selected target, using the local inverse estimate as
maximum Progmax of all goals’ Prog factors(cf. Eq. (2)) follows: A correlate∗d exploratory noise σ is added to the
estimated output qˆ in order to discover and learn new
Prog(gi)= jj==nn2 Ej(gi)−n −jj==1n2 Ej(gi) (2) oreustuclotminegsen(qd+teff=ectoσtr(xp,ots)iti+onqˆx∗t)+t. iqs+tobisserevxeedc.uted and the
A Local Linear Map (LLM) [18], [20], [28] is used as
Prog(g ) Prog
Prog(gi)= Prog i −Progmin an incremental regression algorithm to build and update
max min
the inverse estimates. Note that any incremental regression
ThisProg factorissimilartothecompetenceprogress[12].
techniquecanbeimplemented,wechooseLLMasitdemon-
However, Prog considers only when the robot starts to
strates a very good accuracy for estimating complex models
forget, while competence progress focuses on the learning
(e.g., inverse statics [20]). For redundant kinematics, GB
progress whether it enhances or deteriorates Further com-
tries to select and learn the most efﬁcient solution using the
parison is illustrated in Sec. IV-A.1
2) General Progress Overview: The second forgetting following weighting scheme: 
fBaecctoaruseisththeedagteanesreatlisovneortvisetworeodf, tthheegleenaernrainlgpropgroregsrsesiss. wdtir = 12(1+cos(cid:94)(x∗t −x∗t−1,x+t −x+t−1)
(cid:107) − (cid:107)·(cid:107) − (cid:107)− (6)
measured by the current performance error on the current weff = x+ x+− q+ q+− 1
t t t 1 t t 1
goal relative to the minimum (Emin(gi)) and the maximum wgb =wdir·weff
(Emax(gi)) performance error of−the goal gi (cf. Eq. 3). where t is the time step, x∗ its the detsired tposition, x+ is
REg(g )= E(gi) −Emin(gi) (3) the real end-effector position which corresponds to the real
TFFhecionuitlidalbvealmueesasEumrieidn(egiEit)hme=raxbi(nygfiu),sEinmgEamoxni(neg(ig)oif=)th0e,se∀gfiac∈toGrs. caeolfiﬁngcﬁnigsenuwcrayetliloonfwiqtthh+e,thawcetdtuiirnaltaesnmsdeoesvdsemwonehene,tt.haen(rxdt+hwe,qetaf+cft,uwmalgebma)souivrseesmusethendet
t t t
or as a combination of them (cf. Eq. 4). However, it might to update the local inverse estimate online in a supervised
be better to forget about poorly performing previous learned learning fashion in order to minimize the weighted error Eq
t
e(λxpe=rien0c).esTahnisd atasskuemopntiloynPirsoignvfeascttiogratiendtofucrothnesirdeinratoiounr (qˆc+tf.cFoign.ﬁ2g,urEaqti.o(n7s))abseftowlleoewnitnhge:actual q+tand the estimated
experiments (cf. Sec. IV-A.1). The interest measurement is Eq =wgb(cid:107)q+−qˆ+(cid:107)2
given by the combination of RE and FF (cf. Eq. 5) t t t t
− · ∂Eq (7)
− θ =θ η t
FF(g )=γProg(g )+(1 γ)REg(g ) (4) t+1 t ∂θ
i i i t
interest(g )=λRE(g )+(1−λ)FF (5) where θ are the LLM parameters and η is the learning rate.
i i qhome is used with a weighting scheme in order to control
{ } ∈
where γ, λ [0, 1] are the weighting parameters. Note which solution will be learned. For example, if the robot
that all factors and measurements are normalized in order to startsexploringwithanelbow-downhomeposture,thesam-
have comparable measures for all required goals/tasks. ples with an elbow-down conﬁguration will receive higher
weights than the samples with an elbow-up conﬁguration
B. Interest-Driven Goal Babbling
and vice-versa. xhome has been used as a resting position
GB is a goal-directed method for learning inverse robot (cid:28)
in the original GB with probability ρ 1 in order to avoid
models.Ithasbeenproposedforlearninginversekinematics
drifting [18]. In our proposed scheme, xhome is considered
(IK) [18], i.e., learning the required joint conﬁgurations
as one of the predeﬁned targets, where the robot is able to
∈Q⊂R ∗ ∈P ⊂R
q m toattainsomedesiredpositionsx n. autonomously return to it due to the forgetting factor.
Where m is the number of DoFs, n is the dimension of the
∈{ } C. Incremental Online Associative Radial Basis Function
target variable (e.g. n 2,3 for the spatial position of the
Q P
end-effector), and arethepermissibleconﬁgurationsand Inordertogainmoreﬂexibilityandlearnseveralsolutions
the corresponding positions respectively. IK maps x from with GB, we propose an Online Associative Radial Basis
1338
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 21,2020 at 11:46:26 UTC from IEEE Xplore.  Restrictions apply. Function (OARBF) network which is constructed incremen- [20], while OARBF can model and represent multiple solu-
tally from scratch and its network complexity (e.g., hidden tionsusingmulti-stabledynamicattractors.Bothlearnersare
layer size) is adapted to the learned problem autonomously. constructed online, incrementally and from scratch. Hence,
As illustrated in Fig. 2(a), OARBF consists of three layers: online clustering of the input data is needed for both to add
input, output and hidden layer. The input and output layers the prototypes of LLM and the neurons of OARBF. The
are usually identical with the same number of neurons. The main differences are: First, the basis functions in an LLM
input inp and output out data for learning kinematics are are local linear functions centered around the prototypes, in
∈P ⊂
vectors that concatenate the end effector positions x contrast to the OARBF where activation functions are radial
R ∈ Q ⊂ R
n and conﬁgurations q m (inp = out = basis functions represented by hidden neurons. (cf. Eq. (9)).
∈ R
[x,q]T n+m). Hence, the data should be normalized Second, the input and output dimensions of the LLM and
∈R
in order to have equivalent contributions. The data here OARBF. For LLM the input for learning IK is x n and
− ∈R
is normalized to [ 1, 1], given the joint limits and the the output is q m. While OARBF input and output data
∈R
task dimension. The neurons in the hidden layer are added is deﬁned as [x,q]T n+m.
incrementally based on the online data stream. c and c Therefore, the online clustering is done only for the LLM
x q
determine the RBF centers which are added incrementally in low dimensional Cartesian space. Consequently, when
usinganonlineclusteringalgorithm(cf.Sec.II-D.2).w and the prototypes of LLM are added, the neurons of OARBF
x
w aretheoutputweightsupdatedateachstepbyperforming are added instantaneously. This accelerates the clustering
q
a gradient descent in order to minimizethe weighted error and stabilizes the full learning system as it yields better
(cid:107) − (cid:107) homogeneous distribution of the prototypes. Moreover, only
Et =wgtb inpt ouˆtt 2 one parameter set θ = {η,r} is shared with both learners,
wout =wout−η· ∂Et (8) where η is the learning rate and r is the radius which
t+1 t ∂wout determiners the of the vicinity of each basis function. β =
t q
n
where η is a learning rate, t is a time step, wout = mβx (cf. Eq. (9)). Thus we have to tune only 3 parameters
[w ,w ]T, wgb is the weight of the data sample given for both learners, and only one clustering which increases
x q t
in Eq. (6) and ouˆt is the estimated output of OARBF. the efﬁciency factor up to 4.
The association setup solves the redundancy resolution by 1) Learnerinitialization: TheLLMisinitializedwiththe
utilizing different hidden layer’s activations, i.e., the data ﬁrstlinearfunctionGˆ(1)(x)centeredaroundaprototypevec-
(cid:54) (cid:54)
pairs (x ,q ), (x ,q ) where x = x , q = q ,i = j have torq (1) =xhome correspondingtothehomepostureqhome
i (cid:54) i j j i j i j p
h(x ,q )=h(x ,q ) [24]:  [18][20].OARBFisinitializedwiththeﬁrstneuroncentered
i i j j (cid:80) {
around the ﬁrst received sample c = xhome,c =
−ht(x,q)= −Hif=t1(xfi,(xq,) q) (9) qwheoimghet}s., iT.eh.e, {owutxp1ut=wceqig1,hwtsxa1re=inciqt1ia}xl1iiznedorwdeitrhtothsehiiqfn1tptuhtes
f (x(cid:16), q)=exp((cid:17)β d(x,c )2 β d(q,c )2) output of the network to the current initial sample.
t x x q q
2) Online clustering for the learners: When LLM re-
where β is used to control the spread and the overlap of ceives a new sample with a distance of at least a radius r to
n
RBFs βq = mβx , d is the Euclidean distance between allexistentprototypes,anewlocallinearfunctionGˆ(i+1)(x)
the newly received sample and all existent RBF centers, and willbeaddedandcenteredaroundthenewlyreceivedsample
H is the number of the hidden neurons. The outputs are q (i+1) = x . Gˆ(i+1)(x) is initialized with the last
estimated based on the hidden layer’s activation as follows p new
inverse estimation before adding the new function in order
ouˆt(x,q)=wouth(x,q) (10) toavoidabruptchangesintheinverseestimatefunction,i.e.,
the insertion of the new function will not change the local
Note that the equations Eq. (9) and Eq. (10) are similar behavior of Gˆ(x) at x . LLM parameters are initialized
new
to [24]. However, the output weights are initialized and as in [18], [20]. Accordingly, a new neuron will be added
updated differently. Furthermore, OARBF is constructed to OARBF, initialized with the output weights of its closest
incrementally online and from scratch. IK as well as for- neighbortoavoiddrasticchangesinthelearnedfunctionand
ward kinematics (FK) are learned simultaneously utilizing centered around the newly received sample.
OARBF. Similar to [24], an output feedback-driven loop is
established (cf. Fig. 2(b)) to query the learned model. The III. ONLINEEPISODICMENTALREPLAY
network converges to one of the learned solutions based on For rapid online adaptation and to accelerate the learn-
the previous state of the network. ing process, we propose an online episodic mental replay
(OEMR).Mentalreplaymechanismhasbeenprovedtobean
D. Learners setup and parameter-sharing
essentialcomponentinhumanlearningprocess[29].Several
We propose a parameter-sharing technique in order to replay mechanisms have been proposed for artiﬁcial agents
increase the learning efﬁciency, avoid exhaustive hyper- to speed up the learning process, e.g., Experience Replay
parametertuning,stabilizeandspeedupthelearningprocess. [30], [31] which stores the data set and randomly samples
LLMhasdemonstratedhighstabilityandhighaccuracy[18], againfromitwithminibatchlearning.ImaginaryExperience
1339
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 21,2020 at 11:46:26 UTC from IEEE Xplore.  Restrictions apply. x
0.5 Workspace
RealEnd-EffectorPositions
Training Targets
0
y
-0.5
0.2 0.3 0.4 0.5 0.6 0.7 0.8
(a) Competence-BasedGB (b) OriginalGB
Fig.6. OriginalGBandCompetence-basedGBperformance
(a) 10R reaching the same posi- (b) Virtualtargetgridinvisualized
tionwithdifferentconﬁgurations inrviz
x x
1 Workspace
Fig.4. RobotsSetup 0.5 0.8 0.5 RTreaailn Eign dT aErfgfeectstor Position
0.6 Test Targets
y0 0 y
0.4
-0.5 0.2 -0.5
0
0.4 0.5 0.6 0.7 0.8 0.4 0.5 0.6 0.7 0.8
(a) InterestMeasurement (b) PerformanceResults
Fig.7. Interest-DrivenGB
averagestandarddeviation(std)ofRMSE.Althoughtheavg.
(a) GBLLM (b) AGB-OARBF
RMSE is reasonable for all GB schemes, the original GB,
Fig.5. Interest-DrivenstdRMSE as well as the GB with competence measurement, do not
Replay [32] and Hindsight Experience Replay [33], [34] use guarantee to achieve all the required targets, especially the
sample augmentation in order to sample imaginary goals targets which are difﬁcult to be reached (e.g., the targets
which needs the full goal space to be known in advance. locatedneartotheworkspaceborder)asillustratedinFig.6.
Mental Replay [15] samples several trajectories where each The reason is, on the one hand, the original GB relies on
is replayed once to intensify each experience situation. In a random goal selection. Consequently, all targets receive
contrast to these approaches, OEMR doesn’t require storing similarattentionbytherobot,whileinourproposedscheme,
severalepisodesnorproducingadditionalimaginarysamples. therobotfocusesonhard-to-reachanduntrainedtargets,and
The samples of each last epoch, which consists of M trials it recovers from forgetting previously learned targets due to
ofsomeselectedgoals,arestoredinthereplaybuffer.Atthe the application of FF. On the other hand, the exploration in
endofeachepoch,thesesamplesarereplayedonlinewithout [12]reliesonthecombinationofarandomgoalselectionand
an execution on the robot in order to intensify previous thecompetencemeasurement.Thecompetencemeasurement
experiences. The online replay mechanism accelerates the gives high interest to the area of the highest progress,
convergenceofthelearnerasthemodelisbuiltincrementally whethertherobotislearning(RMSEdecreases)ortherobot
and updated continuously, where only one gradient descent forgets (RMSE increases). While in our learning system is
step is done for each sample (cf. Eq. (7), Eq. (8). completely driven by its interest, concentrating on tasks that
have either been forgotten or not well learned. In addition,
IV. EXPERIMENTALRESULTS
the exploration with competence measurement originally
We ﬁrst implement our proposed scheme in an illustrative
relies on the nearest neighbor strategy [8], [12], [16], which
10 R planar manipulator experiment (cf. Fig. 4) in order to
is not applicable in our proposed methods as the models are
demonstrate the efﬁciency as well as the advantages gained
updated online and no data set is stored.
by our proposed scheme and compare it to the state-of-art
Table I also illustrates the high stability of the Interest-
[12], [18]. The parameters are obtained with pattern search
[35]. Then, we demonstrate our proposed scheme with a 7 DrivenGBwiththeminimumstdRMSE0.8mm(theshaded
area in Fig. 5(a)) which surpasses the other methods and
DoF physical robot manipulator (cf. Fig. 1).
showsarobustperformanceoverallexperiments.Alltraining
A. 10 R planar manipulator Experiment without Replay andtesttargetsarealwaysreachedwithourproposedinterest
1) Hierarchal Interest-Driven Goal Babbling: Our pro- measurement with high accuracy as illustrated in Fig. 7(b),
posed Hierarchal Interest-Driven GB (cf. Sec. II-A, Sec. II- where the red points represent the training targets, the green
B), the original GB [18] and GB with Competence mea- ones represent the test targets, and the blue circles are the
surement [12] have been implemented for a 10R planar observedend-effectorpositions.TableIalsoshowsthatProg
manipulator shown in Fig. 4(a), each link length is 10 cm. factor yields higher stability than Reg, as expected when it
The task is to achieve multiple predeﬁned targets. Each is desirable to forget about the poorly performing previous
experiment was repeated 20 times with 500 epochs, each experiences (cf. Sec.II-A). The validation RMSE has been
epoch consists of 100 samples collected at each time step. computed at each epoch to test the robot’s performance on
Table I illustrates the average validation and test per- all training targets. The validation RMSE converges after 35
formance root mean squared error (RMSE) as well as the epochs as shown in Fig. 5(a). Fig. 7(a) shows the interest
1340
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 21,2020 at 11:46:26 UTC from IEEE Xplore.  Restrictions apply. TABLEI { }
set is η =0.0725,σ =0.0452,r =0.0869β =5
10REXPERIMENTALRESULTSCOMPARISON x
1) HierarchicalInterest-DrivenGoalBabbling: Intheini-
HierarchalGB10RExperimentalResults
tialization phase of the experiment, the experimenter shows
avg.Validation avg.Test avg.RMSE
GoaOlrBigaibnballing raGnodaolmSesleelcetcitoionn R7M.4S··E10[−−m3] R3M.6S··E10[−−m3] s5td·1·[0m−3−] tthheerdoebsoirtewdhweroertkospeaxcpelo.rAebtyhrseiem-dpilmymenosvioinngalitvsiarrtumaltogdriedﬁnoef
RE,Prog 2.7·10−3 1.9· 10− 3 0.8·10−3 targetsisgeneratedinthedetectedworkspaceillustratedina
Interest-Based RE,REg 4.7·10−3 2 1·03− 1.5·10−3 3Dvisualizer (Rvizprovided byROS) asshown inFig. 4(b)
RE,Prog,REg 4.3·10−3 2.3·10−3 1.5·10−3 while the experiment is conducted with the physical robot.
Competence 5.5 103 2.3 103 1.5 103 The targets are scattered in a cuboid shape with a vertical
HierarchicalInterest-DrivenAGBvsAGB
AAGGBB raInndteormests-eBleacsteidon 91.07··1100−−33 98··1100−−33 23.05··1100−−33 andInhtohreiztornaitnalindgisptahnacsee,otfhe10rocbmotbsetatwrteedenexthpelomri.ng from its
homeposture,tryingtoreachthetargetsbasedontheinterest
measurementsoverallepochs.The“yellow”targetsrepresent measurement. Each target trial consists of N intermediate
the most interesting targets for the robot. They represent samples, which varies depending on the distance between
the hard-to-reach targets, e.g., targets near the workspace the targets. Each training episode consists of 1000 samples,
border.Thetargetsindarkblueindicatethattherobotbarely includingallintermediatesamples.OEMRisperformedafter
tries to attain them as the learned model beneﬁts from other each epoch, which took 10 seconds. Only 4 epochs were
experiences to generalize well. needed to learn to reach 41 targets with 6.7 mm training
2) HierarchalInterest-DrivenAssociativeGoalBabbling: RMSE. After 3 hours and 20 min training phase with a
Fig.4(a)showstheexperimentalsetup.First,therobotstarts samplingrateof3secpersample,therobottriedtoreach93
exploring with a curvature-up home posture utilizing our new targets randomly scattered in the explored workspace.
proposed scheme, trying to reach some predeﬁned targets. All the targets were reached with an RMSE of 7.8 mm,
After N epochs, the robot moves to its new home posture, which is acceptable accuracy considering the low positional
±
which is in this setup a curvature-down conﬁguration and accuracy of the Baxter robot of 5 mm.
continuesexploringtryingtoreachthesametargetset.LLM 2) Hierarchical Interest-Driven Associative Goal Bab-
andOARBFareupdatedateachstep.Thetargetsarechosen bling: To test our hierarchical interest-driven AGB with
iteratively based on the interest measurement. 66 neurons Baxter,theexplorationwasdoneasdescribedinIV-A.2with
were added incrementally to OARBF. The robot managed to two different home postures (angles in radian): qhome1 =
− − − −
reach all test and training targets based on the initial robot [ 0.17, 0.25, 0.12,0.93, 0.71,1.72,0.61]T, qhome2 =
− − −
state (curvature-up or down) without inconsistencies with [ 1.20, 0.615,0.38,1.34,0.29,1.28, 0.329]T. The train-
avg. RMSE. 9 mm. Fig. 5(b) shows although it is a highly ing phase is 5 episodes for each exploration phase with
dynamic and very noisy system, the network demonstrates OEMR. 30 virtual Targets were generated in the training
highstabilityduetotheparameter-sharingtechnique.TableI phase.Therobotperformanceisevaluatedon27newtargets
shows the comparison between the associative GB (AGB) with two different initial starting conﬁgurations. The robot
where the goals are chosen randomly and the hierarchical manages to reach all the targets without any inconsistencies,
interest-driven AGB. The later one demonstrates very robust i.e., without switching solutions or averaging between them
performanceillustratedwithminimumstdRMSEof2.5mm. for the OARBF. 22 targets were reached with a test RMSE
Asimilarexperimenthasbeendonein[24],whereARBF of 6.4 mm, and only 5 targets with an avg. RMSE of
istrainedofﬂinewithapre-ﬁxednetworksizeof300neurons 8.6 mm as they were difﬁcult to reach because of self-
to achieve similar accuracy without the possibility of any collision avoidance by the robot system.
online adaptation. The exploration in [24] has been done
in two different phases. The full data sets were stored to be V. CONCLUSION
consolidatedinARBFofﬂine.Incontrast,ourschemeisfully Weproposedfourgeneralmethodsthatareintegratedinto
updated online on a ﬂy with simultaneous exploration and a novel hierarchal online interest-driven learning scheme. It
consolidation,andthenetworksizeistailoredtotheproblem. guarantees to accomplish all required tasks, accelerates the
Besides, two learners need to be tuned and an additional exploration and learning procedures by drastically reducing
clustering phase is required in [24], in contrast to the saved the number of required samples, and demonstrates robust
effortsinoursystemduetotheparameter-sharingtechniques. performance with a smaller RMSE standard deviation com-
pared to the current state of the art. The system is able to
B. 7 DoF physical robot manipulator (Baxter) with Replay
learnmultiplesolutionsforsolvingrequiredtasksﬂexiblyby
Todemonstratetheapplicabilityofourapproachonareal utilizing our proposed OARBF, which is the ﬁrst associative
robot, we utilized the left 7-DoF arm of a Baxter Robot dynamic network to be constructed incrementally with high
by Rethink Robotics (cf. Fig. 1). We chose Baxter because stability. We have demonstrated the applicability of our
of its inaccuracy with a precision of 5 mm [36], which scheme with direct online training on a real 7 DoF physical
is challenging for the learners to cope up with more noisy humanoid arm. The robot learned online from scratch and
data. Baxter sampling rate using MoveIt - Motion Planning accomplished the required task with a reasonable number of
Framework [37] is 3 sec in our experiments. The parameter samples and sufﬁcient accuracy.
1341
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 21,2020 at 11:46:26 UTC from IEEE Xplore.  Restrictions apply. REFERENCES [24] R.ReinhartandM.Rolf,“Learningversatilesensorimotorcoordina-
tion with goal babbling and neural associative dynamics,” in IEEE
[1] D. Wolpert and M. Kawato, “Multiple paired forward and ICDL,Aug2013,pp.1–7.
inverse models for motor control,” Neural Networks, vol. 11, [25] R. F. Reinhart and J. J. Steil, “Learning whole upper body control
no. 7-8, pp. 1317–1329, Oct. 1998. [Online]. Available: https: withdynamicredundancyresolutionincoupledassociativeradialbasis
//doi.org/10.1016/s0893-6080(98)00066-5 functionnetworks,”inIROS. IEEE,2012,pp.1487–1492.
[2] M.RolfandJ.Steil,“Efﬁcientexploratorylearningofinversekinemat- [26] R. Rayyes, H. Donat, and J. Steil, “Hierarchical interest-driven
icsonabionicelephanttrunk,”vol.25,no.6,2014,pp.1147–1160. (associative) goal babbling for efﬁcient exploration video.” [Online].
[3] D.Nguyen-TuongandJ.Peters,“Modellearningforrobotcontrol:a Available:https://www.rob.cs.tu-bs.de/node/809
survey,”CognitiveProcessing,vol.12,no.4,pp.319–340,Apr.2011. [27] M. Rolf, J. J. Steil, and M. Gienger, “Goal babbling permits direct
[4] M. Asada, K. F. MacDorman, H. Ishiguro, and Y. Kuniyoshi, learning of inverse kinematics.” IEEE Trans. Autonomous Mental
“Cognitive developmental robotics as a new paradigm for the Development,vol.2,no.3,pp.216–229,2010.
design of humanoid robots,” Robotics and Autonomous Systems, [28] H. Ritter, “Learning with the Self-Organizing Map,” in ICANN-91,
vol. 37, no. 2-3, pp. 185–193, Nov. 2001. [Online]. Available: T.Kohonen,Ed.,vol.1. NorthHolland,1991,pp.379–384.
https://doi.org/10.1016/s0921-8890(01)00157-9 [29] D. Foster and M. Wilson, “Reverse replay of behavioural sequences
[5] J.Schmidhuber,“Developmentalrobotics,optimalartiﬁcialcuriosity, inhippocampalplacecellsduringtheawakestate,”Nature,vol.440,
creativity,music,andtheﬁnearts,”ConnectionScience,vol.18,no.2, no.7084,pp.680–683,32006.
pp.173–187,2006. [30] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou,
[6] A. Cangelosi, M. Schlesinger, and L. B. Smith, Developmental D.Wierstra,andM.Riedmiller,“Playingatariwithdeepreinforcement
robotics:Frombabiestorobots. MITPress,2015. learning,”arXivpreprintarXiv:1312.5602,2013.
[7] E. Ugur, Y. Nagai, E. Sahin, and E. Oztop, “Staged development of [31] L.-J. Lin, Reinforcement learning for robots using neural networks.
robot skills: Behavior formation, affordance learning and imitation Technicalreport,DTICDocument,1993.
with motionese,” IEEE Transactions on Autonomous Mental Devel- [32] A. Gerken and M. Spranger, “Continuous value iteration (cvi) rein-
opment,vol.7,no.2,pp.119–139,June2015. forcementlearningandimaginaryexperiencereplay(ier)forlearning
[8] S.ForestierandP.Oudeyer,“Modularactivecuriosity-drivendiscovery multi-goal,continuousactionandstatespacecontrollers,”IEEEICRA,
oftooluse,”inIEEE/RSJ,IROS2016,pp.3965–3972. 2019.
[9] M. Rolf, J. J. Steil, and M. Gienger, “Learning ﬂexible full body [33] M.Andrychowicz,F.Wolski,A.Ray,J.Schneider,R.Fong,P.Welin-
kinematicsforhumanoidtooluse,”in2010InternationalConference der,B.McGrew,J.Tobin,P.Abbeel,andW.Zaremba,“Hindsightex-
onEmergingSecurityTechnologies. IEEE,2010,pp.171–176. periencereplay,”AdvancesinNeuralInformationProcessingSystems
[10] J. Schmidhuber, “Formal theory of creativity, fun, and intrinsic mo- 30,pp.5048–5058,2017.
tivation (1990 - 2010),” IEEE Trans. on Auton. Ment. Dev., vol. 2, [34] M. Riedmiller, R. Hafner, T. Lampe, M. Neunert, J. Degrave, T. V.
no.3,pp.230–247,Sep.2010. de Wiele, V. Mnih, N. Heess, and J. T. Springenberg, “Learning by
[11] V. G. Santucci, G. Baldassarre, and M. Mirolli, “Grail: A goal- playing solving sparse reward tasks from scratch,” in Proceedings
discovering robotic architecture for intrinsically-motivated learning,” of the 35th International Conference on Machine Learning, vol. 80.
IEEE Transactionson Cognitiveand Developmental Systems, vol.8, PMLR,10–15Jul2018,pp.4344–4353.
no.3,pp.214–231,Sep.2016. [35] R. Lewis and V. Torczon, “Pattern search algorithms for bound
[12] A. Baranes and P. Oudeyer, “Active learning of inverse models with constrained minimization,” SIAM Journal on Optimization, vol. 9,
intrinsicallymotivatedgoalexplorationinrobots,”Robot.Auton.Syst., no.4,pp.1082–1099,1999.
vol.61,no.1,pp.49–73,2013. [36] “Rethink robotics baxter - hardware speciﬁcations,” http://sdk.
rethinkrobotics.com/wiki/HardwareSpeciﬁcations,accessed:2019-09-
[13] D. Kubus, R. Rayyes, and J. J. Steil, “Learning forward and
12.
inversekinematicsmapsefﬁciently,”in2018IEEE/RSJInternational
[37] “Moveit - motion planning framework,” https://moveit.ros.org/, ac-
Conference on Intelligent Robots and Systems (IROS). IEEE, Oct.
cessed:2019-09-14.
2018.[Online].Available:https://doi.org/10.1109/iros.2018.8593833
[14] P.Oudeyer,F.Kaplan,andV.V.Hafner,“Intrinsicmotivationsystems
for autonomous mental development,” IEEE Transactions on Evolu-
tionaryComputation,vol.11,no.2,pp.265–286,April2007.
[15] D. Tanneberg, J. Peters, and E. Rueckert, “Intrinsic motivation and
mentalreplayenableefﬁcientonlineadaptationinstochasticrecurrent
networks,”CoRR,vol.abs/1802.08013,2018.
[16] S. M. Nguyen and P. Oudeyer, “Socially guided intrinsic motiva-
tion for robot learning of motor skills,” Autonomous Robots, vol.
abs/1804.07269,2014.
[17] V. Santucci, G. Baldassarre, and M. Mirolli, “Which is the best
intrinsic motivation signal for learning multiple skills?” Frontiers in
Neurorobotics,vol.7,p.22,2013.
[18] P. O. Stalph and M. V. Butz, “Learning local linear jacobians for
ﬂexible and adaptive robot arm control,” Genetic Programming and
EvolvableMachines,vol.13,no.2,pp.137–157,2012.
[19] C.vonHofsten,“Anactionperspectiveonmotordevelopment,”Trends
inCogSci,vol.8,pp.266–272,2004.
[20] R. Rayyes, D. Kubus, and J. Steil, “Learning inverse statics models
efﬁciently with symmetry-based exploration,” Frontiers in Neuro-
robotics,vol.12,p.68,2018.
[21] P.Loviken,N.Hemion,A.Laﬂaquiere,M.Spranger,andA.Cangelosi,
“Online learning of body orientation control on a humanoid robot
using ﬁnite element goal babbling,” in 2018 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS). IEEE, Oct.
2018.[Online].Available:https://doi.org/10.1109/iros.2018.8593762
[22] A. K. Philippsen, R. F. Reinhart, and B. Wrede, “Goal babbling of
acoustic-articulatorymodelswithadaptiveexplorationnoise,”in2016
Joint IEEE International Conference on Development and Learning
andEpigeneticRobotics(ICDL-EpiRob). IEEE,Sep.2016.[Online].
Available:https://doi.org/10.1109/devlrn.2016.7846793
[23] R. F. Reinhart, “Autonomous exploration of motor skills by skill
babbling,”Auton.Robots,vol.41,no.7,pp.1521–1537,2017.
1342
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 21,2020 at 11:46:26 UTC from IEEE Xplore.  Restrictions apply. 