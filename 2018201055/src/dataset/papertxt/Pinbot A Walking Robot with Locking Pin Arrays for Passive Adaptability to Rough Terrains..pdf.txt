2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
One-Shot Multi-Path Planning for Robotic Applications Using Fully
Convolutional Networks
Tomas Kulvicius1, Sebastian Herzog1, Timo Lu¨ddecke1, Minija Tamosiunaite 1,2 and Florentin Wo¨rgo¨tter1
Abstract—Path planning is important for robot action ex- approaches, one uses the activation of a target neuron and
ecution, since a path or a motion trajectory for a particular propagates its activity to its neighbouring cells where the
actionhastobedeﬁnedﬁrstbeforetheactioncanbeexecuted.
activity gradient deﬁnes the path. One observes that these
Mostofthecurrentapproachesareiterativemethodswherethe
methods are, thus, related to the Dijkstra algorithm.
trajectory is generated by predicting the next state based on
the current state. Here we propose a novel method by utilising Recently, several path planning methods have been pro-
a fully convolutional neural network, which allows generation posed using deep learning approaches such as deep multi-
of complete paths even for several agents with one network layer perceptrons (DMLP, [25]), long short-term memory
prediction iteration. We demonstrate that our method is able
(LSTM) networks [1], and deep reinforcement learning
to successfully generate optimal or close to optimal paths (less
(deep-RL)approaches[28],[22].Allthesemethodsgenerate
than 10% longer) in more than 99% of the cases for single
pathpredictions in2D and3Denvironments. Furthermore,we pathsiterativelybypredictingthenextstateorthenextaction
show that the network is — without speciﬁc training on such (incaseofdeep-RL)basedontheenvironmentconﬁguration,
cases — able to create (close to) optimal paths in 96% of the the current state, and the target position until the target is
cases for two and in 84% of the cases for three simultaneously
reached. Thus, the network has to be exploited many times
generated paths.
until a complete path can be constructed.
I. INTRODUCTION Path planning for multi-agent systems, usually, is per-
Motion (path) planning is one of the fundamental issues formed by decentralised approaches [29], [4], [3], [7], [20])
in the development of truly autonomous robots, let it be a wherethepathisplannedforeachagentseparatelysuchthat
mobile robot or a robotic-manipulator. In robotics, motion computation time scales with the number of agents. In case
planning is deﬁned as the problem of ﬁnding a temporal ofcentralizedapproaches,pathsarecomputedforeachagent
sequence of valid states (e.g., robot positions or conﬁgu- simultaneously by solving complex optimization problem
rations), which brings a robot (-arm) from a start- to a which does not scale well as the number of agents increases
goal-position (conﬁguration) given some constraints (e.g., [20]. Different from the afore mentioned approaches, in this
obstacles)[18].Inthiswork,wespeciﬁcallyaddresstheissue study we present a method based on a fully convolutional
of multiple path planning for multi-agent systems. network (FCN), which allows multi-path planning in one-
Classical methods for path planning are the Dijkstra algo- shot, i.e., complete multiple paths can be generated by our
rithm[5]andtheA*search[11].DijkstraandA*algorithms network with a single prediction iteration.
provide always the optimal solution (i.e., shortest path) on
II. METHODS
grid-basedpathﬁndingproblems,buttheycanbeslowwhen
having to ﬁnd long paths especially in higher dimensions. A. Overview
Sampling based methods such as the rapidly-exploring
The task is to predict an optimal path (or several paths)
random tree algorithm (RRT, [19], [15], [14], [8]) can also
for an unseen environment with obstacles (not used in the
be used for path ﬁnding and operate well in continuous
training set), given any start- and end-point (goal). In this
spaces. Their performance, however, is worse on grids when
study, we have considered both 2D and 3D environments.
comparing them to Dijkstra or A* [17], [1], i.e., RRT-paths
We use environments deﬁned as occupancy grids in the
areoftennotoptimal.Inaddition,onehastotuneparameters,
whichisnotthecasefortheclassicalalgorithms.Thismakes shape of a binary image of linear size n1. Free spaces are
given as zero (white) and obstacles are given as one (black).
sampling based methods problematic also because they are
For start- and end-points we use two more binary images
on grids computationally more expensive [17], [1].
withstart/endpointsettoone(black)andtheallotherpoints
Bio-inspired neural networks [9], [10], [2], [30], [21],
as zero (see input in Fig. 1).
[26] have also been employed for path ﬁnding. In these
After training, the model predicts a collision-free path
*TheresearchleadingtotheseresultshasreceivedfundingfromtheEuro- again represented as binary image using the same encoding
peanCommunity’sH2020Programme(FutureandEmergingTechnologies, (black) for the path grid elements(see output in Fig. 1).
FET)undergrantagreementno.732266,Plan4Act.
The trajectory for the agent is then created by combining
1T. Kulvicius, S. Herzog, T. Lu¨decke, M. Tamosiunaite
and F. Wo¨rgo¨tter are with Department for Computational forward- with backward-search (from start- and end-point,
Neuroscience, University of Go¨ttingen, 37073 Go¨ttingen, Germany respectively) along the marked path grid elements.
tomas.kulvicius@uni-goettingen.de
2M.TamosiunaiteisalsowiththeFacultyofComputerScience,Vytautas
×
MangnusUniversity,Kaunas,Lithuania 1Notethatourapproachcanalsobeusedforenvironmentsofsizen m.
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 1460
Authorized licensed use limited to: Carleton University. Downloaded on September 22,2020 at 00:57:20 UTC from IEEE Xplore.  Restrictions apply. Detailsofallalgorithmiccomponentsaregivennextusing Euclidean distance between start- and end-points was 5 to
annotations for the 2D case. The 3D case is similar. avoid short (trivial) paths.
×
1,000environmentsofsize15 15withdifferentobstacle
conﬁgurations but ﬁxed start- and end-positions where cre-
(cid:73)(cid:110)(cid:112)(cid:117)(cid:116)(cid:32)(cid:51)(cid:120)(cid:110)(cid:120)(cid:110)
ated for the prediction of multiple paths. We ﬁxed start- and
{ } { } { }
end-points to positions 1,1 , 1,n and n,1 (corners)
{ }
(cid:67)(cid:111)(cid:110)(cid:118)(cid:32)(cid:51)(cid:120)(cid:51) (cid:67)(cid:111)(cid:110)(cid:118)(cid:32)(cid:51)(cid:120)(cid:51) andthegoalwasinthemiddle(position 8,8 )(seeFig.4).
(cid:82)(cid:101)(cid:76)(cid:85) (cid:82)(cid:101)(cid:76)(cid:85)
C. Proposed Network
(cid:69)(cid:110)(cid:118)(cid:105)(cid:114)(cid:111)(cid:110)(cid:109)(cid:101)(cid:110)(cid:116) (cid:83)(cid:116)(cid:97)(cid:114)(cid:116) (cid:71)(cid:111)(cid:97)(cid:108) (cid:46)(cid:46)(cid:46)(cid:54)(cid:52) Network architectures: For the 2D case, a fully convo-
lutional network, as shown in Fig. 1, was used. The three
×
(cid:79)(cid:117)(cid:116)(cid:112)(cid:117)(cid:116)(cid:32)(cid:110)(cid:120)(cid:110) above discussed 2D binary images of size n n constitute
the input layer and the output a 2D image that contains the
(cid:46)(cid:46)(cid:46)
path candidates. 20 identical 2D convolutional layers with
(cid:67)(cid:111)(cid:110)(cid:118)(cid:32)(cid:51)(cid:120)(cid:51) (cid:67)(cid:111)(cid:110)(cid:118)(cid:32)(cid:51)(cid:120)(cid:51) ×
(cid:82)(cid:101)(cid:76)(cid:85) (cid:83)(cid:105)(cid:103)(cid:109)(cid:111)(cid:105)(cid:100) 64 ﬁlters of size 3 3 (stride 1) were used and ﬁnally also
onemoreconvolutionallayerattheendwithjustoneﬁlterof
(cid:46)(cid:46)(cid:46) ×
(cid:54)(cid:52) (cid:80)(cid:114)(cid:101)(cid:100)(cid:105)(cid:99)(cid:116)(cid:101)(cid:100)(cid:32)(cid:112)(cid:97)(cid:116)(cid:104) size3 3.Abatchnormalisationlayer(notshown)wasused
aftereachconvolutionallayer,andafterthelastconvolutional
layer we included a dropout layer with 10% dropout. In all
Fig.1. Proposednetworkarchitecture.Weusedafullyconvolutionalneural
but the last convolutional layer we used ReLU activation
network(FCN)with21convolutionallayers.WeusedReLUactivationunits
inthelayers1to20andasigmoidactivationunitinthelastlayer.Weused units. In the last convolutional layer sigmoid activation was
≤ ≤
a batch normalisation layer after each convolutional layer and in addition used. Network output is given by 0 Oˆ 1. Zero padding
a dropout (10%) layer after the last convolutional layer (not shown). For
was used in all layers to keep the same dimensions.
moredetailspleaserefertothemaintext.
For the 3D case, 3D convolutional layers were used with
× ×
ﬁlters 3 3 3, but we needed more ﬁlters in the ﬁrst
four layers (1024, 512, 256 and 128), because of the higher
B. Data
dimension of the input.
Inputs and maps are deﬁned as described above. In addi- Training procedure: As loss we employed the mean
tiontothecaseofsinglestart-andend-pointswealsopredict squared error (MSE) between network output Oˆ and ground
multiple-paths from several start-positions to the same end truth solution O. Batch sizes were 64 samples (for 2D)
position. In these cases several grid cells in the start map and 16 samples (for 3D), where the latter was limited by
were set to 1. The output map, which contains the found our hardware. If the accuracy on the validation set did not
path, is then represented in the same way as all other maps increase within the last 10 epochs, early validation stopping
bysettingthegrid-cellto1,ifthepathtraversesthisgridcell wasusedtopreventover-ﬁtting.Weused26,000samplesfor
and0everywhereelse.Optimalpathswerecreatedasground trainingand2,000samplesforvalidationin2D,whilein3D
truth by using the A* algorithm, where eight movement 90,000samplesfortrainingand5,000samplesforvalidation
directions (vertical, horizontal and diagonal) were allowed. were used. Finally, all networks were tested on 2,000 new,
Movement costs were calculated using Euclidean metric. unseen samples. The ADAM optimiser with default learning
Data generation: The input maps for training and testing parameters was employed for training both, 2D and 3D,
were generated randomly by setting each grid cell to 1 (i.e, networks.
obstacle) with a probability p = 0.6, or to 0, otherwise. For network implementation we used Tensorﬂow and
o
We disallowed the following two diagonal conﬁgurations of Keras API2, where the A* algorithm was implemented in
{ }
obstacles: Ie = 0,Ie = 1,Ie = 1,Ie = 0 Python 3.6.7. We used a PC with Intel Xeon Silver 4114
{ i,j i,j+1 i+1,j i+1,j+1 }
and Ie = 1,Ie = 0,Ie = 0,Ie = 1 . In (2.2GHz) CPU and NVIDIA GTX 1080 (11GB) GPU.
i,j i,j+1 i+1,j i+1,j+1
these cases A* algorithm would go through two diagonally
D. Path Reconstruction
arranged obstacles that touch each other only by a corner,
whichinrealscenariosisimpossible.Incaseof3Dgrids,we As explained above, the output of the network is a value
≤ ≤
assumed that objects (obstacles) are standing on the ground mapOˆ oflinearsizen,with0 Oˆ 1.Thenumbersreﬂect
(like buildings in a city) and do not ﬂoat in the air. “how certain the network is” that the respective grid cell is
We generated two datasets (2D grids and 3D grids) for part of the path. However, this map does not yet represent
learningandtestingofsinglepathsandoneadditionaldataset the sequence of how to traverse these grid cells one after
for multiple paths with up to three start-positions. Training the other and an additional step, called path reconstruction,
did not include multiple paths. is needed. Here we used bidirectional search such that
In general, we used environments with linear sizes n = we represent forward and the backward path-segments as
10,15, and 20 (see Fig. 2 for n = 15) and here we used
2The data and source code is available online at https:
30,000environmentsfor2Dand97,000for3Dwithdifferent
//alexandria.physik3.uni-goettingen.de/cns-group/
obstacles and different start- and end-points. The minimum datasets/path_planning/
1461
Authorized licensed use limited to: Carleton University. Downloaded on September 22,2020 at 00:57:20 UTC from IEEE Xplore.  Restrictions apply. { }
temporal sequence of points on the grid with xf,yf and where N is the number of successfully found paths and
{ } { t t} S
xb,yb , respectively. Start-point is given by x ,y and N is the total number of tested environments, which gives
t t { } { } s {s } T
end-point by x ,y . Initially we set xf,yf = x ,y a percent value of success.
{ } {g g } 1 1 { s s}
and xb,yb = x ,y . From any current position x ,y Path optimality: We also measured whether successfully
1 1 g g t t
of the forward/backward path-segment, the next position of predicted paths were optimal (i.e., shortest path) or not. For
the forward/backward path is the maximum-valued grid cell that, we compared path lengths of the paths obtained by
{ }
i,j in the nearest neighbourhood of the current position, using the FCN and A* algorithms, denoted as L and
FCN
hence, that cell where: L ∗, respec(cid:80)tively. Path length was computed by (we skip
A ∗
{ } { (cid:54) (cid:54) FCN and A notation for clarity):
x ,y =argmaxOˆ i=x ,j =y , (1)
t+1 t+1 i,j t t − (cid:107) − (cid:107)
∈ − i,j ∈ − } L= n 1 P(x ,y ) P(x ,y ) , (6)
i [x 1,x +1],j [y 1,y +1] . t=1 t+1 t+1 t t
t t t t
Then we set Oˆ = 0 and continue until one of the three where P(x,y) is the path predicted by the FCN or A* algo-
i,j rithm, and n is the number of points in the path sequence.
following conditions is met: (cid:107)·(cid:107)
{ } { } Here, is the Euclidean distance. The path predicted by
1) End-point is reached, xf,yf = x ,y ;
{t t} { g g} the FCN is optimal if L =L ∗. Hence, the percentage
2) Start-point is reached, xb,yb = x ,y ; FCN A
t t{ s} s { } of optimal paths OP is:
3) The paths cross each other, xf,yf = xb,yb or
{ } { } ≤t ≤t k k ·
xb,yb = xf,yf , where 1 k t. OP=100% (N /N ), (7)
t t k k O T
Depending on the condition met, the ﬁnal path P(x,y) is
where N is the number of optimal paths. Because paths
then constructed as follows: O
predictedbyFCNarenotalwaysoptimal,wealsocomputed
{ }
P(x,y)= (xf,yf)...(xf ,yf ) , (2) the path length ratio LR of non-optimal relative to optimal
1 1 M M
paths:
if condition (1) is met;
{ }
P(x,y)= (xb ,yb )...(xb,yb) , (3) LR=L /L ∗. (8)
N N 1 1 FCN A
if condition (2) is met; This allows us to analyse how much longer non-optimal
{
P(x,y)= (xf,yf)...(xf,yf), (4) paths are compared to the A* solutions.
1 1 t }t
(xb− ,yb− )...(xb,yb) , or Algorithmrun-time:Forrun-timeevaluation,pathlength
k 1 k 1 1 1
{ } wasmeasuredinsteps,i.e.,howmanystepsittakestomove
(xf,yf)...(xf− ,yf− ),(xb,yb)...(xb,yb) ,
1 1 k 1 k 1 t t 1 1 from the start-point to the end-point. Thus, for run-time
if condition (3) is met. comparison we measured how much time it takes to predict
Here, M and N are the lengths of the forward and the (in case of FCN) or to ﬁnd (in case of A*) a single path for
backward path, respectively. In real applications, the points different path lengths. For run-time calculation we included
ofthepathP(x,y)canthenbeusedasviapointstogenerate both path search/prediction time and path reconstruction
trajectories using conventional methods such as splines [6], time. Note that A* algorithm (similar to FCN) consist of
[27] or more advanced state-of-the-art methods such as dy- two steps: path search and path reconstruction from visited
namicmovementprimitives(DMPs,[13]),Gaussianmixture states.
models (GMMs, [16]), probabilistic movement primitives Evaluation procedure: Two types of experiments were
(PMPs, [23]) or optimal control primitives (OCPs, [12]). performed: prediction of single paths in 2D and 3D and
Note that in some cases paths could not be reconstructed, prediction of multiple paths in 2D. For the ﬁrst set of
because none of the stopping conditions were met. In this experiments, we trained the network to predict single paths
case we treated network’s prediction as “path not found”. on three different grids with linear sizes n=10,15,20) and
thentestedeachmodelonallthreegrids.Forthesecondtype
E. Evaluation Measures and Procedure
of experiments we trained on single, but tested on multiple
WecomparedourapproachagainsttheA*algorithm[11]. paths (two or three paths). Grids with linear size n = 15
We have chosen A* against rapidly-exploring random trees were used for this.
(RRT [19] or RRT* [15]), since it has been shown that
III. RESULTS
algorithms such as Dijkstra and A* perform better on grid
structures as compared to RRTs [17], [1]. A. General Performance of the Proposed Network
Thus, for comparison we used the following criteria:
In Fig. 2 we show examples of single path predictions on
success rate, path optimality and run-time of the algorithm.
2Dgridsoflinearsizen=15,wherethepathsuggestionsby
Below we describe our evaluation measures and procedure the network are marked by blue dots3. Small dots represent
in more detail.
values close to zero and large dots close to one. Crosses
Successrate:SuccesswasscoredifthepathP(x,y)could mark the optimal solution, which is the one obtained when
be reconstructed, otherwise we counted a failure (path not
found). We deﬁne: 3More examples of 2D and 3D path predictions can be found
· at https://alexandria.physik3.uni-goettingen.de/
SR=100% (N /N ), (5) cns-group/datasets/path_planning/.
S T
1462
Authorized licensed use limited to: Carleton University. Downloaded on September 22,2020 at 00:57:20 UTC from IEEE Xplore.  Restrictions apply. largegridscontainmany“neverseen”longpaths,whichjust
do not exist in the smaller grids.
The above results had shown that the network is most
often able to predict feasible paths, but those are not always
optimal (see Table I). Sub-optimal paths are more often
obtained in larger grids of linear size n=20 (from 58.10%
to 86.55% and from 53.34% to 83.02% for 2D and 3D case,
respectively) as compared to smaller grids of size n = 10
(from98.05%to99.85%andfrom92.88%to93.74%for2D
and 3D, respectively). This is due to the fact that a larger
number of longer paths exist in larger grids and those can
more easily lead to prediction errors.
Finallywemeasuredpathslengthaskinghowmuchlonger
non-optimal paths are relative to the optimal (shortest) path.
As above, better performance is obtained if the network is
trained on the same or on larger grids than those used for
testing.Still,non-optimalpathsare,onaverage,lessthat10%
longer compared to the optimal path (in a range between
5% and 7%, and between 7% and 9% for 2D and 3D,
respectively).
In the last experiment we compared run-time, i.e., how
much time it takes to process one environment, between A*
and FCN. Results obtained on the the 2D and 3D grid of
linearsize20arepresentedinFig.3.Hereweshowrun-time
ofeachrealization(intotal2,000environments)forbothA*
and FCN. As expected, results show that on average run-
Fig. 2. Single path predictions on unseen environments for grids with
linear size n = 15 when training and testing on the same grid size. A) timeofA*increasesquadraticallyifpathsaregettinglonger,
optimal(shortest)paths,B)suboptimalpaths,C)notfoundpaths.Crosses whereasrun-timeofFCNonaverageincreaseslinearly.Note
annotatetheA*solution,wherebluedotsdenotethepredictedpathusing
≈ ≈ that prediction time of the FCN is constant and the linear
ournetwork.Dotsizerepresentsmall( 0.0)andlarge( 1.0)valuesof
the network output. Start and endpoint are shown by green and red dots, increase is due to the path reconstruction time (longer paths
respectively. take more time to reconstruct as compared to shorter paths).
Fitted functions predict that in 2D case for paths longer
than 30 steps (e.g., in larger environments), FCN would
outperformA*,whereasinthe3DcaseFCNoutperformsA*
using the A* algorithm. The network is able to predict
if the paths are longer than 13 steps. Note that in 3D, path
optimal paths in most of the cases (see statistical analysis
reconstruction time for FCN becomes negligible. Moreover,
in Table I). In panel A we show an example of an optimal
ourproposednetworkisabletopredictmultiplepathswithin
path (left), which is identical to the A* solution and an
the same time, whereas in case of A* one would need to
optimal path which differs from the A* path (right). Most
run path search several times, which — as a consequence
often this happens when going around an obstacle from the
— would scale run-time by k, where k is the number of
other side. In panel B one can see paths that are suboptimal
searched paths.
but still feasible. Hence these paths take more steps than
the A* solution but in most of cases they are only less than B. Prediction of Multiple Paths
10% longer. Panel C shows cases where paths could not be In Fig. 4 we show examples of two- (panels B1-B4) and
reconstructed from the network output. three-path predictions (panels C1-C4), where panels A1-A4
In Table I we show a statistical analysis on how our are control cases (single-path predictions). As before, also
networks perform on different 2D- and 3D-grids. When here we show cases of optimal, suboptimal and not found
trained and tested on the same grid size we found a very paths. Our results can be summarized into four different
high success rate, which is above 99.5% for 2D as well cases:
as 3D cases. Remarkably, we also ﬁnd that these networks 1) Optimal multi-paths are found and path ﬁnding is
can predict paths with high reliability even on a grid sizes not affected by adding a second or third source (see
that had not been used for training. With one exception (3D optimal paths in panels A1-C1).
case), success rate is above 98% and 88% for the 2D and 2) One or more paths become suboptimal when adding
3D, respectively. The exception is the “extreme” case of a more sources. One example of this can be found in
network trained on a linear grid size of n = 10 and tested panels A2 and B2. Here an optimal path (A2) turns
on grids of size n = 20. Naturally, performance goes down into a suboptimal one when adding a second source
when training on small and testing on large grids, since the (see panel B2).
1463
Authorized licensed use limited to: Carleton University. Downloaded on September 22,2020 at 00:57:20 UTC from IEEE Xplore.  Restrictions apply. TABLEI
RESULTSFORTHEPREDICTIONOFSINGLEPATHSIN2DAND3DENVIRONMENTSOBTAINEDFROM2,000TESTEDUNSEENENVIRONMENTS.
Testedon
× × × × × × × × ×
2D 10 10 15 15 20 20 10 10 15 15 20 20 10 10 15 15 20 20
±
Successrate(%) Optimalpaths(%) Pathlengthratio(Mean CI[95%])
× ± ± ±
10 10 100 99.75 98.45 99.85 83.51 58.10 1.07 0.039 1.10 0.011 1.21 0.020
Trained × ± ± ±
15 15 99.90 99.95 98.90 97.25 91.00 75.53 1.07 0.012 1.07 0.009 1.09 0.008
on × ± ± ±
20 20 99.90 99.70 99.60 98.05 94.38 86.55 1.07 0.015 1.05 0.006 1.06 0.015
Testedon
× × × × × × × × × × × × × × × × × ×
3D 10 10 10 15 15 15 20 20 20 10 10 10 15 15 15 20 20 20 10 10 10 15 15 15 20 20 20
±
Successrate(%) Optimalpaths(%) Pathlengthratio(Mean CI[95%])
× × ± ± ±
10 10 10 99.85 90.60 62.05 93.74 58.33 53.34 1.07 0.007 1.20 0.014 1.36 0.021
Trained × × ± ± ±
15 15 15 99.70 99.95 88.50 93.18 89.24 61.69 1.09 0.011 1.07 0.008 1.20 0.014
on × × ± ± ±
20 20 20 99.75 99.95 99.80 92.88 88.49 83.02 1.09 0.010 1.07 0.009 1.07 0.009
3) Paths can “disappear” when adding a second or third rate for predicting two paths was still quite high (99.2%).
source (see panels A3, B3 and C3). The here-observed improvement for the two-path-search can
4) Or ﬁnally, as shown in panels A4 and C4, a path can be understood by the fact that there is a higher probability
“appear” when adding more sources. to ﬁnd two paths out of three than two out of two.
For multi-path search we ﬁnd, however, that there are
In Fig. 5 we provide a statistical summary for multi-path
fewer optimal paths (85.88% and 83.33% for two-path and
prediction. We compare here the prediction performance for
three-path search, respectively). Also now paths are getting
the one-path control case with two and three paths. All tests
longer (15% and 22% for two-path and three-path search,
were performed on linear grids of size n=15.
respectively) when the number of searched paths increases.
We obtain success rates of 96.4% and 83.9% for ﬁnding
This is a consequence of the fact that paths are more likely
two paths out of two searched paths and three paths out
tointersectinthecaseofmultiplesourcesandthiswaythey
of three searched paths, respectively. Hence, with increased
are less optimal.
numberofsearchedpathsperformanceisslightlydecreasing.
Remarkably, for the difﬁcult case of trying to ﬁnd three- IV. DISCUSSION
paths, we still achieve always at least one path, and success Inthecurrentstudy,wehaveinvestigatedhowtogenerate
single as well as multiple paths using fully convolutional
networks. To the best of our knowledge, this is novel, since
it allows predicting multiple paths in a “single shot” (one-
(cid:65) (cid:50)(cid:48)
(cid:65)(cid:42) time network prediction). We would like to emphasize that
(cid:70)(cid:67)(cid:78) there are other deep-learning approaches existing (discussed
(cid:115)(cid:41)(cid:49)(cid:53) (cid:65)(cid:42)(cid:32)(cid:102)(cid:105)(cid:116) above)butthosegeneratepathsiterativelyandtheycanonly
(cid:109)
(cid:40) plan a single path [28], [22], [25], [1]. If multiple-paths
(cid:109)(cid:101)(cid:32)(cid:49)(cid:48) (cid:70)(cid:67)(cid:78)(cid:32)(cid:102)(cid:105)(cid:116) are considered then this happens for each agent separately
(cid:45)(cid:116)(cid:105) [20], [3], [7] or they only deal with collision avoidance path
(cid:110)
(cid:82)(cid:117) (cid:53) planning. Hence, path planning of paths for navigation in
maze-like environments is not addressed in these studies.
(cid:48) Recently, an interesting paper by [24] suggested also
(cid:48) (cid:49)(cid:48) (cid:50)(cid:48) (cid:51)(cid:48) (cid:52)(cid:48)
(cid:80)(cid:97)(cid:116)(cid:104)(cid:32)(cid:108)(cid:101)(cid:110)(cid:103)(cid:116)(cid:104)(cid:32)(cid:40)(cid:115)(cid:116)(cid:101)(cid:112)(cid:115)(cid:41) one-shot path planning with a fully convolutional network.
However,theirapproachaddressesonlysinglepathplanning
(cid:66) (cid:53)(cid:48)(cid:48) and they deal with human-aware collision-free navigation in
(cid:65)(cid:42)
(cid:70)(cid:67)(cid:78) rather simple (object free) environments (not mazes). Note
(cid:52)(cid:48)(cid:48)
(cid:41) thattheirmethodisbasedontwosteps:1)Anetworkpredicts
(cid:115)
(cid:109)
(cid:40) (cid:51)(cid:48)(cid:48) (cid:65)(cid:42)(cid:32)(cid:102)(cid:105)(cid:116) thepathand2)thentheRRT*algorithm[15]isemployedfor
(cid:109)(cid:101)(cid:32) path reﬁnement. All this is computationally more expensive
(cid:45)(cid:116)(cid:105) (cid:50)(cid:48)(cid:48) as compared to our approach of path reconstruction by
(cid:110)
(cid:117) bidirectional search.
(cid:82) (cid:49)(cid:48)(cid:48) (cid:70)(cid:67)(cid:78)(cid:32)(cid:102)(cid:105)(cid:116)
We have demonstrated that in case of single path predic-
(cid:48) tionsourproposednetworkisabletopredictoptimalorclose
(cid:53) (cid:49)(cid:48) (cid:49)(cid:53) (cid:50)(cid:48) (cid:50)(cid:53)
tooptimalpathssuccessfullyinmorethan99.5%ofthecases
(cid:80)(cid:97)(cid:116)(cid:104)(cid:32)(cid:108)(cid:101)(cid:110)(cid:103)(cid:116)(cid:104)(cid:32)(cid:40)(cid:115)(cid:116)(cid:101)(cid:112)(cid:115)(cid:41)
(when trained and tested on the same grid size) in both 2D
and 3D environments. Furthermore it generalizes quite well
Fig.3. Resultsfortherun-timecomparisonbetweenA*andFCNobtained on grid sizes different from the trained ones as long as they
from2000testedsampleson2D(panelA)and3Dgrids(panelB)ofsize do not differ too much.
20. We used a ﬁrst order and a second order polynomial functions to ﬁt In case of multiple path prediction, we have shown that,
FCNandA*data,respectively.
although the network has never been trained on multiple
1464
Authorized licensed use limited to: Carleton University. Downloaded on September 22,2020 at 00:57:20 UTC from IEEE Xplore.  Restrictions apply. (cid:65)
(cid:49)(cid:48)(cid:48) (cid:57)(cid:57)(cid:46)(cid:53) (cid:57)(cid:57)(cid:46)(cid:56)(cid:57)(cid:54)(cid:46)(cid:52) (cid:49)(cid:48)(cid:48)(cid:57)(cid:57)(cid:46)(cid:50)
(cid:56)(cid:51)(cid:46)(cid:57)
(cid:37)(cid:41)(cid:56)(cid:48) (cid:49)(cid:32)(cid:112)(cid:97)(cid:116)(cid:104)(cid:32)(cid:102)(cid:111)(cid:117)(cid:110)(cid:100)
(cid:101)(cid:32)(cid:40)(cid:54)(cid:48) (cid:50)(cid:51)(cid:32)(cid:32)(cid:112)(cid:112)(cid:97)(cid:97)(cid:116)(cid:116)(cid:104)(cid:104)(cid:115)(cid:115)(cid:32)(cid:32)(cid:102)(cid:102)(cid:111)(cid:111)(cid:117)(cid:117)(cid:110)(cid:110)(cid:100)(cid:100)
(cid:97)(cid:116)
(cid:115)(cid:32)(cid:114)
(cid:115)(cid:52)(cid:48)
(cid:101)
(cid:99)
(cid:99)
(cid:117)(cid:50)(cid:48) (cid:110)(cid:47)(cid:97) (cid:110)(cid:47)(cid:97)
(cid:83)
(cid:48)
(cid:49) (cid:50) (cid:51)
(cid:35)(cid:32)(cid:111)(cid:102)(cid:32)(cid:115)(cid:101)(cid:97)(cid:114)(cid:99)(cid:104)(cid:101)(cid:100)(cid:32)(cid:112)(cid:97)(cid:116)(cid:104)(cid:115)
(cid:66) (cid:67)
(cid:49)(cid:48)(cid:48) (cid:49)(cid:46)(cid:50)(cid:50)
(cid:57)(cid:51)(cid:46)(cid:56) (cid:56)(cid:53)(cid:46)(cid:57) (cid:56)(cid:51)(cid:46)(cid:51) (cid:49)(cid:46)(cid:50) (cid:49)(cid:46)(cid:48)(cid:55) (cid:49)(cid:46)(cid:49)(cid:53)
(cid:37)(cid:41)(cid:56)(cid:48) (cid:111) (cid:49)
(cid:97)(cid:116)(cid:104)(cid:115)(cid:32)(cid:40)(cid:54)(cid:48) (cid:103)(cid:116)(cid:104)(cid:32)(cid:114)(cid:97)(cid:116)(cid:105)(cid:48)(cid:46)(cid:56)
(cid:109)(cid:97)(cid:108)(cid:32)(cid:112)(cid:52)(cid:48) (cid:104)(cid:32)(cid:108)(cid:101)(cid:110)(cid:48)(cid:48)(cid:46)(cid:46)(cid:52)(cid:54)
(cid:79)(cid:112)(cid:116)(cid:105)(cid:50)(cid:48) (cid:80)(cid:97)(cid:116)(cid:48)(cid:46)(cid:50)
(cid:48) (cid:48)
(cid:49) (cid:50) (cid:51) (cid:49) (cid:50) (cid:51)
(cid:35)(cid:32)(cid:111)(cid:102)(cid:32)(cid:115)(cid:101)(cid:97)(cid:114)(cid:99)(cid:104)(cid:101)(cid:100)(cid:32)(cid:112)(cid:97)(cid:116)(cid:104)(cid:115)
Fig. 5. Results for the prediction of multiple paths obtained from 1,000
testedunseenenvironments.A)Successrate,B)percentageofoptimalpaths,
C) path length ration of non-optimal paths. Error bars in panel C denote
conﬁdenceintervalsofmean(95%).
environments. For example, if the maze changes during one
run of a robot towards its goal, the agent could just use its
current position as the new start-point and perform another
one-shot path ﬁnding procedure. Thus, this method can be
used for fast and efﬁcient on-line (re-)planning in dynamic
environments, too.
Fig. 4. Examples of multi-path predictions on unseen environments of
linearsize15.A)Singlepathpredictions(controlcase),B)predictionsof In summary, there are several quite useful features of our
two paths, and C) prediction of three paths. Crosses denote A* solutions method: 1) single-shot operation, 2) possibility for multi-
where blue dots denote predicted paths using FCN. Size of the dots
path planning, 3) constant prediction run-time, 4) relatively
correspond to small (close to zero) and large (close to one) values of the
network output. Green and red dots correspond to start-points and end- high success and optimality rates, and 5) the ability to
points, respectively. Note that the predicted path shown in panel A2 is an generalise to environments of different sizes. This should
optimal path, and that the predicted path shown in panel A3 is a feasible
make it a valuable and attractive approach for many robotic
path.
applications.
REFERENCES
paths, it is also able to generate paths from multiple sources
toonetarget.Thiscouldbealsousedtosolvesingle-source- [1] M. J. Bency, A. H. Qureshi, and M. C. Yip. Neural path planning:
Fixedtime,near-optimalpathgenerationviaoracleimitation. CoRR,
multi-target (or vice versa) problems. We obtained a success
abs/1904.11102,2019.
rate of 96.4% and 83.9% for the prediction of two and [2] N. Bin, C. Xiong, Z. Liming, and X. Wendong. Recurrent neural
three paths, respectively. Naturally, results should improve network for robot path planning. In Int. Conf. on Parallel and
Distributed Computing: Applications and Technologies, pages 188–
by including samples of multiple paths in the training set.
191,2004.
Theotheroptionwouldbetorepeatthenetwork’sprediction [3] Y. F. Chen, M. Liu, M. Everett, and J. P. How. Decentralized non-
one or several more times by giving locations at which the communicating multiagent collision avoidance with deep reinforce-
mentlearning. In2017IEEEInt.Conf.onRoboticsandAutomation
path reconstruction was lost as new start- and end-points.
(ICRA),pages285–292,2017.
Note, however that the goal of this study was to show how [4] V.R.DesarajuandJ.P.How. Decentralizedpathplanningformulti-
far one can get by not doing this. agentteamswithcomplexconstraints.AutonomousRobots,32(4):385–
403,2012.
An interesting extension of our approach concerns the
[5] E. W. Dijkstra. A note on two problems in connexion with graphs.
possibility to use our method also in dynamic (changing) NumerischeMathematik,1(1):269–271,1959.
1465
Authorized licensed use limited to: Carleton University. Downloaded on September 22,2020 at 00:57:20 UTC from IEEE Xplore.  Restrictions apply. [6] M. Egerstedt and C. F. Martin. Optimal trajectory planning and [30] S. X. Yang and M. Meng. Neural network approaches to dynamic
smoothingsplines. Automatica,37(7):1057–1064,2001. collision-free trajectory generation. IEEE Trans. on Systems, Man,
[7] M. Everett, Y. F. Chen, and J. P. How. Motion planning among andCybernetics,PartB(Cybernetics),31(3):302–318,2001.
dynamic, decision-making agents with deep reinforcement learning.
CoRR,abs/1805.01956,2018.
[8] J. D. Gammell, S. S. Srinivasa, and T. D. Barfoot. Informed RRT*:
Optimalsampling-basedpathplanningfocusedviadirectsamplingof
an admissible ellipsoidal heuristic. In 2014 IEEE/RSJ Int. Conf. on
IntelligentRobotsandSystems,pages2997–3004,2014.
[9] R.Glasius,A.Komoda,andS.Gielen. Neuralnetworkdynamicsfor
pathplanningandobstacleavoidance.NeuralNetworks,8(1):125–133,
1995.
[10] R. Glasius, A. Komoda, and S. Gielen. A biologically inspired
neuralnetfortrajectoryformationandobstacleavoidance. Biological
Cybernetics,74(6):511–520,1996.
[11] P. E. Hart, N. J. Nilsson, and B. Raphael. A formal basis for the
heuristicdeterminationofminimumcostpaths. IEEETrans.Systems
ScienceandCybernetics,4(2):100–107,1968.
[12] S.Herzog,F.Wo¨rgo¨tter,andT.Kulvicius. Generationofmovements
with boundary conditions based on optimal control theory. Robotics
andAutonomousSystems,94:1–11,2017.
[13] J. A. Ijspeert, J. Nakanishi, H. Hoffmann, P. Pastor, and S. Schaal.
Dynamicalmovementprimitives:Learningattractormodelsformotor
behaviors. NeuralComput.,25(2):328–373,February2013.
[14] F. Islam, J. Nasir, U. Malik, Y. Ayaz, and O. Hasan. RRT*-Smart:
RapidconvergenceimplementationofRRT*towardsoptimalsolution.
In 2012 IEEE Int. Conf. on Mechatronics and Automation, pages
1651–1656,2012.
[15] S.KaramanandE. Frazzoli. Sampling-based algorithms foroptimal
motion planning. The International Journal of Robotics Research,
30(7):846–894,2011.
[16] S. M. Khansari-Zadeh and A. Billard. Learning Stable Non-Linear
Dynamical Systems with Gaussian Mixture Models. IEEE Trans.
Robot.,27:943–957,Oct.2011.
[17] L.KnispelandR.Matousek. Aperformancecomparisonofrapidly-
exploring random tree and Dijkstra’s algorithm for holonomic robot
pathplanning. InstituteofAutomationandComputerScience,Faculty
ofMechanicalEngineerig,BrnoUniversityofTechnology,2013.
[18] J.-C.Latombe.Robotmotionplanning,volume124.SpringerScience
&BusinessMedia,2012.
[19] S.M.LaValle. Rapidly-exploringrandomtrees:Anewtoolforpath
planning. TechnicalReport,1998.
[20] P.Long,W.Liu,andJ.Pan. Deep-learnedcollisionavoidancepolicy
fordistributedmultiagentnavigation. IEEERoboticsandAutomation
Letters,2(2):656–663,2017.
[21] J. Ni, L. Wu, P. Shi, and S. X. Yang. A dynamic bioinspired
neuralnetworkbasedreal-timepathplanningmethodforautonomous
underwater vehicles. Computational intelligence and neuroscience,
2017,2017.
[22] A.I.Panov,K.S.Yakovlev,andR.Suvorov. Gridpathplanningwith
deepreinforcementlearning:Preliminaryresults. Procediacomputer
science,123:347–353,2018.
[23] A. Paraschos, C. Daniel, J. Peters, and G. Neumann. Probabilistic
movement primitives. In C.J.C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural
InformationProcessingSystems26,pages2616–2624,2013.
[24] N. Pe´rez-Higueras, F. Caballero, and L. Merino. Learning human-
awarepathplanningwithfullyconvolutionalnetworks. In2018IEEE
Int.Conf.onRoboticsandAutomation(ICRA),pages1–6,2018.
[25] A.H.Qureshi,M.J.Bency,andM.C.Yip.Motionplanningnetworks.
CoRR,abs/1806.05767,2018.
[26] E. Rueckert, D. Kappel, D. Tanneberg, D. Pecevski, and J. Peters.
Recurrent spiking networks solve planning tasks. Scientiﬁc Reports,
6:21142,2016.
[27] B. Siciliano, L. Sciavicco, L. Villani, and G. Oriolo. Robotics:
Modelling, planning and control. Springer Publishing Company,
Incorporated,2009.
[28] L. Tai, G. Paolo, and M. Liu. Virtual-to-real deep reinforcement
learning:Continuouscontrolofmobilerobotsformaplessnavigation.
In2017IEEE/RSJInt.Conf.onIntelligentRobotsandSystems(IROS),
pages31–36,2017.
[29] K.-H. C. Wang and A. Botea. MAPP: A scalable multi-agent path
planning algorithm with tractability and completeness guarantees.
JournalofArtiﬁcialIntelligenceResearch,42:55–90,2011.
1466
Authorized licensed use limited to: Carleton University. Downloaded on September 22,2020 at 00:57:20 UTC from IEEE Xplore.  Restrictions apply. 