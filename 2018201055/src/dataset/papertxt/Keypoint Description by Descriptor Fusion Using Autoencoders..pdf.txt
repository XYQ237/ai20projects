2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
CityLearn: Diverse Real-World Environments for Sample-Efﬁcient
Navigation Policy Learning
Marvin Chanca´n1,2 and Michael Milford1
Abstract—Visualnavigationtasksinreal-worldenvironments
often require both self-motion and place recognition feedback.
Whiledeepreinforcementlearninghasshownsuccessinsolving
these perception and decision-making problems in an end-
to-end manner, these algorithms require large amounts of
experience to learn navigation policies from high-dimensional
data, which is generally impractical for real robots due to
sample complexity. In this paper, we address these problems
withtwomaincontributions.Weﬁrstleverageplacerecognition
and deep learning techniques combined with goal destination
feedback to generate compact, bimodal image representations
that can then be used to effectively learn control policies Fig.1. The CityLearn framework.WeleverageVPRandRLmethods
from a small amount of experience. Second, we present an to learn control policies for goal-driven navigation tasks. Our method is
interactiveframework,CityLearn,thatenablesfortheﬁrsttime efﬁcientandcangeneralizeacrossextremeenvironmentalchanges.
training and deployment of navigation algorithms across city-
sized, realistic environments with extreme visual appearance
changes.CityLearnfeaturesmorethan10benchmarkdatasets,
often used in visual place recognition and autonomous driving
research, including over 100 recorded traversals across 60
cities around the world. We evaluate our approach on two
CityLearn environments, training our navigation policy on a
single traversal per dataset. Results show our method can be
over2ordersofmagnitudefasterthanwhenusingrawimages,
andcanalsogeneralizeacrossextremevisualchangesincluding
day to night and summer to winter transitions.
I. INTRODUCTION Fig.2. Performanceandcomputecharacterization.RLtrainingresults
for(i)ourapproachusingoff-the-shelfVPR(NetVLAD)anddeeplearning
The ability to sense location in time and space is key, for (ResNet-50) models with a number of feature dimensions (e.g. 64, 512,
both robots and living beings, to enable navigation in highly 2048,4096),(ii)abaselineagentthatuses1-dpositionfeedbackinsteadof
images,and(iii)anagenttrainedend-to-endusingrawimages.
dynamic real-world environments. For mobile robots, the
waytheycancreateaparticular,internalworldrepresentation
or reward function values, that eventually increase their
often depends on their perceptual limitations as well as how
network policy architecture and sample complexity.
they interact and make decisions with the environment [1].
Visualplacerecognition(VPR)models,ontheotherhand,
Visual feedback provides high-dimensional information that,
are required to successfully match two image sequences
whenencodedproperly,canbeusedtomakesenseofwhere
of recorded data in real-world environments. While recent
they are and where they need to go. Similarly, self-motion
improvements using deep learning [6]–[8] and algorithmic
feedback also provides information concerning the current
methods [9] have contributed to state-of-the-art results on
position within an environment. These two sensory input
city-sizeddatasets,whetherthosemodelscanenablenaviga-
modalities are concurrent, time-aligned and often comple-
tion capabilities on real robots is not well explored.
mentary during goal-driven navigation tasks.
Inthiswork,weleveragebothVPRandRLtechniquesto
Recentdeepreinforcementlearning(RL)approacheshave
efﬁcientlylearncontrolpoliciesfornavigationtasks(Fig.1).
successfully performed active navigation tasks on simulated
Our resulting control policy is able to perform goal-driven
environmentsusingreal-worldstreetimagery[2]orsynthetic
navigationtasks usingonlytwosensory feedbackmodalities
scenarios [3]–[5]. These algorithms, however, generally uti-
(goal destination and visual representations). The results
lize additional feedback data, e.g. the agent-relative velocity
demonstratethatourpolicyisabletogeneralizeoverarange
of extreme environmental changes on real-world datasets,
The work of M.C. was supported by the Peruvian Government. The
whiledrasticallyreducingtheamountoftrainingexperience,
workofM.M.wassupportedbyARCgrantsFT140101229,CE140100016,
the QUT Centre for Robotics, and the Australian Government via grant e.g. from 29h48m to 11m (Fig. 2). We also show how
AUSMURIB000001associatedwithONRMURIgrantN00014-19-1-2571. our approach can achieve practical sample efﬁciency in an
1 QUT Centre for Robotics, School of Electrical Engineering and
interactive and diverse environment that we call CityLearn.1
Robotics,QueenslandUniversityofTechnology,Brisbane,Australia
2 School of Mechatronics Engineering, Universidad Nacional de Inge-
nier´ıa,Lima,Peru.mchancanl@uni.pe 1Projectpage:mchancan.github.io/projects/CityLearn
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 1697
Authorized licensed use limited to: Carleton University. Downloaded on September 22,2020 at 00:58:19 UTC from IEEE Xplore.  Restrictions apply. The main contributions of this paper are:
1) CityLearn: An interactive open framework with real-
world environments for perception and decision-
making to enable the evaluation of navigation algo-
rithms on more than 10 robotic benchmark datasets
with challenging environmental transitions (Fig. 3).
2) A new approach to sample-efﬁcient RL training for
goal-driven navigation tasks. We use VPR and deep
learning models to encode our sensory input images
which, when combined with goal destination signals,
can generate compact, bimodal representations, from
whichanavigationpolicycanbelearnedtogeneralize
Fig.3. FiveselectedbenchmarkdatasetsthatcanbeusedinCityLearn.
across extreme visual changes such as day to night or
summer to winter cycles.
These approaches, however, often use additional feedback
II. RELATEDWORK data such as reward function values or the agent-relative
velocity that further increase the policy network size and
In robotics research, the use of probabilistic techniques
trainingrequirements.Thesefactorsalsoincreasethenumber
played an important role in solving robotic problems such
of interactions required with the environment, typically to
as how the robot’s sensory information should be integrated
the order of millions of episodes. These systems, more-
to generate internal states and support the decision-making
over, are often evaluated on the same environment used for
process [1]. In the mid-1990s, these methods allowed the
training, thus their generalization capabilities to different
deploymentofnavigationalgorithmsonrealrobotsbyusing
visual conditions are often unknown; alternatively, it is
conditional probability distributions, instead of deterministic
necessary to increase the complexity of their architectures
functions at a ﬁxed time interval (as in classical control), to
for them to successfully train and generalize to challenging
computemoregeneralcontrolactionsthatgoverntherobot’s
environmental conditions.
states [10]–[12]. In the same decade, moreover, the ﬁeld of
RL started to attract the interest of roboticists. RL agents
B. Visual Place Recognition
learn speciﬁc behavior through interactions with dynamic
environments only by reward and punishment signals [13]. VPR methods for sequence-based localization tasks typi-
Therefore, the use of RL to solve more complex robot- callyperformamulti-framematchingprocedurebetweentwo
learning problems started to be extended with the incorpo- or more traversals (query and reference) on stationary real-
ration of neural networks to obtain broader generalization worlddatasets.Bothqueryandreferencesequenceofimages
capabilities. Ideas like hierarchical or curriculum learning oftenincludechallengingappearanceandviewpointchanges
[14] were also proposed to reduce the learning time and betweenthem(e.g.differentweatherorseasonalconditions),
solve these complex, physically realistic robotic problems illuminationchangesduetotimeofday,anddynamicobjects
in simulation environments. (Fig. 3). A VPR algorithm for sequence-based datasets can
be broadly split into two main steps [8], [9], [37]–[54]:
A. Deep Reinforcement Learning based Navigation
(1) feature extraction process utilizing either hand-crafted
The spread of convolutional neural networks (CNN) has or deep-learning-based techniques to obtain compact image
yieldedimpressivestate-of-the-artresultsincomputervision, representations, that can then be (2) matched via conven-
naturallanguageprocessing,andmanyotherrelateddomains tional similarity metrics (e.g. cosine or L distance) or more
2
over the past eight years [15]. Similarly, recent research elaborate multi-frame temporal ﬁltering algorithms such as
incorporating deep neural networks to more advanced RL SeqSLAM [37] and many others [40], [55], [56]. Though
algorithmsfornavigationtaskshaveshownpromisingresults recent improvements using temporal ﬁltering approaches
in simulated environments. have shown state-of-the-art results [9], we note that those
Recent works have trained deep RL agents or deep- methods need to have at least two traversals from the same
learning-based models to perform navigation tasks using environment at the time of performing their ﬁnal matching
real-world images [2], [16]–[24]; typically generalizing well procedure.
over different visual conditions with minimal additional In this work, for the proposed goal-oriented navigation
training and network architecture changes. Similarly, re- task, we use a single traversal to train our control policy
searchershaveusednonend-to-endRLapproacheswithreal network,whichisthenevaluatedontheremainingtraversals.
datathat,whenencodedviaoff-the-shelfdeeplearningmod- Consequently, we choose VPRand deep learning techniques
els, can efﬁciently learn navigation policies in unstructured that are known to obtain better, compact visual representa-
environments [25]. Related deep RL approaches have also tions from raw images, such as ResNet [57] or NetVLAD
shown success on target-driven navigation tasks but have [6]—which performs well compared to related VPR models
only been demonstrated in indoor [26]–[35] or synthetic [58]—rather than full VPR models that use algorithmic
environments [3]–[5], [36]. techniques on top of those deep learned representations.
1698
Authorized licensed use limited to: Carleton University. Downloaded on September 22,2020 at 00:58:19 UTC from IEEE Xplore.  Restrictions apply. TABLEI
CITYLEARN:DETAILEDCOMPARISONWITHRECENTREAL-WORLDENVIRONMENTS
Environment region/dataset #trav #imgs av.step #sensors journey city country
WallStreet 1 9.8m
56k 548.8km NewYork
UnionSquare 1 9.8m
×
1 panoramiccam.
StreetLearn HudsonRive 1 9.9m ◦ USA
(360 view)
[59] CMU 1 9.9m
58k 574.2km Pittsburgh
Allergheny 1 9.8m
SouthShore 1 9.9m
× ◦
6 stereocam.(360 view)
OxfordRobotCar 133 20M 0.2m × × 10km Oxford UK
1 3Dand2 2DLiDAR
×
BerkeleyDeepDrive 100k 120M 30fps 1 cam.(front) 1100h multiple USA
×
Cityspaces 50 - - 1 cam.(front) 100h multiple(50)
Germany
CityLearn KITTI 22 - 10fps 4×cam.(front,rear) 6h Karlsruhe
[Ours] ×
NordlandRailway 4 3.6M 0.05m 1 cam.(front) 728km Trondheim–Bodø Norway
×
Multi-LaneRoad 4 - - 1 cam.(front) 4km GoldCoast(GC)
† ×
GoldCoastDrive 1 - - 1 cam.(front) 87km Brisbane–GC
× Australia
UQSt.Lucia 1 - - 1 cam.(front) 9.5km Brisbane
×
St.LuciaMultipleTimes 10 - 15fps 1 cam.(front) - Brisbane
‡ ×
AlderleyDay/Night 2 31.5k - 1 cam.(front) 16km Brisbane
† ‡
Providesareferencetrajectoryor framecorrespondencesinsteadofGPSdata.
TABLEII
III. THECITYLEARNENVIRONMENT
STREETLEARNVS.CITYLEARN:SUPPORTANDFEATURES
VPR methods are often evaluated on variety-rich, real-
Description StreetLearn[59] CityLearn[Ours]
world datasets collected over long traversals across different
seasons, time of day or weather conditions, including dy- Operatingsystem Ubuntu18.04 Windows/Linux/Mac
Environmentengine StreetLearn Unity/ML-Agents
namic objects, such as cars, trafﬁc, and pedestrians, along
Language/MLframeworks C++,Python/TF C#,Python/TF
with longer-term changes such as construction or roadworks Min.RAMperenv. 12GB 2GB
[37], [44], [60]–[67] (Fig. 3). The data obtained typically Numberofpublicdatasets 1 10+
Numberofcities 2 60+
includesvideosorsequencesofimagesprovidingpanoramic
◦ Numberoftraversals 1 100+
or 360 views from stereo cameras, scans of 2D/3D LiDAR Min.averageagentstep 9.8m 0.05m
sensors,visualodometrydata,andGPS/inertialdatathatcan Multi-environmenttraining  
Featurespublicdatasets  
then be used as ground truth labels.
Appearancechanges  
Weleveragethosereal-worlddatasetstocreateCityLearn, Viewpointchanges  
aninteractiveopenframeworkthatenables,fortheﬁrsttime, Multipletimesofday  
Multipleweather/seasons  
the training and testing of navigation algorithms on city-
sized, realistic environments. Our fully-conﬁgurable frame-
work runs on top of the Unity game engine and their ML- compact goal destinations, resulting in compact, bimodal
Agents framework [68]. CityLearn is related to the recent representationsthatcanthenbeusedtotrainourpolicyusing
StreetLearn work [59] used in [2], [69], [70] but has a a single traversal in our CityLearn environment.
M
range of useful differences. We propose the usage of diverse We use a Markov Decision Process with discrete state
∈ S ∈ A
environments across 5 countries and additionally enable s and action a spaces, and a transition operator
Tt S ×A → S t
loading any other dataset including in-house recorded data; : to model our navigation tasks as a ﬁnite-
∗
see Tables I and II for a detailed comparison. horizon T problem. Our goal is to ﬁnd θ that maximizes
(cid:34) (cid:35)
In Table I, each environment (region/dataset) also in- the objective function: (cid:88)
cludes GPS data; except for Gold Coast Drive and Alderley
Day/Night. Related frameworks for city-scale navigation T
E
based on real-world images were not considered in Table I J(θ)= τ∼πθ(τ) γr(τ) (1)
as they interact differently with the environment via natural t=1
S →P A
language communication [21]–[23]. where π : ( ) is the stochastic navigation policy
θ S×A→R
we want to learn, and r : is the reward function
IV. PROBLEMSTATEMENTANDMETHODS
with discount factor γ. To optimize π , we parameterize it
θ
Our goal is to train a policy network to perform goal- with a neural network that learns θ, as described in Sec. IV-
S
driven navigation tasks. To enable sample-efﬁciency, we use B. isdeﬁnedbyourcompact,bimodalspacerepresentation
either off-the-shelf VPR or deep learning models to encode b generatedbycombiningtheagent’svisualobservationx
t t
our sensory input images and obtain multi-dimensional fea- and a 1-d goal destination g , as also detailed in Secs. IV-
A t
turevectors.Then,usingRL,wecombinethesefeatureswith A and IV-B. is deﬁned over discrete action movements
1699
Authorized licensed use limited to: Carleton University. Downloaded on September 22,2020 at 00:58:19 UTC from IEEE Xplore.  Restrictions apply. in the agent’s action space a . We evaluate our approach on
t
twochallengingCityLearnenvironmentswithextremevisual
changes such as day to night for Oxford RobotCar [60], and
summer to winter for the Nordland [71] dataset (Fig. 5).
A. Visual Observations
We encode our sensory input images – which are either
× ×
1920 1080 RGB for the Nordland dataset or 1280 960
RGB for the Oxford RobotCar dataset – using either off-
Fig. 4. Navigation baseline agents. Our approach (b) uses the goal
the-shelf VPR (NetVLAD) or deep learning (ResNet-50)
models. For NetVLAD [6], we use their best performing dreepsrteinsaetnitoantiogntsabntdwchoimchpaccatnvtihseunalbeobcsoemrvbaitnioendswxitthttohegeangeernatt’es pbriemvoioduasl
network, based on VGG-16 [72] with PCA plus whitening, actionat−1toestimateastochasticnavigationpolicyπandvaluefunction
to encode our images into a range of visual observations V.Wealsotrainabaseline(a)agentusingitscurrentpositionptinstead
consistingof4096-d,2048-d,512-dand64-dfeaturevectors. ofxt,andanotheragentusingraw images(c)fromscratch.
For ResNet-50 [57], we use a network trained on ImageNet Reward design and curriculum learning: We use 7
[73] to extract image representations of 2048-d, which we levels of curriculum learning [14] to encourage the agent to
then reduce to more compact representations such as 512- explore the environment gradually in order to ﬁnd increas-
d and 64-d using the algorithm provided in NetVLAD for ingly distant destinations [2]. Our sparse reward function
dimensionality reduction. gives the agent a reward of +1 only when it ﬁnds the
−
Once we obtain our visual observations, xt, we com- target, potentially receiving a punishment of 1/ms when
bine them with a 1-d goal destination, gt, to generate our it heads away from the required destination, with ms being
compact, bimodal representation, b , that serves as input to the maximum number of agent steps per episode.
t
our navigation policy, see Fig. 4(b). g is encoded as a 1-
t C. Baseline Agents
d feature vector to preserve the compactness of our ﬁnal
bimodalrepresentationb whicharefeaturevectorsof65-d, Wecompareour approach,describedinSectionIV-B,
t
513-d, 2049-d, and 4097-d. against two additional agent architectures: baseline and
raw images, as shown in Figs. 4(a) and (c), respectively.
B. Policy Learning for Visual Navigation In all our experiments, the goal destination, g , is encoded
t
using a 1-d feature vector for fair comparison (Fig. 4), but
Ourobjectiveistolearnapolicyforgoal-directednaviga-
it can easily be adapted to use more complex encoding
tion tasks using a compact, bimodal representation such as
methods, as per previous work [2]. The code of the three
bt.Whiletherehasbeensomesuccessusingdeepreinforce- RL baseline agents including our approach, shown in
ment learning for navigation tasks from raw images [2], [3],
Fig. 4, is made publicly available along with CityLearn.
they require the addition of more feedback modalities (e.g.
Baseline: This baseline agent is a relatively trivial
reward values or agent’s velocity) that eventually increase
baseline, see Fig. 4(a), that uses a 1-d feature vector as its
thenumberofinteractionswiththeenvironmentandtraining
current position p , instead of x . While this substantially
time. We aim to investigate the performance of using bt, simpliﬁes the probtlem, it is a cotmpetitive agent reference
obtained in Sec. IV-A, to train our policy.
since it achieves 100% completed tasks on deployment.
Tasksetup:Wedesignanavigationtaskwhereasuccess- Rawimages:Ourraw imagesagentusesaCNNvisual
fultaskrequiresreasoningusingourvisualobservationsand
moduleof2convolutionallayers,seeFig.4(c),asinprevious
goal destination bt to ﬁnd a required target gt over a single works[3],[77].TheﬁrstCNNlayerhasakernelofsize8×8,
traversal in the CityLearn environment (Figs. 1 and 4). ×
astrideof4 4,and16featuremaps.ThesecondCNNlayer
Our approach: We choose the proximal policy optimiza- has a kernel of size 4×4, a stride of 2×2, and 32 feature
tion(PPO)algorithm[74]tooptimizeourobjectivefunction maps. The input consisted of RGB images of 84×84.
in Eq. (1). PPO is a variation of TRPO [75] that constraints
D. Evaluation Metrics
thepolicyupdate,whilestrikingthebalancebetweensample
complexity and hyperparameters tuning to achieve state-of- Visual place recognition: VPR performance using our
the-art results on a range of benchmark RL problems. Our encoded visual observations are reported via area under the
agent network architecture, see Fig. 4(b), comprises of curve (AUC) metrics across a number of feature dimensions
a single linear multi-layer perceptron (MLP) of 512 units (Fig.6).Wetrainaclassiﬁeroneachreferencetraversalusing
that encodes b , see Fig. 4(b). We then combine it with a single MLP that receives our encoded visual observations.
t
the agent’s previous action, a − , using a single recurrent We then use this trained classiﬁer to evaluate the remaining
t 1
layer long short-term memory (LSTM) [76] of 256 units, to query traversals. Once we have the scores for both query
estimatetherequiredactionsfromtheestimatedpolicyπand and reference, we compute the precision-recall curves from
thevaluefunctionV.Additionally,weimplementtwopolicy where we can obtain the overall AUC performance.
networks for comparison purposes, also shown in Figs. 4(a) Goal-oriented navigation: We evaluate the RL training
and (c), whose details are provided in Sec. IV-C. performanceonboththetraversalusedfortrainingandother
1700
Authorized licensed use limited to: Carleton University. Downloaded on September 22,2020 at 00:58:19 UTC from IEEE Xplore.  Restrictions apply. Fig.5. Two diverse real-world benchmark datasetsusedinourexperiments.TheNordlanddataset(lefttoright-center)includingsummer,fall,and
wintertraversals.TheOxfordRobotCardataset(left-centertoright)includingday,overcast,andnighttraversals.
Fig. 6. AUC place recognition performance, on the Nordland (left) and Oxford Robotcar (right) datasets, evaluated under moderate and extreme
environmental changes. We use off-the-shelf place recognition (NetVLAD: NV) and deep learning (ResNet-50: RN) models to encode our RGB images
intoarangeoffeaturedimensions:64-d,512-d,2048-d,4096-d).
Fig. 7. RL training curves. Our approach uses NetVLAD and ResNet-50 models, with 64-d, 512-d, 2048-d, 4096-d feature representations, can
efﬁciently train our RL navigation policy compared to an agent trained end-to-end using raw images (light blue). We also show the results for a
baselineagent(blue)thatusessimple1-d goalandpositionrepresentations.
two testing traversals with extreme visual changes (e.g. day For Oxford RobotCar, moderate viewpoint changes
toafternoon/nightforOxfordRobotCar,andsummertowin- Day/sunny to overcast results in lower global performance,
ter/fallforNordland).Wealsolimitthemaximumnumberof when compared to Nordland which does not include view-
agentstepsinanepisodetothenumberofimageswithinthe point changes (see Fig. 6 (right)). In contrast, for extreme
traversal,measuringinthiswayhowwelltheagentcanﬁnda appearance changes, such as summer to winter or day to
target destination with a moderate, environment-appropriate night, we can observe that the global AUC performance is
number of steps. We provide statistics on the number of compromised, reducing to less than half for Nordland or
navigation tasks that our policy can achieve by reporting even to less than a quarter for Oxford RobotCar compared
the percentage of the deployment results in two categories: to small appearance changes. It is worth noting we are
(1) completed tasks, when the agent reaches the target using performing only a single-frame matching procedure here;
theminimumnumberofstepsasdeﬁnedabove,or(2)failed theresultsmaynotbeasgoodasexpectedforthesestate-of-
tasks, otherwise. the-artmethodssincemulti-framealgorithmictechniquesare
typically incorporated on top of those single-frame results,
V. EXPERIMENTS:RESULTS
as previously described in Sec. II-B.
We ﬁrst conduct conventional, single-frame VPR experi-
ments using our visual observations on two stationary real-
B. Sample-Efﬁcient Navigation Policy Training
world datasets (Fig. 5). We then use these compact place
WeillustrateRLtrainingcurvesinFig.7;complete-related
representations to train our policy network for efﬁciently
visualization as a function of the required training time is
learning goal-driven navigation tasks using CityLearn.
presented in Fig. 2. In Fig. 7 (left), we observe that our
A. Place Recognition Experiments approach, with 64-d representations, achieves comparable
The trade-off of using compact visual observations for average reward performance compared to the baseline
VPR is shown in Fig. 6. We report the results of our single- agent; being 92% for NetVLAD, 80% for ResNet-50 and
frame VPR experiments, as described in Secs. IV-A and IV- 99% for the baseline agent. This small difference be-
D. AUC performance decreases as we decrease the feature tween these three agents is reﬂected in Fig. 7 (right), where
dimensionfrom4096-dallthewayto64-dinbothNetVLAD the number of agent steps stabilizes slightly below 50 at
and ResNet-50 models. We can also observe how well these 10,000 episodes for the baseline agent, while for the
networksgeneralizewhenfacingsmallappearancevariations remaining two agents (NetVLAD and ResNet-50 with 64-
such as summer to fall for Nordland, see Fig. 6 (left). d) this occurs slightly above 50 steps at 18,000 episodes.
1701
Authorized licensed use limited to: Carleton University. Downloaded on September 22,2020 at 00:58:19 UTC from IEEE Xplore.  Restrictions apply. Fig.8. Deployment.Navigationpolicyevaluationstatisticsonthetraversalitwastrained.Nordland(left):summer,OxfordRobotCar(right):day.
Fig. 9. RL generalization results. Evaluation statistics over moderated (blue) and extreme (green) appearance environmental changes. For Nordland
(left):fall(blue)andwinter(green)traversals.ForOxfordRobotCar(right):overcast(blue)andnight(green).
This behavior is consequently observed again in Fig. 7,
as we increase the visual feature dimensions from 512-d to
4096-d. The ﬁnal average number of steps for these agents
is around 75 and the required number of training episodes
increases as we increase the feature dimension; except for
4096-d that stabilizes at 50,000 episodes, which is lower
than 2048-d that requires 60,000 episodes. We additionally
provide two training results for 4096-d, where this behavior Fig.10. Deploymentcomparison.ThepoliciesweretrainedontheOxford
RobotCardataset(day:top-left)andevaluatedunderextremevisualchanges
is again shown in curve 4096-d*. It is worth noting the
(night).Our approachcompletedthetaskusingNetVLAD64-d(center-
training curves in Fig. 7 were obtained by averaging 5 trials left),whiletheraw imagesagentfailed(bottom-left).
each using different seed numbers, and then applying curve
smoothingwithweight0.9toenablecleanervisualizationof (using the policy network described in Sec. IV-C) in a route
our results. In all our experiments, we used 16 concurrent of the Oxford RobotCar dataset. Both agents starting at the
agents for training our policy network using the CityLearn same location with a common goal.
framework.
VI. CONCLUSIONS
C. Deployment and Generalization Results We conducted comprehensive experiments applying VPR
We report evaluation statistics of our trained navigation and RL techniques to examine the value of using visual and
policy on both the reference traversal used for training (Fig. self-motion(intermsoftheagent’spreviousactions)sensory
8) and query traversals used to test their generalization feedback to learn navigation policies on diverse robotic
capabilities (Fig. 9) across our two datasets alongside with datasets.ToenableefﬁcientRLtraining,weuseVPRmodels
the CityLearn environment using the Nordland (left) and to encode real sensory data that, when combined with the
Oxford RobotCar (right) datasets. We evaluated our trained goaldestination,generatescompactbimodalrepresentations.
stochastic policy every 100 episodes and calculated the Once trained, we showed that smaller visual representations
number of completed and failed navigation tasks. From such as 64-d generalized better than larger features over a
Figs. 8 and 9, it can be observed that when using compact range of environmental transitions, while being around 2
representations (64-d) we can achieve better generalization orders of magnitude faster and requiring a small fraction of
results, even under extreme environmental changes such as the amount of experience in terms of training time.
summer to winter for Nordland, shown in Fig. 9 (left), or Theproposedinteractiveenvironment,CityLearn,canalso
day to night for Oxford RobotCar, see Fig. 9 (right). While be used to load any other benchmark dataset (or even
increasing the feature dimension in VPR tasks results in custom in-house recorded data), such as those from drones
better AUC performance (see Fig. 6), the opposite seems to or underwater robots, to train and test many different types
occur for navigation tasks; smaller representations are better of navigation algorithms as well as to further build and
for both ﬁnal average performance and sample efﬁciency in investigatetheperformanceofadvancedRLalgorithmsusing
terms of training time or number of episodes, as well as in realistic images. Future research could include other RL
generalization capabilities, at least in an RL context. algorithms such as [77], modular architectures for transfer
Fig. 10 shows deployment comparisons between our ap- learning to new cities [78], and adding more functionality to
proach and an agent trained end-to-end using raw images the environment such as creating 2D geometric maps.
1702
Authorized licensed use limited to: Carleton University. Downloaded on September 22,2020 at 00:58:19 UTC from IEEE Xplore.  Restrictions apply. REFERENCES [23] H.Chen,A.Suhr,D.Misra,N.Snavely,andY.Artzi,“Touchdown:
Natural language navigation and spatial reasoning in visual street
[1] S.Thrun,W.Burgard,andD.Fox,ProbabilisticRobotics,1sted.,ser.
environments,”inProceedingsoftheIEEEConferenceonComputer
IntelligentRoboticsandAutonomousAgents. TheMITPress,2005.
VisionandPatternRecognition,2019,pp.12538–12547.
[2] P.Mirowski,M.Grimes,M.Malinowski,K.M.Hermann,K.Ander-
[24] V.Dhiman,S.Banerjee,B.Grifﬁn,J.M.Siskind,andJ.J.Corso,“A
son, D. Teplyashin, K. Simonyan, A. Zisserman, R. Hadsell, et al.,
critical investigation of deep reinforcement learning for navigation,”
“Learningtonavigateincitieswithoutamap,”inAdvancesinNeural
arXivpreprintarXiv:1802.02274,2018.
InformationProcessingSystems,2018,pp.2419–2430.
[25] J. Bruce, N. Su¨nderhauf, P. Mirowski, R. Hadsell, and M. Milford,
[3] P.W.Mirowski,R.Pascanu,F.Viola,H.Soyer,A.Ballard,A.Banino,
“Learning deployable navigation policies at kilometer scale from
M. Denil, R. Goroshin, L. Sifre, K. Kavukcuoglu, D. Kumaran, and
a single traversal,” in Proceedings of The 2nd Conference on
R. Hadsell, “Learning to navigate in complex environments,” arXiv
Robot Learning, ser. Proceedings of Machine Learning Research,
preprintarXiv:1611.03673,2016.
A. Billard, A. Dragan, J. Peters, and J. Morimoto, Eds., vol. 87.
[4] A.Banino,C.Barry,B.Uria,C.Blundell,T.Lillicrap,P.Mirowski,
PMLR, 29–31 Oct 2018, pp. 346–361. [Online]. Available:
A.Pritzel,M.J.Chadwick,T.Degris,J.Modayil,etal.,“Vector-based
http://proceedings.mlr.press/v87/bruce18a.html
navigationusinggrid-likerepresentationsinartiﬁcialagents,”Nature,
[26] Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei, and
vol.557,no.7705,pp.429–433,2018.
A. Farhadi, “Target-driven visual navigation in indoor scenes using
[5] C. J. Cueva and X.-X. Wei, “Emergence of grid-like representations
deepreinforcementlearning,”in2017IEEEinternationalconference
bytrainingrecurrentneuralnetworkstoperformspatiallocalization,”
onroboticsandautomation(ICRA),2017,pp.3357–3364.
arXivpreprintarXiv:1803.07770,2018.
[27] S.Gupta,J.Davidson,S.Levine,R.Sukthankar,andJ.Malik,“Cog-
[6] R. Arandjelovic´, P. Grona´t, A. Torii, T. Pajdla, and J. Sivic,
nitivemappingandplanningforvisualnavigation,”inProceedingsof
“NetVLAD: CNN Architecture for Weakly Supervised Place Recog-
the IEEE Conference on Computer Vision and Pattern Recognition,
nition,” 2016 IEEE Conference on Computer Vision and Pattern
2017,pp.2616–2625.
Recognition(CVPR),pp.5297–5307,2016.
[28] E.Kolve,R.Mottaghi,D.Gordon,Y.Zhu,A.Gupta,andA.Farhadi,
[7] Z. Chen, A. Jacobson, N. Su¨nderhauf, B. Upcroft, L. Liu, C. Shen,
“AI2-THOR: An Interactive 3D Environment for Visual AI,” arXiv
I.D.Reid,andM.Milford,“Deeplearningfeaturesatscaleforvisual
preprintarXiv:1712.05474,2017.
placerecognition,”2017IEEEInternationalConferenceonRobotics
[29] M.Savva,A.X.Chang,A.Dosovitskiy,T.Funkhouser,andV.Koltun,
andAutomation(ICRA),pp.3223–3230,2017.
“MINOS: Multimodal indoor simulator for navigation in complex
[8] S.Garg,N.Su¨enderhauf,andM.Milford,“Lost?appearance-invariant
environments,”arXivpreprintarXiv:1712.03931,2017.
place recognition for opposite viewpoints using visual semantics,”
[30] G.Kahn,A.Villaﬂor,P.Abbeel,andS.Levine,“Composableaction-
ProceedingsofRobotics:ScienceandSystemsXIV,2018.
conditioned predictors: Flexible off-policy learning for robot naviga-
[9] S.Hausler,A.Jacobson,andM.Milford,“Multi-processfusion:Visual
tion,”arXivpreprintarXiv:1810.07167,2018.
place recognition using multiple image processing methods,” IEEE
[31] G. Brunner, O. Richter, Y. Wang, and R. Wattenhofer, “Teaching a
RoboticsandAutomationLetters,vol.4,no.2,pp.1924–1931,2019.
machine to read maps with deep reinforcement learning,” in Thirty-
[10] H.F.Durrant-Whyte,“Anautonomousguidedvehicleforcargohan-
SecondAAAIConferenceonArtiﬁcialIntelligence,2018.
dling applications,” The International Journal of Robotics Research,
[32] Y. Wu, Y. Wu, G. Gkioxari, and Y. Tian, “Building generalizable
vol.15,pp.407–440,1996.
agents with a realistic and rich 3D environment,” arXiv preprint
[11] I. R. Nourbakhsh, J. Bobenage, S. Grange, R. Lutz, R. Meyer, and
arXiv:1801.02209,2018.
A. Soto, “An affective mobile robot educator with a full-time job,”
[33] S.Bansal,V.Tolani,S.Gupta,J.Malik,andC.Tomlin,“Combining
ArtiﬁcialIntelligence,vol.114,no.1,pp.95–124,1999.
optimal control and learning for visual navigation in novel environ-
[12] F. Dellaert, D. Fox, W. Burgard, and S. Thrun, “Monte carlo local-
ments,”arXivpreprintarXiv:1903.02531,2019.
ization for mobile robots,” in Proceedings 1999 IEEE International
[34] T.-H. Wang, H.-J. Huang, J.-T. Lin, C.-W. Hu, K.-H. Zeng, and
Conference on Robotics and Automation (ICRA), vol. 2, May 1999,
M. Sun, “Omnidirectional cnn for visual place recognition and nav-
pp.1322–1328.
igation,” in 2018 IEEE International Conference on Robotics and
[13] L. Lin, “Reinforcement learning for robots using neural networks,”
Automation(ICRA),2018,pp.2341–2348.
Ph.D.dissertation,CarnegieMellonUniversity,Pittsburgh,PA,1992.
[35] A. Mousavian, A. Toshev, M. Fisˇer, J. Kosˇecka´, A. Wahid, and
[14] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, “Curriculum
J.Davidson,“Visualrepresentationsforsemantictargetdrivennaviga-
learning,”inProceedingsofthe26thAnnualInternationalConference
tion,”in2019InternationalConferenceonRoboticsandAutomation
on Machine Learning, ser. ICML 09. New York, NY, USA:
(ICRA). IEEE,2019,pp.8846–8852.
Association for Computing Machinery, 2009, pp. 41–48. [Online].
[36] D.S.Chaplot,E.Parisotto,andR.Salakhutdinov,“Activeneurallo-
Available:https://doi.org/10.1145/1553374.1553380
calization,”inInternationalConferenceonLearningRepresentations,
[15] I. G. Goodfellow, Y. Bengio, and A. C. Courville, “Deep learning,”
2018.
Nature,vol.521,pp.436–444,2015.
[37] M. J. Milford and G. F. Wyeth, “SeqSLAM: Visual route-based
[16] S. Brahmbhatt and J. Hays, “Deepnav: Learning to navigate large
navigationforsunnysummerdaysandstormywinternights,”in2012
cities,” in 2017 IEEE Conference on Computer Vision and Pattern
IEEEInternationalConferenceonRoboticsandAutomation(ICRA),
Recognition(CVPR),July2017,pp.3087–3096.
May2012,pp.1643–1649.
[17] A.Khosla,B.AnAn,J.J.Lim,andA.Torralba,“Lookingbeyondthe
[38] Z.Chen,O.Lam,A.Jacobson,andM.Milford,“Convolutionalneural
visible scene,” in Proceedings of the IEEE Conference on Computer
network-based place recognition,” arXiv preprint arXiv:1411.1509,
VisionandPatternRecognition,2014,pp.3710–3717.
2014.
[18] A. Carballo, S. Seiya, J. Lambert, H. Darweesh, P. Narksri, L. Y.
[39] T.Naseer,L.Spinello,W.Burgard,andC.Stachniss,“Robustvisual
Morales, N. Akai, E. Takeuchi, and K. Takeda, “End-to-end au-
robot localization across seasons using network ﬂows,” in Twenty-
tonomousmobilerobotnavigationwithmodel-basedsystemsupport,”
eighthAAAIconferenceonartiﬁcialintelligence,2014.
Journal of Robotics and Mechatronics, vol. 30, no. 4, pp. 563–583,
[40] E.Pepperell,P.I.Corke,andM.J.Milford,“All-environmentvisual
2018.
place recognition with smart,” in IEEE International Conference on
[19] A.Amini,G.Rosman,S.Karaman,andD.Rus,“Variationalend-to-
RoboticsandAutomation(ICRA),2014,pp.1612–1618.
endnavigationandlocalization,”in2019InternationalConferenceon
[41] M.Milford,W.Scheirer,E.Vig,A.Glover,O.Baumann,J.Mattingley,
RoboticsandAutomation(ICRA),2019,pp.8958–8964.
andD.Cox,“Condition-invariant,top-downvisualplacerecognition,”
[20] H.Xu,Y.Gao,F.Yu,andT.Darrell,“End-to-endlearningofdriving
in2014IEEEInternationalConferenceonRoboticsandAutomation
modelsfromlarge-scalevideodatasets,”inProceedingsoftheIEEE
(ICRA),2014,pp.5571–5577.
Conference on Computer Vision and Pattern Recognition (CVPR),
[42] S.Lowry,N.Snderhauf,P.Newman,J.J.Leonard,D.Cox,P.Corke,
2017,pp.2174–2182.
andM.J.Milford,“Visualplacerecognition:Asurvey,”IEEETrans-
[21] H.deVries,K.Shuster,D.Batra,D.Parikh,J.Weston,andD.Kiela,
actionsonRobotics,vol.32,no.1,pp.1–19,Feb2016.
“Talkthewalk:Navigatingnewyorkcitythroughgroundeddialogue,”
[43] N. Su¨nderhauf, S. Shirazi, F. Dayoub, B. Upcroft, and M. Milford,
arXivpreprintarXiv:1807.03367,2018.
“On the performance of convnet features for place recognition,” in
[22] V. Cirik, Y. Zhang, and J. Baldridge, “Following formulaic map
2015 IEEE/RSJ International Conference on Intelligent Robots and
instructions in a street simulation environment,” in 2018 NeurIPS
Systems(IROS),2015,pp.4297–4304.
Workshop on Visually Grounded Interaction and Language, vol. 1,
2018.
1703
Authorized licensed use limited to: Carleton University. Downloaded on September 22,2020 at 00:58:19 UTC from IEEE Xplore.  Restrictions apply. [44] N. Su¨nderhauf, S. Shirazi, A. Jacobson, F. Dayoub, E. Pepperell, [61] Z. Chen, F. Maffra, I. Sa, and M. Chli, “Only look once, mining
B. Upcroft, and M. Milford, “Place recognition with convnet land- distinctive landmarks from convnet for visual place recognition,” in
marks:Viewpoint-robust,condition-robust,training-free,”Proceedings 2017 IEEE/RSJ International Conference on Intelligent Robots and
ofRobotics:ScienceandSystemsXII,2015. Systems(IROS),2017,pp.9–16.
[45] B. Ferrarini, M. Waheed, S. Waheed, S. Ehsan, M. Milford, and [62] A. J. Glover, W. P. Maddern, M. J. Milford, and G. F. Wyeth,
K.D.McDonald-Maier,“Visualplacerecognitionforaerialrobotics: “FAB-MAP+RatSLAM:Appearance-basedSLAMformultipletimes
Exploringaccuracy-computationtrade-offforlocalimagedescriptors,” of day,” in 2010 IEEE International Conference on Robotics and
in 2019 NASA/ESA Conference on Adaptive Hardware and Systems Automation,May2010,pp.3507–3512.
(AHS),2019,pp.103–108. [63] M. Milford, S. Lowry, N. Sunderhauf, S. Shirazi, E. Pepperell,
[46] N. Merrill and G. Huang, “Lightweight unsupervised deep loop B. Upcroft, C. Shen, G. Lin, F. Liu, C. Cadena, and I. Reid, “Se-
closure,”arXivpreprintarXiv:1805.07703,2018. quencesearchingwithdeep-learntdepthforcondition-andviewpoint-
[47] S.LowryandH.Andreasson,“Lightweight,viewpoint-invariantvisual invariantroute-basedplacerecognition,”in2015IEEEConferenceon
place recognition in changing environments,” IEEE Robotics and ComputerVisionandPatternRecognitionWorkshops(CVPRW),June
AutomationLetters,vol.3,no.2,pp.957–964,2018. 2015,pp.18–25.
[48] T.Sattler,W.Maddern,C.Toft,A.Torii,L.Hammarstrand,E.Sten- [64] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, “Vision meets
borg, D. Safari, M. Okutomi, M. Pollefeys, J. Sivic, et al., “Bench- robotics: The KITTI dataset,” The International Journal of Robotics
marking 6dof outdoor visual localization in changing conditions,” in Research,vol.32,no.11,pp.1231–1237,2013.[Online].Available:
ProceedingsoftheIEEEConferenceonComputerVisionandPattern https://doi.org/10.1177/0278364913491297
Recognition,2018,pp.8601–8610. [65] T.Naseer,W.Burgard,andC.Stachniss,“Robustvisuallocalization
[49] J.Mount,L.Dawes,andM.J.Milford,“Automaticcoverageselection across seasons,” IEEE Transactions on Robotics, vol. 34, no. 2, pp.
forsurface-basedvisuallocalization,”IEEERoboticsandAutomation 289–302,April2018.
Letters,vol.4,no.4,pp.3900–3907,2019. [66] J. Guo, U. Kurup, and M. Shah, “Is it safe to drive? an overview
[50] A. Torii, H. Taira, J. Sivic, M. Pollefeys, M. Okutomi, T. Pajdla, of factors, challenges, and datasets for driveability assessment in
and T. Sattler, “Are Large-Scale 3D Models Really Necessary autonomousdriving,”arXivpreprintarXiv:1811.11277,2018.
for Accurate Visual Localization?” IEEE Transactions on Pattern [67] H.Caesar,V.Bankiti,A.H.Lang,S.Vora,V.E.Liong,Q.Xu,A.Kr-
Analysis and Machine Intelligence, 2019. [Online]. Available: ishnan,Y.Pan,G.Baldan,andO.Beijbom,“nuScenes:Amultimodal
https://doi.org/10.1109/TPAMI.2019.2941876 dataset for autonomous driving,” arXiv preprint arXiv:1903.11027,
[51] S. Hausler, A. Jacobson, and M. Milford, “Filter early, match late: 2019.
Improving network-based visual place recognition,” arXiv preprint [68] A. Juliani, V.-P. Berges, E. Vckay, Y. Gao, H. Henry, M. Mattar,
arXiv:1906.12176,2019. and D. Lange, “Unity: A general platform for intelligent agents,”
[52] S. Lowry, “Similarity criteria: evaluating perceptual change for vi- arXiv preprint arXiv:1809.02627, 2018. [Online]. Available: https:
sual localization,” in 2019 European Conference on Mobile Robots //github.com/Unity-Technologies/ml-agents
(ECMR),2019,pp.1–6. [69] K. M. Hermann, M. Malinowski, P. Mirowski, A. Banki-Horvath,
[53] A.Khaliq,S.Ehsan,Z.Chen,M.Milford,andK.McDonald-Maier, K.Anderson,andR.Hadsell,“Learningtofollowdirectionsinstreet
“A holistic visual place recognition approach using lightweight view,”arXivpreprintarXiv:1903.00401,2019.
cnns for signiﬁcant viewpoint and appearance changes,” IEEE [70] A. Li, H. Hu, P. Mirowski, and M. Farajtabar, “Cross-view policy
Transactions on Robotics, pp. 1–9, 2019. [Online]. Available: learning for street navigation,” in Proceedings of the IEEE Interna-
https://doi.org/10.1109/TRO.2019.2956352 tionalConferenceonComputerVision,2019,pp.8100–8109.
[54] M. Chanca´n, L. Hernandez-Nunez, A. Narendra, A. B. Barron, and [71] N. Su¨nderhauf, P. Neubert, and P. Protzel, “Are we there yet? chal-
M. Milford, “A hybrid compact neural architecture for visual place lenging seqslam on a 3000 km journey across all four seasons,” in
recognition,”IEEERoboticsandAutomationLetters,vol.5,no.2,pp. Proc. Workshop Long-Term Autonomy 2013 IEEE Int. Conf. Robot.
993–1000,April2020. Autom.(ICRA),2013.
[55] B. Talbot, S. Garg, and M. Milford, “OpenSeqSLAM2.0: An Open [72] K. Simonyan and A. Zisserman, “Very deep convolutional networks
Source Toolbox for Visual Place Recognition Under Changing Con- for large-scale image recognition,” arXiv preprint arXiv:1409.1556,
ditions,” in 2018 IEEE/RSJ International Conference on Intelligent 2014.
RobotsandSystems(IROS),Oct2018,pp.7758–7765. [73] J.Deng,W.Dong,R.Socher,L.Li,KaiLi,andLiFei-Fei,“Imagenet:
[56] S.Garg,N.Suenderhauf,andM.Milford,“Don’tlookback:Robusti- Alarge-scalehierarchicalimagedatabase,”in2009IEEEConference
fyingplacecategorizationforviewpoint-andcondition-invariantplace onComputerVisionandPatternRecognition,June2009,pp.248–255.
recognition,”in2018IEEEInternationalConferenceonRoboticsand [74] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,
Automation(ICRA),2018,pp.3645–3652. “Proximal policy optimization algorithms,” arXiv preprint
[57] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for arXiv:1707.06347,2017.
image recognition,” in 2016 IEEE Conference on Computer Vision [75] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust
andPatternRecognition(CVPR),June2016,pp.770–778. regionpolicyoptimization,”inInternationalConferenceonMachine
[58] M. Zaffar, A. Khaliq, S. Ehsan, M. Milford, and K. D. McDonald- Learning,2015,pp.1889–1897.
Maier, “Levelling the playing ﬁeld: A comprehensive comparison of [76] S.HochreiterandJ.Schmidhuber,“Longshort-termmemory,”Neural
visualplacerecognitionapproachesunderchangingconditions,”arXiv Computation,vol.9,pp.1735–1780,1997.
preprintarXiv:1903.09107,2019. [77] L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih,
[59] P. Mirowski, A. Banki-Horvath, K. Anderson, D. Teplyashin, T. Ward, Y. Doron, V. Firoiu, T. Harley, I. Dunning, S. Legg,
K. M. Hermann, M. Malinowski, M. K. Grimes, K. Simonyan, andK.Kavukcuoglu,“IMPALA:ScalableDistributedDeep-RLwith
K. Kavukcuoglu, A. Zisserman, et al., “The streetlearn environment Importance Weighted Actor-Learner Architectures,” arXiv preprint
anddataset,”arXivpreprintarXiv:1903.01292,2019. arXiv:1802.01561,2018.
[60] W. Maddern, G. Pascoe, C. Linegar, and P. Newman, “1 year, [78] C. Devin, A. Gupta, T. Darrell, P. Abbeel, and S. Levine, “Learn-
1000 km: The oxford robotcar dataset,” The International Journal ing modular neural network policies for multi-task and multi-robot
of Robotics Research, vol. 36, no. 1, pp. 3–15, 2017. [Online]. transfer,” in 2017 IEEE International Conference on Robotics and
Available:https://doi.org/10.1177/0278364916679498 Automation(ICRA),2017,pp.2169–2176.
1704
Authorized licensed use limited to: Carleton University. Downloaded on September 22,2020 at 00:58:19 UTC from IEEE Xplore.  Restrictions apply. 