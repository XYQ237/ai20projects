2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
VALID: A Comprehensive Virtual Aerial Image Dataset
Lyujie Chen1, Feng Liu1, Yan Zhao1, Wufan Wang1, Xiaming Yuan1, and Jihong Zhu2
Abstract—Aerial imagery plays an important role in land-
use planning, population analysis, precision agriculture, and
unmanned aerial vehicle tasks. However, existing aerial image
datasetsgenerallysufferfromtheproblemofinaccuratelabel-
ing, single ground truth type, and few category numbers. In
this work, we implement a simulator that can simultaneously
acquire diverse visual ground truth data in the virtual envi-
ronment. Based on that, we collect a comprehensive Virtual
AeriaL Image Dataset named VALID, consisting of 6690 high-
resolution images, all annotated with panoptic segmentation
on 30 categories, object detection with oriented bounding box,
and binocular depth maps, collected in 6 different virtual
scenes and 5 various ambient conditions (sunny, dusk, night,
snow and fog). To our knowledge, VALID is the ﬁrst aerial
image dataset that can provide panoptic level segmentation
andcompletedensedepthmaps.Weanalyzethecharacteristics
of VALID and evaluate state-of-the-art methods for multiple
tasks to provide reference baselines. The experiment results
demonstratethatVALIDiswellpresentedandchallenging.The
datasetisavailableathttps://sites.google.com/view/
valid-dataset/.
I. INTRODUCTION
Therecentsuccessoflearning-basedmethodsincomputer
vision researches has motivated the use of large public
benchmarks, such as ImageNet [1] and COCO [2]. Due to
the prevalence of UAV in both civil and military use, aerial
image datasets recently got the increasing attention of the
community. However, the scale and quality of these datasets
are still insufﬁcient. It is mainly because that the aerial im-
agesarecollectedbysatellitesorplanes,whicharefarmore
expensive and difﬁcult to acquire than everyday life images.
Inaddition,theobjectsinaerialimagesappearwithdifferent
visual characteristics, arbitrary orientations, multiple scales,
and complex categories, resulting in difﬁculties for efﬁcient
and accurate labeling.
The existing aerial image datasets [3], [4], [5], [6], [7],
[8], [9], [10] generally have the problems of too little
data, few category numbers, single ground truth type, and Fig. 1: Illustration of VALID.
inaccurate labeling. Early datasets which designed for scene
classiﬁcationandobjectdetectiontaskssuchasUCM-21[3],
AID [5], NWPU-RESISC45 [6] and UCAS-AOD [4] have datasets under aerial scenes such as DeepGlobe [9], Inria
only approximately several thousand images or instances. 2Dsemantic[7]andISPRS2Dsemantic[11]haveveryfew
Recent large-scale aerial image datasets such as DOTA [8] segmentation categories. More importantly, these mentioned
and xView [10] focus on object recognition task, with- datasets are all labeled manually, which is laborious and
out pixel-level annotation. In addition, other segmentation leads to human error. In general, it is essential to efﬁciently
and accurately collect a comprehensive large-scale aerial
1The authors are with the Department of Computer Science
image dataset which can meet the demand of all kinds of
and Technology, State Key Laboratory of Intelligent Technology
and Systems, Beijing National Research Center for Information Sci- major visual tasks.
ence and Technology, Tsinghua University, Beijing, 100084 (e-mail: Due to the ability to automatically collect data and gen-
jimchen0605@gmail.com).
erate annotation in diversiﬁed environments, virtual datasets
2Jihong Zhu is with the Department of Precision Instrument, Tsinghua
University,Beijing,100084(e-mail:jhzhu@tsinghua.edu.cn). become the most feasible way to provide large amounts of
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 2009
Authorized licensed use limited to: Carleton University. Downloaded on September 22,2020 at 01:41:39 UTC from IEEE Xplore.  Restrictions apply. altitude images captured from satellites and low-altitude
images collected from planes. Among them, there are three
common types of ground truth data, i.e., classiﬁcation label,
detection bounding box, and segmentation maps.
For high-altitude situations, early aerial datasets such as
UCM-21 [3], AID [5] and NWPU-RESISC45 [6] provide
the classiﬁcation label of aerial scenes or single object with
clear background. Although only a little effort is needed
to label the entire image and the annotation categories are
abundant, these datasets have limited help for real UAV
tasks. Some large-scale useful aerial image datasets for
object detection task such as DOTA [8], xView [10] and
visDrone [24] have just appeared in the past two years due
to the relatively high difﬁculty for labeling. The number
of instances in these datasets can reach up to millions.
Each image has more than thousands of pixels in width and
height, containing a large number of object instances. The
annotationofaerialsegmentationdatasets[11],[7],[9],[25]
is the most difﬁcult. Since most of the objects are too tiny
to distinguish manually, only buildings, roads, and terrain
surface types can be generally labeled. What’s more, most
Fig.2:MoresamplesofVALID.Fortheannotationofobject of the annotation of them is completely pixel-level ﬁne-
detection, blue color horizontal bounding box and red color grained because polygons are used at the edges of objects
oriented bounding box of each instance are both displayed. foreaseoflabeling.Incomparison,low-altitudeaerialimage
datasets [26], [27], [28], [29], [30] pay more attention to the
behaviorofpeopleandvehiclesfortaskssuchassurveillance
data. Synthetic data has been widely used in recent years and tracking. In recent months, a small low-altitude aerial
within the computer vision and robotics community. For image dataset [31] has been published which focuses on the
instance, [12], [13], [14] for autonomous driving, [15], [16] semantic understanding of urban scenes, consisting of 400
for indoor simulation, [17] for UAV obstacle avoidance and high-resolution images over 20 semantic classes.
[18] for human motions. Based on these datasets, a lot of Ingeneral,theexistingdatasetsupportsonlyasingletype
works [19], [20], [21], [22], [17] use domain adaptation ofannotation,whichisdifﬁculttomeetthediversedemands
to work on real images when trained in virtual environ- of different tasks. What is needed now is a comprehensive
ments. At present, there is no synthetic dataset speciﬁcally datasetthatcancaptureallkindsofdataatmultiplealtitudes.
designed for aerial scenes. To this end, inspired by a recent In this paper, we take advantage of automatically collecting
research [17] that uses Airsim [23] simulator to construct data with the corresponding annotation in the virtual envi-
a dense forest trail dataset in Unreal Engine 4 (UE4), we ronment to construct our aerial image dataset.
build a variety of realistic environments in UE4 and create
B. Virtual image dataset
a virtual aerial benchmark suite to enable access to more
abundantannotationthatprovidedforhigh-levelUAVvisual Althoughthereiscurrentlynovirtualdatasetdesignedfor
tasks. Finally, we collect a Virtual AeriaL Image Dataset aerialscenes,syntheticbenchmarkshaveplayedanimportant
(VALID) which contains 6,690 realistic synthetic images role in many other areas. In the ﬁeld of unmanned vehi-
with a resolution of 1024×1024, all annotated with ground cles, [12] uses the real-to-virtual world cloning method to
truthdata,includingpanopticsegmentation,detectionbound- generate the Virtual KITTI dataset for multi-object tracking.
ing box, and binocular depth maps, as illustrated in Fig. 1. [13] releases an image collection of urban scenes named
VALID provides more accurate and ﬁne-grained annotation SYNTHIA for semantic segmentation task. [14] presents
thanexistingmanuallylabeleddatasets.Toprovidereference a virtual benchmark suite with diverse annotated ground
baselines, we evaluate the performance of state-of-the-art truth data for both low-level and high-level vision tasks.
models for semantic segmentation, instance segmentation, [17] collects a virtual forest trail dataset to train the UAVs
object detection and monocular depth prediction on the to ﬂy a collision-free trajectory. In addition, many other
presented benchmark. The results indicate that our dataset virtual datasets have been used for various ﬁelds, including
is quite challenging. SUNCG[20],MINOS[15],SceneNet[32]forindoorscenes,
PHAV [33], SURREAL [18] for synthetic humans.
II. RELATEDWORK
Virtual datasets are suitable for the situation where real
A. Aerial image dataset image annotation is hard to acquire. Since we have full
Asfarasweknow,almostallexistingaerialimagedatasets control of synthetic objects in virtual environments, it is
are collected in real-world environments, including high- feasible to collect various types of ground truth data.
2010
Authorized licensed use limited to: Carleton University. Downloaded on September 22,2020 at 01:41:39 UTC from IEEE Xplore.  Restrictions apply. TABLE I: Category of VALID.
III. COLLECTIONOFVALID
A. Scene construction
Main Detailed Main Detailed
Type Type
It is considerably difﬁcult and expensive with known category category category category
techniques to collect a large-scale aerial image dataset with tree tree stuff animal animal thing
plant plant stuff person person thing
various ground truth data in the real world. So we instead
road road stuff chair thing
collect data in the virtual environment. In order to offer
pavement pavement stuff low fence stuff
the most visually realistic simulations, we build a variety land land stuff obstacle garbagebin thing
of conditions and environments in Unreal Engine 4 to re- water stuff lowobstacle stuff
water
pool thing telegraphpole stuff
constructreal-worldscenes.Unliketheurbanscenesthatare
ice ice stuff trafﬁclight thing
widelyusedinunmannedvehiclesvirtualdatasets,theaerial high
stone stuff busstop thing
rock obstacle
image datasets cover a wide geographical range. To increase pierrubble stuff lamp thing
the diversity of data, we select six typical environments, as bridge bridge thing highobstacle stuff
sign sign thing tunnel tunnel thing
shown in Fig. 1, including crowded airports, downtown with
smallvehicle thing ship ship thing
skyscrapers, the neighborhood with villas, European-style vehicle
largevehicle thing plane plane thing
streets, seaside hilly town, and primitive snow mountains. building building thing harbor harbor thing
The weather and lighting conditions in the scenery cover
sunny, dusk, night, snow and fog. More than that, there
are thousands of pre-made environments in the marketplace
quadrotor drone to ﬂy at several different altitudes. During
of Unreal Engine and third-party websites. Therefore, it is
the entire ﬂight, the drone remains parallel to the ground,
convenient and efﬁcient to build the required aerial scene
using a down-headed binocular camera to simultaneously
according to the given task.
acquire original images and corresponding annotation. The
B. Category selection annotation methodology of each type of ground truth is
Due to the variety of scenes, VALID contains many described below.
different kinds of objects. We set up a two-level hierarchical Segmentation Our segmentation method is a sort of
category rule. As shown in Tab. I, VALID consists of 20 image collection process. The simulator simultaneously cap-
maincategories,i.e.,tree,plant,road,pavement,land,water, tures the original RGB image and the corresponding seg-
ice, rock, bridge, sign, vehicle, building, animal, person, mentation image from two separate views, one of which
low obstacle, high obstacle, tunnel, ship, plane and harbor. rendersanenvironmentwithrealisticappearanceandlighting
Among them, we have reﬁned several categories into detail. while the other one assigns the unique pre-allocated colors
Water is further divided into natural water and swimming to each mesh available in the environment according to
pools.Forrock,wedistinguishbetweenthestoneontheland its name. However, a “thing” instance that has complete
and the rubble near the port. Vehicle is divided into small semantic information (e.g., house) is usually composed of
vehicle and large vehicle according to size. For the obstacle several kinds of meshes (e.g., door, window, wall, roof,
category, chair, fence, and garbage bin are listed separately etc). In order to render every “thing” instance has the same
from low obstacle as detailed categories. Also, we separate unique color, we rename the meshes of each instance to
thetelegraphpole,trafﬁclight,busstopandlampfromhigh havethesamepreﬁxandthenperformtheregularexpression
obstacle. Finally, 30 detailed categories are formed. In order matching during the color allocation process, thus obtaining
toachievepanopticsegmentation,17ofthemarerecognized both semantic and instance-level segmentation annotations.
as “thing”, and the rest 13 are classiﬁed as “stuff”. Instance Naturally, by combining these two, panoptic segmentation
segmentationandobjectdetectiontasksarebasedon“thing” ground truth data is obtained.
categories while semantic and panoptic segmentation tasks Detection For a virtual scene, each instance has been
use all 30 detailed categories. assigned a unique color. In other words, given an RGB
image, we can ﬁnd the area of all instances based on
C. Image collection and annotation
the panoptic segmentation map. Then we can calculate the
Afterdeterminingthevirtualscenesandobjectcategories, minimum horizontal bounding box (HBB) and minimum
the next step is to collect and label the data. Our approach oriented bounding box (OBB) using methods provided by
is inspired by recent research [17] that uses Airsim [23] many third-party open-source image processing libraries.
simulator to construct a dense forest trail dataset in UE4. Depth The acquisition of depth value is relatively sim-
However,theoriginalannotationtechniquespresentedinthis ple. Since the coordinates of each object in the virtual
prior work can not meet our demand: original segmentation environment can be accessed, the depth is calculated by
labeling is based on object name, which can not provide direct conversion of the difference of the coordinates. In our
the correct semantic correspondence, and fails to distinguish paper, we collect the depth ground truth by Airsim’s own
between different “thing” instances of the same category. tool. Note that VALID uses a perspective strategy for depth
To this end, we implement an advanced version of Airsim, calculation, which means the depth value is the distance
whichenablesthecollectionofpixel-levelgroundtruthdata. difference between the object and the focus of the camera.
Speciﬁcally, with the Airsim simulator, we can operate a The ﬁeld of view (FOV) of two cameras in both horizontal
2011
Authorized licensed use limited to: Carleton University. Downloaded on September 22,2020 at 01:41:39 UTC from IEEE Xplore.  Restrictions apply. TABLE II: Comparison among VALID and other datasets.
Dataset Annotation #cat #images Imagewidth
Inria2Dsemantic[7] Semantic 1 180 5000
ISPRS2Dsemantic[11] Semantic 6 33+38 2000∼6000
DeepGlobebuilding[9] Semantic 1 24586 650
DeepGlobelandcover[9] Semantic 6 1146 2448
SemanticDrone[31] Semantic 20 400 6000
isAID[25] Instance 15 2806 800∼13000
VALID-Seg Panoptic 30 6690 1024
(a) Comparison among VALID-Seg and previous aerial image
datasets for segmentation task.
Dataset Annotation #cat #instances #images Imagewidth
NWPUVHR-10[34] HBB 10 3651 800 ∼1000
SZTAKI-INRIA[35] OBB 1 665 9 ∼800
HRSC-2016[36] OBB 1 2976 1061 ∼1100
UCAS-AOD[4] OBB 2 14596 1510 ∼1000
DOTA[8] OBB 14 188282 2806 800∼4000
VALID-Det OBB 17 144164 6467 1024
(b)ComparisonamongVALID-Detandotheraerialimagedatasets
for object detection task.
Fig.3:(a)Instancenumberpercategory.(b),(c)Aspectratio
Dataset Scene #cam Annotation Maxdepth #images
of HBB and OBB for instances in VALID-Det. (d) Depth
NYUDepthV2[37] indoor 1 dense - 1449
KITTI[38] outdoor 2 sparse ∼80m 42382 distribution of VALID-Depth. The distribution of images
ScanNet[39] indoor 1 dense - 2.5million collected in different altitudes is marked in different color.
VALID-Depth aerial 2 pixellevel ∼300m 6690
(c) Comparison among VALID-Depth and other general depth
prediction datasets.
TABLE III: Instance size distribution of different datasets.
Dataset 0-10px 10-50px 50-300px above300px
PASCALVOC[40] - 0.14 0.61 0.25
MSCOCO[2] - 0.43 0.49 0.08
NWPUVHR-10[34] - 0.15 0.83 0.02
DOTA[8] - 0.57 0.41 0.02
VALID-Det 0.25 0.42 0.29 0.04
Fig. 4: Area size of 30 detailed categories.
and vertical directions is 90 degrees (focal length is half of
the image size) and the baseline distance is 25 centimeters.
However, although the image resolution can be up to 4K,
D. Dataset splits
the spatial resolution is only about 30 centimeters, which
In order to ensure that the data distribution of each
is hard to recognize small objects like the person and road
category is roughly the same, we follow [8] by randomly
infrastructure. For low-altitude aerial image dataset such as
assigning1/2ofthedataasthetrainingset,1/6asvalidation
SemanticDrone [31]whichatanaltitudeof5to30meters,
setand1/3asthetestingset.Asubsetofdifferentaltitudesin
even if the resolution is as high as 6000×4000, the object
each scene satisfy the proportion of this allocation as well.
information it contains is actually limited. What’s more,
For object detection and instance segmentation tasks, it is
these ultra-large images will be sliced or resized during the
worth noting that some low-altitude images may not include
algorithm’s training. So this so-called large size and high
detection instances. So we only split the images that contain
resolution are not practical. To be able to contain more
instances as the aforesaid way.
information while ensuring the clearness, we have captured
IV. PROPERTIESOFVALID the images at a resolution of 1024×1024 at three different
altitudes (20m, 50m, 100m).
We collect a total of 6,690 images captured in six virtual
scenes with ﬁve different ambient conditions: sunny, dusk,
A. VALID-Seg
night, snow and fog. Each image in VALID is annotated
withpixel-levelpanopticsegmentation,objectdetection,and For most existing aerial datasets that designed for seg-
binoculardepthmaps.Hence,wecanseparateintothreesub- mentation task, too few class remains a signiﬁcant problem,
datasets, i.e., VALID-Seg, VALID-Det, and VALID-Depth. as shown in Tab. IIa. In contrast, the VALID dataset labels
Image size Aerial images are usually very large in size up to 30 categories. Some relatively more diverse datasets
becausetheoriginaldataaremostlyacquiredfromsatellites. with more than 5 categories such as ISPRS Vaihingen &
2012
Authorized licensed use limited to: Carleton University. Downloaded on September 22,2020 at 01:41:39 UTC from IEEE Xplore.  Restrictions apply. TABLE IV: Evaluation of baseline models on VALID
InstancepercategoryInFig.3,wecalculatetheinstance
Method meanIoU PixelAcc. number per category in VALID-Det. The most common
FCN[41] 21.56 73.97 categories are small vehicle and building, which is similar
PSPNet[42] 53.76 92.70
to real-world scenes.
(a) Evaluation of baseline models on semantic segmentation. Instance size of categories According to the convention
in [47], instance size can be measured by the height of the
Method AP AP AP APbb APbb APbb
50 75 50 75 horizontal bounding box and categorized as follows: small
MaskR-CNN[43] 28.92 52.47 28.59 47.38 72.49 51.90
for range from 10px to 50px, middle for range from 50px
(b) Evaluation of baseline models on instance segmentation.
to 300px, and large for range above 300px. In VALID-Det
Method RMSE rel log10 δ<1.25 δ<1.252 δ<1.253 dataset,wedividealltheinstancesintofoursplitsbyadding
FCRN[44] 13.10 0.2180 0.0912 0.6368 0.9261 0.9726 an ultra-small class for the range below 10px. Instances of
(c) Baseline results of monocular depth prediction. thedifferentcategorieshavevarioussizes,andeveninstances
of the same category have different sizes due to multiple
YOLOv3[45] FasterR-CNN[46]
ﬂight altitudes. Tab. III illustrates the percentage of instance
pool 90.91 90.88
bridge 90.80 90.91 splitsindifferentdatasets.Becausemanyselectedcategories
sign 29.68 14.45 are common objects in life, VALID-Det is dominated by
smallvehicle 90.64 49.26
small instances. In particular, about 25% instances are ultra-
largevehicle 87.90 57.34
building 90.67 79.59 small size, which makes the dataset more challenging and
animal 13.71 9.09 puts higher requirements on the detection algorithms for
person 33.99 9.09
ﬂexibly handling extremely tiny and large objects.
chair 24.41 9.09
garbagebin 26.49 9.09 Aspect ratio of instances The aspect ratio (AR) greatly
trafﬁclight 26.50 9.09 affects the performance of the anchor-based detection algo-
busstop 77.25 41.59
rithms such as Faster R-CNN [46] and YOLOv3 [45]. In
lamp 67.01 41.92
tunnel 81.49 77.92 Fig. 3, we illustrate the distribution of AR for both HBB
ship 90.89 82.05 and OBB for instances in VALID-Det, providing a reference
plane 88.94 90.91
for better model design. It is observed that instances vary
harbor 88.60 90.07
mAP 64.70 50.14 greatly in aspect ratio.
(d) Average per-category and global accuracy of baseline models C. VALID-Depth
evaluated with HBB ground truth.
Existing aerial image datasets do not provide depth infor-
mation or so-called altitude information because no sensor
can provide a dense depth map at such a high altitude.
Potsdam [11], DeepGlobe land cover [9] and Semantic However, altitude information is of great signiﬁcance for
Drone [31] have much fewer images than VALID-Seg. recognizing the terrain and guiding the mission of UAVs
Per-category areas The area of different categories di- such as landing. VALID-Depth provides a binocular depth
rectlyaffectstheperformanceofbeingaccuratelysegmented mapwhichcansupportmostofmainstreamdepthprediction
in the algorithm. In Fig. 4, we count the per-category areas algorithms. Compared with some depth datasets [37], [38],
in VALID-Seg. The categories with the largest area are [39] in other areas, as shown in Tab. IIc, VALID-Depth has
water and land due to their prevalence in all virtual scenes. awiderdepthrangeandmorepreciseannotation.Itisworth
However, some of the most commonly appeared categories noting that the depth maps in VALID are completely pixel-
such as person and chair are still small in area size, which level dense, much more accurate than NYU Depth V2 [37]
makes them hard to distinguish. and KITTI [38] which use interpolation methods to fulﬁll
AnnotationaccuracyComparedtotheexistingaerialseg- the depth maps.
mentation datasets [11], [7], [9], [31] that use polygons for Depth distribution Because most of the objects in aerial
labeling, the annotation of VALID-Seg is completely pixel- images are far away from the onboard cameras, the depth
level and ﬁne-grained. As shown in Fig. 1, it is especially values are generally distributed around the three altitudes
obvious for objects with particularly irregular edges such as of our ﬂight, illustrated in Fig. 3. Also, when the altitude is
trees and people. More importantly, VALID-Seg is the ﬁrst higher,thespatialareacoverageofthecamerawillbelarger,
aerialimagedatasetthatprovidespanoptic-levelgroundtruth leading to shorter ﬂight time and fewer data.
data.
V. EVALUATIONS
B. VALID-Det
In this section, we present the evaluation of state-of-the-
As mentioned before, some low-altitude images may not art methods on VALID for semantic segmentation, instance
includedetectioninstances.VALID-Dethas6467frames,all segmentation, object detection, and monocular depth predic-
ofwhichannotatedwithhorizontalboundingbox(HBB)and tion. Our primary goal is to assess the challenge of VALID
oriented bounding box (OBB). Comparison with previous and provide reference baselines for these high-level vision
datasets is listed in Tab. IIb. tasks. Among them, models are trained on the training and
2013
Authorized licensed use limited to: Carleton University. Downloaded on September 22,2020 at 01:41:39 UTC from IEEE Xplore.  Restrictions apply. validation set while evaluated on the testing set. Due to the
GPU memory limitation, the resolution of images in VALID
in the following experiments is resized to half its original
size (512×512).
A. Semantic segmentation
For semantic segmentation, we carefully select FCN-8s
[41] and Pyramid Scene Parsing Network (PSPNet) [42] as
our benchmark testing algorithms due to their good perfor-
manceandpopularity.Detailednetworkdesignisthesameas
those provided in the original paper. The backbones of two Fig.5:VisualizationresultscomparisonbetweenFCN-8sand
models are VGG16 [48] and dilated ResNet-18 [49], [50] PSPNetonsemanticsegmentationbaseline.Fromlefttoright
respectively. For measurement, pixel-wise accuracy (Pixel are RGB image, ground truth, FCN-8s result, and PSPNet
Acc.) and mean intersection over union (Mean IoU) are result respectively.
used. As illustrated in Tab. IVa and Fig. 5, both numerical
and qualitative results indicate that PSPNet outperforms
FCN-8s by a large margin. However, PSPNet still cannot
accuratelyidentifyultrasmallsizecategoriessuchasperson
and animal.
B. Instance segmentation
Forinstancesegmentation,webenchmarktheperformance
of Mask R-CNN [43] since it is a landmark work in this
area. As mentioned before, only 6467 valid images are used
for this task, where 4311 for training and 2156 for testing.
Network design and parameter settings are referenced to the
original paper where the backbone is ResNet-50 [50] and
FPN [51]. To measure the instance segmentation accuracy,
we compute the mask-level and bounding box-level average Fig. 6: Visualization results of baseline experiments on
precision (AP) among all categories. The numerical results VALID-Det. Top and bottom illustrate the results for Faster
are presented in Tab. IVb. R-CNN and YOLOv3.
C. Object detection
As for the object detection task, we perform the experi- network design and parameter settings of the original paper,
mentsonaone-stagedetectorYOLOv3 [45]andatwo-stage except for the resolution of the image is further resized to
detector Faster R-CNN [46] respectively. Here we evaluate 256×256. In Tab. IVc, the commonly used metrics in this
thebaselineperformanceofdetectorsonhorizontalbounding area are presented, i.e., root mean squared error (RMSE),
box ground truth. We use the same dataset splits as that for absolutemeanrelativeerror(rel),averagelog RMSEerror
10
instance segmentation task. The backbone of Faster R-CNN (log10) and threshold accuracy (δ <1.25, δ <1.252, δ <
and YOLOv3 is ResNet-101 [50] and darknet-53 [52]. We 1.253)
set the initial learning rate to 0.001 and decrease 10 times
VI. CONCLUSIONS
when the training loss is relatively stable. During the test
phase, we keep the threshold of non-maximum suppression In this paper, we collect a comprehensive aerial image
(NMS) as 0.25 for both methods. we report the average per- dataset named VALID in the virtual environment to provide
categoryandglobalaccuracyfor17categoriesoftwomodels a variety of ground truth data. Compared with previous
in Tab. IVd. It is observed that YOLOv3 performs much aerial datasets that have single annotation type and less
better on small size categories (animal, person, lamp, small data, VALID has some compelling characteristics, includ-
vehicle) and large aspect ratio categories (large vehicle). ing various realistic environments, diverse highly accurate
More visualization results are presented in Fig. 6. annotation and rapid data extensibility. In addition, VALID
is the ﬁrst aerial image dataset that offers completely ﬁne-
D. Monocular depth prediction
grained panoptic segmentation and dense binocular depth
Although binocular depth maps are provided in VALID- maps, which provides the most important foundation for the
Depth,weonlyevaluatethebaselineperformanceofmonoc- research of many visual algorithms and aerial applications.
ular depth prediction since it is a more challenging problem. For the future work, we will enrich the data of VALID
We select FCRN [44] as the evaluation model of which in different virtual scenes to adapt it to various speciﬁc
backbone is ResNet-50 [50]. The dataset splits are the same tasks,includingUAVlanding,groundtargetattack,androad
as that for semantic segmentation task. We refer to the detection.
2014
Authorized licensed use limited to: Carleton University. Downloaded on September 22,2020 at 01:41:39 UTC from IEEE Xplore.  Restrictions apply. REFERENCES [22] X. Guo, H. Li, S. Yi, J. Ren, and X. Wang, “Learning monocular
depth by distilling cross-domain stereo networks,” in Proceedings of
[1] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei,
theEuropeanConferenceonComputerVision(ECCV),2018,pp.484–
“Imagenet:Alarge-scalehierarchicalimagedatabase,”2009.
500.
[2] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
[23] S. Shah, D. Dey, C. Lovett, and A. Kapoor, “Airsim: High-ﬁdelity
P. Dolla´r, and C. L. Zitnick, “Microsoft coco: Common objects in
visualandphysicalsimulationforautonomousvehicles,”inFieldand
context,”inEuropeanconferenceoncomputervision. Springer,2014,
servicerobotics. Springer,2018,pp.621–635.
pp.740–755.
[24] P. Zhu, L. Wen, D. Du, X. Bian, H. Ling, Q. Hu, H. Wu, Q. Nie,
[3] Y.YangandS.Newsam,“Bag-of-visual-wordsandspatialextensions
H.Cheng,C.Liu,etal.,“Visdrone-vdt2018:Thevisionmeetsdrone
for land-use classiﬁcation,” in Proceedings of the 18th SIGSPATIAL
videodetectionandtrackingchallengeresults,”inProceedingsofthe
internationalconferenceonadvancesingeographicinformationsys-
EuropeanConferenceonComputerVision(ECCV),2018,pp.0–0.
tems. ACM,2010,pp.270–279.
[25] S. Waqas Zamir, A. Arora, A. Gupta, S. Khan, G. Sun, F. Shah-
[4] H. Zhu, X. Chen, W. Dai, K. Fu, Q. Ye, and J. Jiao, “Orientation
bazKhan,F.Zhu,L.Shao,G.-S.Xia,andX.Bai,“isaid:Alarge-scale
robust object detection in aerial images using deep convolutional
datasetforinstancesegmentationinaerialimages,”inProceedingsof
neural network,” in 2015 IEEE International Conference on Image
the IEEE Conference on Computer Vision and Pattern Recognition
Processing(ICIP). IEEE,2015,pp.3735–3739.
Workshops,2019,pp.28–37.
[5] G.-S.Xia,J.Hu,F.Hu,B.Shi,X.Bai,Y.Zhong,L.Zhang,andX.Lu,
[26] M. Bonetto, P. Korshunov, G. Ramponi, and T. Ebrahimi, “Privacy
“Aid:Abenchmarkdatasetforperformanceevaluationofaerialscene
in mini-drone based video surveillance,” in 2015 11th IEEE Inter-
classiﬁcation,”IEEETransactionsonGeoscienceandRemoteSensing,
national Conference and Workshops on Automatic Face and Gesture
vol.55,no.7,pp.3965–3981,2017.
Recognition(FG),vol.4. IEEE,2015,pp.1–6.
[6] G. Cheng, J. Han, and X. Lu, “Remote sensing image scene classi-
[27] A. Robicquet, A. Sadeghian, A. Alahi, and S. Savarese, “Learning
ﬁcation: Benchmark and state of the art,” Proceedings of the IEEE,
socialetiquette:Humantrajectoryunderstandingincrowdedscenes,”
vol.105,no.10,pp.1865–1883,2017.
in European conference on computer vision. Springer, 2016, pp.
[7] E.Maggiori,Y.Tarabalka,G.Charpiat,andP.Alliez,“Cansemantic
549–565.
labelingmethodsgeneralizetoanycity?theinriaaerialimagelabeling
[28] M. Mueller, N. Smith, and B. Ghanem, “A benchmark and simula-
benchmark,” in 2017 IEEE International Geoscience and Remote
tor for uav tracking,” in European conference on computer vision.
SensingSymposium(IGARSS). IEEE,2017,pp.3226–3229.
Springer,2016,pp.445–461.
[8] G.-S. Xia, X. Bai, J. Ding, Z. Zhu, S. Belongie, J. Luo, M. Datcu,
[29] S. Li and D.-Y. Yeung, “Visual object tracking for unmanned aerial
M. Pelillo, and L. Zhang, “Dota: A large-scale dataset for object
vehicles:Abenchmarkandnewmotionmodels,”inAAAI,2017.
detection in aerial images,” in Proceedings of the IEEE Conference
[30] P.Zhu,L.Wen,X.Bian,L.Haibin,andQ.Hu,“Visionmeetsdrones:
onComputerVisionandPatternRecognition,2018,pp.3974–3983.
Achallenge,”arXivpreprintarXiv:1804.07437,2018.
[9] I.Demir,K.Koperski,D.Lindenbaum,G.Pang,J.Huang,S.Basu,
[31] “Semantic drone dataset,” http://dronedataset.icg.tugraz.at/, accessed
F. Hughes, D. Tuia, and R. Raska, “Deepglobe 2018: A challenge
Jan25,2019.
to parse the earth through satellite images,” in 2018 IEEE/CVF
[32] J.McCormac,A.Handa,S.Leutenegger,andA.J.Davison,“Scenenet
Conference on Computer Vision and Pattern Recognition Workshops
rgb-d: Can 5m synthetic images beat generic imagenet pre-training
(CVPRW). IEEE,2018,pp.172–17209.
on indoor segmentation?” in Proceedings of the IEEE International
[10] D. Lam, R. Kuzma, K. McGee, S. Dooley, M. Laielli, M. Klaric,
ConferenceonComputerVision,2017,pp.2678–2687.
Y. Bulatov, and B. McCord, “xview: Objects in context in overhead
imagery,”arXivpreprintarXiv:1802.07856,2018. [33] C.R.deSouza12,A.Gaidon,Y.Cabon,andA.M.Lo´pez,“Procedural
[11] “Isprs 2d semantic labeling dataset,” http://www2.isprs.org/ generationofvideostotraindeepactionrecognitionnetworks,”2017.
commissions/comm3/wg4/semantic-labeling.html. [34] G.Cheng,P.Zhou,andJ.Han,“Learningrotation-invariantconvolu-
[12] A. Gaidon, Q. Wang, Y. Cabon, and E. Vig, “Virtual worlds as tionalneuralnetworksforobjectdetectioninvhropticalremotesens-
proxyformulti-objecttrackinganalysis,”inProceedingsoftheIEEE ingimages,”IEEETransactionsonGeoscienceandRemoteSensing,
conference on computer vision and pattern recognition, 2016, pp. vol.54,no.12,pp.7405–7415,2016.
4340–4349. [35] C. Benedek, X. Descombes, and J. Zerubia, “Building development
[13] G.Ros,L.Sellart,J.Materzynska,D.Vazquez,andA.M.Lopez,“The monitoringinmultitemporalremotelysensedimagepairswithstochas-
synthia dataset: A large collection of synthetic images for semantic ticbirth-deathdynamics,”IEEETransactionsonPatternAnalysisand
segmentationofurbanscenes,”inProceedingsoftheIEEEconference MachineIntelligence,vol.34,no.1,pp.33–50,2012.
oncomputervisionandpatternrecognition,2016,pp.3234–3243. [36] Z.Liu,H.Wang,L.Weng,andY.Yang,“Shiprotatedboundingbox
[14] S. R. Richter, Z. Hayder, and V. Koltun, “Playing for benchmarks,” spaceforshipextractionfromhigh-resolutionopticalsatelliteimages
in Proceedings of the IEEE International Conference on Computer with complex backgrounds,” IEEE Geoscience and Remote Sensing
Vision,2017,pp.2213–2222. Letters,vol.13,no.8,pp.1074–1078,2016.
[15] M.Savva,A.X.Chang,A.Dosovitskiy,T.Funkhouser,andV.Koltun, [37] P. K. Nathan Silberman, Derek Hoiem and R. Fergus, “Indoor seg-
“Minos: Multimodal indoor simulator for navigation in complex mentationandsupportinferencefromrgbdimages,”inECCV,2012.
environments,”arXivpreprintarXiv:1712.03931,2017. [38] A.Geiger,P.Lenz,C.Stiller,andR.Urtasun,“Visionmeetsrobotics:
[16] S.Song,F.Yu,A.Zeng,A.X.Chang,M.Savva,andT.Funkhouser, Thekittidataset,”InternationalJournalofRoboticsResearch(IJRR),
“Semantic scene completion from a single depth image,” in Pro- 2013.
ceedings of the IEEE Conference on Computer Vision and Pattern [39] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and
Recognition,2017,pp.1746–1754. M.Nießner,“Scannet:Richly-annotated3dreconstructionsofindoor
[17] L.Chen,W.Wang,andJ.Zhu,“Learningtransferableuavforforest scenes,” in Proc. Computer Vision and Pattern Recognition (CVPR),
visual perception,” in Proceedings of the 27th International Joint IEEE,2017.
ConferenceonArtiﬁcialIntelligence. AAAIPress,2018,pp.4883– [40] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and
4889. A. Zisserman, “The pascal visual object classes (voc) challenge,”
[18] G.Varol,J.Romero,X.Martin,N.Mahmood,M.J.Black,I.Laptev, International Journal of Computer Vision, vol. 88, no. 2, pp. 303–
andC.Schmid,“Learningfromsynthetichumans,”inProceedingsof 338,June2010.
the IEEE Conference on Computer Vision and Pattern Recognition, [41] J.Long,E.Shelhamer,andT.Darrell,“Fullyconvolutionalnetworks
2017,pp.109–117. forsemanticsegmentation,”IEEETransactionsonPatternAnalysis&
[19] D. Vazquez, A. M. Lopez, J. Marin, D. Ponsa, and D. Geronimo, MachineIntelligence,vol.39,no.4,pp.640–651,2014.
“Virtual and real world adaptation for pedestrian detection,” IEEE [42] H.Zhao,J.Shi,X.Qi,X.Wang,andJ.Jia,“Pyramidsceneparsing
transactions on pattern analysis and machine intelligence, vol. 36, network,”inProceedingsofIEEEConferenceonComputerVisionand
no.4,pp.797–809,2014. PatternRecognition(CVPR),2017.
[20] B. Sun and K. Saenko, “From virtual to reality: Fast adaptation of [43] W. Abdulla, “Mask r-cnn for object detection and instance segmen-
virtual object detectors to real domains.” in BMVC, vol. 1, no. 2, tation on keras and tensorﬂow,” https://github.com/matterport/Mask
2014,p.3. RCNN,2017.
[21] P. P. Busto, J. Liebelt, and J. Gall, “Adaptation of synthetic data for [44] I. Laina, C. Rupprecht, V. Belagiannis, F. Tombari, and N. Navab,
coarse-to-ﬁneviewpointreﬁnement.”inBMVC,2015,pp.14–1. “Deeperdepthpredictionwithfullyconvolutionalresidualnetworks,”
2015
Authorized licensed use limited to: Carleton University. Downloaded on September 22,2020 at 01:41:39 UTC from IEEE Xplore.  Restrictions apply. in2016FourthInternationalConferenceon3DVision(3DV). IEEE,
2016,pp.239–248.
[45] J. Redmon and A. Farhadi, “Yolov3: An incremental improvement,”
2018.
[46] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: towards real-
timeobjectdetectionwithregionproposalnetworks,”inInternational
ConferenceonNeuralInformationProcessingSystems,2015.
[47] S.Yang,P.Luo,C.-C.Loy,andX.Tang,“Widerface:Afacedetection
benchmark,” in Proceedings of the IEEE conference on computer
visionandpatternrecognition,2016,pp.5525–5533.
[48] K. Simonyan and A. Zisserman, “Very deep convolutional networks
forlarge-scaleimagerecognition,”CoRR,vol.abs/1409.1556,2014.
[49] L.-C.Chen,G.Papandreou,I.Kokkinos,K.Murphy,andA.L.Yuille,
“Semanticimagesegmentationwithdeepconvolutionalnetsandfully
connectedcrfs,”arXivpreprintarXiv:1412.7062,2014.
[50] K.He,X.Zhang,S.Ren,andJ.Sun,“Deepresiduallearningforimage
recognition,” in Proceedings of the IEEE conference on computer
visionandpatternrecognition,2016,pp.770–778.
[51] T.-Y.Lin,P.Dolla´r,R.Girshick,K.He,B.Hariharan,andS.Belongie,
“Featurepyramidnetworksforobjectdetection,”inProceedingsofthe
IEEEConferenceonComputerVisionandPatternRecognition,2017,
pp.2117–2125.
[52] J. Redmon, “Darknet: Open source neural networks in c,” http://
pjreddie.com/darknet/,2013–2016.
2016
Authorized licensed use limited to: Carleton University. Downloaded on September 22,2020 at 01:41:39 UTC from IEEE Xplore.  Restrictions apply. 