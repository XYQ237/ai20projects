2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
Self-Supervised Learning for Alignment of Objects and Sound
Xinzhu Liu, Xiaoyu Liu, Di Guo, Huaping Liu, Fuchun Sun and Haibo Min
Abstract—The sound source separation problem has many
useful applications in the ﬁeld of robotics, such as human-
robotinteraction,sceneunderstanding,etc.However,itremains
a very challenging problem. In this paper, we utilize both
visual and audio information of videos to perform the sound
source separation task. A self-supervised learning framework
is proposed to implement the object detection and sound sep-
aration modules simultaneously. Such an approach is designed
to better ﬁnd the alignment between the detected objects and
separated sound components. Our experiments, conducted on
boththesyntheticandrealdatasets,validatethisapproachand
demonstratetheeffectivenessoftheproposedmodelinthetask
of object and sound alignment.
Fig.1. Ourmodelsimultaneouslydetectsvisualobjectsandseparatesthe
I. INTRODUCTION soundofeachobjectfromthesoundofthevideo.Intheabovevideo,two
typesofvisualobjectsaredetectedandthesoundisdividedintotwoparts.
Most audio signals are mixtures of several audio sources Then, the objects and the sound are matched, so that the sound of each
(speech, music, noises). The task of sound source separation objectcanbeobtained.
requiresthesedifferentsourcestobeseparated.Ithasawide
rangeofapplicationsintheﬁeldofrobotics[1].Inahuman-
robot interaction scene, the robot needs to distinguish the input mixtures [15]. In another recent paper, permutation
humanvoicefromthebackgroundnoisetobetterunderstand invarianttrainingisproposedforspeaker-independentspeech
what the human is saying [2]. In a rescue situation, the separation [16]. However, all the above-proposed methods
robot needs to detect human voice from the mixed sound require restrictive assumptions and the performance is still
picked up by its sensors so as to better rescue people from not satisfying.
dangerous situations [3]. Sound separation is also useful in On the other hand, videos contain both visual images and
the tasks of speech recognition and dialogue management audio information at the same time. Since the visual and
[4],searchoftheclosestsoundsource[5]andauditoryscene audio information are complementary to each other, we can
understanding [6]. In addition, sound separation is a vital resort to both the visual and audio information contained in
component in sound source detection and localization tasks the video clip for sound separation task [17]. Ravulapalli
[7]–[9]. et al. [18] propose to separate multiple concurrent audio
Even though the study of sound source separation has and video events with a feature-based approach. To identify
a long and rich history, the variant of the problem with sources in video and separate them into different audios,
only audio modality available (an example being the famous a canonical correlation analysis method is employed for
“cocktail party problem” [10]) remains to this day a very audio-visual source separation [19]. Gao et al. utilize a deep
difﬁcult task [11]. Early literature in this ﬁeld began by multi-instance multi-label learning framework to separate
focusing on signal processing methods [12]. For instance, sound source in videos [20]. Ephrat et al. present a model
sparse coding methods are proposed to extract sources to isolate a single speech signal from a mixture of other
from real-world sound signals [13]. Non-negative matrix speakers’ sound and background noise [21]. At the same
factorization is a similarly popular signal processing method time, scholars also conduct research on the sound local-
[14]. More recently, some literature has started to provide a ization problem. In [22], a low-rank and sparsity method
supervised learning treatment to sound separation, applying is represented to localize visual objects associated with an
deep learning methods to this problem [12]. In [15], a audio source and separate the audio signal meanwhile. A
deep learning framework is proposed to assign contrastive deep visual-audio speech enhancement network is proposed
embedding vectors to regions of different time-frequency in to separate a speakers voice with the information of lip
a spectrogram to predict the target spectrogram from the regions in the corresponding video [23]. Zhao et al. present
the PixelPlayer, a system that learns to separate the sound
XZ.Liu,D.Guo,HP.LiuandFC.SunarewithBeijingNationalResearch
of each pixel from the input sounds [24]. Furthermore, they
Center for Information Science and Technology, Institute for Artiﬁcial
Intelligence,TsinghuaUniversity,andtheDepartmentofComputerScience modifythemodeltodoindependentimagesegmentationand
and Technology, Tsinghua University, Beijing, 100084, China. XY. Liu is sound source separation for input videos [25]. For learning
withSchoolofInformationandElectronics,BeijingInstituteofTechnology,
representation,Owensetal.proposeaself-supervisedneural
China. H. Min is with Beijing HiBingo Hi-Tech Co. Ltd. Corresponding
author:HuapingLiu(hpliu@tsinghua.edu.cn). network to learn aligned visual and audio representation,
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 1588
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 14:19:23 UTC from IEEE Xplore.  Restrictions apply. which is applied to the on/off-screen audio sound separation may be invisible in the image (e.g. wind). In this paper,
[26]. Some semantic information from vision is also very we try to recover these alignment relationship and use this
usefulforsoundseparation.Forexample,Gaoetal.propose relation to help separating the sound sources. Speciﬁcally,
O I
a model to generate the sound of instruments with object- thegoalistogeneratetheobjectssets ( ),thesoundsets
O S I
category labels [27]. The object detection results are used to ( ), and more importantly, to get the alignment sets
S
helpseparatetheon-screenandoff-screensounds[28],[29]. A V { | ⊆{ ··· }×{ ··· }}
( )= (O ,S )(n,k) 1,2, ,N 1,2, ,K ,
However,inreal-lifescenarioswhenworkingwithrobots, n k
the sound separation results in pixel-level is not sufﬁcient where the sound source S ∗ is emitted by the visual object
∈AkV
for applications, because robots cannot know what each O ∗, for (O ∗,S ∗) ( ).
n n k
pixel represents in the real environment. What are of greater To solve this problem from the viewpoint of machine
interest to robots are the actual objects and the sound learning, we collect a lot of unlabeled video clips which
generated by each object, not individual pixels. Therefore, contain objects of interest and their sounds, and construct a
we propose a self-supervised model to separate the sound self-supervised deep learning architecture to learn the intrin-
in object-level without any label information. In practice, if sic relationship between visual objects and sound sources. It
several objects are sounding simultaneously, as is shown in should be noted this work is novel compared with existing
Fig.1, the robot can detect every object in the environment work [24], [25], [20]. In fact, Refs. [24] and [25] study the
andseparatethesoundofeveryobjectfromthemixedsound. pixel-levelandsegmentation-levelsound,butdonotconsider
The main contributions are summarized as follows: the object-level alignment. The most relevant work is [20],
1) A novel object detection and sound separation frame- whichintroducesvideo-levelobjectpredictionandconstructs
work is established to perform object detection and a multi-instance multi-label network. In their work, sound
sound source separation tasks simultaneously. separation is performed using Non-negative Matrix Factor-
2) A self-supervised learning network is developed to ization (NMF), while our work recovers the audio-visual
build the association between detected objects and alignmentusingdeeplearningarchitecture,andleveragesthe
sound components to predict the emitted sound for advantages of visual object detection and sound separation.
every object.
III. NETWORKARCHITECTURE
3) Evaluative experiment is conducted to measure the
The model is composed of three main modules: visual
performance of the proposed model, with data from
object detection module, sound feature extraction module,
realistic scenes used for model validation.
and sound separation module, which is presented in Fig. 2.
This paper is organized as follows. In section II, we
The visual object detection module detects visual objects in
illustrate the problem of buliding the alignment between
the key-frame of every video, and extracts visual features of
visual objects and sound sources. In section III, the network
each detected object. The sound feature extraction module
structureofourmodelispresented.InsectionIV,wefurther
is utilized to extract features of the sound, and divide the
explain the process of object detection and sound separation
features into several components that may contain speciﬁc
with our model. Finally, experiments to evaluate our model
semantic meanings. The sound separation module integrates
are presented in section V.
the information obtained from visual object detector and
II. PROBLEMFORMULATION soundfeatureextractortogeneratethesoundofeachdetected
V I object.
Forashortvideoclip ,usuallyakey-frame issufﬁcient
to capture most of the visual objects. We therefore use A. Visual Object Detection
O I { ··· } Visual object detection module takes the key-frame as
( )= O ,O , ,O ,
I 1 2 N input, which is the mid-point frame of videos. The Faster-
torepresentthesetofthedetectedvisualobjectsinthevideo RCNN [30] network is utilized to detect objects and si-
V
. Please note that O represents the n-th detected object multaneously extract visual features. For each key-frame
n I
andN isthenumberofthedetectedobjectsinthekey-frame, , the detection boxes and visual features of each object
which is simply set as the middle one of the video frames. are recorded, and the set of visual features of objects is
S
For the video associated sound track , represented as
{ ··· }
O S { ··· } f1,f2, ,fN ,
( )= S ,S , ,S ,
S 1 2 K
where f is the visual feature of the n-th detected object
n
whereS representsthek-thsoundofanisolatedsourceand from the last feature layer of the Faster-RCNN network.
k
K is the number of the sound sources.
B. Sound Feature Extraction
In many cases, even if we can successfully detect the
S
objects in the visual images and separate the sound sources Asfortheinputsound ,theShort-TimeFourierTransfor-
for the concerned video, it is still very challenging to m (STFT) is used to convert the audio signal into the spec-
construct the alignment between visual objects and sound trogram. This spectrogram is resampled on a log-frequency
P
sources. The main reason is that not all of the visual objects scale to obtain a T-F spectrogram . Then, the U-Net [31]
emit sound (e.g., building), while the sound-emitting objects structure is applied to the spectrogram to extract the sound
1589
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 14:19:23 UTC from IEEE Xplore.  Restrictions apply. Fig.2. Thestructureoftheproposedmodel.
F
feature and divide the sound into K components, which is is deﬁned as a linear layer, which combines the K-
C
represented as the set dimensionalvisualfeaturevectorswithK soundcomponents
C { ··· } to perform the spectrogram masks prediction.
= a1,a2, ,aK . After separation, the mixed sound spectrogram and the
These K components can be regarded as the basic units of maskareusedtorecoverthesoundwaveformofeveryobject.
sound sources, and the feature of every sound source can Finally, the model can detect every object in the video, and
be obtained by weighting the feature of those components separate the sound of every object.
together. The components can have some semantic informa- D. Training Process
tion, so that combining different components in a speciﬁc
Infact,trainingdirectlyonindividualvideodataisdifﬁcult
way can get the sound of a speciﬁc object.
to implement, due to the much little extra information
C. Sound Separation Network obtained during the self-supervised learning process. For
those videos with only one object, the model cannot learn
The visual features of objects and sound components F
the separation function by training on individual videos
belong to different modalities, so it is difﬁcult to build
becausethereisnothingusefultolearnforsoundseparation.
connection directly between visual and auditory modalities.
For those videos with several objects, it is difﬁcult to obtain
In order to establish the association between two modalities,
precise ground truth of object sound from the video itself.
a two-layer convolution module is utilized to project the
To train the model more effectively, we generate training
visual features ﬁrst into the space, in which the mapped
samples manually. We mix the sound of two videos, and
features can be compared and processed jointly with sound
G formulate the objective of the training process to separate
components. The set of visual features after projection is
the sound of individual video from the mixed sound of two
represented as
videos.
G { ··· }
= g ,g , ,g , In the training process, in order to generate training
1 2 N V V
∈R samples, we select two videos (1), (2) from the training
where gn K is the n-th object’s mapped visual feature. dataset randomly. Then we mix the sound of each video to
Then, the sound separation network matches every object S
generate the mixed sound
with a speciﬁc sound source by processing the projected mix
S S ◦S
visual features and the sound components jointly. To make mix = (1) (2),
◦
the training process easier, the separation network is utilized where represents the mix operation of two sounds. The
S
topredictthe maskM forthe visual objectO ,whichcan set of sound component features of sound obtained
n n C mix
separatethesoundofanobjectfromthewholevideosound. by the sound feature extraction is . Given the key-
I I mix
The set of the mask for every video is obtained as frame (1), (2) of the two videos, the set of projected
G G
{ ··· } visual features are (1) and (2). Our model is trained
M ,M , ,M .
1 2 N to minimize the difference between predicted spectrogram
F C F
In fact, the separation network learns the function ( ,g ) masks Mask(1), Mask(2) obtained by function and real
n
topredictthemaskM .Intheproposedmodel,thefunction masks of the two videos.
n
1590
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 14:19:23 UTC from IEEE Xplore.  Restrictions apply. The ground truth of the spectrogram masks of the two
videos is deﬁned as
P P
(1) (2)
Gt(1) = P Gt(2) = P ,
mix mix
P P P
where (1), (2), represent the spectrogram of the
mix
input sound and the mixed sound respectively. The loss
function is deﬁned as the distance between predicted masks
and the ground truth masks,
Loss=Dis(Gt(1),Mask(1))+Dis(Gt(2),Mask(2)), Fig.3. Sampleframesfromtheselecteddataset.
where Dis is the pixelwise distance between the separated
spectrogram masks and its corresponding ground truth. The
achieve, we choose 8 categories, containing Male Speech,
distance needs to be as small as possible.
Female Speech, Baby cry, Bus, Truck, Motorcycle, Train
It should be noted that the training process is a form
horn, and Race car. The dataset is divided into two splits:
of self-supervised learning process. Although ground truth
9000videosinthetrainingsetand1000videosinthetesting
masks are provided for our model to learn, we do not use
set. Some sample frames are shown in Fig.3.
anyvideolabelinformationorotherextrainformationexcept
All the video data can be divided into two more general
the videos themselves. We use the manually mixed sound as
categories, the speaking people and sounding vehicles. Dur-
training data, and the ground truth sound, which is exactly
ing the training and testing process, we choose two videos
thesoundofindividualvideos,isnaturallyavailablewithout
randomly from those two general categories respectively
any additional supervision information. Hence, the ground
and mix their sound. Then our model separates the sound
truthcanbeobtainedeasilyandnoextrainformationisused
of each video from the whole sound. Among 9000 videos
during the self-supervised learning procedure.
in the training set, 4500 videos belong to the category of
IV. OBJECTDETECTIONANDSOUNDSEPARATION person speech, and the other 4500 videos belong to the
category of vehicle’s sound. Besides, both 500 videos are
Thegoalofourmodelistoﬁndallthevisualobjectsinthe
belonging to the category of person speech and vehicle’s
videoandseparatethesoundofeveryvisualobjectfromthe
sound respectively in the testing set.
mixedsoundsignalofthevideosimultaneously.Speciﬁcally,
this model is designed to address the problem of particular B. Data Processing
kindsofnoisethatoccurfrequentlyinreal-lifeduringhuman
The data is preprocessed to obtain audio-visual pairs for
speech.Therefore,wemainlyfocusonseparatingthesounds
training, The middle frame of videos is extracted as the
of people and other interfering sounds.
key-frame of each video. Each audio sample is about 10
Aftertraining,themodelshouldbeappliedtovideoswith
seconds, and we resample the audio signals to 11kHz. After
multiple sound sources in the prediction process, to perform
that, the signals are converted into spectrograms using the
object detection and sound separation tasks simultaneously.
Short-Time Fourier Transform (STFT) with a window size
The model predicts the spectrogram mask for each object in ×
of 1022 and a hop length of 256, which results in 256
the video, and recovers the sound waveform of each object
512Time-Frequency(T-F)representationsofsound.Tospeed
with the mask and the mixed spectrogram. As the ground
up, we further resample signals on a log-frequency scale to
truth of object sound in videos with multiple sound sources ×
obtain a 256 256 T-F spectrograms, which is similar to
is difﬁcult to obtain, only qualitative analysis is completed
the application of Mel-Frequency scale.
in the prediction phase.
The difference between the prediction process and the C. Evaluation Criteria
training process is the type of processed data and the target. To evaluate the trained model, we use the model to
In the prediction process, the model is utilized to separate separatethesoundthatismanuallymixedfromtwodifferent
the object sound from an individual video sound. However, video samples. Quantitative analysis and qualitative analysis
in the training process, the model tries to separate the sound are conducted to assess our method.
of individual videos from the mixed sound of two videos. In In the testing process, we randomly choose two videos in
fact,whatthemodellearnsinthetrainingprocessiseffective the testing set and mix the sound of each video manually.
and useful in object sound separation in the prediction Then, we use our model to separate the sound, and some
process. sound separation metrics are applied to evaluate the perfor-
mance. For each video sample representing human speech,
V. EXPERIMENT
we randomly select a video of vehicle type and mix the
A. Dataset
soundoftwovideos.Weconduct300timesofmixingtotally,
We build a new video dataset for our experiment. Our where 4500 pairs of people samples and vehicle samples
dataset is a subset of AudioSet [32], which contains 10000 are mixed each time. To quantitatively evaluate the effect of
videos with 10 seconds. According to goals we want to soundsourceseparation,weusedthreecommonmetrics:The
1591
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 14:19:23 UTC from IEEE Xplore.  Restrictions apply. Fig. 4. Qualitative analysis results of several samples. We show the key frame of the mixed two videos, the spectrogram of mixed sound and the
spectrogram of separated sound with different sound source separation methods. For each sample, the ﬁrst column is the key frame of two videos. The
third,forthandﬁfthcolumnsaretheseparatedspectrogramwithRPCA,HPSS,andNMFmethodsrespectively.Thesixthcolumnistheseparatedsound
spectrogram with the Sound-of-Pixels model. The seventh column is the separated spectrogram with our model. The eighth column is the ground truth
soundspectrogram,whichisthesoundspectrogramofeachvideo’saudioclip.
Source-to-Distortion Ratio (SDR), Source-to-Interferences measures the absence of artifacts, so actually SAR is not
Ratio (SIR) and Source-to-Artifacts Ratio (SAR) proposed precise enough to reﬂect the separation accuracy [27].
in [33]. The metrics are calculated using The BSS-EVAL The separated sound segment and predicted sound spec-
Toolbox [33]. trogram are shown for qualitative analysis.
Withthegroundtruthofthesound,wecandecomposethe
D. Quantitative Analysis
output sound into several parts: target, interferences, noise
and artifacts We compare the quantitative performance of our methods
with several audio-only sound source separation baselines
s=s +e +e +e
target interf noise artif including RPCA, HPSS, and NMF methods. At the same
SDRisthematrixtoevaluatetheextentofgeneraldistortion, time, we also compare our method with the sound source
SIR is used to evaluate the amount of interferences caused separation model proposed in [24], which utilizes visual-
by other sources, while SAR is to evaluate the amount of audio features. We refer to that model as Sound-of-Pixels
artifacts errors terms. The deﬁnitions are as follows [33]: in the following parts of this paper.
The Source-to-Distortion Ratio (SDR): The results are shown in Table I. Compared with those
(cid:107) (cid:107) audio-only sound separation baselines, our model achieves
s 2
SDR=10log10(cid:107)einterf +etanrogiseet +eartif(cid:107)2. tHhPeShSigmheestthoindsSoDuRtp.eHrfoorwmevoeur,rimnotdereml.sForofmSItRh,eNdeMﬁFnitainodn
The Source-to-Interferences Ratio (SIR): of SDR and SIR, it is noticed that SIR compares the
(cid:107) (cid:107) separatedsoundwithonlytheamountofinterferences,while
s 2
SIR=10log10(cid:107)eitnatregreft(cid:107)2. SthDeRuncreolmatpeadressignthaelssienpcaluradtiendg sthoeunadmowuintht otfheintseurmfereonfceasll,
The Source-to-Artifacts Ratio (SAR): noise, etc. Hence, SDR is a better metrics reﬂecting the
(cid:107) (cid:107) accuracy of the sound separation task. Although SIR of
s +e +e 2
SAR=10log10 target (cid:107)enaoritsife(cid:107)2 interf . NarMe Floawn,dwHhPiSchS minedtihcoadtessartehahtigthheerr,eSDisRaoflathrgoesenmoeistheodins
From the deﬁnition, we can know that SIR and SDR the separated target sound obtained by NMF and HPSS
can evaluate the accuracy of sound separation results. SAR methods. Furthermore, in the part of qualitative analysis,
1592
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 14:19:23 UTC from IEEE Xplore.  Restrictions apply. TABLEI
QUANTITATIVEANALYSISINDIFFERENTMETHODS
Methods SAR SIR SDR
RPCA 4.3025 -0.1598 -4.2977
HPSS 9.4705 2.8645 0.1160
NMF 0.6633 4.6153 -4.2616
Sound-of-Pixels 6.6016 1.0348 -3.3530
OurModel 15.0934 1.2884 0.1342
we show the spectrogram of each separated sound with
different sound separation methods. The results show that
our model performs better in sound separation than other
audio-only sound separation models. Our model can extract
visual features, and those features can guide and support the
process of sound separation to obtain better results.
In addition, our method also outperforms the visual-audio
Sound-of-Pixels model in SAR, SIR and SDR. That pixel-
level sound separation model utilizes the visual features of Fig. 5. Results in real-world scenarios. We show the results of object
detection and sound separation tasks. Then, the detected objects and the
each pixel and the segmented audio features to accomplish
separatedaudioclipsarealigned.
thesoundseparationtogether.Infact,themodelcannotknow
what each pixel represents in the real scene, and cannot
build the connection between adjacent pixels or related separation results. Through comprehensively analyzing the
pixels.Ourmodelextractstheobject-levelvisualfeaturesfor experiment results in quantitative and qualitative ways, our
everydetectedobject.Theobject-levelfeaturescontainmore model performs better than other methods.
semantic information than the pixel-level features, hence it
can better guide the sound source separation. F. Real-World Scenarios
E. Qualitative Analysis To further illustrate the effectiveness of our model, we
capture some real-life videos. In one of the videos we
ThequalitativeresultsareshowninFig.4.Wegive6sam-
capture, a student is standing on the street reporting the
ples of the sound separation task in the testing set. For each
weather condition,with a large noisyvehicle passing behind
sample, we show the separated sound spectrogram of each
her.Thesoundofthatvehicle’senginemixeswiththespeech
video with different sound separation methods mentioned in
ofthatstudent.Withourmodel,thesoundofthatstudentcan
quantitativeanalysis.Fromthoseperformance,wecanobtain
be extracted clearly, which is shown in Fig. 5. The detected
results as follows:
student and that vehicle are marked with boxes. And the
1) Theperformanceofsoundseparationwithourmodelis
soundofthewholevideoisseparatedinto2parts:thesound
better than other methods, and the difference between
of the student and the sound of the vehicle. The results of
separatedspectrogramwithourmodelandgroundtruth
separated audios are shown in the attached video.
spectrogram is minimal.
2) The difference between the separated sound spec- VI. CONCLUSION
trogram with the Sound-of-Pixels model and ground
Inthispaper,wedevelopaself-supervisedlearningmodel
truth is larger than that between our model’s result
toperformobjectdetectionandsoundsourceseparationtasks
and ground truth. The separated sound gained by the
forvideosinreal-worldscenariossimultaneously.Theresults
Sound-of-Pixels model lose some information of the
of experiments on manually synthetic data and captured
origin sound sometimes.
videos in real scenes indicate that the model can build
3) The NMF method does not separate two sound ef-
alignment between objects and sounds by exploiting the
fectively, and the two sound spectrograms are similar
visual and audio information at the same time. However, the
sometimes.
model also have limitations in a more complex setting. For
4) TheperformanceofHPSSandRPCAmethodisalittle
example, the proposed model cannot separate the sounds in
better than NMF method, but still not as good as our
instance-level. We will try to solve that task in the future.
method. The separated sound results of person speech
with RPCA method lose information sometimes.
ACKNOWLEDGMENT
Thequalitativeresultsdemonstratetheactualperformance
of sound separation with different models. Though SIR This work was supported in part by the National Natural
of NMF and HPSS methods is a little higher, the actual Science Foundation of China under Grants U1613212 and
sound separation performance indicates they have worse 91848206.
1593
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 14:19:23 UTC from IEEE Xplore.  Restrictions apply. REFERENCES [22] J. Pu, Y. Panagakis, S. Petridis, and M. Pantic, “Audio-visual object
localizationandseparationusinglow-rankandsparsity,”in2017IEEE
[1] H.Liu,F.Sun,andX.Zhang,“Roboticmaterialperceptionusingac- InternationalConferenceonAcoustics,SpeechandSignalProcessing
tivemultimodalfusion,”IEEETransactionsonIndustrialElectronics, (ICASSP),2017,pp.2901–2905.
vol.66,no.12,pp.9878–9886,2018.
[23] T.Afouras,J.S.Chung,andA.Zisserman,“Theconversation:Deep
[2] H.G.OkunoandK.Nakadai,“Robotaudition:Itsriseandperspec- audio-visualspeechenhancement,”arXivpreprintarXiv:1804.04121,
tives,” in 2015 IEEE International Conference on Acoustics, Speech
2018.
andSignalProcessing(ICASSP),2015,pp.5610–5614.
[24] H.Zhao,C.Gan,A.Rouditchenko,C.Vondrick,J.McDermott,and
[3] T.Morito,O.Sugiyama,R.Kojima,andK.Nakadai,“Partiallyshared A. Torralba, “The sound of pixels,” in The European Conference on
deep neural network in sound source separation and identiﬁcation ComputerVision(ECCV),September2018.
using a uav-embedded microphone array,” in 2016 IEEE/RSJ Inter-
[25] A. Rouditchenko, H. Zhao, C. Gan, J. McDermott, and A. Torralba,
nationalConferenceonIntelligentRobotsandSystems(IROS),2016, “Self-supervised audio-visual co-segmentation,” in ICASSP 2019 -
pp.1299–1304. 2019IEEEInternationalConferenceonAcoustics,SpeechandSignal
[4] M.Frchette,D.Ltourneau,J.-M.Valin,andF.Michaud,“Integration Processing(ICASSP),2019,pp.2357–2361.
of sound source localization and separation to improve dialogue
[26] A. Owens and A. A. Efros, “Audio-visual scene analysis with self-
managementonarobot,”in2012IEEE/RSJInternationalConference supervised multisensory features,” in The European Conference on
onIntelligentRobotsandSystems(IROS),2012,pp.2358–2363. ComputerVision(ECCV),September2018.
[5] Q.NguyenandauthorJongSukChoi,“Selectionoftheclosestsound
[27] R.GaoandK.Grauman,“Co-separatingsoundsofvisualobjects,”in
source for robot auditory attention in multi-source scenarios,” in ICCV,2019.
JournalofIntelligentRoboticSystems,2016,pp.239–251.
[28] F. Wang, D. Guo, H. Liu, J. Zhou, and F. Sun, “Sound-indicated
[6] R. Kojima, O. Sugiyama, R. Suzuki, K. Nakadai, and C. E. Taylor, visualobjectdetectionforroboticexploration,”in2019International
“Semi-automatic bird song analysis by spatial-cue-based integration Conference on Robotics and Automation (ICRA), 2019, pp. 8070–
ofsoundsourcedetection,localization,separation,andidentiﬁcation,”
8076.
in2016IEEE/RSJInternationalConferenceonIntelligentRobotsand
[29] J.Zhou,F.Wang,D.Guo,H.Liu,andF.Sun,“Video-guidedsound
Systems(IROS),2016,pp.1287–1292. sourceseparation,”ICIRA2019:IntelligentRoboticsandApplications,
[7] H.Liu,Z.Zhang,Y.Zhu,andS.-C.Zhu,“Self-supervisedincremental
pp.415–426,2019.
learningforsoundsourcelocalizationincomplexindoorenvironment,”
[30] S.Ren,K.He,R.Girshick,andJ.Sun,“Fasterr-cnn:Towardsreal-
in2019InternationalConferenceonRoboticsandAutomation(ICRA), timeobjectdetectionwithregionproposalnetworks,”inAdvancesin
2019,pp.2599–2605. neuralinformationprocessingsystems,2015,pp.91–99.
[8] W.He,P.Motlicek,andJ.-M.Odobez,“Deepneuralnetworksformul-
[31] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional
tiplespeakerdetectionandlocalization,”in2018IEEEInternational networksforbiomedicalimagesegmentation,”inMedicalImageCom-
ConferenceonRoboticsandAutomation(ICRA),2018,pp.74–79. putingandComputer-AssistedIntervention–MICCAI2015. Cham:
[9] F. Grondin and F. Michaud, “Noise mask for tdoa sound source
SpringerInternationalPublishing,2015,pp.234–241.
localization of speech on mobile robots in noisy environments,” in
[32] J. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen, W. Lawrence,
2016 IEEE International Conference on Robotics and Automation
R.C.Moore,M.Plakal,andM.Ritter,“Audioset:Anontologyand
(ICRA),2016,pp.4530–4535. human-labeled dataset for audio events,” in Acoustics, Speech and
[10] E.C.Cherry,“Someexperimentsontherecognitionofspeech,with SignalProcessing(ICASSP),2017IEEEInternationalConferenceon.
one and with two ears,” in The Journal of the Acoustical Society of
IEEE,2017,pp.776–780.
America,1953,p.975979.
[33] E.Vincent,R.Gribonval,andC.Fe´votte,“Performancemeasurement
[11] J. H. McDermott, “The cocktail party problem,” in Current Biology, inblindaudiosourceseparation,”IEEEtransactionsonaudio,speech,
2009,p.R1024R1027. andlanguageprocessing,vol.14,no.4,pp.1462–1469,2006.
[12] D. Wang and J. Chen, “Supervised speech separation based on deep
learning: An overview,” IEEE/ACM Transactions on Audio, Speech,
andLanguageProcessing,vol.26,no.10,pp.1702–1726,Oct2018.
[13] T. Virtanen, “Sound source separation using sparse coding with
temporalcontinuityobjective,”ProcIcmc,vol.2003,2003.
[14] ——, “Monaural sound source separation by nonnegative matrix
factorization with temporal continuity and sparseness criteria,” IEEE
Transactions on Audio, Speech, and Language Processing, vol. 15,
no.3,pp.1066–1074,March2007.
[15] J.R.Hershey,Z.Chen,J.L.Roux,andS.Watanabe,“Deepclustering:
Discriminativeembeddingsforsegmentationandseparation,”in2016
IEEE International Conference on Acoustics, Speech and Signal
Processing(ICASSP),2016,pp.31–35.
[16] D. Yu, M. Kolbk, Z.-H. Tan, and J. Jensen, “Permutation invariant
training of deep models for speaker-independent multi-talker speech
separation,” in 2017 IEEE International Conference on Acoustics,
SpeechandSignalProcessing(ICASSP),2017,pp.241–245.
[17] H. Liu, D. Guo, X. Zhang, W. Zhu, and F. Sun, “Toward
image-to-tactilecross-modalperceptionforvisually-impairedpeople,”
IEEE Transactions on Automation Science and Engineering, DOI:
10.1109/TASE.2020.2971713.
[18] S.RavulapalliandS.Sarkar,“Associationofsoundtomotioninvideo
using perceptual organization,” in 18th International Conference on
PatternRecognition(ICPR’06),2006,pp.1216–1219.
[19] C.Sigg,B.Fischer,B.Ommer,V.Roth,andJ.Buhmann,“Nonneg-
ativeccaforaudiovisualsourceseparation,”in2007IEEEWorkshop
onMachineLearningforSignalProcessing,2007,pp.253–258.
[20] R. Gao, R. Feris, and K. Grauman, “Learning to separate object
sounds by watching unlabeled video,” in The European Conference
onComputerVision(ECCV),September2018.
[21] A. Ephrat, I. Mosseri, O. Lang, T. Dekel, K. Wilson, A. Hassidim,
W.T.Freeman,andM.Rubinstein,“Lookingtolistenatthecocktail
party: A speaker-independent audio-visual model for speech separa-
tion,” ACM Transactions on Graphics (Proc. SIGGRAPH), vol. 37,
2018.
1594
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 14:19:23 UTC from IEEE Xplore.  Restrictions apply. 