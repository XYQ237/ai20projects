2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
Knowledge-Guided Reinforcement Learning Control for Robotic Lower
Limb Prosthesis
Xiang Gao, Jennie Si, Fellow, IEEE, Yue Wen, Minhan Li, and He (Helen) Huang, Senior Member, IEEE
Abstract—Robotic prostheses provide new opportunities to a problem, including learning directly from data without
better restore lost functions than passive prostheses for trans- requiringanexplicitmathematicaldescriptionoftheenviron-
femoralamputees.Butcontrollingaprosthesisdeviceautomat-
mentandtheinteractingdynamicsbetweenthecontrollerand
ically for individual users in different task environments is an
the environment. This has given RL an expanded domain of
unsolved problem. Reinforcement learning (RL) is a naturally
promising tool. For prosthesis control with a user in the loop, controlapplicationsbeyondthecapacityoftraditionalcontrol
it is desirable that the controlled prosthesis can adapt to theory and practice. There have been several successful
differenttaskenvironmentsasquicklyandsmoothlyaspossible. demonstrations of RL applications to solving challenging
However, most RL agents learn or relearn from scratch when
roboticcontrolproblems.Amongthose,deepRLmethodsat-
the environment changes. To address this issue, we propose
tractedmostattentions.Forexample,Nairetal.[8]employed
the knowledge-guided Q-learning (KG-QL) control method as
a principled way for the problem. In this report, we collected deepdeterministicpolicygradients(DDPG)foraroboticarm
anduseddatafromtwoable-bodied(AB)subjectswearingaRL block stacking task with sparse reward. The authors of [9]
controlledroboticprostheticlimbwalkingonlevelground.Our proposed deep latent policy gradient (DLPG) for learning
ultimategoalistobuildanefﬁcientRLcontrollerwithreduced
locomotionskills.However,deepRLbasedmethodsmaybe
time and data requirements and transfer knowledge from AB
not suitable for biomedical applications such as the human-
subjectstoamputeesubjects.Towardthisgoal,wedemonstrate
itsfeasibilitybyemployingOpenSim,awell-establishedhuman prosthesis control problem being discussed in this paper,
locomotionsimulator.OurresultsshowtheOpenSimsimulated because training data involving amputee subjects are usually
amputee subject improved control tuning performance over difﬁcult to acquire and expensive to collect. Additionally,
learningfromscratchbyutilizingknowledgetransferfromAB
experimentalsessionsinvolvinghumansubjectsusuallycan-
subjects. Also in this paper, we will explore the possibility of
not last more than one hour because of human fatigue and
information transfer from AB subjects to help tuning for the
amputee subjects. safety considerations. To tackle this challenge, we proposed
several sample-efﬁcient and easy-to-implement RL methods
I. INTRODUCTION inourpreviousworks[10]-[13]allowingfordirectlylearning
from data. In our application of prosthesis control, it is very
The rapid development of robotic prostheses in both
common that the robotic prosthesis need to be adapted for
researchandcommercialproductsbringsthemclosertoreal-
a new user. However, these RL methods, as well as most
life scenarios. Compared to passive devices, robotic lower
existing RL methods, are designed to learn from scratch
limbprosthesespromisetoprovidebetterfunctionstorestore
wheneveranewtaskornewmodelispresented,andthusnot
natural gaits for amputees, such as decreased metabolic
readilycapableofstoringandtransferringknowledgegained
consumption[1],improvedadaptationtovariousterrains[2],
from one subject to another.
[3] or walking speed [4], and enhanced balance and stability
It is therefore of high priority that the RL agent is
[5]. In robotic lower limb prosthetics, ﬁnite state impedance
designedtobetrainingtimeandsampleefﬁcientwhentuned
control (FS-IC) [6], [7] is still the most common approach
for a new user. To take advantage of previous knowledge
inbothprototypesorcommercialdevices.However,inorder
and information, we consider building a representation for
to maximize the performance for each user, there are a large
potentially transferable knowledge across subjects. In the
number of control parameters in these devices that need to
current study, we consider extracting knowledge from able-
be tuned by experienced clinicians.
bodied (AB) subjects and use that for future RL control
Reinforcement learning allows learning from interacting
designforamputeesubjects.Itisknownthattransferlearning
with the environment to generate suitable actions while
has attracted great attention in the machine learning ﬁeld
maximizing a performance reward in a particular situation.
whereitistypicallyconsideredforstoringknowledgegained
Learning can take place under different formulations of
while solving one problem and applying it to a different
but related problem [14]. In the context of general transfer
*This work was partly supported by National Science Foundation:
#1563921and#1808752forJ.Si,#1563454and#1808898forH.Huang. learning in the literature, our prosthesis parameter tuning
Correspondence:J.SiandH.Huang. problem has the same state and action while the problem
X. Gao and J. Si are with the the Department of Electrical, Computer,
calls for gaining knowledge from tuning parameters for AB
andEnergyEngineering,ArizonaStateUniversity,Tempe,AZ85287USA
si@asu.edu subjects (source task) and using that for tuning parameters
Y. Wen, M. Li and H. Huang are with the Department of Biomedical for amputee subjects (target task).
Engineering, North Carolina State University, Raleigh, NC 27695 USA,
Recently, many successful applications of structural
andalsowiththeUniversityofNorthCarolinaatChapelHill,ChapelHill,
NC27599USAhhuang11@ncsu.edu knowledge transfer have been reported in the literature.
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 754
Authorized licensed use limited to: Heriot-Watt University. Downloaded on September 21,2020 at 04:57:13 UTC from IEEE Xplore.  Restrictions apply. Fig.1. Schematicofknowledge-guidedRLcontrol.(a)Knowledgerepresentationbyregressionmodelsofthesystemareobtainedusingdatafromtwo
able-bodiedsubjects.(b)TheonlinelearningprocessinOpenSim:anactor-criticagentistrainedtooptimizetheimpedanceparametersbyinteractingwith
(cid:48)
thelowerlimbwalkingmodelinOpenSim,withthehelpofthetransferredknowledgerepresentedinthefunctionofQ(x ,u ).(c)Graydashedline:
k k
normalkneekinematics,blueline:actualmeasuredkneekinematics.
Barretoetal.[15]solvedtheproblemwhererewardschange A. FiniteStateImpedanceControlofHuman-ProsthesisSys-
butenvironmentsremainthesameusingsuccessorfeatures,a tem
valuefunctionrepresentationthatdecouplesthedynamicsof
FromtheperspectiveofaRLagent,theintegratedhuman-
theenvironmentfromtherewards.Asadietal.[16]proposed
prosthesis system can be treated as a nonlinear dynamic
a learning architecture which transfers control knowledge in
system of the form
the form of behavioral skills and representation hierarchy,
which separates the subgoals so that a more compact state x =F(x ,u ),k =0,1,2,... (1)
k+1 k k
space can be learned. In [17], researchers demonstrated that
where k is the discrete time index or gait cycle in this
Schema network is capable to perform zero-shot transfer ∈ R ∈ R
study, x 2 is the state vector, u 3 is the action
between tasks where cause-effect relationship remains un- k k
or control vector, and F describes the intrinsic human-
changed, such as learning to play the breakout game with
prosthesis system dynamics of how a new state at k + 1
differentmaps.In[18],targetapprenticelearningisproposed
evolves from a current state and control at k. Speciﬁcally,
for cross-domain transfer, e.g. from balancing a cart-pole to
state x is deﬁned as the differences (errors) between the
balancing a bike. k
measured knee angle proﬁle and the target knee proﬁle at
Unlike the above approaches, we propose a new knowl- the feature points. The target knee proﬁle is identical to
edge transfer framework for the class of problems that those normative knee kinematics reported in [19]. In Fig.
transfer from a source task to a target task while main- 1(c),foreachofthefourphasesthereisapairofsuchfeature
taining the same state and control problems. We built a points with black and red markers, where their vertical and
knowledge representation from AB subjects into the actor- horizontal differences are peak error ∆P ∈R and duration
critic update, and the knowledge transfer schedule results error ∆D ∈R, respectively: k
k
in a diminishing inﬂuence of previous knowledge, which
simultaneously allows for increased attention to learning of xk =[∆Pk,∆Dk]T. (2)
the target task on hand. Speciﬁcally, we ﬁrst collected data
The RL controller is realized within an established FS-
from AB subjects, then we built regression models on these
IC platform. In FS-IC, a complete gait cycle is divided into
data,whichthenbecametransferredknowledgetoguideaQ-
four sequential gait phases based on knee joint kinematics
learningprocess,namelyourproposedknowledgeguidedQ-
and ground reaction force (GRF) by a ﬁnite state machine
learning (KG-QL) process. Our method introduces two new
(FSM). These four gait phases are stance ﬂexion (STF),
advances from the existing transfer learning methods. First,
stance extension (STE), swing ﬂexion (SWF) and swing
we provided a ﬂexible framework where the representation
extension(SWE).Inreal-timeexperiments,phasetransitions
of the transferred knowledge can be either a value function
arerealizedasthosein[7]basedonDempster-Shafertheory
or a regression model or both. Second, the amount of
(DST). In each phase, the prosthetic system mimics a pas-
transferred knowledge into a new task can be programmed
sive spring-damper-system with a group of three predeﬁned
in a convenient way to address different applications’ needs.
impedance parameters as
∈R
I =[K ,B ,θ ] 3, (3)
II. METHODS k k k e,k
where K is stiffness, B is damping coefﬁcient and θ
k k e,k
Fig. 1 is a schematic diagram of our proposed transfer is equilibrium position. For all four phases, there are 12
learning-based or knowledge guided reinforcement learning impedance parameters in total. Four RL controllers sharing
framework for prosthesis parameter tuning. identicalstructurearedesignedseparatelyforthefourphases
755
Authorized licensed use limited to: Heriot-Watt University. Downloaded on September 21,2020 at 04:57:13 UTC from IEEE Xplore.  Restrictions apply. , and each of them has its own parameters. The knee joint C. Extracting Knowledge from Human Gait Data
∈R
torqueT isgeneratedbasedonthefollowingimpedance We performed linear regression to establish a relationship
control law
between prosthesis impedance parameters and the human-
prosthesis system kinematics as follows,
− F F
Tk =Kk(θ θe,k)+Bkω. (4) xk+1 = (xk,uk)= (zk)=β0+β1zk+e, (6)
∈ R ∈ R
Cadojrursetsmpoenntdsin∆glIyk,tthoethaectiiomnpuedkaonfcethpeaRraLmaegteernstIiks,deﬁnedas wβ[x1he∈,rueRx]2×k∈+5R1i5s itshet2hreiesgprtrehesedsiincoetnoxrtcvosatearfitﬁaecb,ileβen0fto(romretdhisebtyshleothpienet)ce,urzcrkreepn=tt,
k k ∈R
state x and action u , and e 2 is the error term. Least-
u =∆I =[∆K ,∆B ,∆θ ]. (5) k k
k k k k e,k square solution of the coefﬁcients β and β can be found
0 1
using the (x ,u ,x ) tuples. Equation (6) characterizes
B. Human Gait Data Collection k k k+1
the human-prosthesis system qualitatively because when a
controller enables the human-prosthesis system to generate
To perform knowledge transfer from a source task to a
improvedlocomotionperformance,wegenerallyobservethat
target task, ﬁrst we need to obtain the transferable knowl- | |≤| |
edge, which can be represented in the form of raw data, xk+1 xk . F
After the regression model is obtained, we can formu-
policy, value function, or others. Here we deﬁne a function (cid:48) F (cid:48)
Q(cid:48)(x ,u ) to store such information for transfer. It takes late Q(xk,uk) based on (xk,uk). How Q is formulated
k k alsorelatestothestagerewardorcostinRL.Inourwork,we
state and action as input to generate the state-action value
function or Q-value function Q(x,u) as in the RL literature. set the stage cost variable rk =0 for success and rk =1 for
(cid:48) failure(see(9)),whichimpliedthatthegoalfortheRLagent
Although Q can be a previously learned Q-value function
was to minimize the total cost-to-go. Accordingly, inspired
from a RL agent, it can also be represented in other forms.
(cid:48)
In this work, we construct Q(cid:48) using regression model based by LQR control objective fu(cid:48)n≥ction, Q was formulated as a
on the source task data. qua≥dratic form su≥ch that Q 0,which was consistent with
r 0 and Q 0 (Q is the iterative Q-value function
Source task data was collected from two AB participants k i i
deﬁned in (14) and (15)):
(both male, 25-35 years old) while walking at a constant
(cid:48) F
speed of 0.6 m/s on a treadmill with force platforms em- Q =0.02x2 =0.02( (x ,u ))2. (7)
k+1 k k
bedded within each belt. All participants provided written
(cid:48)
informedconsentpriortoparticipatingaccordingtoprotocols NotethatheretheformofQ wasmanuallydeﬁnedandwas
(cid:48)
approvedbytheInstitutionalReviewBoardatNorthCarolina not unique. The ratio of 0.02 was set manually to make Q
State University. A certiﬁed prosthetist aligned the robotic within a comparable range of the (cid:48)stage cost rk. As shown
knee prosthesis for each subject. The AB subjects used an later, knowledge represented in Q can be adopted by the
designer at a preferred rate. Fig. 2 Illustrates kinematics and
L-shapeadaptor(withonelegbent90degrees)towalkwith
Q-values as knowledge representations.
the robotic knee prosthesis (Fig. 1(a)) [11].
The gait data used in this study includes a total of D. Knowledge Guided Reinforcement Learning
N = 1120 pairs of state-action tuples (x ,u ) from the
k k
twoABsubjects(AB1:480pairs,AB2:640pairs)usingthe
Algorithm 1 Knowledge Guided Q-Learning (KG-QL) for
sameprosthesisdevice.Duringdatacollection,theprosthesis
prosthesis control with a human in the loop
impedance parameters were controlled by the dHDP based (cid:48)
Input: Transferred knowledge Q from a source task
RLapproachthatweinvestigatedpreviously[11],[20].Note
Initialization: Random actor NN weights and critic NN
that the dHDP was only to provide some control to the
weights. i=0,k =0.
prosthesis instead of providing optimal control to achieve a
performance measure. In other words, the data were drawn 1: Start from a random initial state x0.
from the online learning process of the dHDP RL controller 2: repeat
rather than generated by a well-learned policy to provide 3: Get uk from xk according to actor NN (-greedy).
sufﬁcient exploration of the control policy space. Actually, a 4: Take action uk, observe cost rk and next state xk+1.
RLcontrollerisnotuniquefordatacollection.Anysampling 5: Update actor NN weights to minimize (22).
method is acceptable as long as it sufﬁciently samples the 6: Updat←e critic NN weights to minimize (24).
control parameter space, and it maintains practical stability 7: xk←+1 xk.←
of the human-prosthesis system. Note that during data col- 8: i i+1,k k+1.
lection, the impedance parameters I were updated every 9: until xk is a terminal state
k
seven gait cycles, and state x was averaged by the seven
k (cid:48)
gait cycles conditioned on the same impedance parameters We have introduced how the transferred knowledge Q
I .Thatistosay,toreducestep-by-stepvariabilityinfeature is obtained through regression. Now we can move onto the
k
measurements, the time index k here corresponds to every onlinelearningprocessoftheRLagentasshowninFig.1(b).
seven gait cycles. We call this RL algorithm a knowledge-guided Q-learning
756
Authorized licensed use limited to: Heriot-Watt University. Downloaded on September 21,2020 at 04:57:13 UTC from IEEE Xplore.  Restrictions apply. (cid:48)
algorithm (KG-QL) because when determining a best action whereQ isanpositivesemi-deﬁnitefunctionthatrepresents
≤ ≤
for the next step, its decision is guided and biased by the previouslylearnedknowledge,and0 α 1isaweighting
(cid:48) ≤ →i → ∞
transferred knowledge Q. factor such that α α ,and α 0 when i .
i+1 i i
At time index k, the RL agent starts from state x and Here we simply let α be a uniformly decreasing sequence
k i
takes the action u . Then it ends up at the next state x , of0.5,0.49,0.48,...,0asiincreases.Combiningtheabovetwo
k k+1
and receives a cost r . This process repeats for k =1,2,... equations, we have
k
untilaterminalstateisreached.Thetotalcost-to-gofunction
(cid:88) Q (x ,u )=r +γmin[Q (x ,u )
or value function is deﬁned as∞ i+1 k k k (cid:48)uk+1 i k+1 k+1 (19)
+α Q(x ,u )]
− i k+1 k+1
J(xk,uk)= γj krk, (8) E. Actor-Critic Implementation
j=k Algorithm 1 summarizes our implementation of the the
whererk =r(xk,uk)isthestagecost,andγ isthediscount proposedKG-QLmethod.Notethatiincreaseswithk atthe
factor with 0<γ <1. In our work, we deﬁned rk as same time in our implementation. These two indexes i and
0, if x is a success state k have different meanings, and they are not equal in general
k+1 (though they are equal in this work), so we did not combine
rk =r(xk,uk)= 1, if xk+1 is a failure state (9) them.WeimplementedKG-QLwithanactor-criticstructure
0.01, otherwise [22],[23],where(17)wasimplementedbyanactor,and(18)
wasimplementedbyacritic.Bothactorandcriticwerefeed-
In this work, a success state is deﬁned as when the state is
forward neural networks (NN) with one hidden layer (5-6-1
in the target range, and a failure state is deﬁned as the state
forthecritic,and2-6-3fortheactor).Thecritichasthestate
is out of the safety range. Further details can be found in
III-A. xk andtheactionuk asinputs,andoutputsanapproximation
Equation (8) can be written as of the Q-value function, denoted by Qˆ(xk,uk). The actor
has state x as inputs, and outputs the control action u .
k k
J(xk,uk)=rk+γJ(xk+1,uk+1). (10) Theactorusedatangentsigmoidactivationfunctionϕ(v)in
both the hidden layer and output layer,
AccordingtoBellman’soptimalityprinciple[21],theoptimal
− −
∗
cost function J satisﬁes the action dependent discrete time ϕ(v)= 1 exp(−v) (20)
Hamilton–Jacobi–Bellman (HJB) equation 1+exp( v)
∗ ∗ where v is the input vector for the activation function. Note
J (x ,u )=r +γminJ (x ,u ). (11) −
k k k uk+1 k+1 k+1 that 1 < ϕ(v) < 1. For the critic, it also used the same
Besides, the optimal control π∗ can be expressed as tan-sigmoid function ϕ(v) in its hidden layer. But it used a
linear activation function φ(v)=v in its output layer.
∗ ∗
π (x )=argminJ (x ,u ). (12) During training, the actor and critic back-propagated their
k k k
uk prediction error to update their weights (Steps 5 & 6 in
Bysubstituting(12)into(11),thediscretetimeHJBequation Algorithm 1). The prediction error of the actor e ∈ R
a,k
becomes is to realize (17),
∗ ∗ ∗ (cid:48)
J (x ,u )=r +γJ (x ,π (x )). (13) e =Qˆ (x ,u )+α Q(x ,u ). (21)
k k k k+1 k+1 a,k i k k i k k
For an actor-critic agent, we have the following structure, Then the squared error E for the actor is
a
1
πi(xk)=argminQi(xk,uk), (14) Ea = 2e2a,k. (22)
uk The prediction error of the critic e ∈ R is the temporal
Qi+1(xk,uk)=rk+γQi(xk+1,πi(xk+1)), (15) difference (TD) error of (18), c,k
wcohnetrreolipioslitchyeaintedraittievreatiivnedeQx,-vπailuaendfunQcitioanre, rtehsepeitcetriavteilvye. ec,k =rk+γ(cid:48)[Qˆi(xk+1,πi(xk+1)−) (23)
+α Q(x ,π (x ))] Qˆ (x ,u )
Combining (14) and (15), we have i k+1 i k+1 i+1 k k
which is the difference between the left-hand side and right-
Q (x ,u )=r +γminQ (x ,u ). (16) hand side of (18). To correct the prediction error, the weight
i+1 k k k i k+1 k+1
uk+1 update objective was to minimize the squared performance
Accordingly, the knowledge-guided form of actor-critic
error E ,
learning can be written as c 1
E = e2 . (24)
c 2 c,k
(cid:48)
π (x )=argmin[Q (x ,u )+α Q(x ,u )], (17) III. RESULTS
i k i k k i k k
uk In the following experiments, knowledge was extracted
Q (x ,u )=r +γ[Q (x ,π (x )) from AB subjects and then transferred to an OpenSim
i+1 k k k (cid:48) i k+1 i k+1 (18)
+α Q(x ,π (x ))], simulated amputee subject.
i k+1 i k+1
757
Authorized licensed use limited to: Heriot-Watt University. Downloaded on September 21,2020 at 04:57:13 UTC from IEEE Xplore.  Restrictions apply. B. Knowledge Representation Results
Fig. 2 depicts the regression results data from two AB
subjects in the SWF phase. In Fig 2(a), the z-axis is the
next peak error ∆P , which is the ﬁrst element of the
k+1
next state x . Its values were obtained from the linear
k+1
regressionmodel(6)byvaryingoneofthestatevariablepeak
error ∆P and one of the action variable ∆θ , while other
k e,k
state and action variables remain unchanged. We can learn
how the next peak angle θ may change from Fig 2(a).
k+1 − −
(a) (b) For example, suppose ∆P = 5°. If ∆θ = 2°, then
− k e,k
Fig. 2. Knowledge extraction and representation based on AB human ∆Pk+1 < 5° acco−rding toFig 2(a). Vice versa, if ∆θe,k =
subjects.DatashownhereisfromtheSWFphase.(a)Theregressionmodel 2°, then ∆P > 5°. So 2° may be a better choice than
(cid:48) − k+1
in(6).(b)KnowledgerepresentationintheformofQ in(7). 2° for ∆θ in this case, as it makes the deviation of the
e,k
nextpeakerror∆P smaller.Fig2(b)showsthevaluesof
(cid:48) k+1 (cid:48)
Q which are computed from (7). Q has a minimum value
(cid:48)
0 at (0,0). Larger Q value indicates greater cost, which is
unfavorable by the RL agent.
C. Results of Reinforcement Learning with Knowledge
Transfer
Fig. 3 shows the knee kinematics with different initial
impedance parameters in the 10 simulation sessions were
distant from the target proﬁle, especially the peak angle
errors.Clearly,aftertheimpedanceparameterswereadjusted
(a) (b)
by the proposed RL controller, knee kinematics of the ﬁnal
Fig.3. Kneeangleproﬁles.(a)BeforeTuning(b)AfterTuning
acclimationstagesapproachedthetargetpoints.Speciﬁcally,
theaveragedabsolutevaluesofthepeakerrorsoverthethree
± ±
sessions deceased from 1.23° 0.77° to 0.36° 0.32° for
± ±
A. OpenSim Experiment Setup STF, from 3.13° 0.31° to 0.52° 0.24° for STE, from
± ±
5.53° 0.89° to 0.63° 0.68° degrees for SWF and from
± ±
The OpenSim lower limb walking model (Fig. 1(a)) used 2.72° 1.67° to 0.12° 0.25° for SWE. The results indicate
in this work is adopted from [24] and identical to the one that the proposed knowledge guided QL controller is able to
in [10]. In this model, ﬁve rigid-body segments linked by adjust the prosthetic knee kinematics to reproduce the target
one degree-of-freedom pin joints are used to model human knee proﬁle under different initial conditions.
walking dynamics. For the tuning task, we deﬁned a target Fig. 4 illustrates the evolution of the state, i.e. peak errors
± ±
range of 1° and 0.01 s for peak error ∆Pk and duration ∆Pk and duration errors ∆Dk, during the experimental
error ∆D , respectively. Only if for all four phases both session under one of the sets of initial parameters. Since
| | k | |
∆P < 1° and ∆D < 0.01s were met then we said similar results were obtained from other experiment ses-
k k | | | |
the state x was in the target range. If ∆P or ∆D are sions, hereafter we only present the result from the ﬁrst
k k k
greater than some preset values, then state x was out of session as an example. Because all four phases were tuned
k
the safety range and the control system resets to the default simultaneously, the parameter changes in one phase would
position of initial impedance parameters to ensure human affect its subsequent phases. In Fig. 4, notice that the sharp
subjectsafety.Moredetailsaboutthetargetrangeandsafety edgesonthecurvesindicatetheimpedanceparametersbeing
range can be found in our previous work [10, Table I]. reset to their initial values, because failure occurred. In this
An episode is the process from learning step k = 0 until example episode, the KG-QL agent was able to reduce all
termination which can either be that the state x enters the peak errors and duration errors to zero after approximately
k
target range for 10 consecutive gait cycles or runs out of 150 gait cycles.
safety range. If terminated, the state x was reset with the Fig. 5 illustrates the averaged root-mean-square error
k
initial impedance and initial state as the next episode began. (RMSE) of the gait knee proﬁle over the 10 experimental
Each OpenSim session consisted of multiple episodes with sessions. With the transferred knowledge from AB subjects,
a total of maximum 500 gait cycles. the RMSE of the proposed KG-QL algorithm drops faster
ThecommonparametersusedintheOpenSimexperiment than the QL without knowledge transfer, i.e., learning with
are listed as follows except those mentioned elsewhere. The α = 0 in (18). Our proposed KG-QL achieved a RMSE
i
discount factor γ was set to 0.95, the initial NN weights for performancelessthan1°afteronly100gaitcycles,however,
−
both actor and critic were uniformly distributed between 1 QL without knowledge transfer can only achieved similar
and 1. performance after about 400 gait cycles. Fig. 4 and Fig. 5
758
Authorized licensed use limited to: Heriot-Watt University. Downloaded on September 21,2020 at 04:57:13 UTC from IEEE Xplore.  Restrictions apply. (a) (b) (c) (d)
Fig.4. Evolutionofstatesinthefourgaitphases(a)phaseSTF(b)phaseSTE(c)phaseSWF(d)phaseSWE.
simulated amputee subjects, where in the latter case, the
forward dynamics model should capture such relationships.
Based on experimental measurements from two AB sub-
jects, we established a knowledge representation in the form
ofaregressionmodelofthehuman-prosthesisdynamics,and
aQ-valueintegrationofthisknowledgefortransferringtothe
targettask.WedemonstratedtheeffectivenessofthisKG-QL
control framework. Our results show that, with transferred
knowledge,QLwasabletoreachacomparableperformance
inthetargettaskofanOpenSimsimulatedsubject,butsaving
at least 60% of the learning time.
Our contribution is not limited to the demonstration of
the feasibility of such transfer learning. It also includes
our proposed RL control design framework that allows for
ﬂexible knowledge representation in the value function or
Fig. 5. Comparison of root-mean-square error (RMSE) for the with
knowledgeguidecaseandthewithoutknowledgeguidecase. systemdynamicsorboth.Inaddition,weprovidedadditional
ﬂexibilitybyallowingforadesignertodeterminehowmuch
information can be transferred from the source task to the
showthatthetargettasktimewassigniﬁcantlyreducedwith target task. In addition, our KG-RL control framework pro-
knowledge transfer. vides a principled way to solving transfer learning problems
that involve the same states and controls. Thus, it can be
IV. DISCUSSIONSANDCONCLUSIONS integrated with other TD-based methods such as SARSA
and value iteration, as well as their deep learning variants,
We developed a new KG-QL framework to integrate
to name a few.
and transfer knowledge from AB subjects to OpenSim
In this work, we demonstrated the feasibility of KG-QL
simulated amputee subjects with a common goal of opti-
based control to automatically tune robotic prostheses. To
mizing impedance parameters for robotic knee prosthesis.
validatethefullpotentialofourapproach,weneedtofurther
The knowledge for transfer can be obtained ofﬂine using
evaluate it with transfers between AB subjects as well as
historical data, aka, from AB subjects in our study, to
amputees.Also,thenormative(target)kneekinematicsbeing
facilitateonlinereinforcementlearningforamputeesubjects.
used in this paper may not be an ideal design goal for the
OurOpenSimsimulationresultsvalidatedthisnewapproach
RL agent. We will explore other design goals that better
and showed that our new scheme can help restore near-
quantifythehuman-prosthesisgaitperformance,suchasgait
normal knee kinematics, in a time and sample-efﬁcient
symmetry index and stability margin.
manner compared to the naive learner. Our results suggested
that the proposed KG-QL controller is a promising new
framework when performing the cross-subject learning for REFERENCES
the robotic knee prosthesis with human in the loop. Our
[1] S. K. Au, J. Weber, and H. Herr, “Powered ankle-foot prosthesis
demonstrated effectiveness of transfer learning from AB improveswalkingmetaboliceconomy,”IEEETrans.Robot.,2009.
subjects to OpenSim simulated amputee subject may be [2] S. Au, M. Berniker, and H. Herr, “Powered ankle-foot prosthesis to
assistlevel-groundandstair-descentgaits,”NeuralNetworks,vol.21,
due to the fundamental principle guiding human gaiting.
no.4,pp.654–666,May2008.
Or in other words, the underlying physiology and physics [3] A.H.ShultzandM.Goldfarb,“AUniﬁedControllerforWalkingon
represented in the relationships from impedance parameters even and Uneven Terrain with a Powered Ankle Prosthesis,” IEEE
Trans.NeuralSyst.Rehabil.Eng.,2018.
in the FS-IC to knee joint torque and further to locomotion,
[4] D.Quintero,D.J.Villarreal,D.J.Lambert,S.Kapp,andR.D.Gregg,
should be preserved in both AB subjects and the OpenSim “Continuous-Phase Control of a Powered Knee-Ankle Prosthesis:
759
Authorized licensed use limited to: Heriot-Watt University. Downloaded on September 21,2020 at 04:57:13 UTC from IEEE Xplore.  Restrictions apply. Amputee Experiments Across Speeds and Inclines,” IEEE Trans.
Robot.,2018.
[5] B. E. Lawson, H. A. Varol, and M. Goldfarb, “Standing stability
enhancement with an intelligent powered transfemoral prosthesis,”
IEEETrans.Biomed.Eng.,2011.
[6] F.Sup,A.Bohara,andM.Goldfarb,“DesignandControlofaPowered
TransfemoralProsthesis.”Int.J.Rob.Res.,vol.27,no.2,pp.263–273,
Feb2008.
[7] M. Liu, F. Zhang, P. Datseris, and H. H. Huang, “Improving Finite
State Impedance Control of Active-Transfemoral Prosthesis Using
Dempster-ShaferBasedStateTransitionRules,”J.Intell.Robot.Syst.
TheoryAppl.,vol.76,no.3-4,pp.461–474,Dec2014.
[8] A.Nair,B.McGrew,M.Andrychowicz,W.Zaremba,andP.Abbeel,
“Overcoming Exploration in Reinforcement Learning with Demon-
strations,”inProc.-IEEEInt.Conf.Robot.Autom.,2018.
[9] S.ChoiandJ.Kim,“Trajectory-basedDeepLatentPolicyGradientfor
Learning Locomotion Behaviors,” in IEEE Int. Conf. Robot. Autom.,
2019.
[10] Y. Wen, J. Si, X. Gao, S. Huang, and H. H. Huang, “A New Pow-
ered Lower Limb Prosthesis Control Framework Based on Adaptive
Dynamic Programming,” IEEE Trans. Neural Networks Learn. Syst.,
vol.28,no.9,pp.2215–2220,Sep2017.
[11] Y.Wen,J.Si,A.Brandt,X.Gao,andH.Huang,“OnlineReinforce-
ment Learning Control for the Personalization of a Robotic Knee
Prosthesis,”IEEETrans.Cybern.,pp.1–11,Jan2019.
[12] M.Li,X.Gao,W.Yue,S.Jennie,andH.He,“OfﬂinePolicyIteration
Based Reinforcement Learning Controller for Online Robotic Knee
Prosthesis Parameter Tuning,” in Proc. - IEEE Int. Conf. Robot.
Autom.,2019.
[13] X.Gao,Y.Wen,M.Li,J.Si,andH.Huang,“RoboticKneeParameter
Tuning Using Approximate Policy Iteration,” in Cogn. Syst. Signal
Process., F. Sun, H. Liu, and D. Hu, Eds. Singapore: Springer,
Singapore,Nov2019,pp.554–563.
[14] M. E. Taylor and P. Stone, “Transfer Learning for Reinforcement
Learning Domains : A Survey,” J. Mach. Learn. Res., vol. 10, pp.
1633–1685,2009.
[15] A.Barreto,W.Dabney,R.Munos,J.J.Hunt,T.Schaul,H.VanHas-
selt, and D. Silver, “Successor features for transfer in reinforcement
learning,”inAdv.NeuralInf.Process.Syst.,2017.
[16] M.AsadiandM.Huber,“Effectivecontrolknowledgetransferthrough
learning skill and representation hierarchies,” in IJCAI Int. Jt. Conf.
Artif.Intell.,2007.
[17] K. Kansky, T. Silver, D. A. Mély, M. Eldawy, M. Lázaro-Gredilla,
X.Lou,N.Dorfman,S.Sidor,S.Phoenix,andD.George,“Schema
networks:Zero-shottransferwithagenerativecausalmodelofphysics
intuitive,”in34thInt.Conf.Mach.Learn.ICML2017,2017.
[18] G. Joshi and G. Chowdhary, “Cross-Domain Transfer in Reinforce-
ment Learning Using Target Apprentice,” in Proc. - IEEE Int. Conf.
Robot.Autom.,2018.
[19] M.P.Kadaba,H.K.Ramakrishnan,andM.E.Wootten,“Measurement
oflowerextremitykinematicsduringlevelwalking,”J.Orthop.Res.,
vol.8,no.3,pp.383–392,May1990.
[20] J. Si and Y. T. Wang, “Online learning control by association and
reinforcement,”IEEETrans.NeuralNetworks,vol.12,no.2,pp.264–
76,Mar.2001.
[21] R. Bellman, Dynamic programming. Princeton University Press,
1957.
[22] R.S.SuttonandA.G.Barto,Reinforcementlearning:anintroduction,
2nded. Cambridge,MA:MITPress,2018.
[23] J. Si, A. G. Barto, W. B. Powell, and D. C. Wunsch, Handbook of
learningandapproximatedynamicprogramming. IEEEPress,2004.
[24] Jennifer Hicks, “From the Ground Up: Building a Passive
Dynamic Walker Model,” 2014. [Online]. Available: https:
//simtk-conﬂuence.stanford.edu:8443/display/OpenSim33/From+the+
Ground+Up{%}3A+Building+a+Passive+Dynamic+Walker+Model
760
Authorized licensed use limited to: Heriot-Watt University. Downloaded on September 21,2020 at 04:57:13 UTC from IEEE Xplore.  Restrictions apply. 