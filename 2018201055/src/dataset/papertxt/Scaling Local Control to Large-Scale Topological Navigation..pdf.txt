2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
Learning View and Target Invariant Visual Servoing for Navigation
Yimeng Li and Jana Kosˇecka
Abstract—The advances in deep reinforcement learning re-
centlyrevivedinterestindata-drivenlearningbasedapproaches
to navigation. In this paper we propose to learn viewpoint
invariant and target invariant visual servoing for local mobile
robot navigation; given an initial view and the goal view or
an image of a target, we train deep convolutional network
controller to reach the desired goal. We present a new archi-
tecture for this task which rests on the ability of establishing
correspondences between the initial and goal view and novel
rewardstructuremotivatedbythetraditionalfeedbackcontrol
error. The advantage of the proposed model is that it does not
require calibration and depth information and achieves robust
visualservoinginavarietyofenvironmentsandtargetswithout
any parameter ﬁne tuning. We present comprehensive evalua-
tion of the approach and comparison with other deep learning
architectures as well as classical visual servoing methods in
visually realistic simulation environment [1]. The presented
Fig.1. Visualizationoftheproposedmethod.Thethreeimagesonthetop
model overcomes the brittleness of classical visual servoing
are current view, target view and Q-value table. Brightest color shows up
based methods and achieves signiﬁcantly higher generalization intheleftblockoftheQ-valuetableindicatestakingaturning-rightaction.
capability compared to the previous learning approaches. Thethreeimagesatbottomarethevisualizationsofdensecorrespondence
maponx/y-axisandthefeaturemapoutputbytheperceptionmodule.
I. INTRODUCTION
Traditional approaches to navigation in novel environ-
ments often required solution to many components, includ- jointly with the perception component. The contributions of
ing mapping, motion planning and low level control. The this work are as follows:
reliance of motion planning on high quality geometric maps (i) We show how general image correspondence map can
and trajectory following on perfect localization, resulted in be distilled by deep convolutional networks and trained in
fragmented and brittle solutions which had to be ﬁne-tuned an end-to-end manner to learn a policy for target reach-
for particular environments. In contrast to these methods ing and goal view reaching task; (ii) We design a novel
biological systems have more ﬂexible representations of dense reward structure and train the model using deep
environments and control policies which enable them to reinforcement learning (DRL) framework; (iii) We present
robustly navigate in previously unseen environments. These a comprehensive comparison of our model with alternative
observations and the emergence of effective data driven deep CNN architectures and training approaches proposed
techniques for learning control policies directly from obser- for this task as well as classical image based visual servoing
vations recently spurred large body of research in learning techniques, showing superior performance of our approach.
navigation. In this paper we propose a learning approach to Comprehensive evaluation is carried out using visually real-
visual servoing for mobile robots in indoors environments. istic simulated household environments [1] with variety of
In the broader context of navigation task, visual servoing targets and goals, demonstrating good generalization ability
discussed here can be viewed as local navigation skill for of the approach.
viewbasednavigation,wherethegoalistoreachthedesired
view or navigate towards the target of interest. While there II. RELATEDWORK
is a large body of work on classical approaches to visual
Here we review the related work focusing on local navi-
servoing, they rely on the extraction, tracking and matching
gation skills most relevant to our approach. A class of local
ofasetofvisualfeatureswhicharedifﬁcultandbrittletasks.
navigation methods assumes that the goal is speciﬁed in
Relatedattemptstoovercomethesedifﬁcultiesusinglearning
an agent’s local coordinate frame, often assuming perfect
based approaches have been recently considered in [2], with
localization. Authors in [3] learn a local navigation policy
the focus on improving the pose estimation and perception
using deep reinforcement learning and Value iteration net-
component for 6-DOF pose based visual servoing in table
works, [4] learn how to predict the next waypoint given
top environments. We study the problem in mobile robot
a long-range goal and use traditional optimal control to
navigation setting and propose to learn the control policy
compute the desired trajectory given local ego-centric map
of the environment. Kumar et al. [5] consider visual-teach-
George Mason University, Computer Science Department [yli44,
kosecka]@gmu.edu and-repeat approach where the environment is represented
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 658
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 15:56:02 UTC from IEEE Xplore.  Restrictions apply. in terms of trajectories experienced in exploration. Efﬁcient observation I . State s is characterized by the pose of the
0
retrieval of the views along with the actions enables novel agent x,y coordinates and the heading angle θ in a world
traversals of the environment. reference frame unknown to the agent. The observation I
Locomotion policies proposed in [6] and [7] use siamese can be either an RGB image or an RGB-D image including
network to extract feature maps from the input images and depthinformationasadditionalchannel.Atargetviewimage
estimate the discrete motion. Pathak et al. [8] combines I taken at goal position s is provided to the agent. I is
g g g
forward dynamics with inverse dynamics model to solve assumed to have some overlap with the initial ﬁeld of view
the bi-modal ego-motion estimation problem. Savinov et I .Thegoalfortheagentistolearnapolicyforreachingthe
0
al. [9] stack the start and goal image up as an input to goalstates byexecutingasequenceofactionsa.Theaction
g
theirlocomotionnetwork,trainingthemodelinasupervised space A can be either continuous or discrete depending on
way. Disadvantage of these models is poor generalization the control method.
capabilityandverydensesamplingoftheintermediateviews We approach this problem using learned data-driven strat-
to represent the trajectories. egyandcompareanddiscusstheadvantagesofthisapproach
Related problem of semantic target driven navigation was to classical image based visual servoing [20] and learned
considered by [10], [11] where object is speciﬁed as an visual servoing approaches [2].
imageorasansemanticcategory[12]consideringmid-range Classical visual servoing computes correspondences at
navigation tasks and more loose deﬁnition of goal success. fewselectedpointsbetweencurrentobservationI andtarget
t
Similarly to us many of the learning based methods train view I followed by analytic derivation of the feedback
g
theirmodelsinsimulatedenvironments,followedbytransfer control law for continuous velocities at each each step.
or adaptation of the learned policies to real robots. Authors LearningmethodlearnsamappingfromI andI toactions
t g
in [13] and [3] successfully transfer their model trained on in reinforcement learning framework and predicts at each
GibsonEnv [1] to the real world. state a discrete action a moving towards the goal location.
Besides going to a semantic goal, people have also ex- The action space includes seven actions, which are moving
− − −
plored other goal representations; Amini et al. [14] use a forward 0.1m in 7 orientations [ π, π, π,0, π,π,π].
4 6 15 15 6 4
local topological map and Codevilla et al. [15] steer a toy
truck through high-level map related commands. We will start our discussion with the geometry of the
There is also a large body of work on classical visual problem, discussing the brittleness and drawbacks of the
servoing methods. More recent discussion can be found geometric methods to motivate our learning based solution.
in [16]. Image based visual servoing has been adapted
A. Classical Visual Servoing
for short range mobile robots navigation in [17] and pose
based visual servoing task was studied in [2] where 6-DOF Visual servoing control has been developed in early 90’s
pose regression is estimated by deep networks followed by with the goal of increasing the accuracy of the control of
traditional control. robot end effector by using visual feedback control. Two
The work closest to ours is Sadeghi et al. [10], [18] main classes of systems are position based and image based
and Yexin et al. [19]. They both study the target reaching visual servo. While the position based visual servo strives to
problem and use DRL to train the model in simulation. ﬁrst estimate the relative pose between the initial and target
In [10] the policy is guided by heatmap obtained through view, the image based visual servo derives the error signal
the correlation in the feature spaces of object and current directly from measurements. The image features f can vary
robot’s view. The attention mask obtained from semantic dramatically (e.g. points or lines), but the effective relation-
segmentation is computed by [19] and used as an input for shipsbetweenrobot’sposeanditsvelocitiesischaracterized
the policy learning. Both approaches focus on the problem by feature Jacobian J (s) which can be derived analytically
f
of reaching semantic target and rest on the availability of
f˙=J (s)s˙ (1)
powerful architectures and representations pre-trained for f
objectdetectionandsemanticsegmentation.Furthermorethe where s˙ = [ν ,ν ,ν ,ω ,ω ,ω ]T. J (s) is also referred
x y z x y z f
attentionmechanismdoesnotlenditselftohomingscenarios to as interaction matrix. For image points the relationship
in navigation, where the goal of the agent is speciﬁed as the between image velocities and motion of the end effector is
goal view and may not contain interesting objects. characterized by the well known optical ﬂow equation [16].
For a holonomic mobile robot moving on the xz-plane, the
III. APPROACH
vel(cid:20)ocity(cid:21)spa(cid:34)ce is three-dimensi(cid:16)onal [νx,νz,(cid:17)ωy(cid:35)]T(cid:34) and(cid:35)the
Consider an agent that operates in a novel indoor envi- optical ﬂow equation reduces to:
ronment. Instead of navigating with the help of a pre-built
metricmap,ouragentisprovidedwithasetofimagestaken u˙ −λ (u−u0) − λ+ (u−u0)2 νx
= Z Z λ ν
at different locations, forming a view based topological map v˙ 0 (v−v0) −(u−u0)(v−v0) ωz
of the environment [9]. Our goal is to train the agent to Z λ y (2)
do short-range navigation to reach desired location or target where u˙ and v˙ is the optical ﬂow, λ is the focal length
of interest in the ﬁeld of view of the agent. Let’s assume for the camera, Z is the depth of the feature point and
that the agent starts at some random state s and obtains an (u ,v ) is the image principal point. In practice, if N >=2
0 0 0
659
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 15:56:02 UTC from IEEE Xplore.  Restrictions apply. correspondences are detected, we can stack up rows J to
f
get the interaction matrix for all features J. The control law
can then be obtained by computing the pseudo-inverse of
∗ ∗−
Eq. (2) for the desired camera motion s˙ =J+(f f).
f
For image based visual servoing (IBVS), in addition
to challenges of detection, matching and tracking of geo-
metric features, there are additional well-known difﬁculties
for visual navigation tasks [16]. It is possible to have an
inconsistent set of feature velocities such that no possible
camera motion will result in the required image motion.
For example, nearly collinear features will cause very small Fig.2. LearnedVisualServoingArchitecture.Visualrepresentationisthe
camera motion. The performance of the model relies on the dense correspondence extracted from two input images It,Ig. Perception
module extracts features from the visual representation. Action module
featuredepthZ inEquation(2).Someapproachesconsidered predictstheQ-valuesforeachactionbasedonthefeatureinput.
using a constant depth for all the feature points or depth
estimates from motion is also helpful. The motion based
methods often failed when camera motion got smaller. For mapisﬂattenedintoa256-dimensionalvectorfollowedbya
the conﬁgurations where the desired orientation is notably fully-connected layer which outputs Q-values for all actions
different from the current orientation and differential drive in our feedback policy (see Figure 1). Note that our model’s
robots with non-holonomic constraints, the overlap between architectureishighlyﬂexible.Givendifferentkindsofvisual
current view and goal view often becomes too small result- input, we can vary the design of the perception module and
ing in the failure of the correspondence computation. The leave the action module untouched. We also tried to insert
proposed learning based approach helps to overcome the an LSTM layer in front of the FC-layer to learn a recurrent
brittleness of the traditional methods. policy. We didn’t get a performance boost compared to the
reactive policy. All the experiment results mentioned in the
B. Learned Visual Servoing
paper are achieved without using an LSTM layer.
We formulate the problem of learning visual servoing InputVisualRepresentations:Motivatedbyclassicalvisual
(LVS) in the reinforcement learning framework. An agent servoing, the input to our perception module is the map of
starts at a state s in the environment and the goal is to dense correspondences. In the ﬁrst stage we compute the
0
navigate to the given target view. At each s , the agent dense correspondences (shown in Figure 2) using ground
t
receives an observation I . Given the observation, the agent truth information of known pose and camera depth. Learned
t
takes action a and receives a reward r(s ,a ). We are and ﬁne-tuned dense correspondences as in [22] can be later
t t t
interested in learning control policy a =φ (ψ(I ,I )) and incorporatedinourmodel.Theinputisatwo-channelimage
t θ t g
representation of current view and goal view, that predicts where each channel represents the relative offset on x/y-
the action advancing the agent towards the goal, where axis from each point in the current view to a corresponding
φ is the action module and ψ is the perception module. point in the target view. We also observe that applying
θ
In traditional Q-learning approach, the goal is to learn a image smoothing to the dense correspondence map during
Q-function Q(s ,a ) quantifying the goodness of different testing stage improves the model’s stability, as smoothed
t t
actions in each state. By following a policy π , the agent correspondencemaphasfewerdepthdiscontinuitiesthatcan
θ
produces a sequence of Q-states (s ,a )T after T steps. negativelyaffectingtheﬁnalprediction.Inexperiments(Exp
t t t=0
Our goal is to train a policy π that maximizes the total D) we corrupt the high-quality dense correspondences in
θ
reward received through the trajectory. various ways to see the effect of the noise on the ﬁnal
Deep Q Network: For large state spaces the Q function robustness of our model.
cannot be computed exactly. We use a Deep Q Network Reward: For the task of driving to a target view, the robot
(DQN) [21]toapproximatethevalueofaQ-state,Q(s ,a ), traverses a trajectory so as to minimize the error between
t t
the action-value of unobserved pose s of the mobile agent. startviewandgoalview.Sinceviewdifferencebetweentwo
t
Our DQN architecture has two components: a perception differentlocationswithsimilarorientationisrathersmall,the
module and an action module as shown in Figure 2. The imageerrorisaweaksignalforourlearningtask.Tomeasure
perception module consists of four convolutional layers, all the ’progress’ to the goal during navigation, we compute
using ReLU activation and batch normalization for efﬁcient d distance between poses (x ,y ,θ ) and (x ,y ,θ ).
polar t t t g g g
training.Theﬁrstthreeconvolutionallayershavekernelsize d isexpressedinthecoordinateframeofthegoal,where
× × × × × × polar
5 5 16,3 3 32,3 3 64, strides of 2,1,1 and ρisthedistancebetweenrobot’scenterandthegoalposition,
each of them is followed by a max pooling layer to reduce αistheanglebetweenrobot’sreferenceframeandthevector
spatial support of the feature map. The last CNN layer is connecting center of the robot with the goal position and β
× × ×
a 1 1 convolutional layer which outputs a 16 16 1- is the angle difference between current orientation of the
sizedfeaturemap.Asinputweexamineseveralintermediate robot and heading direction. The distance to the goal d is a
representations of RGB views before passing them into the weighted sum of these position and angular distances. Note
perception module. For the action component the feature thatwhenatthegoalallρ=α=β =0.Thedistancetothe
660
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 15:56:02 UTC from IEEE Xplore.  Restrictions apply. goal is related to(cid:112)feedback control law error for differential
drive robots outlined in [16].
− −
ρ= (x x )2+(y y )2
g t− g− t −
α=arctan(y y ,x x ) θ
− g t −g t− t (3)
β =θ arctan(y y ,x x )
g g t g t
d =ρ+λ ρ(α,0)+λ ρ(β,0)
polar α β
d distanceenforcestheagenttotraveltowardsthetarget
polar
and rotate to align with target pose when the agent is in the
vicinity of the target. In practice, λ and λ are both set
α β
to 0.2. We also consider d distance [23] for comparison
pose
and use d for clarity to denote the distance below. Fig. 3. Occupancy map of the ’Dardan’ scene in the testing dataset. We
The reward is designed to encourage the robot to move sample 7 starting viewpoints in the environment represented by the red
triangles.Thebluetrianglesarethetargetposes.Theirdistancetothestart
in the directio(cid:18)n to mi(cid:18)nimize th(cid:19)e distanc(cid:18)e to the (cid:19)go(cid:19)al. The
viewpointandorientationvaryfromeachother.
reward function is,
d − − d
R=max 0,min t 1,1 min t ,1 (4)
d d
init init
where d is the initial distance between the agent and the
init
goal and d is the current distance of the agent to the goal.
t
Therightterminthemaxfunctionmeasurestheprogressof
the agent towards the goal after taking action a .
t
Note that we are having a dense reward setup as the
environment is simulated. We leave the design of sparse
rewards to future work when we train an agent in real-world
scenes.
IV. EXPERIMENTS
Datasets: All the experiments are completed in Gibso-
nEnv [1]. Gibson environment contains visually realistic
reconstructions of indoors scenes, with varying appearances
Fig.4. Visualizationsofstartviewsandtargetimagesfromdifferenttest
andlayouts.Werandomlyselect16indoorscenesastraining scenes.Firstandfourthcolumnshowsthestartviewswhilesecond,third,
data and 6 others for testing. During training, we randomly ﬁfthandsixthcolumnshowsthetargetviews.
sample the starting robot poses. Examples of sampling on
one test scene is shown by Figure 3. Sampled initial views
A. Intermediate Representations for LVS
and target views are shown by Figure 4. The target views
are chosen at certain distance and orientations away from Wecomparetheperformanceofdifferentperceptionmod-
the starting point. The distance ranges from 0.5 to 4.0m. ule architectures while using the same reward structure and
The orientation and target pose heading angle ranges from action module in DQN.
−
π/4 to π/4 with respect to the initial pose. Usually it will LocomotionNet [9]. Input to the network is a triplet of
taketheagentfrom5to35stepstoreachthegoal.Wemake RGB images comprising of previous observation, current
×
sure that the target view and start view has at least 16 16 observation and target image. The input images are stacked
pixel overlapping area. During testing, for each scene, we up into a 9-channel image and put through a sequence
manually select approximately 10 starting poses where there of convolutional layers to extract features. Convolving the
is enough open space in front of them. The target images current view with previous view will provide the knowledge
are chosen in a similar way as training data. We evaluate of the previous action. Convolving the current view with
different approaches by computing the success rate of visual target image indicates the signal to the goal.
servoing results. The success criteria is that the robot pose FlowNet. We combine current view and target image into a
is within 0.2m to the target pose during navigation. pair and use FlowNet [24] to compute the optical ﬂow. The
Training Details: We use Actor-Critic architecture [21] and optical ﬂow is represented as a two-channel ﬂow map that
replay memory when training our model. Each minibatch is an input of our perception module.
consists of losses over 128 random state-action tuples sam- SiameseNet.Theperceptionmodulefollowsthearchitecture
pled from replay memory. Each tuple contains the state proposed by [6]. It is modeled as a siamese network for
representation, the action and the reward after taking the computingfeaturemapsrespectivelyfrominputimages.The
−
action. We use RMSprop Optimizer, learning rate of 10 5. current observation and target image are used as inputs. The
Themodelisoptimizedfor50000iterationsonasingleGPU, output feature maps are ﬂattened into a vector and then
which takes 6 hours. concatenated into a large feature vector as the ﬁnal output.
661
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 15:56:02 UTC from IEEE Xplore.  Restrictions apply. Fig.5. Examplesofarobotdrivingtoaview(ﬁrstrow)anddrivingtoanobject(secondrow).Theﬁrstcolumnshowsthetargetviewandthetarget
object.Thelastcolumnistheoverviewofthetwotrials.Betweenthemaretheobservationsalongthedriving.
TABLEII
InverseDynamics.Weadoptthearchitectureof [8]tolearn
REWARDSTRUCTURE
the network weights and train the model through supervised
learning. Then we ﬁx the weights of the trained model and
Reward DistanceMetric SuccessRate
use it as the perception module.
Correlation Map [25]. The perception module computes DistMinimize dpolar 83.5%
Progress d 73.2%
the correlation between patches from two feature maps and Progress dppoolsaer 54.9%
outputsthecorrelationmapastheﬁnalvisualrepresentation.
The feature map extraction architecture is similar to our
perception module but having two more pooling layers. We
× × Table II shows the results. Using both d metric and
computetheco×rrela×tion×betweentwo8 8 64featuremaps using DistMinimize reward boosts the spuocsceess rate for
and obtain a 8 8 8 8 correlation map. more than 10%. The advantage of DistMinimize over
We separate the test cases into short-range navigation
Progressisthatinthelatter,therobotwillreceiveapositive
where the target location is within 10 steps starting from
rewardeventhoughthedistancetothegoalisnotshortened.
the initial pose and longer-range navigation where the agent
This makes the agent hesitant to move towards the goal.
needs at least 15 steps to reach the goal. It takes the robot
Furthermore, using d distance forces the robot to head
more than 25 steps to reach the furthest goal. pose
towardsthegoalandavoidstheriskoflosingcorrespondence
(overlap)tothetargetview.Thisishelpfulforsomedifﬁcult
TABLEI
test case as shown in Figure 6.
VISUALREPRESENTATION
C. Classical Visual Servoing Evaluation
Approach Short/Long-Range
Here we evaluate IBVS on a non-holonomic robot from
LocomotionNet[9] 26.5%/18.2%
two aspects: image point features and depth data. We ex-
SiameseNet[6] 14.3%/28.8%
CorrespondenceMap 85.7%/80.3% periment with three point feature variations: ground-truth
SmoothedCorrespondenceMap 90.9%/86.9% sparse correspondence (GtCorresp), learned sparse corre-
spondence (LearnedCorresp) and SIFT. For ground-truth
Table I shows the experiment results. We didn’t get correspondence, we deliberately select 4 correspondences
with the largest offset, as these are the feature points most
satisfying results for some of the approaches so their results
indicative of the motion between images. We don’t evaluate
are missing from the table. The optical ﬂow detected by
dense correspondence since the interaction matrix inverse
FlowNet is not very helpful because FlowNet is trained to
computation is time-consuming and it is more likely that
learn small displacement while our displacement spans over
the feature velocities are inconsistent when you have a large
80 pixels in some test cases. In the following experiment,
number of features. We take an off-shelf learned correspon-
we use smoothed correspondence map as the default input
dence[26]methodasinput.Falselydetectedcorrespondence
when we evaluate our LVS model.
are removed through geometric veriﬁcation, both in case
B. Reward
of learned correspondences and SIFT features. We try four
Here we hold up the input visual representation to be the depth variations: ground-truth depth (GtDepth), constant
dense correspondence map, but vary the reward structure. depth (ConstantDepth), noisy depth (NoisyDepth) and no
The dense correspondence which we denote as ground-truth depth. Ground-truth depth image is taken directly form the
correspondence is computed for each pixel in the common simulated environment. Constant depth is setup to be 4.0 as
ﬁeldofviewusingtheavailabledepthinformation.Wecom- all the target viewpoints are within 4 meters. Noisy depth is
pare the performance of using two distance metric, d computed by adding a gaussian noise with µ = 0,σ = 0.5
polar
distance and d distance [23], and two reward setups, to the ground-truth depth image. For the no depth setup,
pose
distance minimization reward DistMinimize used by our we only compute the angular velocity ω and use constant
y
model and Sadeghi’s progressive reward Progress [10]. velocity ν =0.1.
662
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 15:56:02 UTC from IEEE Xplore.  Restrictions apply. TABLEIII
DIFFERENTIBVSSETUPS
Feature+Depth Textured Nontextured Corridor
GtCorr+GtDepth 80.4% 72.2% 80.4%
LearnedCorr+GtDepth 40.0% 23.2% 32.9%
SIFT+GtDepth 41.3% 18.5% 24.7%
GtCorr+noisyDepth 52.0% 42.5% 46.8%
GtCorr+constantDepth 57.0% 56.2% 48.3%
GtCorr+noDepth 32.8% 29.9% 28.2%
Fig.7. Comparisonoftheperformanceofthemodelunderdifferentnoise
GtDenseCorr+noDepth 87.6% 88.8% 84.8%
settings. Left image demonstrates the Gaussian noise on correspondence
offset. Right image demonstrates the uniform noise on correspondence
coverage
the performance is still above 75% when 50% of features
is missing. The smoothing reduces the sparsity and has
favorable effect on the resulting policy. This demonstrates
that our model is robust to several types of correspondence
Fig.6. Visualizationofatestcasewherecorrespondenceismissingduring errors.
navigation.Giventhestartviewandgoalview,therobotshouldmoveright
thenturnleft.UsingIBVS,robotstopsduetolossofFOVoverlap.Using
LVS,robotisabletoreachthegoal.
E. Target Object Servoing
Even though our model is not trained on object targets, it
Table III, compares the success rate of different IBVS
can be easily adapted to navigate towards semantic targets.
methods under various environments. The results show that
To identify the target of interest, we run an object detec-
classical visual servoing’s performance is highly correlated
tor [27] on the images collected by randomly driving the
with the quality of feature points and depth data. Using
robot in the environment and crop the detected object of
carefully selected ground-truth correspondences boosts the
interest. The crop is then used as the target image, followed
model’s performance by more than 40% than using detected by computation of the correspondence map between current
features. Using constant depth instead of ground-truth depth
view and target view. The correspondence map is input to
degradestheperformanceby20%.Usingnoisydepthresults the model and we drive the robot by following the action
in 28% drop in success rate demonstrating that IBVS is predictions.Figure5givesoutoneexampleofarobotdriving
sensitive to the depth changes. We also evaluate our LVS
to an object. This suggests that the proposed method might
method (GtDenseCorr + noDepth) on the same test environ-
be suitable for semantic target navigation considered previ-
ment. It outperforms IBVS by at least 8% in textured and ously [10]. We don’t do large-scale quantitative evaluations
nontextured environments especially in the following case:
of our model’s performance on the driving-to-object task as
when the target location is on the opposite of the robot’s
the object locations are not labeled in the environment.
orientation, the robot might lose the common area between
current observation and the goal image. In another word,
thereisnocorrespondencebetweenI andI .Arobotusing V. CONCLUSION
t g
IBVS will stop under this condition while it will continue
move forward using LVS based on its learning experience. Wedemonstratedlearningviewandtargetinvariantvisual
Figure 6 demonstrates this hard case. servoing for local navigation. We examine the performance
of a variety of input representations and train the model
D. Noisy Correspondences
using deep Q-learning framework. Both the input of our
To evaluate the robustness of our trained model we add model and the reward structure are motivated by classical
gaussiannoisetothedisplacementateachpixelinthecorre- visualservoingmethods.Theabilitytotrainthemodelinan
spondence map. The variance σ of the gaussian distribution end-to-endfashionsigniﬁcantlyimprovestherobustnessand
controls the amount of offset. We also vary the density of performance of the approach compared to classical visual
the detected features using uniform distribution. Parameter servoing methods, where a small set of ﬁxed features is
Coverage controls the probability of a pixel having a selected for the task. We present comprehensive comparison
correspondence. Before we input the noisy correspondence of the model to alternative representations proposed in the
map to the model, we convolve the map with averaging literature.Whilethecurrentmodelassumesdensecorrespon-
smoothing ﬁlter. dence, it is robust to errors in the correspondence maps. In
Figure 7 shows the results. Our model is robust to large the future work we plan to incorporate depth information
offset errors and still achieves 80% success rate when the explicitly, carry our more extensive experiments with target
offset deviates from the ground-truth for up to 32 pixels. objects and presence of obstacles and validate our approach
For the missed correspondence noise, it is surprising that on mobile robot platform.
663
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 15:56:02 UTC from IEEE Xplore.  Restrictions apply. REFERENCES [25] I.Rocco,R.Arandjelovic´,andJ.Sivic,“End-to-endweakly-supervised
semantic alignment,” in Proceedings of the IEEE Conference on
[1] F.Xia,A.R.Zamir,Z.He,A.Sax,J.Malik,andS.Savarese,“Gibson ComputerVisionandPatternRecognition,2018,pp.6917–6925.
env: Real-world perception for embodied agents,” in Proceedings of
[26] D. DeTone, T. Malisiewicz, and A. Rabinovich, “Superpoint: Self-
the IEEE Conference on Computer Vision and Pattern Recognition, supervised interest point detection and description,” in Proceedings
2018,pp.9068–9079. oftheIEEEConferenceonComputerVisionandPatternRecognition
[2] Q. Bateux, E. Marchand, J. Leitner, F. Chaumette, and P. Corke, Workshops,2018,pp.224–236.
“Visual servoing from deep neural networks,” arXiv preprint
[27] F. Massa and R. Girshick, “maskrcnn-benchmark: Fast,
arXiv:1705.08940,2017.
modular reference implementation of Instance Segmen-
[3] S.Gupta,J.Davidson,S.Levine,R.Sukthankar,andJ.Malik,“Cog-
tation and Object Detection algorithms in PyTorch,”
nitivemappingandplanningforvisualnavigation,”inProceedingsof
https://github.com/facebookresearch/maskrcnn-benchmark,2018.
the IEEE Conference on Computer Vision and Pattern Recognition,
2017,pp.2616–2625.
[4] S.Bansal,V.Tolani,S.Gupta,J.Malik,andC.Tomlin,“Combining
optimal control and learning for visual navigation in novel environ-
ments,”arXivpreprintarXiv:1903.02531,2019.
[5] A. Kumar, S. Gupta, D. Fouhey, S. Levine, and J. Malik, “Visual
memoryforrobustpathfollowing,”inAdvancesinNeuralInformation
ProcessingSystems,2018,pp.765–774.
[6] P. Agrawal, J. Carreira, and J. Malik, “Learning to see by moving,”
in Proceedings of the IEEE International Conference on Computer
Vision,2015,pp.37–45.
[7] D. Jayaraman and K. Grauman, “Learning image representations
tied to egomotion from unlabeled video,” International Journal of
ComputerVision,vol.125,no.1-3,pp.136–161,2017.
[8] D.Pathak,P.Mahmoudieh,G.Luo,P.Agrawal,D.Chen,Y.Shentu,
E.Shelhamer,J.Malik,A.A.Efros,andT.Darrell,“Zero-shotvisual
imitation,”inProceedingsoftheIEEEConferenceonComputerVision
andPatternRecognitionWorkshops,2018,pp.2050–2053.
[9] N.Savinov,A.Dosovitskiy,andV.Koltun,“Semi-parametrictopolog-
icalmemoryfornavigation,”arXivpreprintarXiv:1803.00653,2018.
[10] F.Sadeghi,“Divis:Domaininvariantvisualservoingforcollision-free
goalreaching,”arXivpreprintarXiv:1902.05947,2019.
[11] Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei, and
A. Farhadi, “Target-driven visual navigation in indoor scenes using
deepreinforcementlearning,”in2017IEEEinternationalconference
onroboticsandautomation(ICRA). IEEE,2017,pp.3357–3364.
[12] A. Mousavian, A. Toshev, M. Fisˇer, J. Kosˇecka´, A. Wahid, and
J.Davidson,“Visualrepresentationsforsemantictargetdrivennaviga-
tion,”in2019InternationalConferenceonRoboticsandAutomation
(ICRA). IEEE,2019,pp.8846–8852.
[13] X. Meng, N. Ratliff, Y. Xiang, and D. Fox, “Neural au-
tonomous navigation with riemannian motion policy,” arXiv preprint
arXiv:1904.01762,2019.
[14] A.Amini,G.Rosman,S.Karaman,andD.Rus,“Variationalend-to-
endnavigationandlocalization,”in2019InternationalConferenceon
RoboticsandAutomation(ICRA). IEEE,2019,pp.8958–8964.
[15] F. Codevilla, M. Miiller, A. Lo´pez, V. Koltun, and A. Dosovitskiy,
“End-to-enddrivingviaconditionalimitationlearning,”in2018IEEE
InternationalConferenceonRoboticsandAutomation(ICRA). IEEE,
2018,pp.1–9.
[16] P. Corke, Robotics, vision and control: fundamental algorithms in
(cid:13)
MATLABR second,completelyrevised. Springer,2017,vol.118.
[17] S. R. Bista, “Indoor navigation of mobile robots based on visual
memoryandimage-basedvisualservoing,”Ph.D.dissertation,2016.
[18] F. Sadeghi, A. Toshev, E. Jang, and S. Levine, “Sim2real viewpoint
invariantvisualservoingbyrecurrentcontrol,”inProceedingsofthe
IEEEConferenceonComputerVisionandPatternRecognition,2018,
pp.4691–4699.
[19] X. Ye, Z. Lin, J.-Y. Lee, J. Zhang, S. Zheng, and Y. Yang, “Gaple:
Generalizableapproachingpolicylearningforroboticobjectsearching
inindoorenvironment,”IEEERoboticsandAutomationLetters,2019.
[20] S. Hutchinson, G. Hager, and P. Corke, “A tutorial on visual servo
basedcontrol,”IEEETransactionsonRoboticsandAutomation,2018.
[21] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou,
D.Wierstra,andM.Riedmiller,“Playingatariwithdeepreinforcement
learning,”arXivpreprintarXiv:1312.5602,2013.
[22] C.B.Choy,J.Gwak,S.Savarese,andM.Chandraker,“Universalcor-
respondencenetwork,”inAdvancesinNeuralInformationProcessing
Systems,2016,pp.2414–2422.
[23] S. M. LaValle, Planning algorithms. Cambridge university press,
2006.
[24] A.Dosovitskiy,P.Fischer,E.Ilg,P.Hausser,C.Hazirbas,V.Golkov,
P. Van Der Smagt, D. Cremers, and T. Brox, “Flownet: Learning
opticalﬂowwithconvolutionalnetworks,”inProceedingsoftheIEEE
internationalconferenceoncomputervision,2015,pp.2758–2766.
664
Authorized licensed use limited to: University of New South Wales. Downloaded on September 20,2020 at 15:56:02 UTC from IEEE Xplore.  Restrictions apply. 