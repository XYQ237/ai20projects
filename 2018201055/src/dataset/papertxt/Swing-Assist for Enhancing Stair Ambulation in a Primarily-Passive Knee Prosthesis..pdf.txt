2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
Controlling Assistive Robots with Learned Latent Actions
Dylan P. Losey1, Krishnan Srinivasan1, Ajay Mandlekar1, Animesh Garg2, Dorsa Sadigh1
Abstract—Assistive robotic arms enable users with physical an approach for controlling the robot in continuous spaces
disabilities to perform everyday tasks without relying on a usinglow-DoFactionslearnedfromdata.Ourinsightisthat:
caregiver. Unfortunately, the very dexterity that makes these
arms useful also makes them challenging to teleoperate: the High-DoF robot actions can often be embedded into
robothasmoredegrees-of-freedomthanthehumancandirectly intuitive, human-controllable, and low-DoF latent actions
coordinate with a handheld joystick. Our insight is that
we can make assistive robots easier for humans to control Latentactionsarealow-DoFrepresentationthatcapturesthe
by leveraging latent actions. Latent actions provide a low- mostimportantaspectsofhigh-DoFactions.Returningtoour
dimensional embedding of high-dimensional robot behavior:
pouringexample:thehumanwantstherobotarmto(a)carry
for example, one latent dimension might guide the assistive
thecuplevelwiththetableand(b)performapouringaction.
arm along a pouring motion. In this paper, we design a
teleoperation algorithm for assistive robots that learns latent Intuitively, this should be reﬂected in the latent actions: one
actions from task demonstrations. We formulate the control- latent dimension should cause the robot to move the cup
lability, consistency, and scaling properties that user-friendly along the table, while the other should make the robot pour
latent actions should have, and evaluate how different low-
more or less water (see Fig. 1).
dimensional embeddings capture these properties. Finally, we
We explore methods for learning these low-DoF latent
conduct two user studies on a robotic arm to compare our
latentactionapproachtobothstate-of-the-artsharedautonomy actionsfromtask-speciﬁctrainingdata.Weenvisionsettings
baselinesandateleoperationstrategycurrentlyusedbyassistive wheretherobothasaccesstodemonstrationsofrelatedtasks
arms.Participantscompletedassistiveeatingandcookingtasks (potentially provided by the caregiver), and the user—in an
more efﬁciently when leveraging our latent actions, and also
online setting—wants to control the robot to perform a new
subjectively reported that latent actions made the task easier
task: e.g., now the cup is located in a different place, and
to perform. The video accompanying this paper can be found
at: https://youtu.be/wjnhrzugBj4. the person only wants half a glass of water. In practice, we
ﬁndthatsomemodelsresultinexpressiveandintuitivelatent
Index Terms—Physically assistive devices, cognitive human-
robot interaction, human-centered robotics actions,andthatuserscancontrolrobotsequippedwiththese
models to complete eating and cooking tasks.
I. INTRODUCTION Overall, we make the following contributions:
ForthenearlyonemillionAmericanadultsthatneedassis- Formalizing Desirable Properties of Latent Actions. We
tance when eating, taking a bite of food or pouring a glass formally specify a list of properties that user-friendly latent
of water presents a signiﬁcant challenge [1]. Wheelchair- actionsshouldsatisfy.Thisincludescontrollability,i.e.,there
mounted robotic arms and other physically assistive robotic must be a sequence of latent actions that move the robot
devices provide highly dexterous tools for performing these to the desired state, and consistency, i.e., the robot should
tasks without relying on help from a caregiver. In order to always behave similarly under a given learned latent action.
be effective, however, these assistive robots must be easily LearningLatentActionsthroughAutoencoders.Welearn
controllable by their users. latent actions using different autoencoder models, and com-
Consider a person controlling a robotic arm to pour water pare how these models perform with respect to our desired
into a glass. Because of the physical limitations of their properties.Weﬁndthatmodelswhichareconditionedonthe
users, today’s assistive arms utilize low-dimensional control robot’s current state accurately reconstruct high-DoF actions
interfaces, such as joysticks [2]. But the robot arm is high- from human-controllable, low-DoF inputs.
dimensional:ithasmanydegrees-of-freedom(DoFs),andthe Evaluating Latent Actions with User Studies. We imple-
human must precisely coordinate all of these interconnected ment our approach on a robot arm, and compare to state-
DoFs to pour the water without spilling. In practice, user of-the-art shared autonomy baselines and a current control
studies have demonstrated that controlling assistive arms is strategyforassistiveroboticarms.Weﬁndthat—duringeat-
quite challenging due to the unintuitive mapping from low- ingandcookinguserstudies—learnedlatentactionsresulted
DoF human inputs to high-DoF robot actions [3], [4]. in improved objective and subjective performance.
Current approaches solve this problem when the human’s
goals are discrete: e.g., when the robot should pour water II. RELATEDWORK
into either glass A or glass B. By contrast, we here propose In this paper we leverage learning techniques to identify
low-DoF latent actions for teleoperating high-DoF robots.
1Stanford Intelligent and Interactive Autonomous Systems Group (IL- Prior works on shared autonomy have addressed (a) using
IAD),DeptofComputerScience,StanfordUniversity,Stanford,CA94305.
predeﬁned mappings from low-DoF human inputs to robot
2AnimeshGargiswithUniversityofToronto,VectorInstitute,andNvidia.
(e-mail:dlosey@stanford.edu) actions, and (b) learning mappings when the human’s action
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 378
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 14:09:58 UTC from IEEE Xplore.  Restrictions apply. Fig.1. Humanteleoperatingarobotarmusinglatentactions.BecausetherobothasmoreDoFsthanthehumancandirectlycontrol,weleveragelow-DoF
embeddingstolearnalatentspacezfortheusertointuitivelyinteractwith.Heretherobothasbeentrainedondemonstrationsofpouringtasks,andlearns
a2-DoFlatentspace.Theﬁrstlatentdimensionz1 movesthecuplevelwiththetable,andthesecondlatentdimensionz2 tiltsthecup.Weexplorehow
conditional autoencoders—such as the one shown on right—can be leveraged to learn these intuitive and human-controllable latent actions. We train an
encoderthatﬁndsourlow-DoFembeddingz giventhestatesandhigh-DoFactiona.Thedecoderthenrecoversahigh-DoFactionaˆbasedonz ands.
Duringcontrol,thelow-DoFhumaninputzisenoughtoreconstructtheirintendedhigh-DoF,continuousrobotactionaˆconditionedonthecurrentstates.
∈A⊆R T
space has the same number of DoFs as the robot. Previous is the state space, a m is the action space, (s,a)
{ }
researchonlatentrepresentations(c)focusedonautonomous isthetransitionfunction,R(s)=1 task is solved in s isa
∈
robots that are acting without a human in-the-loop. sparse reward function that indicates task success, γ [0,1)
·
Shared Autonomy. Under shared autonomy, the robot com- isthediscountfactor,andρ0()istheinitialstatedistribution.
We assume access to a dataset of task demonstrations, and
bines user inputs with autonomous assistance: this method
want to learn the latent action space by leveraging this
hasbeenappliedtosettingswherethehuman’sinputislow-
dataset. Formally, we have a dataset of state-action pairs
dimensional [4]–[6]. Recent works explored tasks where the D { }
human wants their assistive arm to reach a goal (e.g., pick = Z(s0⊂,aR0),(s1,a1),... ,andseektolearnalatentaction
space d that is of lower dimension than the original
up a cup) [2], [7]–[10]. Here the robot maintains a belief Z×S (cid:55)→A
over possible goals, and increasingly assists the human as action space (d<m), along with a function φ:
that maps latent actions to robot actions.
it becomes conﬁdent about their intended goal [7], [8]. We
Recall our motivating example, where the human is lever-
point out that—similar to [11]—the mapping between the
aging latent actions to make their assistive robot pour water.
human’s low-DoF input and the robot’s high-DoF action is
There are several properties that the human expects latent
pre-deﬁned, and not learned from data.
actions to have: e.g., the human should be able to guide the
Other shared autonomy works consider settings where the
robot by smoothly changing the joystick direction, and the
human’s input has the same number of DoFs as the robot’s
robot should never abruptly become more sensitive to the
action space [12]–[15]. Our paper is most related to shared
human’sinputs.Inwhatfollows,weformalizetheproperties
autonomy research by Reddy et al. [15], where the robot
thatmakelatentactionsintuitive.Thesepropertieswillguide
learns a mapping between humans inputs and their intended
our approach, and provide a principled way of assessing the
actions using reinforcement learning; however, in [15] the
usefulness of latent actions with humans in-the-loop.
human inputs are the same dimension as the robot action,
∈ D
and thus there is no need to learn an embedding for control. Latent Controllability. Let s ,s be two states from
i j
Learning Latent Representations. Recent works have the dataset of demonstrations, and let s1,s2,...,sK be the
sequence of states that the robot visits when starting in
shown that alternate action representations can facilitate
learning efﬁciency in manipulation [16], however this action state s0 =si and taking latent actions z1,...,zK. The robot
transitionsbetweenthevisitedstatesusingthelearnedlatent
spaceishanddesigned.Instead,weaimtoidentifylow-DoF T
embeddings of complex state-space models. Hence we turn space: sk = (sk−1,Zφ(zk−1,sk−1)). Formally, we say that
alatentactionspace iscontrollableifforeverysuchpairs
to works that learn latent representations from data. Recent
research has learned latent dynamics [17], trajectories [18], o{f s}tates (si∈,sZj) there exists a sequence of latent actions
plans[19],policies[20],andskillsforreinforcementlearning zk Kk=1,zk suchthatsj =sK.Inotherwords,alatent
action space is controllable if it can move the robot between
[21]. These methods typically leverage autoencoder models
pairs of start and goal states from the dataset.
[22],butdonotincludeahumanin-the-looporteleoperation.
Z
Here, we leverage autoencoders to learn a consistent and Latent Consistency. We deﬁne a latent action space as
∈Z
controllablelatentrepresentationforassistiverobotics.Previ- consistentifthesamelatentactionz hasasimilareffect
ousteleoperationliteraturehasexploredprincipalcomponent on how the robot behaves in nearby states. We formulate
analysis(PCA)forreducingtheuser’sinputdimension[23], this similarity via a task-dependent metric d : e.g., in
M
[24]. We will compare our method to this PCA baseline. pouringtasksd couldmeasuretheorientationoftherobot’s
M
end-effector. Applying this metric, consistent latent actions
T T
III. PROBLEMSTATEMENT should satisfy: d ( (s ,φ(z,s )), (s ,φ(z,s )))< for
(cid:107) − (cid:107) M 1 1 2 2
We formulate a task as a discrete-time Markov Decision s1 s2 <δ for some ,δ >0.
Process (MDP) M=(S,A,T,R,γ,ρ ). Here s∈S ⊆Rn LatentScaling.Finally,alatentactionspaceZ isscalableif
0
379
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 14:09:58 UTC from IEEE Xplore.  Restrictions apply. Algorithm 1 Learning Latent Control for Assistive Robots meaningswithlatentactions,weconditiontheinterpretation
D { }
1: Collect dataset = (s ,a ),(s ,a ),... from kines- of the latent action on the robot’s current state. Deﬁne φ :
0 0 1 1 Z×S →A
thetic demonstrations asadecoderthatnowmakesdecisionsbasedon
D
2: Train autoencoder to minimize loss L(s,a) on bothz ands.Weexpectthatconditionalautoencoders(cAE)
3: Align the learned latent space and conditional variational autoencoders (cVAE) which use
←
4: for t 1,2,...,T do φ will learn more expressive and controllable actions than
5: Set latent action z as human’s joystick input their non-state conditioned counterparts.
t ←
6: Execute reconstructed action aˆ φ(z ,s )
t t t V. SIMULATIONS
7: end for
To test if the proposed low-DoF embeddings capture our
desired user-friendly properties, we perform simulations on
robotarms.ThesimulatedrobotshavemoreDoFthanneeded
applyinglargerlatentactionsleadstolargerchangesinstate.
(cid:107) − (cid:48)(cid:107)→∞ (cid:107) (cid:107)→∞ to complete the task, and thus must learn to coordinate their
In other words, we would like s s as z ,
(cid:48) T redundant joints when decoding human inputs.
where s = (s,φ(z,s)).
Setup. We simulate one- and two-arm planar robots, where
IV. METHODS each arm has ﬁve revolute joints and links of equal length.
Nowthatwehaveformallyintroducedthepropertiesthata The state s ∈ Rn is the robot’s joint position, and the
user-friendlylatentspaceshouldsatisfy,wewillexplorelow- action a∈Rn is the robot’s joint velocity. Hence, the robot
·
DoF embeddings that capture these properties. We are inter- transitions according to: s = s + a dt, where dt is
t+1 t t
ested in models that balance expressiveness with intuition: the step size. Demonstrations consist of trajectories of state-
the embedding must reconstruct high-DoF actions while action pairs: in each of different simulated tasks, the robot
remaining controllable, consistent, and scalable. We assert trains with a total of 10000 state-action pairs.
that only models which reason over the robot’s state when
Tasks. We consider four different tasks. Two are reported
decoding the human’s inputs can accurately and intuitively
here, and two additional tasks are included in our supple-
interpret the latent action. Our approach for training and
mental material (https://arxiv.org/abs/1909.09674)
leveraging these models is shown in Algorithm 1.
1) Sine: a single 5-DoF robot arm moves its end-effector
A. Models along a sine wave with a 1-DoF latent action
ReconstructingActions.Letusreturntoourpouringexam- 2) Rotate: two robot arms are holding a box, and rotate
thatboxaboutaﬁxedpointusinga1-DoFlatentaction
ple: when the person applies a low-DoF joystick input, the
robot completes a high-DoF action. We use autoencoders ModelDetails.WeexaminemodelssuchasPCA,AE,VAE,
to move between these low- and high-DoF action spaces. and state conditioned models such as cAE and cVAE. The
S × A → Z
Deﬁne ψ : as an encoder that embeds the encoders and decoders contain between two and four linear
(cid:48) Z → ·
robot’s behavior into a latent space, and deﬁne φ : layers (depending on the task) with a tanh() activation
A
as a decoder that reconstructs a high-DoF robot action function. The loss function is optimized using Adam with
∈ A −
aˆ from this latent space (see Fig. 1). To encourage a learning rate of 1e 2. Within the VAE and cVAE, we set
models to learn latent actions that accurately reconstruct the normalization weight <1 to avoid posterior collapse.
high-DoF robot behavior, we incorporate the reconstruction Dependent Measures. To determine accuracy, we measure
(cid:107) − (cid:107)
error a aˆ 2 intothemodel’slossfunction,whichmeasures the mean-squared error between the intended actions a and
the difference between the demonstrated action a and the reconstructed actions aˆ on a test set of state-action pairs
model’s estimate aˆ. (s,a) drawn from the same distribution as the training set.
Regularizing Latent Actions. When the user slightly tilts To test model controllability, we select pairs of start and
the joystick, the robot should not suddenly pour its entire goal states (s ,s ) from the test set, and solve for the latent
i j
glassofwater.Tobetterensurethisconsistencyandscalabil- actionsz thatminimizetheerrorbetweentherobot’scurrent
ity,weincorporateanormalizationtermintothemodel’sloss state and s . We then report this minimum state error.
function. Let us deﬁne ψ :S×A→Rd×Rd as an encoder Wejointljymeasureconsistencyandscalability:todothis,
+
thatoutputsthemeanµandcovarianceσ ofthelatentaction we select 25 states along the task, and apply a ﬁxed grid of
−
space. We penalize the divergence between this latent action latentactionsz from[ 1,+1]ateachstate.Forevery(s,z)
N (cid:107) N i
space and a normal distribution: KL( (µ,σ) (0,1)). pairwerecordthedistanceanddirectionthattheend-effector
Variationalautoencoder(VAE)modelstrade-offbetweenthis travels (e.g., the direction is +1 if the end-effector moves
normalization term and reconstruction error [22]. right). We then ﬁnd the best-ﬁt line relating z to distance
Conditioning on State. Importantly, we recognize that the times direction, and report its R2 error.
meaningofthehuman’sjoystickinputoftendependsonwhat Our results are averaged across 10 trained±models of the
therobotisdoing.Whentherobotisholdingaglass,pressing same type, and are listed in the form mean SD.
down on the joystick indicates that the robot should pour Hypotheses. We have the following two hypotheses:
water; but—when the robot’s gripper is empty—it does not H1. Only models conditioned on the state will ac-
makesensefortherobottopour!Sothatrobotscanassociate curately reconstruct actions from low-DoF inputs.
380
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 14:09:58 UTC from IEEE Xplore.  Restrictions apply. Desired VAE cVAE
A B
A B
z=-1 Latent Action z=+1 Fig.3. ResultsfortheRotatetask.(A)therobotusestwoarmstoholda
lightbluebox,andlearnstorotatethisboxaroundtheﬁxedpointshownin
teal.Eachstatecorrespondstoadifferentﬁxedpoint,andpositivezcauses
counterclockwiserotation.Onrightweshowhowz affectstherotationof
the box at each state. (B) rollout of the robot’s trajectory when the user
start appliesz=+1forVAEandcVAEmodels,wherebothmodelsstartinthe
samestate.UnliketheVAE,thecVAEmodelcoordinatesitstwoarms.
C D
Fig.2. ResultsfortheSinetask.(A)mean-squarederrorbetweenintended
and reconstructed actions normalized by PCA test loss. (B) effect of the
latent action z at three states along the sine wave for the cVAE model.
Darkercolorscorrespondtoz>0andlightercolorssignifyz<0.Above
weplotthedistancethat theendeffectormovesalongthesine waveasa
functionofz ateachstate.(C)rolloutofrobotbehaviorwhenapplyinga
constantlatentinputz=+1,wherebothVAEandcVAEstartatthesame
Fig.4. Experimentalsetupforourﬁrstuserstudy.(Left)inthiseatingtask,
state.(D)end-effectortrajectoriesformultiplerolloutsofVAEandcVAE.
theparticipantusesatwo-DoFjoysticktoguidetherobottopickuptheir
desired morsel from a discrete set. (Right) we compare our latent action
H2. State conditioned models will learn a latent approachtosharedautonomybaselinesfromtheHARMONICdataset.
spacethatiscontrollable,consistent,andscalable.
orientation for the end-effectors of both arms (i.e., ignoring
Sine Task. This task and our results are shown in Fig. 2.
their location). Each model exhibits a linear relationship
We ﬁnd that including state conditioning greatly improves ±
betweenzandorientation:R2 =0.995 0.004forcVAEand
accuracywhencomparedtothePCAbaseline:AEandVAE ±
± ± R2 = 0.996 0.002 for cVAE. In other words, there is an
incur 98.0 0.6% and 100 ±0.8% of the PCA±loss, while approximately linear mapping between z and the orientation
cAE and cVAE obtain 1.37 1.2% and 3.74 0.4% of of the box that the two arms are holding.
the PCA loss, respectively. We likewise observe that the
Summary. The results of our Sine and Rotate tasks support
state conditioned models are more controllable than their
hypotheses H1 and H2. The state conditioned models more
alternatives. When using the learned latent actions to move
accurately reconstruct high-DoF actions from low-DoF em-
between 1000 randomly selected pairs of states along the
beddings (H1), and also exhibit the user-friendly properties
sine wave, cAE and cVAE are on average 5.6% and 11.1%
of controllability, consistency, and scalability (H2).
as far from the goal as PCA. By contrast, models without
state conditioning (i.e., AE and VAE) performed worse than VI. USERSTUDIES
the PCA baseline, with 104% error and 106% error. To evaluate whether actual humans can use learned latent
When evaluating consistency and scalability, we discover
actionstoteleoperaterobotsandperformeverydaytasks,we
that every model’s relationship between latent actions and
conducted two user studies on a 7-DoF robotic arm (Fetch,
robotbehaviorcanbemodeledasapproximatelylinear:PCA
FetchRobotics).Intheﬁrststudy,wecomparedourproposed
has the highest R2±= 0.99, while cAE a±nd cVAE have the approach to state-of-the-art shared autonomy methods when
lowest R2 =0.94 0.04 and R2 =0.95 0.01. the robot has a discrete set of possible goals. In the second
RotateTask.Wesummarizetheresultsforthistwo-armtask
study,participantscompletedacookingtaskwithcontinuous
inFig.3.LikeintheSinetask,themodelsconditionedonthe
goal spaces using either a teleoperation method currently
current state are more accurate than their non-conditioned
employedbyassistivearmsorourlearnedlatentactions.For
± ±
counterparts:AEandVAEhave28.7 4.8%and38.0 5.8% both studies the participants controlled the robot arm with a
of the PCA baseline loss, while cAE and cVAE reduce this
low-DoF teleoperation interface (a handheld joystick).
± ±
to 0.65 0.05% and 0.84 0.07%. The state conditioned
modelsarealsomorecontrollable:whenusingthelearnedz A. DiscreteGoalSpace:LatentActionsvs.SharedAutonomy
± ±
torotatethebox,AEandVAEhave56.8 9%and71.5 8% Inourﬁrstuserstudyweimplementedtheassistiveeating
asmuchend-effectorerrorasthePCAbaseline,whereascAE taskfromtheHARMONICdataset[25](seeFig.4).Herethe
± ±
and cVAE achieve 5.4 0.1% and 5.9 0.1% error. human is guiding the robot to pick up a bite of food. There
When testing for consistency and scalability, we measure arethreemorselsneartherobot—i.e.,threepossiblegoals—
therelationshipbetweenthelatentactionz andthechangein andthehumanwantstherobottoreachoneofthesediscrete
381
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 14:09:58 UTC from IEEE Xplore.  Restrictions apply. *
*
*
Fig.6. Objectiveresultsfromtheeatinguserstudy.WefoundthatcVAEled
Fig. 5. End-effector trajectories from High Assist and cVAE conditions.
tofastertaskcompletionwithlessuserinputandend-effectormotion.The
Therobotstartsattheblackdot,andmovestopositionitselfovertheplate. Full Assist condition performed worse than High Assist across the board
∗
(omitted for clarity). Error bars show the 10 and 90 percentiles, and
goals. The HARMONIC dataset reports the performance of denotesstatisticalsigniﬁcance(p<.05).
24 people who completed this task with different levels
performed independent t-tests. We found that participants
of shared autonomy [7]. Under shared autonomy, the robot
that used the cVAE model took statistically signiﬁcant lower
infers from the human’s inputs which goal they are trying
Completion Time (t(158) = 2.95, p < .05), Joystick Input
to reach, and then provides assistance towards that goal. We
(t(158) = 2.49, p < .05), and Trajectory Length (t(158) =
here conduct an additional experiment to compare our latent
9.39, p<.001), supporting our hypothesis H3.
action method to this set of baselines.
Independent Variables. We manipulated the robot’s tele- B. Continuous Goal Space: Latent Actions vs. End-Effector
operation strategy with ﬁve levels: the four conditions from Real-world tasks often move beyond discrete goals: in-
the HARMONIC dataset plus our proposed cVAE method. steadofreachingforanobjectthatmusteitherbeinposition
In the ﬁrst four conditions, the robot provided no assistance A or position B, objects may lie in continuous regions (i.e.,
(No Assist), or interpolated between the human’s input and anywhereonashelf).Inourseconduserstudy,wefocusona
anassistiveaction(LowAssist,HighAssist,andFullAssist). cooking scenario with continuous goals spaces (see Fig. 7).
High Assist was the most effective strategy from this group: The user wants their assistive robot to help them make a
when interpolating, here the assistive action was given twice recipe: this requires picking up ingredients from the shelf,
the weight of the human’s input. In our cVAE approach, the pouring them into a bowl, recycling empty containers—or
human’s joystick inputs were treated as latent actions z (and returninghalf-ﬁlledcontainerstotheshelf—andthenstirring
therobotprovidednootherassistance).WetrainedourcVAE the mixture. Shared autonomy approaches like [7], [8], [25]
model on demonstrations from the HARMONIC dataset. are not suitable within this setting because: (a) the goals
lie in continuous regions and (b) the user needs to control
Dependent Measures. We measured the fraction of trials
both the goal that the robot reaches for and the style of the
in which the robot picked up the correct morsel of food
reachingtrajectory(e.g.,pouring,orkeepingupright).Hence,
(Success Rate), the amount of time needed to complete the
we compare our latent action method against a switching
task (Completion Time), the total magnitude of the human’s
teleoperation strategy currently used by assistive robots [3],
input(JoystickInput),andthedistancetraveledbytherobot’s
[13], where the joystick inputs alternatively control the
end-effector (Trajectory Length).
position and orientation of the robot’s end-effector.
Hypothesis. We had the following hypothesis:
Independent Variables. We tested two teleoperation strate-
H3. Teleoperating with learned latent actions will
gies: End-Effector and cVAE. Under End-Effector the user
improvetasksuccesswhilereducingthecompletion
inputsapplieda6-DoFtwisttotherobot’send-effector,con-
time, joystick inputs, and trajectory length.
trollingitslinearandangularvelocity.Participantsinteracted
Participantsand Procedure.Ourparticipantpoolconsisted with two 2-DoF joysticks, and were given a button to toggle
of ten Stanford University afﬁliates who provided informed betweenlinearandangularmotion[3],[7],[25].Bycontrast,
±
consent (3 female, average participant age 23.9 2.8 years). in cVAE the participants could only interact with one 2-DoF
∈R
Following the same protocol as the HARMONIC dataset, joystick, i.e., the latent action was z =[z ,z ] 2.
1 2
each participant was given up to ﬁve minutes to familiarize Dependent Measures – Objective. We measured the total
themselves with the task and joystick, and then completed amountoftimeittookforparticipantstocompletetheentire
ﬁve recorded trials using our cVAE approach. cooking task (Completion Time), as well as the magnitude
Results. We display example robot trajectories in Fig. 5 and of their inputs (Joystick Input).
report our dependent measures in Figs. 4 and 6. Inspecting Dependent Measures – Subjective. We administered a 7-
these example trajectories, we observe that the cVAE model point Likert scale survey after each condition. Questions
learned latent actions that move the robot’s end-effector into wereseparatedintosixscales,suchaseaseofperformingthe
a region above the plate. Users controlling the robot with task (Ease) and consistency of the controller (Consistent).
cVAEreachedtheirdesiredmorselin44ofthe50totaltrials, Once users had completed the task with both strategies,
yielding a higher Success Rate than the assistance baselines. we asked comparative questions about which they preferred
To better compare cVAE to the High Assist condition, we (Prefer), which was Easier, and which was more Natural.
382
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 14:09:58 UTC from IEEE Xplore.  Restrictions apply. Fig.7. Setupforourseconduserstudy.(Toprow)theparticipantisteleoperatinganassistiverobottomaketheirrecipe.Thisrecipeisbrokendowninto
threesub-tasks.Onlefttherobotpicksupeggs,poursthemintothebowl,thendropsthecontainerintotherecycling.Inmiddletherobotpicksupﬂour,
poursitintothebowl,thenreturnsthecontainertotheshelf.Onrighttherobotgraspsanapple,placesitinthebowl,thenstirsthemixture.(Middlerow)
examplerobottrajectorieswhenthepersondirectlycontrolstherobot’sEnd-Effector.(Bottomrow)exampletrajectorieswhenusingcVAEtolearnlatent
actions.Comparingtheexampletrajectories,weobservethatcVAEresultedinrobotmotionsthatmoresmoothlyanddirectlyaccomplishedthetask.
Hypotheses. We had the following hypotheses: VII. DISCUSSIONANDCONCLUSION
H4.Userscontrollingtherobotarmwithlow-DoF Summary. We focused on assistive robotics settings where
latent actions will complete the cooking task more the user interacts with low-Dof control interfaces. In these
quickly and with less overall effort. settings, we showed that intelligent robots can embed their
H5. Participants will perceive the robot as easier high-DoF, dextrous behavior into low-DoF latent actions
toworkwithinthecVAEcondition,andwillprefer for the human to control. We determined that autoencoders
the cVAE over End-Effector teleoperation. conditioned on the system state accurately reconstructed the
human’s intended action, and also produced controllable,
Experimental Setup. We designed a cooking task where consistent, and scalable latent spaces.
the person is making a simpliﬁed “apple pie.” As shown in One key advantage to latent actions is that—unlike com-
Fig. 7, the assistive robot must sequentially pour eggs, ﬂour, parable shared autonomy approaches—they can assist the
and an apple into the bowl, dispose of their containers, and human during tasks with either discrete or continuous goal
stirthemixture.Theusersatnexttotherobotandcontrolled spaces. We validated this in our two user studies. In the
its behavior with a handheld joystick. ﬁrst (discrete), latent actions resulted in higher success
Participants and Procedure. We used a within-subjects than shared autonomy baselines. In the second (continuous),
design and counterbalanced the order of our two conditions. participants controlled both the robot’s goals and movement
Eleven members of the Stanford University community (4 style: compared against a teleoperation strategy currently
female, age range 27.4 ± 11.8 years) provided informed employed by assistive arms, latent actions led to improved
consent to participate in this study. Four subjects had prior objective and subjective performance.
experienceinteractingwiththerobotusedinourexperiment. Howpracticalisthisapproach?Inourcookinguserstudy,
the robot was trained with less than 7 minutes of kinesthetic
Results – Objective. Our objective results are summarized
demonstrations.Weattributethisdataefﬁciencyinparttothe
in Fig. 7. When using cVAE to complete the entire recipe,
− simplicity of our model structure: we used standard cVAEs
participantsﬁnishedthetaskinlesstime(t(10)= 6.9−,p< that we trained within the robot’s on-board computer. We
.001), and used the joystick less frequently (t(10) = 5.1, believe this makes our approach very efﬁcient, accurate, and
p<.001) as compared to direct End-Effector teleoportation. easy to use in practice as compared to alternatives.
Results – Subjective. We display the results of our 7-point Limitations and Future Work. Although latent actions
LikertscalesurveysinFig.7.Beforereportingtheseresults, provide an intuitive control interface for assistive robots,
we ﬁrst conﬁrmed the reliability of our scales. We then this alone is not sufﬁcient for precise manipulation tasks.
leveraged paired t-tests to compare user ratings for End- Futureworkwillfocusoninterweavinglearnedlatentactions
Effector and cVAE conditions. Participants perceived cVAE with shared autonomy, so that robots can perform both
as requiring less user effort (t(10) = 2.7, p < .05) than coarsereachingmotionstowardsthegoal,andthenmaintain
End-Effector. Participants also indicated that it was easier to that goal while the user performs ﬁne-grained manipulation.
completethetaskwithcVAE(t(10)=2.5,p<.05),andthat Follow-up experiments that demonstrate how latent actions
cVAE causedtherobottomovemorenaturally(t(10)=3.8, enable humans to perform dexterous manipulation are in-
p<.01). The other scales were not signiﬁcantly different. cluded in https://arxiv.org/abs/1909.09674.
383
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 14:09:58 UTC from IEEE Xplore.  Restrictions apply. REFERENCES
[1] D.M.Taylor,AmericansWithDisabilities. USCensusBureau,2018.
[2] D. Gopinath, S. Jain, and B. D. Argall, “Human-in-the-loop opti-
mization of shared autonomy in assistive robotics,” Robotics and
AutomationLetters,vol.2,no.1,pp.247–254,2016.
[3] L.V.Herlant,R.M.Holladay,andS.S.Srinivasa,“Assistiveteleop-
erationofrobotarmsviaautomatictime-optimalmodeswitching,”in
Int.Conf.onHumanRobotInteraction(HRI),2016,pp.35–42.
[4] B.D.Argall,“Autonomyinrehabilitationrobotics:Anintersection,”
AnnualReviewofControl,Robotics,andAutonomousSystems,2018.
[5] T. Carlson and J. d. R. Millan, “Brain-controlled wheelchairs: A
roboticarchitecture,”Robotics&AutomationMagazine,2013.
[6] K. Muelling, A. Venkatraman, J.-S. Valois, J. Downey, J. Weiss,
S. Javdani, M. Hebert, A. B. Schwartz, J. L. Collinger, and J. A.
Bagnell, “Autonomy infused teleoperation with application to bci
manipulation,”arXivpreprintarXiv:1503.05451,2015.
[7] S. Javdani, H. Admoni, S. Pellegrinelli, S. S. Srinivasa, and J. A.
Bagnell, “Shared autonomy via hindsight optimization for teleopera-
tion and teaming,” The International Journal of Robotics Research,
vol.37,no.7,pp.717–742,2018.
[8] A. D. Dragan and S. S. Srinivasa, “A policy-blending formalism
for shared control,” The International Journal of Robotics Research,
vol.32,no.7,pp.790–805,2013.
[9] R.M.Aronson,T.Santini,T.C.Kübler,E.Kasneci,S.Srinivasa,and
H.Admoni,“Eye-handbehaviorinhuman-robotsharedmanipulation,”
inInt.Conf.onHuman-RobotInteraction(HRI),2018,pp.4–13.
[10] A. Broad, T. Murphey, and B. Argall, “Learning models for shared
control of human-machine systems with unknown dynamics,” in
Robotics:ScienceandSystems(RSS),2018.
[11] F. Abi-Farraj, C. Pacchierotti, and P. R. Giordano, “User evaluation
ofahaptic-enabledshared-controlapproachforrobotictelemanipula-
tion,”inInt.Conf.onIntelligentRobotsandSystems(IROS),2018.
[12] D. P. Losey, C. G. McDonald, E. Battaglia, and M. K. O’Malley,
“Areviewofintentdetection,arbitration,andcommunicationaspects
of shared control for physical human–robot interaction,” Applied
MechanicsReviews,vol.70,no.1,p.010804,2018.
[13] D.Rakita,B.Mutlu,andM.Gleicher,“Amotionretargetingmethod
foreffectivemimicry-basedteleoperationofrobotarms,”inInt.Conf.
onHuman-RobotInteraction(HRI),2017,pp.361–370.
[14] F.Abi-Farraj,T.Osa,N.P.J.Peters,G.Neumann,andP.R.Giordano,
“A learning-based shared control architecture for interactive task
execution,”inInt.Conf.onRoboticsandAutomation(ICRA),2017.
[15] S.Reddy,A.D.Dragan,andS.Levine,“Sharedautonomyviadeep
reinforcementlearning,”inRSS,2018.
[16] J.Buchli,E.Theodorou,F.Stulp,andS.Schaal,“Variableimpedance
control a reinforcement learning approach,” Robotics: Science and
SystemsVI,pp.153–160,2011.
[17] M.Watter,J.Springenberg,J.Boedecker,andM.Riedmiller,“Embed
to control: A locally linear latent dynamics model for control from
rawimages,”inNeurIPS,2015,pp.2746–2754.
[18] J.Co-Reyes,Y.Liu,A.Gupta,B.Eysenbach,P.Abbeel,andS.Levine,
“Self-consistent trajectory autoencoder: Hierarchical reinforcement
learningwithtrajectoryembeddings,”inICML,2018.
[19] C. Lynch, M. Khansari, T. Xiao, V. Kumar, J. Tompson, S. Levine,
and P. Sermanet, “Learning latent plans from play,” arXiv preprint
arXiv:1903.01973,2019.
[20] A.Edwards,H.Sahni,Y.Schroecker,andC.Isbell,“Imitatinglatent
policiesfromobservation,”inICML,2019.
[21] K. Hausman, J. T. Springenberg, Z. Wang, N. Heess, and M. Ried-
miller, “Learning an embedding space for transferable robot skills,”
2018.
[22] D.P.KingmaandM.Welling,“Auto-encodingvariationalbayes,”in
Int.Conf.onLearningRepresentations(ICLR),2014.
[23] P. K. Artemiadis and K. J. Kyriakopoulos, “EMG-based control of
a robot arm using low-dimensional embeddings,” Transactions on
Robotics,vol.26,no.2,pp.393–398,2010.
[24] M.T.CiocarlieandP.K.Allen,“Handposturesubspacesfordexterous
robotic grasping,” The International Journal of Robotics Research,
vol.28,no.7,pp.851–867,2009.
[25] B.A.Newman,R.M.Aronson,S.S.Srinivasa,K.Kitani,andH.Ad-
moni, “HARMONIC: A Multimodal Dataset of Assistive Human-
RobotCollaboration,”ArXive-prints,July2018.
384
Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on September 20,2020 at 14:09:58 UTC from IEEE Xplore.  Restrictions apply. 