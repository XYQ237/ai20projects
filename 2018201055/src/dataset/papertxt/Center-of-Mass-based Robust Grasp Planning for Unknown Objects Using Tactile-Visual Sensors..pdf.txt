2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
High Resolution Soft Tactile Interface for Physical Human-Robot
Interaction
Isabella Huang1 and Ruzena Bajcsy1
Abstract—Ifrobotsandhumansaretocoexistandcooperate
in society, it would be useful for robots to be able to engage in
tactile interactions. Touch is an intuitive communication tool
as well as a fundamental method by which we assist each
other physically. Tactile abilities are challenging to engineer
inrobots,sincebothmechanicalsafetyandsensoryintelligence
areimperative.Existingworkrevealsatrade-offbetweenthese
principles— tactile interfaces that are high in resolution are
not easily adapted to human-sized geometries, nor are they
generally compliant enough to guarantee safety. On the other
hand,softtactileinterfacesdeliverintrinsicallysafemechanical
properties, but their non-linear characteristics render them
difﬁcult for use in timely sensing and control. We propose a Fig. 1: The proposed soft interface enables safe and precise
robotic system that is equipped with a completely soft and tactile interaction between a human and a robot.
therefore safe tactile interface that is large enough to interact
with human upper limbs, while producing high resolution
tactile sensory readings via depth camera imaging of the soft
interface. We present and validate a data-driven model that
Our tactile interface consists of a pneumatically pressur-
maps point cloud data to contact forces, and verify its efﬁcacy
bydemonstratingtworeal-worldapplications.Inparticular,the ized domed contact membrane molded from silicone. The
robotisabletoreacttoahumanﬁnger’spokesandchangeits compliant properties of this soft device make it inherently
posebasedonthetactileinput.Inaddition,wealsodemonstrate safe for physical interaction with humans. Moreover, our
that the robot can act as an assistive device that dynamically
design belongs to an up-and-coming class of optical tactile
supports and follows a human forearm from underneath.
sensors that utilizes embedded depth-sensing cameras to
I. INTRODUCTION extract interaction information from a soft contact region.
This type of camera offers a richness in visual data that is
Physical interaction amongst humans is a natural and
unmatchedbytraditional2Dcamerasandultimatelyenables
essential method by which we communicate with and assist
highsensingaccuracy.Speciﬁcally,therearetwomaingoals
each other. As a communication tool, physical interaction
that are desired in tactile sensing. Firstly, it is useful to infer
can be more clear and intuitive than providing auditory or
the physical geometry of the object in contact, and it has
visualcues.Forexample,onecanswiftlygrabafriend’sarm
been shown in our previous work that our design does so
tostopthemfromwalkingintoincomingtrafﬁc,ormanually
extremely efﬁciently [1]. Secondly, a more difﬁcult goal is
adjust their form when teaching them a new task, such as
to infer the force that is being applied on the sensor. It
how to swing a golf club properly. As a means of assistance
is not possible to directly read force from vision, so some
for those who have limited mobility, physical interaction is
sort of mapping is required. This is challenging even for a
even more vital. This is especially clear for caretakers who
sensor like ours that outputs a high-ﬁdelity 3D membrane
aid clinical patients or the elderly in everyday tasks such as
representation, as non-linear constituent elastic stress-strain
helping them walk, sit up in bed, or change their clothes.
equations require ﬁnite element methods that are compu-
If robots are to coexist with and be useful to humans,
tationally intensive and highly sensitive to manufacturing
endowing them with similar capabilities of physical inter-
parameters. In this work, following the recent successes in
action would be hugely advantageous and would facilitate
the ﬁeld, data-driven techniques will be employed in the
a myriad of exciting applications with considerable societal
modeling of the soft contact region.
impact. A necessary component of such a robot capable of
physical interaction is a tactile interface, which should not The space of all contact interactions and resultant forces
only ensure a human’s safety during interaction, but also for which to build this mapping is extremely vast, but
enablethecorrectsensingofinteractionforces.Inthiswork, will be constrained in this work based on two proposed
weproposeatactileinterfacethatstrivestoaddressthesetwo physical human-robot interaction (pHRI) applications that
concerns of safety and accurate sensing. both involve interaction with the human upper limb. These
applications are inspired by our motivating examples for
*ThisworkwassupportedbyNSFAward#1545126 physicalinteractiontobeusedincommunicationaswellasin
1The authors are with the Department of Electrical Engineering and
assistance. To demonstrate tactile communication, we show
Computer Sciences, University of California, Berkeley, CA 94270, USA
{isabella.huang, bajcsy}@eecs.berkeley.edu that a human can successfully alter a robot’s end effector
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 1705
Authorized licensed use limited to: University of New South Wales. Downloaded on September 19,2020 at 21:48:50 UTC from IEEE Xplore.  Restrictions apply. pose by indenting the tactile interface in the appropriate state-of-the-art depth camera technology and thus renders
location with their ﬁngertip. To exemplify an assistive task, our design ineffective for very ﬁne manipulation tasks, its
we demonstrate that our soft interface can act as a soft larger geometry actually makes it well suited for human-
forearm support that adapts to and follows the human’s level interaction. Thus, our ﬁnal design aims to collect the
dynamic forearm motions by interpreting the shear and features across tactile sensors that are best suited towards
torque forces applied by the human forearm to the tactile pHRI— high resolution, intrinsic softness, variable stiffness,
interface. and appropriate geometric scale.
II. RELATEDWORKS
B. Methods for Inferring Interaction Forces Visually
A. Design of Tactile Sensors for pHRI
It is challenging to interpret forces from visual data pro-
The objective of tactile sensing is to physically interact
vided by optical tactile sensing methods. To this end, ﬁnite
with the external environment and extract knowledge about
element modeling techniques have been applied to generate
it by interpreting salient signals. To this end, many exist-
a mapping between visual data and contact forces, but this
ing soft tactile sensing designs in the past have relied on
approach is computationally heavy and not suitable for real
embedding photoreﬂective, piezoelectric [2], strain-sensitive
time control [11]. In the recent years, data-driven methods
[3], electromagnetic [4], and other electronic modules [5]
have proven themselves to be fast and accurate for use with
withinacompliantmaterial.Suchdesignsareeasilyarranged
modeling optical tactile sensors. Within the TacTip family,
into a ﬂat skin-like conﬁguration that then covers a robot
deeplearningenabledthefastdevelopmentofedgedetection
bodytoenabletactilesensingcapabilitiesforinteractionwith
andcontourfollowingcapabilities.Forevenmoreﬁnetasks,
humans or the environment. One main advantage of such
forexample,theGelSighthasbeenasuccessfulplatformfor
skins is that the spatial resolution can be made to be as ﬁne
learning physical models to detect object hardness [12], or
as around the millimeter scale, depending on the engineered
toperformcomplicatedmanipulationtasksviareinforcement
density of sensing elements [6]. However, these constituent
learning [13], or even to detect differences in fabric texture
sensors limit the material strain and compliance that make
[14]. As a result of these successes, convolutional neural
soft sensors desirable [7]. Moreover, even if the skin itself
networks for processing visual data in tactile sensing has
is soft, overlaying it on top of a hard robot chassis severely
becomeapromisingmethodinthecommunity.Inthiswork,
limits the achievable compliance during interaction, which
weadoptasimilararchitecturewhencreatingourownmodel.
poses a safety threat when humans are involved.
Since high-compliance interaction is of the utmost impor- C. Existing Applications in pHRI
tance for safety, in this work we do not improve traditional
The space of existing pHRI applications in general is
tactile skins further, but rather develop a new design for a
limitedwithrespecttopurposeandprecision.Inrigidrobots,
softrobotendeffectorinterface.Anexampleofexistingwork
force-torque sensors are the primary transducers for pHRI,
basedonasimilardesigngoalisthedevelopmentofapairof
enabling interactions for example with robot arms [15]
pneumatically pressurized ﬁngers that can carefully pick up
and full-bodied partner ballroom dancing robots [16]. The
fragile objects by sensing an overall pressure change in the
sensory capabilities of these robots are very powerful, but
ﬁngers upon contact [8]. However, this sensory information
their pHRI applications are limited due to risk of unsafe
is low in resolution, and can neither sense the objects’
interactions with the hard chassis. The scope of existing
shapes nor even localize the point of contact. Recently, the
soft pHRI robots is unsurprisingly even more narrow due
leading technique in tactile sensing that does not require
to challenging constraints in mechanical design and sensing.
embedded sensing elements is optical tactile sensing, in
Some noteworthy examples of existing soft pHRI systems
whichdevicesimagevisualcuesfromadeformedcompliant
include RI-MAN, a humanoid robot designed to help lift
membrane to reconstruct the interaction effects between the
and carry humans [17], and Huggable, a soft robot pet
environment and the sensor. Some well-known examples of
surrogate [18] that responds to affective touch-based inter-
soft optical tactile sensing devices are the TacTip sensors,
actions. There is an apparent need in the ﬁeld to develop
which image the distortion of internal papillae of a soft gel-
robots that can perform useful and precise pHRI task while
ﬁlled membrane, as well as the extremely high-resolution
remaining compliant enough to assure human safety. Thus,
MIT GelSight sensor, which makes use of the superposition
this work presents two useful applications of the proposed
of different colored light waves in a gel medium to infer
robotic system that involve completely soft interactions and
contact information [9]. On the other hand, the optical
donotrequirebulkytransducersapartfromasmallinternally
tactilesensorproposedinourpriorworkisahigh-resolution
embedded camera.
sensor that directly images the contact membrane via depth
camera without needing to reconstruct papillae positions or
III. DESIGNOFSYSTEM
process color changes. Moreover, its stiffness can be altered
pneumatically, which can advantageously modulate different Thesystemiscomposedofasofttactileinterfacemounted
interaction impedances [10]. Furthermore, its simple design onto the end of a lightweight LBR iiwa 14 R820 robot arm
is easy to manufacture and extremely scalable. While the from KUKA AG with its constituent components labeled
lower bound on its geometry is restricted by the range of in Fig. 2. In order to access ground truth force-torque
1706
Authorized licensed use limited to: University of New South Wales. Downloaded on September 19,2020 at 21:48:50 UTC from IEEE Xplore.  Restrictions apply. readings to train the data-driven models, an ATI Axia 6-
dof transducer is mounted between the end of the robot arm
andthetactilesensingmodule.Thetactilesensorconsistsof
a hemispherical membrane that acts as the tactile interface
sealedoverapneumatically-pressurizedrigidcylindricalcap-
sule 3D printed from PLA polyester. Though this particular
capsuleisrigid,itcouldbeshieldedwithcompliantmaterial,
or better yet, replaced entirely with a soft structure. Inside
the capsule is an embedded Camboard pico ﬂexx time-of-
ﬂight depth camera from PMD Technologies, the smallest
commercially available depth camera available, which im-
agesthemembraneinternally.Thelongitudinaldistancefrom
the camera to the base of the membrane is 10 cm, which
Fig.2:Componentsofthecompositesofttactileendeffector
is the camera’s lowest nominal range for which a depth
interface.
image can be resolved correctly. The membrane, molded
from EcoﬂexTM 50 silicone, has an outer and inner diameter
of 100 and 94 mm respectively with a uniform thickness of
conﬁguration through interaction with the tactile interface.
3 mm. A 30 mm-long skirt extending from the hemisphere’s
Here, interactions for this application are limited to single
baseallowsthesiliconemembranetobesecurelyfastenedto
ﬁnger poking so that the contact geometry can be kept
the cylinder, with three external cable ties tightly enforcing
simple. Thus, we avoid more complicated interactions such
a hermetic seal. The cylinder surface is fully covered with a
as those from multiple ﬁngers and/or the palm.
coatofXTC-3D®resintopreventairleakage,andtheinterior
of the cylinder is painted with a matte black acrylic paint to B. Dynamic Support of the Human Forearm.
absorb any stray reﬂected rays from the camera laser that The second application enables the robot to act as an
lead to distorted camera readings. Ports on the cylinder that assistive device for the human. Speciﬁcally, it assists with
allowfortheconnectionofthepicoﬂexxUSBcableaswell upper limb motion, a common feature in ergonomic design
as the pneunatic tubing are also sealed with the resin and and exoskeletons that helps minimize the effort required to
additionalmodelingclaywherenecessary.Anaircompressor support the weight of one’s arm [21]. In this application,
enables the modulation of pressure state inside the sensor. the human’s forearm rests on top of and is fully supported
Since the tactile membrane is designed to be thin so that it by the soft tactile interface. However, such a support system
would be sensitive enough to contact-induced deformations, is only useful if the human has autonomy over their own
it inﬂates slightly upon internal pressurization. Thus, each movements. Thus, the device follows and dynamically sup-
internal pressure state is linked to a certain amount of ports the forearm as it translates in space, as well as rotates
inﬂation. In this work, all development was performed for with it when a torque about the sensor axis is applied.
one ﬁxed pressure state, particularly the one at which the
membrane’s inner radius was inﬂated to 50 mm, compared V. DATACOLLECTION
to its neutral 47 mm radius when free air exchange between Both applications can only be implemented if the robot is
thecapsuleandtheatmosphereispermitted.Allsensingand aware of the forces and torques being applied to it during
actuation is connected to Robot Operating System (ROS) interaction. In its most useful form, the robot should only
nodes. Since the KUKA robot does not have native ROS have access to the depth camera readings, since being out-
support, an open source ROS stack is used to control the ﬁtted with a force-torque sensor renders the overall system
KUKA’sjointpositions[19]andtheTRAC-IKlibraryisused less compliant. Thus, mappings that predict the force-torque
for computing inverse kinematics [20]. responsibleforameasuredmembranepointcloudneedtobe
availabletotherobot.Tothisend,wesystematicallycollecta
IV. ADMISSIBLEINTERACTIONS
datasetofadmissibleinteractionsandtheirresultinginduced
The space of admissible interactions studied in this work force,torque,andpointcloudsinordertotrainadata-driven
was limited by design based on the two different pHRI model that provides this mapping. There were two datasets,
applicationswewishedtoachieve.Bothapplicationsinvolve one for each application, that were collected. Note that the
speciﬁc interactions with the human upper limb, and illus- coordinate system is deﬁned to be ﬁxed to the ATI force-
trate the usage of physical interaction as a means of human- torquesensor.Theprincipalcameraaxesalsosharethesame
robot communication and assistance respectively. orientation as the force-torque coordinate frame.
A. Robot End Effector Pose Correction A. Finger Poking Dataset
The ﬁrst application utilizes the tactile interface as a Fingertip apparatus. A rigid sphere 1 cm in diameter
communication tool from the human to the robot. In this printed from PLA was used in the data collecting exper-
regime, the human physically corrects the end effector pose iments as an approximate duplicate of an average human
of the robot by prodding the robot towards the desired ﬁngertip. Since minimal slip and shear was expected for the
1707
Authorized licensed use limited to: University of New South Wales. Downloaded on September 19,2020 at 21:48:50 UTC from IEEE Xplore.  Restrictions apply. pokinginteractions,itwasreasonabletoneglectthefrictional
effects of skin versus PLA.
Pre-indentation contact points. The sphere was then used
to indent the membrane at a variety of locations at different
angles and depths, and the resultant membrane point cloud
and force-torque measurements at each state were recorded.
The interaction states were discretized systematically. First,
a set of initial contact points on the membrane surface
were selected. As illustrated in Fig. 3, this was achieved
Fig. 3: Polar view of Fig. 4: Side view of the
by designating 40 equally spaced quarter-circle arcs that run
the pre-indentation ﬁnger- pre-indentation contact
from the pole to the base of the hemisphere (parameterized
tipcontactpointsthatlieon points and the indentation
by φ = πj,j ∈ [0..39]). Along each quarter-circle arc, ﬁve
20 40 evenly-spaced quarter- locations associated with
pointswereselectedwithprojectionsonthexy-planethatare
circle arcs. each.
equally spaced with radii 0 to 4 cm from the pole, leaving
enoughofadistancefromthebaseofthehemispheresothat
boundary contact conditions could be avoided. These initial
contact points indicate the location at which the spherical
obstacle ﬁrst makes contact with the membrane to begin
indentation.
Indentation locations Then, the sphere indented the mem-
brane at varying depths and angles from the z-axis. As (a) θ= π rad, δ= (b)θ= π rad,δ= (c) θ= π rad, δ=
18 18 18
1.5 cm 1.5 cm 1.5 cm
illustrated in Fig. 4, each pre-indentation contact point was
the starting point for 10 different indentation directions
(θ = πk,k ∈ [0..9]). Along each indentation direction, ﬁve
18
differentindentationdepthsδ equallyspacedfrom0.5to2.5
cmwerevisited.Allindentationlocationsthatstemmedfrom
any one pre-indentation contact point were constrained to
(d)θ= π rad,δ= (e) θ= π rad, δ= (f) θ= π rad, δ =
oneplane,andinteractionswereactuatedbytheKUKAarm 18 18 18
2.5 cm 2.5 cm 2.5 cm
pushing into the ﬁxed sphere at the appropriate angles and
depths. In total, there were 10,000 unique states for which
data was collected.
Visualization of example data. Figs. 5a, 5d, and 5g are
radial plots of ground truth τ at a particular indentation
x
state parameterized by θ and δ starting from every pre- (g)θ= 7π rad,δ= (h)θ= 7π rad,δ= (i) θ= 7π rad, δ=
18 18 18
indentation contact state, where the size of each marker is 2.5 cm 2.5 cm 2.5 cm
scaled according to the magnitude of τ . Between Figs. 5a
x Fig. 5: Polar plot of τ in the ﬁngertip dataset starting at all
x
and 5d, the magnitude of the measured τ is greater in 5d,
x pre-indentation contact points for selected values of θ and
due to its indentation depth being 1 cm deeper. Fig. 5g
δ. Columns separate ground truth readings (left), prediction
depicts the torque value when the θ is large enough that
learned from poking data (middle), and prediction learned
torque applied is in the opposite direction compared to a
from both poking and forearm data (right).
smaller θ such as in Fig. 5d. The magnitude is also larger
because the moment arm to the x-axis attached to the force-
torque sensor becomes longer.
displacement of the pole. Fig. 6 illustrates the indentation
B. Forearm Shearing and Torquing Dataset locations visited by the forearm contact point. Then, the
Forearm apparatus. An attempt to replicate a generic device moved in a speciﬁc direction parameterized by φ
forearm geometry using a simple 3D-printed ellipsoid was in the xy-plane, deﬁned in the same way as in Fig. 3 of
unsuccessfulduetothedifferentmaterialpropertiesbetween theﬁngertipexperiment,andstoppedat5differentdistances
PLA and human skin yielding different frictional forces from 0.3 to 1.5 cm away. This shearing distance was limited
during interaction. Thus, data was collected using the re- to avoid material slip. This process was then repeated for
searcher’s real forearm. The forearm was placed directly φ= 2πl,l∈[0..40] rad and also for δ = m,m∈[0..8] cm.
40 4
underneath the sensor during data collection such that its Torquing. In the same manner as the shearing experiments,
axis was centered on the membrane. A ﬁxed brace was used thedevicepushedintotheforearmuntilanindentationdepth
to keep the forearm consistently in the same place between δ was achieved. Then, the device rotated about its z-axis in
data collection sessions. one direction, stopping at angles ψ = πn,n ∈ [0..8]. for
24
Shearing. The device pushed into the forearm until an data recording. The maximum rotation angle achieved, π,
3
indentation depth δ was achieved, as measured from the z- was limited to avoid material slip from excessive torquing.
1708
Authorized licensed use limited to: University of New South Wales. Downloaded on September 19,2020 at 21:48:50 UTC from IEEE Xplore.  Restrictions apply. (a) Ground truth (b) Forearm model (c) Combined
data. prediction. model prediction.
Fig. 6: Side view of the forearm indentation states. Arm is Fig. 8: Predicted τ on example forearm torquing validation
z
not drawn to scale. data as a function of the yaw angle for different indentation
depths δ.
This was repeated for rotation in the opposite direction, and
thenalsoforδ = p,p∈[0..8]cm.Forboththeshearingand
4 a validation MSE of 0.03 after training with a batch size of
torquing motions, data was collected for 1726 interaction
500. The forearm dataset, with a total of 8640 datapoints,
states. In order to achieve a better distribution of samples,
yielded a ﬁnal validation MSE of 0.057 after training with
the forearm experiments were repeated 4 more times so that
a batch size of 400. The combined dataset yielded a ﬁnal
a total of 8640 datapoints were collected.
validation MSE of 0.1 with a training batch size of 500. All
VI. FORCE-DEFORMATIONMODEL force-torque values were recorded in SI units.
A. Architecture
The neural network architecture used to map the input
B. Prediction Results
of a 171×224 organized point cloud into a 6-dimensional
force-torque vector is illustrated in Fig. 9. Each point cloud
We found that in general, the two models trained on the
input, an array of depth measurements, is passed through
individual datasets performed very well in validation. For
a series of two convolutional and max-pooling layers, and
example, in Fig. 5 we can compare the performance of the
then through a fully-connected regression network to yield
pokingdatasetmodel(middlecolumm)withthegroundtruth
thepredictedresultantforce-torquemeasurement.Wetrained
(left column) and note that the prediction is qualitatively
three different models with the same architecture. The ﬁrst
accurate. However, it is apparent that the combined model
modelcombinedthedatafromthetwodatasets,andtheother
(right column) yields less accurate predictions, which can
two were trained on either just the poking or the forearm
be vastly different from the ground truth. For example, in
dataset.Sincethetwointeractionregimesarequitedifferent,
Fig. 5f, the sign on the τ reading in the third quadrant
x
wewantedtoexplorewhetherthecombineddatasetnetwork
is evidently wrong. The performance of the forearm model
would be able to generalize and perform as well as each of
wasalso verygood inpredicting forearmshearand torquing
theindividialdatasetmodels.Thenetworksweretrainedwith
forces.Fig.7visualizesexamplevalidationdataforboththe
theSGDoptimizerandalearningrateof0.01withnodecay,
forearm and the combined models on a set of ground truth
andadropoutof0.2wasimplementedinthefully-connected forearm shearing readings at φ = 3π at a depth of δ =
4
layers to prevent overﬁtting. An 80:20 train/test split was 0.75 cm. Qualitatively, both models perform well. However,
selectedforhyperparametertuningandvalidation.Theﬁnger
whenpredictingforearmtorques,withexamplepredictionon
poking dataset, with 10,000 total datapoints, leveled out to
validation data in Fig. 8, the combined model fails to output
good prediction. One explanation is that while shearing and
poking interactions involve pushing, the twisting involved in
torquing yields more complicated membrane structures, and
there was not enough training data for the combined model
to generalize well.
(a) Predicted F (b) Predicted F (c) Predicted F
x y z
Fig.7:Forearmshearingexamplevalidationdataatφ=3π4
and δ = 0.75 cm comparing the predictions of the forearm
and combined models with the ground truth. Fig. 9: Architecture of the force-torque-deformation model.
1709
Authorized licensed use limited to: University of New South Wales. Downloaded on September 19,2020 at 21:48:50 UTC from IEEE Xplore.  Restrictions apply. (a)Userusestheirﬁnger (b) Robot changes posi- (c) User pokes at the in- (d) Robot changes posi-
to poke along and oppo- tion proportional to its terfaceatanangletothe tion and orientation ac-
site to the z-axis of the sensed contact force. z-axis. cording to the user’s
robot. force-torque input.
Fig. 10: Real-world validation of the robot end effector pose correction application.
(a) User begins by rest- (b) User applies a com- (c) Robot reacts by (d) User attempts to ex- (e)Robotreactsbytwist-
ing their forearm on the binedshearandtorsional changing both its ert a pure torque on the inginthesamedirection
soft interface. force to the robot. position and orientation. interface. as the applied torque.
Fig. 11: Real world validation of the dynamic forearm support.
VII. REAL-WORLDRESULTS VIII. LIMITATIONSANDFUTUREWORK
We have presented a promising new design for a tactile
We perform a proof-of-concept demonstration of the two interfaceinpHRIapplicationswithalearnedmodelmapping
applications in which the robot uses its learned force- point cloud measurements to contact forces and torques.
deformation models. For each application, we utilize the We validated the model with two real-world applications
individuallytrainedmodelsratherthanthecombinedonedue motivated by relevant challenges in pHRI today. However,
to their higher accuracy. Example snapshots are depicted in ourworkislimitedinmanyways.Firstly,thegeneralizability
Figs. 10 and 11. For the two cases, we implement the same of the learned force-deformation model is nowhere near
simple control loop at a slow 1 Hz rate for safety purposes, what can be attained using physical constituent relations.
at which the robot changes its conﬁguration based on its the As evident from the combined model performing unreli-
predicted force and torque from the point cloud stream. At ably compared to the individually trained models, a much
each update step, where values are measured in SI units, broader set of interactions must be collected if pure data-
the robot position in the world frame p = [px py pz]T driven techniques are to be employed. An imminent goal
is updated according to pi ← pi + 0.1Fi,i ∈ {x,y,z}. for future work is to develop a model-based framework
Additionally, a rotation about each of the principal axes in that leverages known physical relationships in elastic shell
thebodyframeattachedtothebaseofthecylinder(ie.atthe theory. This would enable more complicated interactions
forcesensor),isappliedaccordingtoRi(2τi)whereRiisthe with the interface such as those with different parts of the
rotationoperatorabouttheithprincipalaxis.Theseconstants upper limb or complex multiﬁngered interactions such as
were chosen empirically such that they yield a large enough pinching. For instance, our current system would not even
changeinrobotjointconﬁgurationateachstepwithoutbeing be able to react to forces directed away from the camera.
too sensitive to sensor noise. In the forearm application, the Thoughnotexploredhere,physics-basedmodelsshouldalso
force is initially zeroed at the resting position (ie. when help generalize performance across different human users
the forearm’s weight is fully supported by the robot). As with different limb sizes. Moreover, running the system at a
expected from the good validation results in the models, the highercontrolratewouldbemorecompellingforrealworld
system was reliable and consistently responded as expected. applications, and it would be way to test and develop real-
However, an in-depth user study was not performed, so the time HRI algorithms for human-robot teaming and teaching.
usability and comfort of the system is not conﬁrmed here.
1710
Authorized licensed use limited to: University of New South Wales. Downloaded on September 19,2020 at 21:48:50 UTC from IEEE Xplore.  Restrictions apply. REFERENCES [16] K. Kosuge et al. “Dance partner robot - Ms DanceR”.
In:Proceedings2003IEEE/RSJInternationalConfer-
[1] R. Bajcsy I. Huang J. Liu. “A Depth Camera-Based
ence on Intelligent Robots and Systems (IROS 2003)
Soft Fingertip Device for Contact RegionEstimation
(Cat. No.03CH37453). Vol. 4. Oct. 2003, 3459–3464
and Perception-Action Coupling”. In: 2019 Inter-
vol.3.
national Conference on Robotics and Automation
[17] T. Mukai et al. “Development of the Tactile Sensor
(ICRA). 2019.
System of a Human-Interactive Robot ”RI-MAN””.
[2] J.Dargahi.“Athreesensingelementpiezoelectrictac-
In: IEEE Transactions on Robotics 24.2 (Apr. 2008),
tilesensorforroboticandprostheticapplications”.In:
pp. 505–512.
Sensors and Actuators A-physical 80 (2000), pp. 23–
[18] W.D. Stiehl et al. “The design of the huggable: A
30.
therapeutic robotic companion for relational, affective
[3] Jonathan Engel, Jack Chen, and Chang Liu. “Devel-
opment of polyimide ﬂexible tactile sensor skin”. In: touch”. In: vol. 2005. Sept. 2005, pp. 408–415. ISBN:
0-7803-9274-4.
J.Micromech.Microeng13(May2003),pp.359–366.
[19] Christoph Hennersperger et al. “Towards MRI-based
[4] S. Takenawa. “A soft three-axis tactile sensor based
autonomous robotic US acquisitions: a ﬁrst feasibility
on electromagnetic induction”. In: 2009 IEEE Inter-
study”.In:IEEEtransactionsonmedicalimaging36.2
national Conference on Mechatronics. 2009, pp. 1–6.
(2017), pp. 538–548.
[5] N. Lu and D.H. Kim. “Flexible and Stretchable Elec-
[20] P. Beeson and B. Ames. “TRAC-IK: An open-
tronics Paving the Way for Soft Robotics”. In: Soft
sourcelibraryforimprovedsolvingofgenericinverse
Robotics 1.1 (2014), pp. 53–62.
kinematics”. In: 2015 IEEE-RAS 15th International
[6] Brenna D. Argall and Aude G. Billard. “A survey of
Conference on Humanoid Robots (Humanoids). Nov.
Tactile Human-Robot Interactions”. In: Robotics and
2015, pp. 928–935.
Autonomous Systems 58.10 (2010), pp. 1159–1176.
[21] MohammadRahmanetal.“DevelopmentandControl
ISSN: 0921-8890.
of a Robotic Exoskeleton for Shoulder, Elbow and
[7] R.S. Fearing. “Tactile Sensing Mechanisms”. In: The
Forearm Movement Assistance”. In: Applied Bionics
InternationalJournalofRoboticsResearch9.3(1990),
and Biomechanics 9 (July 2012), pp. 275–292.
pp. 3–23.
[8] Joohyung Kim, Alexander Alspach, and Katsu Ya-
mane. “3D printed soft skin for safe human-robot
interaction”. In: Sept. 2015, pp. 2419–2425.
[9] R. Li et al. “Localization and manipulation of
small parts using GelSight tactile sensing”. In: 2014
IEEE/RSJ International Conference on Intelligent
Robots and Systems. 2014, pp. 3988–3993.
[10] Giovanni Tonietti, R. Schiavi, and Antonio Bicchi.
“Design and Control of a Variable Stiffness Actuator
forSafeandFastPhysicalHuman/RobotInteraction”.
In: May 2005, pp. 526–531. ISBN: 0-7803-8914-X.
[11] Thomas H. Speeter. “Three-dimensional Finite Ele-
ment Analysis of Elastic Continua for Tactile Sens-
ing”. In: The International Journal of Robotics Re-
search 11.1 (1992), pp. 1–19.
[12] W. Yuan et al. “Shape-independent hardness estima-
tionusingdeeplearningandaGelSighttactilesensor”.
In: 2017 IEEE International Conference on Robotics
and Automation (ICRA). May 2017, pp. 951–958.
[13] A. Gupta et al. “Learning dexterous manipulation for
a soft robotic hand from human demonstrations”. In:
2016 IEEE/RSJ International Conference on Intelli-
gentRobotsandSystems(IROS).Oct.2016,pp.3786–
3793.
[14] Rui Li and Edward H. Adelson. “Sensing and Recog-
nizingSurfaceTexturesUsingaGelSightSensor”.In:
TheIEEEConferenceonComputerVisionandPattern
Recognition (CVPR). June 2013.
[15] Andrea Bajcsy et al. “Learning robot objectives from
physical human interaction”. In: Nov. 2017.
1711
Authorized licensed use limited to: University of New South Wales. Downloaded on September 19,2020 at 21:48:50 UTC from IEEE Xplore.  Restrictions apply. 