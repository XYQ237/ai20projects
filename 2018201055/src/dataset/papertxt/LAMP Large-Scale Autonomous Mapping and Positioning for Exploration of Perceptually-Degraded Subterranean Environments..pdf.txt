2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
SMArT: Training Shallow Memory-aware Transformers
for Robotic Explainability
Marcella Cornia1, Lorenzo Baraldi1, and Rita Cucchiara1
Abstract—The ability to generate natural language expla- interaction between vision and language has been modeled
nations conditioned on the visual perception is a crucial step either using Recurrent Neural Networks or exploring more
towards autonomous agents which can explain themselves and
recent alternatives – like one-dimensional convolutions or
communicatewithhumans.Whiletheresearcheffortsinimage
fully-attentive models such as the Transformer [10], [11].
and video captioning are giving promising results, this is often
done at the expense of the computational requirements of the Most of the last advancements in the ﬁeld, however, are
approaches, limiting their applicability to real contexts. In due to approaches which rely on complex forms of attention
this paper, we propose a fully-attentive captioning algorithm and of interactions between the visual and the textual do-
which can provide state-of-the-art performances on language
main[12],[13],[11].Thisisoftendoneattheexpenseofthe
generation while restricting its computational demands. Our
computational demands of the algorithm, thus limiting the
model is inspired by the Transformer model and employs only
two Transformer layers in the encoding and decoding stages. applicability of these results to embedded agents and robots.
Further, it incorporates a novel memory-aware encoding of Further, for these approaches to be adapted in robotics, they
image regions. Experiments demonstrate that our approach need to be re-thought in terms of efﬁciency, memory and,
achieves competitive results in terms of caption quality while
power consumption as well as in terms of their adaptability
featuringreducedcomputationaldemands.Further,toevaluate
in real contexts.
itsapplicabilityonautonomousagents,weconductexperiments
on simulated scenes taken from the perspective of domestic Following recent research lines on the investigation of
robots. fully-attentivemodelsforimagecaptioning[11],inthispaper
weproposeashallowandcomputationallyefﬁcientmodelfor
I. INTRODUCTION
image captioning. Our model is inspired by the Transformer
Recent advancements at the intersection of Computer approach and incorporates a novel memory-aware image
Vision,NaturalLanguageProcessingandRoboticshavetried encoder which can model the relationships between image
to bring together the understanding of the visual world and regions by also memorizing knowledge learned from data in
that of natural language from the perspective of robots and a computationally friendly manner. Further, we demonstrate
embodied agents [1], [2], [3], [4], [5]. Such research effort state-of-the-art performances using solely two attentive lay-
has the ﬁnal goal of developing autonomous agents which ers in both the encoder and the decoder. This is in contrast
cannaturallyperceive,understand,andperformactionsinthe with both machine translation models based on attention,
surroundingworld,whilestillhavingacomprehensionofthe and recent attempts to develop captioning systems based on
human language which is fundamental to interact with the the Transformer, which tend to use six or more encoding
ﬁnaluser.Theabilitytodescribeimagesandvideosisoneof and decoding layers. Our approach is competitive in terms
thecorechallengesinthisdomainandacrucialachievement of caption quality and has the additional beneﬁt of having
towardsmachineintelligence[6],[7],[8],[9].Itrequiresnot reducedcomputationaldemandswhencomparedwithrecent
only to recognize salient objects in an image, understand approaches.
their interactions, but also to verbalize them using natural Summarizing, our contributions are as follows: (i) we in-
language, which makes itself very challenging. troduce SMArT, a Shallow and Memory-Aware Transformer
Noticeably,thegenerationofnaturallanguageconditioned for the task of image captioning; (ii) our model incorporates
on the visual input is also a critical step in the direction self-attention and a memory-aware image encoding layer,
of cognitive system explainability and trustworthy Artiﬁcial in which self-attention is endowed with memory vectors;
Intelligence, as it endows an autonomous system with the (iii) we demonstrate competitive results on the reference
ability to describe the reason of its choices, actions and to benchmark for image captioning (COCO) using only two
demonstrate its perception capabilities to the user. To this encoderandtwodecoderlayers.Finally,wedemonstratethe
aim, the research efforts in language generation and image applicabilityofourapproachtosimulatedscenestakenfrom
captioning are giving promising results. the perspective of domestic robots.
Inspired by the developments in natural language pro-
cessing and by the advancements in attention modeling, II. RELATEDWORK
image captioning algorithms have relied on the combination A large variety of methods has been proposed for the
of an image encoder and a natural language decoder. The imagecaptioningtask.Whileearlyapproacheswerebasedon
captiontemplatesﬁlledbyusingtheoutputofpre-trainedob-
1Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara are with the
ject detectors [14], [15], almost all recent captioning models
Department of Engineering “Enzo Ferrari”, University of Modena and
{ }
ReggioEmilia,Modena,Italy name.surname @unimore.it integrate recurrent neural networks as language models [6],
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 1128
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 11:45:15 UTC from IEEE Xplore.  Restrictions apply. [16],[17],[7]withavisualfeatureextractorforconditioning A. Transformer layers
the language model on the visual input.
Both encoder and decoder consist of a stack of Trans-
Therepresentationoftheimagehasbeeninitiallyobtained
former layers which act, respectively, on image regions
from the output of one or more layer of a CNN [18],
and words. In the following, we revise their fundamental
[19], [20], [6]. Then, with the integration of attentive mech-
features. Each encoder layer consists of a self-attention and
anisms [21], the visual representation has turned into a
feed-forward layer, while each decoder layer is a stack of
time-varying vector extracted from a grid of CNN fea-
one self-attentive and one cross-attentive layer, plus a feed-
tures using the hidden state of the language model as the
forward layer. Both attention layers and feed-forward layers
query [22], [23], [6], [24], [8]. Recently, integrating image
are encapsulated into “add-norm” operations, described in
regions eventually extracted from a detector as attention
the following.
candidates has become the predominant strategy in caption-
a) Multi-head attention: the core component of both
ing architectures [25], [8]. On this line, Jiang et al. [13]
self-attentionandcross-attentionlayersisanattentionmech-
proposed a recurrent fusion network to integrate the output
anism [21] with multiple heads with different learned
ofmultipleimageencoders.Yaoetal.[12]haveexploredthe
weights. Attention is applied using scaled dot-products as
incorporation of relationships between image regions, both
similarity measure [10] while keys, queries, and values are
from a semantic point of view and using geometric features
computed through linear transformations.
such as the position and the spatial extent of the region.
Formally, given two sequences x ,x ,...,x and
Their work exploits semantic relationships predictor which 1 2 N
xˆ ,xˆ ,...,xˆ of d-dimensional input vectors, each head
are trained separately, and whose outputs are incorporated 1 2 M
applies two linear transformations to the ﬁrst sequence to
inside a Graph Convolutional Neural Network. Regarding
form key and value vectors:
the training strategies, notable advances have been made
by using Reinforcement Learning to train non-differentiable k =W x , v =W x , (1)
t k t t v t
captioning metrics [26], [16], [20].
where W and W are the key and value transformation
While RNNs have been a popular choice for deﬁning k v ×
matrices, with size d d, where d = d/H is the
the language model, they also suffer from their sequential h h
dimensionality of a single head, and H is the number of
nature and limited representation power. For this reason,
heads. Analogously, a linear transformation is applied to the
researchers have also investigated the use of alternatives
second sequence to obtain query vectors:
which have demonstrated to be compelling for machine
translation. In this context, Aneja et al. [27] have deﬁned a q =W xˆ , (2)
t q t
fully-convolutional captioning model using one-dimensional
convolution and attention over image features. More re- where W has the same size of W and W . Query
q k v
cently, the Transformer model [10] has been proposed as vectors are used to compute a similarity score with key
a powerful and fast language model for translation tasks vectors, and generate a weight distribution over values.
and unsupervised pre-training [28]. Herdade et al. [11] have The similarity score between a query q and a key k is
t c
ﬁrst investigated the usage of the Transformer model for computed as√a scaled dot-product between the two vectors,
(cid:124)
image captioning, proposing a deep architecture which is i.e. (q v )/ d . Each head then produces a vector by
t c h { }
conditioned on geometric region features. In contrast to this averaging the values v with the weights deﬁned by an
c c
concurrentwork,weproposeashallowarchitecturebasedon attentive distribution o(cid:88)ver the similarity scores:
theTransformerandtheuseofanovelmemory-awareregion
N
encoder. When compared to this proposal, our approach y = α v , where (3)
t tc c
demonstratestobebothlesscomputationallydemandingand (cid:80) √
c=1 (cid:124)
more accurate in terms of CIDEr.
exp(q v / √d )
α = t (cid:124)c h . (4)
tc
exp(q v / d )
III. PROPOSEDMETHOD i t i h
Results from different heads are then concatenated and
Our fully-attentive captioning model consists of an en- projected to a vector with dimensionality d through a ﬁnal
coder module, in charge of encoding image regions, and a linear transformation.
decoder module, which is conditioned on the encoder and In the encoding stage, the sequence of image regions is
generatesthenaturallanguagedescription.Incontrasttopre- used to infer queries, keys and values, thus creating a self-
vious captioning approaches, which employed RNNs as the attention pattern in which pairwise region relationships are
language model, we propose to use a fully-attentive model modelled. In the decoder, we instead apply both a cross-
forboththeencodingandthedecodingstage,buildingonthe attention and a masked self-attention pattern. In the former,
Transformermodel[10]formachinetranslation.Inaddition, the sequence of words is used to infer queries and image
we propose a self-attention region encoder with memory regions are used as keys and values. In the latter, the left-
vectors to encode learned knowledge and relationships on hand side of the textual sequence is used to generate keys
the visual input. A summary of our approach and of its and values for each element of the sequence, thus enforcing
components is visually presented in Fig. 1. causality in the generation.
1129
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 11:45:15 UTC from IEEE Xplore.  Restrictions apply. Input Image Object Regions
SMArT
GeneratedCaption
Object  A street with cars and 
Encoder Decoder a motorcycle next to 
Detector
some buildings.
Add-norm
feed-forward
query FC Add-norm
FC Encoder Layer 1 Decoder Layer 2 cross-attention
attention FC
Encoder Layer 2 Decoder Layer 1
memory vectors Add-norm FC query
keyvectors feed-forward self-attention
Add-norm
valuevectors
Fig.1. Overviewofourimagecaptioningapproach.BuildingonaTransformer-likeencoder-decoderarchitecture,ourapproachincludesamemory-aware
regionencoderwhichaugmentsself-attentionwithmemoryvectors.Further,ourapproachisshallow,asitrequiresonlytwoencodinganddecodinglayers.
{ }
b) Position-wise feed-forward layers: the second com- where the ordinary keys k are computed through linear
i i
ponent of a Transformer layer is a fully-connected forward transformations from the sequence (Eq. 1), and memory key
{ }
layer which is applied time-wise over the input sequence. vectors km arelearnableweightsthatactasmemoryslots.
i i
Thisconsistsoftwoafﬁnetransformationswithasinglenon- Asitcanbeseen,memoryslotsareindependentontheinput
linearity, sequenceofdetections,andthereforestoreknowledgewhich
FF(x )=Uσ(Vx +b)+c, (5) does not depend on the input or the context.
t t
where σ(x) = max(x,0) is the RELU activation function, Similarly, thevalues V of an head are extended with
learnable memory slots as well,
and V and ×U are learna×ble weight matrices, respectively  
with sizes d df and df d; b and c are bias terms. The (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
size of the hidden layer d is usually chosen to be larger
f V = v ,v ,...,v ,vm,vm,...,vm , (8)
than d, e.g. four times d in most implementations [10]. 1 2 N 1 2 M
c) Skipconnectionandlayernormalization: Eachsub- ordinaryvalues memoryvaluevectors
layer (attention or position-wise feed-forward) is encapsu- { }
where vm are learnable weights with the same dimen-
lated within a residual connection [29] and layer normaliza- i i
sionality of a value. By deﬁning memory keys and memory
tion [30]. This “add-norm” operation is deﬁned as
values as separate weights, we break the linear dependency
AddNorm(x )=LayerNorm(x +f(x )), (6) between keys and values, thus letting the network learn
t t t
unrelated sets of keys and values. This is in contrast with
wheref indicateseitheranattentionlayeroraposition-wise
concurrent approaches in machine translation which have
feed-forward layer.
investigated the use of persistent memories [31]. Given M
B. Memory-augmented region encoder
as the number of key and value memory slots for each head,
·
Recent captioning literature has demonstrated that regions our model overall learns a set of 2M H memory slots.
identiﬁed by an object detector are the ideal attention can-
didates for encoding the visual input [8]. A stack of self- C. Fully-attentive decoder
attentive layers can naturally model the relationships be-
The language model of our approach is composed of a
tween regions; however, it can not naturally encode a-priori
stack of two decoder layers, each performing self-attention
knowledge learned from data. To overcome this limitation
and cross-attention operations. As mentioned, each cross-
and enhance the visual encoding capabilities of the model,
attentionlayerusesthedecoderoutputsequencetoinferkeys
we endow our encoder with memory slots. In practice, we
andvalues,whileself-attentionlayersrelyexclusivelyonthe
extend each self-attention layer of the encoder so that the
inputsequenceofthedecoder.However,keysandvaluesare
key and value sets of each head contain an additional set of
masked so that each query can only attend to keys obtained
learned parameters.
  frompreviouswords,i.e.thesetofkeysandvaluesforquery
Formally, the set of keys K of each head is extended as { } { }
q are, respectively, k ≤ and v ≤ .
follows:   t i i t i i t
  At training time, the input of the encoder is the ground-
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) { }
truth sentence BOS,w ,w ,...,w , and the model is
1 2 n
K = k ,k ,...,k ,km,km,...,km , (7) trained with a cross-entropy loss to predict the shifted
1 2 N 1 2 M { }
ordinarykeys memorykeyvectors ground-truth sequence, i.e. w1,w2,...,wn,EOS , where
1130
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 11:45:15 UTC from IEEE Xplore.  Restrictions apply. TABLEI TABLEII
CAPTIONINGPERFORMANCE(WITHOUTMEMORY)ASWEVARYTHE THEPERFORMANCEOFOURMODELASWEVARYTHENUMBEROF
NUMBEROFENCODERANDDECODERLAYERS. MEMORYSLOTSFOREACHHEAD.
N. Layers B-1 B-4 M R C S N. Memories B-1 B-4 M R C S
1 80.1 38.1 28.7 58.0 127.9 22.7 No memory 80.2 38.0 28.9 58.3 128.9 22.4
2 80.2 38.0 28.9 58.3 128.9 22.4 20 80.7 38.5 28.9 58.2 129.4 22.4
3 79.4 36.4 27.7 56.8 123.7 20.9 40 80.4 38.1 28.8 58.2 129.7 22.2
4 79.2 36.3 28.0 57.1 121.7 21.6 100 80.7 38.5 29.0 58.4 128.6 22.6
5 79.2 36.2 27.5 56.7 120.5 20.6
6 (as [10], [11]) 79.1 36.1 27.9 56.8 121.9 21.0
withclutteredsurfaces,anddayandnightlightingconditions.
Authors have simulated domestic service robots of multiple
BOS and EOS are special tokens to indicate the start and sizes, resulting in sequences with three different camera
the end of the caption. heights above the ground plane. We employ the validation
While at training time the model jointly predicts all setofthisdataset,forwhichground-truthobjectinformation
output tokens, the generation process at prediction time is is available. This consists of over 21000 images in four
sequential. At each iteration, the model is given as input the simulated indoor video sequences, containing a subset of
partially decoded sequence; it then samples the next input COCO classes.
token from its output probability distribution, until a EOS
B. Evaluation protocols
marker is generated.
Following previous works [26], [20], [8], after a pre- Regardingevaluation,weemploypopularcaptioningmet-
training step using cross-entropy, we further optimize the rics whenever ground-truth captions are available, to eval-
sequence generation using Reinforcement Learning. Speciﬁ- uate both ﬂuency and semantic correctness: BLEU [35],
cally,weemployavariantoftheself-criticalsequencetrain- ROUGE [36], METEOR [37], and CIDEr [32]. BLEU is
ingapproach[20]whichappliestheREINFORCEalgorithm a form of precision of word n-grams between predicted and
on sequences sampled using Beam Search [8]. Further, we ground-truthsentences.Asdoneinpreviousworks,weevalu-
baselinetherewardusingthemeanoftherewardsratherthan ateourpredictionswithBLEUusingn-gramsoflenght1and
greedy decoding as done in [20], [8]. 4.ROUGEcomputesanF-measurewitharecallbiasusinga
Speciﬁcally, given the output of the decoder we sample longest common subsequence technique. METEOR, instead,
the top-k words from the decoder probability distribution scores captions by aligning them to one or more ground-
at each timestep, and always maintain the top-k sequences truths. Alignments are based on exact, stem, synonym, and
withhighestprobability.Wethencomputetherewardofeach paraphrase matches between words and phrases. CIDEr,
sentence wi and backpropagate with respect to it. The ﬁnal ﬁnally, computes the average cosine similarity between n-
gradient expression fo(cid:88)r on(cid:0)e sample is thus: (cid:1) grams found in the generated caption and those found in
reference sentences, weighting them using TF-IDF. While
∇ −1 k − ∇ it has been shown experimentally that BLEU and ROUGE
L((cid:0)θ(cid:80))= (cid:1) (r(wi) b) logp(wi) (9)
θ k θ have lower correlation with human judgments than the other
i=1 metrics [32], the common practice in the image captioning
where b = r(wi) /k is the baseline, computed as the literature is to report all the mentioned metrics. To ensure
i
mean of the rewards obtained by the sampled sequences. a fair evaluation, we use the Microsoft COCO evaluation
To reward the overall quality of the generated caption, we toolkit to compute all scores.
useimagecaptioningmetricsasareward.Followingprevious Whenonlyobject-levelinformationisavailableasground-
works [8], we employ the CIDEr metric (speciﬁcally, the truth,suchasintheACVRdataset,weevaluatethecapability
CIDEr-D score) which has been shown to correlate better of our captioning approach to name objects on the scene.
with human judgment [32]. To assess how the predicted caption covers all the objects,
we also deﬁne a soft coverage measure between the ground-
IV. EXPERIMENTALEVALUATION truthsetofobjectclassesandthesetofnamesinthecaption.
A. Datasets Givenapredictedcaptiony,weﬁrstlyextractallnounsfrom
For comparison with the state of the art, we employ the the sentence. We compute the optimal assignment between
∗
Microsoft COCO dataset, which contains 123287 images them and the set of ground-truth classes c , using distances
labeled with 5 captions each. We employ the data splits between word vectors and the Hungarian algorithm [38].
deﬁned in [33], where 5000 images are used for validation, We then deﬁne an intersection score between the two sets
5000 images for testing and the rest for training. Further, to as the sum of assignment proﬁts. Our coverage measure
assesstheperformanceofourapproachonimagestakenfrom is computed as the ratio of the intersection score and the
arobot-centricpointofview,weemploytheACVRRobotic number of ground-truth object classes:
∗
Vision Challenge dataset [34] which contains simulated data ∗ I(y,y )
from a domestic robot scenario. The dataset contains scenes Cov(y,c )= ∗ , (10)
#c
1131
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 11:45:15 UTC from IEEE Xplore.  Restrictions apply. TABLEIII TABLEIV
COMPARISONWITHTHESTATEOFTHEARTFORIMAGECAPTIONINGON OBJECTCOVERAGEANALYSISONTHEACVRROBOTICVISION
THETESTSETOFTHECOCODATASET. CHALLENGEDATASET,WHENVARYINGTHEMINUMUMOBJECTAREA
THRESHOLD.
Method B-1 B-4 M R C S
Coverage
FC-2K [20] - 31.9 25.5 54.3 106.3 -
Att2all [20] - 34.2 26.7 55.7 114.0 - Method >1% >3% >5% >10%
Up-Down [8] 79.8 36.3 27.7 56.9 120.1 21.4
RFNet [13] 79.1 36.5 27.7 57.3 121.9 21.2 SMArT w/o memory 0.747 0.806 0.836 0.846
SGAE [43] 80.8 38.4 28.4 58.6 127.8 22.1 SMArT (m=20) 0.751 0.808 0.841 0.846
GCN-LSTM [12] 80.9 38.3 28.6 58.5 128.7 22.1 SMArT (m=40) 0.762 0.821 0.848 0.850
ORT [11] 80.5 38.6 28.7 58.4 128.3 22.6 SMArT (m=100) 0.757 0.814 0.843 0.846
SMArT w/o memory 80.2 38.0 28.9 58.3 128.9 22.4
SMArT (m=40) 80.4 38.1 28.8 58.2 129.7 22.2
numberofencodinganddecodinglayersequal.TableIshows
the results obtained after a full training with RL ﬁnetuning.
· ·
where I(, ) is the intersection score, and the # operator Noticeably, the performance obtained with six layers (as in
represents the cardinality of the two sets of nouns. theoriginalTransformermodel[10],andasinthecaptioning
Since images may contain small objects which not neces- model of [11]) is lower than the one obtained when using 1,
sarilyshouldbementionedinacaptiondescribingtheoverall 2 or 3 layers. While the best results are obtained with two
scene, we also deﬁne a variant of the Coverage measure by layers(128.9CIDEr),wenoticethatusingjustonelayerisa
thresholding over the minimum object area. In this case, we compellingalternative,whichstillobtainsaCIDErof127.9.
∗
considerc asthesetofobjectswhoseboundingboxescover 2) Persistent memory vectors: We then evaluate the role
an area higher than the threshold. of using persistent memory vectors in the encoder. Table II
reports the performance obtained by our model with two
C. Implementation and training details
layers and a number of memory slots per head varying from
As mentioned, we use two layers in both the encoder and 0 to 100. As it can be seen, using 40 memory slots for each
thedecoder.Thedimensionalityofalllayers,d,issetto512 headfurtherincreasestheCIDErmetricfrom128.9to129.7.
and we use H = 8 heads. The dimensionality of the inner 3) Comparison with the state of the art: We report the
feed-forward layer, df, is 2048. We use dropout with keep performances of our model in comparison with different
probability 0.9 after each attention layer and after position-
captioning models. In particular, we compare with: FC-
wise feed-forward layers. Input words are represented with
2K [20], an LSTM baseline using a global feature vector as
one-hot vectors and then linearly projected to the input
image descriptor; Att2all [20], which uses additive attention
dimensionality of the model, d. We also employ sinusoidal over the grid of image features extracted from a CNN; Up-
positional encodings [10] to represent word positions inside
Down [8], which employs a two-LSTM layer model with
the sequence, and sum the two embeddings before the ﬁrst
bottom-up features extracted from Faster R-CNN. Also, we
encoding layer.
compare with RFNet [13], which fuses encoded features
To represent image regions, we use Faster R-CNN [39]
from multipleCNN networks; SGAE[43], which introduces
with ResNet-101 [29]. In particular, we employ the model
auto-encoding scene graphs into its model. Finally, we also
ﬁnetunedontheVisualGenomedataset[40]providedby[8].
considerGCN-LSTM[12],whichexplorestheroleofvisual
To compute the intersection score and for extracting nouns
relationships between image regions and, importantly, with
from captions, we use the spaCy NLP toolkit1. We use
ORT[11],whichemployedaTransformer-basedmodelwith
GloVe [41] as word vectors.
six layers.
The model is trained using Adam [42] as optimizer with
TableIIIreportstheperformancesofallthementionedap-
β1 = 0.9 and β2 = 0.98. The learning rate is varied proaches at the end of training with reinforcement learning.
during training using the strategy of [10], i.e. according to
− · − · − Firstly, we notice that our approach overcomes both LSTM-
the formula: d 0.5 min(s 0.5,s w 0.5), where s is the based approaches and ORT [11] according to the CIDEr
current optimization step and w is a warmup parameter, set metric.Noticeablyalso,theperformanceofashallowmodel
to 10000 in all our experiments. After the pre-training with withoutmemoryisinlinewiththestateoftheart.Onallthe
cross-entropy loss, we ﬁnetune the model again using Adam
other metrics, which are less aligned with human judgment,
−
and with a ﬁxed learning rate of 5e 6. our approach achieves competitive performances.
D. Captioning results 4) Performanceonrobot-centricimages: TableIVreports
the Coverage measure for different variants of our approach
1) Shallow vs. deep models: Firstly, we investigate the
onthevalidationsetoftheACVRRoboticVisionChallenge
performance of fully-attentive captioning models as we vary
dataset. As it can be observed, our approach is capable of
the number of encoding and decoding layers. In particular,
mentioning most of the objects on the scene which cover
we start from our model without memory, and keep the
at least 10% of the image, and achieves a 0.76 coverage
1https://spacy.io/ score when thresholding at 1% of the area, thus taking into
1132
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 11:45:15 UTC from IEEE Xplore.  Restrictions apply. A kitchen with a microwave and  A banana sitting on a table next to  A room with two chairs and a table. A person standing in a dark room  A white bathroom with a sink and 
other objects on a table. a couch. with a couch. a toilet.
A person standing on the floor  A bedroom with a bed and a chair. A black stove in a living room with  A chair and a refrigerator in a  A dark room with a few chairs and 
next to a couch. a table. room. a table.
Fig.2. SentencesgeneratedontheACVRRoboticVisionChallengedataset.Wereportobjectsdetectedonthesceneandunderlinetheirmentionsinthe
caption.Objectspresentinthesceneandnotmentionedinthecaptionareshowningray.
and on the same GPU (NVIDIA 1080Ti). To exclude the
effect of different implementations, we re-implement all the
approaches and use the same framework (i.e. PyTorch) and
the same routines whenever possible. As it can be observed,
SMArT is more computationally efﬁcient than ORT [11],
reducing the prediction times by a signiﬁcant margin. Also,
the use of persistent memories does not signiﬁcantly impact
prediction performance. When comparing with Up-Down,
instead, we notice that the mean prediction time is lower
for small batch sizes, while for larger batch sizes the perfor-
mances of Up-Down and of our approach are comparable.
ConsideringthecaptioningqualitygivenbyUp-Down(120.1
CIDEr)andthatofourapproach(129.7CIDEr),ourmethod
provides better caption quality with a comparable computa-
Fig. 3. Prediction times when varying the batch size, for SMArT (w/ tional cost. Most importantly, the reduction in the number
and w/o memory), ORT, and Up-Down. SMArT features higher ﬂuency
of layers provides better caption quality (129.7 CIDEr vs.
and correctness with comparable prediction times compared to LSTM-
basedapproaches;lowerexecutiontimesandcomparablecorrectnesswhen 128.3 CIDEr) and reduced prediction times compared to
comparedtoTransformer-basedapproaches. other Transformer-based approaches.
V. CONCLUSION
accountalsosmallobjects.Reportedresultsalsoconﬁrmthe
ﬁndingsontheeffectivenessofthepersistentmemoryvectors InthispaperwehavepresentedSMArT,anovelapproach
observed on COCO. To further highlight the adaptability of for image captioning, based on a shallow Transformer net-
our solution to robot-centric images, Figure 2 shows some workendowedwithpersistentmemoryvectors.Oursolution
predicted captions on sample images of the ACVR dataset. ismotivatedbytheneedofeffectivebridgesbetweenvision,
language and action that can be deployed on autonomous
E. Computational analysis agents. As shown in the experimental evaluation, SMArT
achieves state of the art caption quality with reduced com-
Wecompleteourexperimentalevaluationbyanalyzingthe
putational needs. Additionally, we have demonstrated the
computational demands of our solution in comparison with
applicability of our approach to robot-centric images from
ORT [11] and Up-Down [8]. We compare with ORT [11]
simulated environments.
as it is the only Transformer-based competitor, and with
Up-Down [8] as it is the most lightweight among recent
ACKNOWLEDGMENT
proposals based on LSTM. Adapt-Att [6], RFNet [13] and
GCN-LSTM [12] can indeed be seen as extensions of Up- Thisworkwaspartiallysupportedbythe“IDEHA-Inno-
Down [8] which add elements to its computational graph, vationforDataElaborationinHeritageAreas”project(PON
thus potentially increasing processing times. ARS01 00421), funded by the Italian Ministry of Education
Figure 3 shows the mean and standard deviation of pre- (MIUR). We also gratefully acknowledge the NVIDIA AI
diction times as a function of the batch size. For a fair Technology Center, EMEA, for its support and access to
evaluation, we run all approaches on the same workstation computing resources.
1133
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 11:45:15 UTC from IEEE Xplore.  Restrictions apply. REFERENCES [21] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation
by jointly learning to align and translate,” in Proceedings of the
[1] P. Anderson, A. Chang, D. S. Chaplot, A. Dosovitskiy, S. Gupta, InternationalConferenceonLearningRepresentations,2015.
V. Koltun, J. Kosecka, J. Malik, R. Mottaghi, M. Savva, et al., [22] K.Xu,J.Ba,R.Kiros,K.Cho,A.Courville,R.Salakhutdinov,R.S.
“On evaluation of embodied navigation agents,” arXiv preprint Zemel,andY.Bengio,“Show,attendandtell:Neuralimagecaption
arXiv:1807.06757,2018. generationwithvisualattention,”inProceedingsoftheInternational
[2] P.Anderson,Q.Wu,D.Teney,J.Bruce,M.Johnson,N.Su¨nderhauf, ConferenceonMachineLearning,2015.
I.Reid,S.Gould,andA.vandenHengel,“Vision-and-languagenav- [23] Q. You, H. Jin, Z. Wang, C. Fang, and J. Luo, “Image captioning
igation:Interpretingvisually-groundednavigationinstructionsinreal with semantic attention,” in Proceedings of the IEEE Conference on
environments,”inProceedingsoftheIEEEConferenceonComputer ComputerVisionandPatternRecognition,2016.
VisionandPatternRecognition,2018. [24] M.Cornia,L.Baraldi,G.Serra,andR.Cucchiara,“Payingmoreatten-
[3] X.Wang,W.Xiong,H.Wang,andW.YangWang,“Lookbeforeyou tiontosaliency:Imagecaptioningwithsaliencyandcontextattention,”
leap:Bridgingmodel-freeandmodel-basedreinforcementlearningfor ACM Transactions on Multimedia Computing, Communications, and
planned-aheadvision-and-languagenavigation,”inProceedingsofthe Applications,vol.14,no.2,p.48,2018.
EuropeanConferenceonComputerVision,2018. [25] M.Pedersoli,T.Lucas,C.Schmid,andJ.Verbeek,“Areasofattention
[4] D. Fried, R. Hu, V. Cirik, A. Rohrbach, J. Andreas, L.-P. Morency, forimagecaptioning,”inProceedingsoftheInternationalConference
T. Berg-Kirkpatrick, K. Saenko, D. Klein, and T. Darrell, “Speaker- onComputerVision,2017.
followermodelsforvision-and-languagenavigation,”inAdvancesin [26] M. Ranzato, S. Chopra, M. Auli, and W. Zaremba, “Sequence level
NeuralInformationProcessingSystems,2018. trainingwithrecurrentneuralnetworks,”inProceedingsoftheInter-
[5] F.Landi,L.Baraldi,M.Corsini,andR.Cucchiara,“EmbodiedVision- nationalConferenceonLearningRepresentations,2015.
and-Language Navigation with Dynamic Convolutional Filters,” in [27] J. Aneja, A. Deshpande, and A. G. Schwing, “Convolutional image
ProceedingsoftheBritishMachineVisionConference,2019. captioning,” in Proceedings of the IEEE Conference on Computer
[6] J.Lu,C.Xiong,D.Parikh,andR.Socher,“Knowingwhentolook: VisionandPatternRecognition,2018.
Adaptive attention via a visual sentinel for image captioning,” in [28] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-
ProceedingsoftheIEEEConferenceonComputerVisionandPattern trainingofDeepBidirectionalTransformersforLanguageUnderstand-
Recognition,2017. ing,”inProceedingsoftheAnnualConferenceoftheNorthAmerican
[7] J. Lu, J. Yang, D. Batra, and D. Parikh, “Neural Baby Talk,” in ChapteroftheAssociationforComputationalLinguistics,2019.
ProceedingsoftheIEEEConferenceonComputerVisionandPattern [29] K.He,X.Zhang,S.Ren,andJ.Sun,“Deepresiduallearningforimage
Recognition,2018. recognition,” in Proceedings of the IEEE Conference on Computer
[8] P.Anderson,X.He,C.Buehler,D.Teney,M.Johnson,S.Gould,and VisionandPatternRecognition,2016.
L. Zhang, “Bottom-up and top-down attention for image captioning [30] J.L.Ba,J.R.Kiros,andG.E.Hinton,“LayerNormalization,”arXiv
and visual question answering,” in Proceedings of the IEEE Confer- preprintarXiv:1607.06450,2016.
enceonComputerVisionandPatternRecognition,2018. [31] S. Sukhbaatar, E. Grave, G. Lample, H. Jegou, and A. Joulin,
[9] M.Cornia,L.Baraldi,andR.Cucchiara,“Show,ControlandTell:A “Augmenting Self-attention with Persistent Memory,” arXiv preprint
Framework for Generating Controllable and Grounded Captions,” in arXiv:1907.01470,2019.
ProceedingsoftheIEEEConferenceonComputerVisionandPattern [32] R. Vedantam, C. Lawrence Zitnick, and D. Parikh, “CIDEr:
Recognition,2019. Consensus-based Image Description Evaluation,” in Proceedings of
[10] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. the IEEE Conference on Computer Vision and Pattern Recognition,
Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in 2015.
AdvancesinNeuralInformationProcessingSystems,2017. [33] A.KarpathyandL.Fei-Fei,“Deepvisual-semanticalignmentsforgen-
erating image descriptions,” in Proceedings of the IEEE Conference
[11] S. Herdade, A. Kappeler, K. Boakye, and J. Soares, “Image Cap-
onComputerVisionandPatternRecognition,2015.
tioning: Transforming Objects into Words,” in Advances in Neural
[34] D. Hall, F. Dayoub, J. Skinner, H. Zhang, D. Miller, P. Corke,
InformationProcessingSystems,2019.
G. Carneiro, A. Angelova, and N. Su¨nderhauf, “Probabilis-
[12] T.Yao,Y.Pan,Y.Li,andT.Mei,“ExploringVisualRelationshipfor
tic Object Detection: Deﬁnition and Evaluation,” arXiv preprint
Image Captioning,” in Proceedings of the European Conference on
arXiv:1811.10800,2018.
ComputerVision,2018.
[35] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “BLEU: a method
[13] W.Jiang,L.Ma,Y.-G.Jiang,W.Liu,andT.Zhang,“RecurrentFusion
forautomaticevaluationofmachinetranslation,”inProceedingsofthe
Network for Image Captioning,” in Proceedings of the European
AnnualMeetingonAssociationforComputationalLinguistics,2002.
ConferenceonComputerVision,2018.
[36] C.-Y.Lin,“Rouge:Apackageforautomaticevaluationofsummaries,”
[14] B. Z. Yao, X. Yang, L. Lin, M. W. Lee, and S.-C. Zhu, “I2t: Image
in Proceedings of the Annual Meeting on Association for Computa-
parsingtotextdescription,”inProceedingsoftheIEEE,2010.
tionalLinguisticsWorkshops,2004.
[15] R. Socher and L. Fei-Fei, “Connecting modalities: Semi-supervised
[37] S. Banerjee and A. Lavie, “METEOR: An automatic metric for
segmentationandannotationofimagesusingunalignedtextcorpora,”
MTevaluationwithimprovedcorrelationwithhumanjudgments,”in
in Proceedings of the IEEE Conference on Computer Vision and
ProceedingsoftheAnnualMeetingonAssociationforComputational
PatternRecognition,2010.
LinguisticsWorkshops,2005.
[16] S. Liu, Z. Zhu, N. Ye, S. Guadarrama, and K. Murphy, “Improved
[38] H. W. Kuhn, “The Hungarian method for the assignment problem,”
image captioning via policy gradient optimization of spider,” in
NavalResearchLogisticsQuarterly,vol.2,no.1-2,pp.83–97,1955.
Proceedings of the International Conference on Computer Vision,
[39] S.Ren,K.He,R.Girshick,andJ.Sun,“FasterR-CNN:Towardsreal-
2017.
timeobjectdetectionwithregionproposalnetworks,”inAdvancesin
[17] J. Johnson, A. Karpathy, and L. Fei-Fei, “Densecap: Fully convolu-
NeuralInformationProcessingSystems,2015.
tional localization networks for dense captioning,” in Proceedings of
[40] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz,
the IEEE Conference on Computer Vision and Pattern Recognition,
S. Chen, Y. Kalantidis, L.-J. Li, D. A. Shamma, M. Bernstein, and
2016.
L.Fei-Fei,“VisualGenome:ConnectingLanguageandVisionUsing
[18] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan, “Show and Tell:
Crowdsourced Dense Image Annotations,” International Journal of
Lessons Learned from the 2015 MSCOCO Image Captioning Chal-
ComputerVision,vol.123,no.1,pp.32–73,2017.
lenge,” IEEE Transactions on Pattern Analysis and Machine Intelli-
[41] J.Pennington,R.Socher,andC.Manning,“GloVe:Globalvectorsfor
gence,vol.39,no.4,pp.652–663,2017.
wordrepresentation,”inProceedingsoftheConferenceonEmpirical
[19] J. Donahue, L. Anne Hendricks, S. Guadarrama, M. Rohrbach,
MethodsinNaturalLanguageProcessing,2014.
S. Venugopalan, K. Saenko, and T. Darrell, “Long-term recurrent
[42] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimiza-
convolutional networks for visual recognition and description,” in
tion,” in Proceedings of the International Conference on Learning
ProceedingsoftheIEEEConferenceonComputerVisionandPattern
Representations,2015.
Recognition,2015.
[43] X.Yang,K.Tang,H.Zhang,andJ.Cai,“Auto-EncodingSceneGraphs
[20] S. J. Rennie, E. Marcheret, Y. Mroueh, J. Ross, and V. Goel, “Self- for Image Captioning,” in Proceedings of the IEEE Conference on
criticalsequencetrainingforimagecaptioning,”inProceedingsofthe ComputerVisionandPatternRecognition,2019.
IEEEConferenceonComputerVisionandPatternRecognition,2017.
1134
Authorized licensed use limited to: Carleton University. Downloaded on September 21,2020 at 11:45:15 UTC from IEEE Xplore.  Restrictions apply. 