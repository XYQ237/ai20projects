2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
FADNet: A Fast and Accurate Network for Disparity Estimation
∗ ∗ † †
Qiang Wang1, , Shaohuai Shi1, , Shizhen Zheng1, Kaiyong Zhao1, , Xiaowen Chu1,
Abstract—Deepneuralnetworks(DNNs)haveachievedgreat
successintheareaofcomputervision.Thedisparityestimation
problem tends to be addressed by DNNs which achieve much
better prediction accuracy in stereo matching than traditional
hand-craftedfeaturebasedmethods.Ononehand,however,the
designed DNNs require signiﬁcant memory and computation
resources to accurately predict the disparity, especially for
those 3D convolution based networks, which makes it difﬁ- (a)Inputimage (b)PSMNet[6]
cult for deployment in real-time applications. On the other
hand, existing computation-efﬁcient networks lack expression
capability in large-scale datasets so that they cannot make an
accuratepredictioninmanyscenarios.Tothisend,wepropose
anefﬁcientandaccuratedeepnetworkfordisparityestimation
namedFADNetwiththreemainfeatures:1)Itexploitsefﬁcient
2Dbasedcorrelationlayerswithstackedblockstopreservefast
computation;2)Itcombinestheresidualstructurestomakethe (c)OurFADNet (d)Groundtruth
deeper model easier to learn; 3) It contains multi-scale predic- Fig. 1: Performance illustrations. (a) a challenging input
tions so as to exploit a multi-scale weight scheduling training
image.(b)ResultofPSMNet[6]whichconsumes13.99GB
technique to improve the accuracy. We conduct experiments
to demonstrate the effectiveness of FADNet on two popular GPU memory and runs 399.3 ms for one stereo image pair
datasets, Scene Flow and KITTI 2015. Experimental results on an Nvidia Tesla V100 GPU. (c) Result of our FADNet,
showthatFADNetachievesstate-of-the-artpredictionaccuracy, which only consumes 1.62 GB GPU memory and runs 18.7
and runs at a signiﬁcant order of magnitude faster speed than msforonestereoimagepairontheNvidiaTeslaV100GPU.
existing 3D models. The codes of FADNet are available at
https://github.com/HKBU-HPML/FADNet.
learning (AutoML) for neural architecture search (NAS) on
I. INTRODUCTION stereo matching. In practice, to measure whether a DNN
model is good enough, we not only need to evaluate its
It has been seen that deep learning has been widely
accuracy on unseen stereo images (whether it can estimate
deployed in many computer vision tasks. Disparity estima-
the disparity correctly), but also need to evaluate its time
tion (also referred to as stereo matching) is a classical and
efﬁciency (whether it can generate the results in real-time).
important problem in computer vision applications, such as
3D scene reconstruction, robotics and autonomous driving. In ED-Conv2D methods, stereo matching neural networks
While traditional methods based on hand-crafted feature [2][3][5] are ﬁrst proposed for end-to-end disparity estima-
extraction and matching cost aggregation such as Semi- tionbyexploitinganencoder-decoderstructure.Theencoder
GlobalMatching(SGM)[1])tendtofailonthosetextureless part extracts the features from the input images, and the
and repetitive regions in the images, recent advanced deep decoder part predicts the disparity with the generated fea-
neural network (DNN) techniques surpass them with decent tures.Thedisparitypredictionisoptimizedasaregressionor
generalization and robustness to those challenging patches, classiﬁcation problem using large-scale datasets (e.g., Scene
and achieve state-of-the-art performance in many public Flow [5]) with disparity ground truth. The correlation layer
datasets [2][3][4][5][6][7]. The DNN-based methods for [10][5] is then proposed to increase the learning capability
disparity estimation are end-to-end frameworks which take of DNNs in disparity estimation, and it has been proved to
stereo images (left and right) as input to the neural network besuccessfulinlearningstrongfeaturesatmultiplelevelsof
and predict the disparity directly. The architectures of DNN scales [10][5][11][12][13]. To further improve the capability
are very essential to achieve accurate estimation, and can be ofthemodels,residualnetworks[14][15][16]areintroduced
categorized into two classes, encoder-decoder network with into the architecture of disparity estimation networks since
2D convolution (ED-Conv2D) and cost volume matching the residual structure enables much deeper network to be
with3Dconvolution(CVM-Conv3D).Besides,recentstudies easier to train [17]. The ED-Conv2D methods have been
[8], [9] begin to reveal the potential of automated machine provedcomputingefﬁcient,buttheycannotachieveveryhigh
estimation accuracy.
∗
Authorshavecontributedequally. To address the accuracy problem of disparity estimation,
†
Correspondingauthors. researchers have proposed CVM-Conv3D networks to better
{ 1Department of Computer Science, Hong Kong Baptis}t University, capture the features of stereo images and thus improve the
qiangwang,csshshi,szzheng,kyzhao,chxw
@comp.hkbu.edu.hk estimation accuracy [3][18][6][7][19]. The key idea of the
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 101
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 10:19:04 UTC from IEEE Xplore.  Restrictions apply. CVM-Conv3D methods is to generate the cost volume by multi-view images. Although monocular vision is low cost
concatenating left feature maps with their corresponding and commonly available in practice, it does not explicitly
right counterparts across each disparity level [18][6]. The introduce any geometrical constraint, which is important
features of cost volume are then automatically extracted by for disparity estimation[20]. On the contrary, stereo vision
3D convolution layers. However 3D operations in DNNs are leverages the advantages of cross-reference between the left
computing-intensive and hence very slow even with current and the right view, and usually show greater performance
powerfulAIaccelerators(e.g.,GPUs).Althoughthe3Dcon- androbustnessingeometricaltasks.Inthispaper,wemainly
volution based DNNs can achieve state-of-the-art disparity discuss the work related to stereo images for disparity
estimation accuracy, they are difﬁcult for deployment due estimation, which is classiﬁed into two categories: 2D based
to their resource requirements. On one hand, it requires a and 3D based CNNs.
large amount of memory to install the model; so only a In 2D based CNNs, end-to-end architectures with mainly
limited set of accelerators (like Nvidia Tesla V100 with convolution layers [5][21] are proposed for disparity esti-
32GB memory) can run these models. On the other hand, it mation, which use two stereo images as input and generate
takes several seconds to generate a single result even on the the disparity directly and the disparity is optimized as a
verypowerfulTeslaV100GPUusingCVM-Conv3Dmodels. regression task. However, the models are pure 2D CNN
The memory consumption and the inefﬁcient computation architectures which are difﬁcult to capture the matching fea-
make the CVM-Conv3D methods difﬁcult to be deployed in turessuchthattheestimationresultsarenotgood.Toaddress
practice. Therefore, it is crucial to address the accuracy and the problem, the correlation layer which can express the
efﬁciency problems for real-world applications. relationshipbetweenleftandrightimagesisintroducedinthe
To this end, we propose FADNet which is a Fast end-to-end architecture (e.g., DispNetCorr1D [5], FlowNet
and Accurate Disparity estimation Network based on ED- [10], FlowNet2 [22], DenseMapNet [23]). The correlation
Conv2D architectures. FADNet can achieve high accuracy layersigniﬁcantlyincreasestheestimatingperformancecom-
while keeping a fast inference speed. As illustrated in Fig. pared to the pure CNNs, but existing architectures are still
1, our FADNet can easily obtain comparable performance not accurate enough for production.
as state-of-the-art PSMNet [6], while it runs approximately 3D based CNNs are further proposed to increase the
× ×
20 faster than PSMNet and consumes 10 less GPU estimation performance [3][18][6][7][19], which employ 3D
memory. In FADNet, we ﬁrst exploit the multiple stacked convolutions with cost volume. The cost volume is mainly
2D-basedconvolutionlayerswithfastcomputation,andthen formed by concatenating left feature maps with their cor-
wecombinestate-of-the-artresidualarchitecturestoimprove responding right counterparts across each disparity level
the learning capability, and ﬁnally we introduce multi-scale [18][6], and the features of the generated cost volumes can
outputs for FADNet so that it can exploit the multi-scale be learned by 3D convolution layers. The 3D based CNNs
weight scheduling to improve the training speed. These can automatically learn to regularize the cost volume, which
features enable FADNet to efﬁciently predict the disparity have achieved state-of-the-art accuracy of various datasets.
with high accuracy as compared to existing work. Our However, the key limitation of the 3D based CNNs is
contributions are summarized as follows: their high computation resource requirements. For example,
• We propose an accurate yet efﬁcient DNN architecture training GANet [7] with the Scene Flow [5] dataset takes
fordisparityestimationnamedFADNet,whichachieves weeks even using very powerful Nvidia Tesla V100 GPUs.
comparablepredictionaccuracyasCVM-Conv3Dmod- Eventheyachievegoodaccuracy,itisdifﬁculttodeploydue
elsanditrunsatanorderofmagnitudefasterspeedthan to their very low time efﬁciency. To this end, we propose a
the 3D-based models. fast and accurate DNN model for disparity estimation.
• We develop a multiple rounds training scheme with
III. MODELDESIGNANDIMPLEMENTATION
multi-scaleweightschedulingforFADNetduringtrain-
ing, which improves the training speed yet maintains Our proposed FADNet exploits the structure of DispNetC
the model accuracy. [5] as a backbone, but it is extensively reformed to take
• We achieve state-of-the-art accuracy on the Scene Flow care of both accuracy and inference speed, which is lacking
× ×
datasetwithupto20 and45 fasterdisparitypredic- in existing studies. We ﬁrst change the structure in terms of
tionspeedthanPSMNet[6]andGANet[7]respectively. branchdepthandlayertypebyintroducingtwonewmodules,
Therestofthepaperisorganizedasfollows.Weintroduce residual block and point-wise correlation. Then we exploit
some related work in DNN based stereo matching problems the multi-scale residual learning strategy for training the
in Section II. Section III introduces the methodology and reﬁnement network. Finally, a loss weight training schedule
implementation of our proposed network. We demonstrate is used to train the network in a coarse-to-ﬁne manner.
our experimental results in Section IV. We ﬁnally conclude
A. Residual Block and Point-wise Correlation
the paper in Section V.
DispNetC and DispNetS which are both from the study
II. RELATEDWORK in [5] basically use an encoder-decoder structure equipped
There exist many studies using deep learning methods with ﬁve feature extraction and down-sampling layers and
in estimating image depth using monocular, stereo and ﬁve feature deconvolution layers. While conducting feature
102
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 10:19:04 UTC from IEEE Xplore.  Restrictions apply. L
Dual Point-Wise
ResBlock Correlation
DeConvolution Disparity
R
Element-Wise
Concatenate
Addition
Å
Conv, 3x3, Å Å Å Å Å Å Å
stride=1
Conv, 3x3, L
Conv, 3x3, stride=2
stride=1
Å
Conv, 3x3,
stride=2 Conv, 3x3,
stride=2
Warped
Conv, 3x3, L
Dual Conv stride=2
R
Å
Dual ResBlock
Fig. 2: The model structure of our proposed FADNet.
extractionand down-sampling,DispNetCand DispNetSﬁrst where k is the kernel size of cost matching, x and x
1 2
adopt a convolution layer with a stride of 1 and then a are the centers of two patches from f and f respectively.
1 ×2 × ×
convolution layer with a stride of 2 so that they consistently Computingallpatchcombinationsinvolvesc K2 w2 h2
×
shrink the feature map size by half. We call the two-layer multiplication and produces a cost matching map of w h.
convolutions with size reduction as Dual-Conv, which is GivenamaximumsearchingrangeD,weﬁxx andshiftthe
− 1
shownintheleft-bottomcornerofFig.2.DispNetCequipped x onthex-axisdirectionfrom DtoDwithastrideoftwo.
2 × ×
with Dual-Conv modules and a correlation layer ﬁnally Thus, the ﬁnal output cost volume size will be w h D.
achieves an end-points error (EPE) of 1.68 on the Scene However,thecorrelationoperationassumesthateachpixel
Flow dataset, as reported in [5]. inthepatchcontributesequallytothepoint-wiseconvolution
The residual block originally derived in [14] for image results,whichmaylosstheabilitytolearnmorecomplicated
classiﬁcation tasks is widely used to learn robust features matching patterns. Here we propose point-wise correlation
and train a very deep networks. The residual block can composed of two modules. The ﬁrst module is a classical
×
welladdressthegradientvanishproblemwhentrainingvery convolution layer with a kernel size of 3 3 and a stride of
deep networks. Thus, we replace the convolution layer in 1. The second one is an e(cid:88)lement-wise multiplication which
the Dual-Conv module by the residual block to construct a is deﬁned by Eq. (2).
new module called Dual-ResBlock, which is shown in the (cid:104) (cid:105)
c(x ,x )= f (x ),f (x ) , (2)
left-bottom corner of Fig. 2. With Dual-ResBlock, we can 1 2 1 1 2 2
make the network deeper without training difﬁculty as the whereweremovethepatchconvolutionmannerfromEq.(1).
residualblockallowsustotrainverydeepmodels.Therefore, Since the maximum valid disparity is 192 in the evaluated
we further increase the number of feature extraction and datasets, the maximum search range for the original image
down-sampling layers from ﬁve to seven. Finally, DispNetC resolution is no more than 192. Remember that the correla-
and DispNetS are evolving to two new networks with better tion layer is put after the third Dual-ResBlock, of which the
learning ability, which are called RB-NetC and RB-NetS output feature resolution is 1/8. So a proper searching range
respectively, as shown in Fig. 2. value should not be less than 192/8=16. We set a marginally
One of the most important contributions of DispNetC larger value 20. We also test some other values, such as 10
is the correlation layer, which targets at ﬁnding corre- and 40, which do not surpass the version of using 20 in
spondences between the left and right images. Given two the network. The reason is that applying too small or large
multi-channel feature maps f ,f with w,h and c as their search range value may lead to under-ﬁtting or over-ﬁtting.
1 2
width, height and nu(cid:88)mber of channels, the correlation layer Table I lists the accuracy improvement brought by apply-
calculates the cost volume of them using Eq. (1). ing the proposed Dual-ResBlock and point-wise correlation.
We train them using the same dataset as well as the training
(cid:104) (cid:105)
c(x1,x2)= f1(x1+o),f2(x2+o) , (1) schemes.ItisobservedthatRB-NetCoutperformsDispNetC
∈− ×−
o [ k,k] [ k,k] withamuchlowerEPE,whichindicatestheeffectivenessof
103
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 10:19:04 UTC from IEEE Xplore.  Restrictions apply. the residual structure. We also notice that setting a proper Note that d is the ground truth disparity of scale 1
s 2s
searching range value of the correlation layer helps further and dˆ is the predicted disparity of scale 1 . The loss
s 2s
improve the model accuracy. function is separately applied in the seven scales of outputs,
which generates seven loss values. The loss values are then
TABLE I: Model accuracy improvement of Dual-ResBlock accumulated with loss weights.
and point-wise correlation with different D.
Model D TrainingEPE TestEPE TABLE II: Multi-scale loss weight scheduling.
DRiBsp-NNeettCC 2100 22..8298 22..8006 Round w0 w1 w2 w3 w4 w5 w6
1 0.32 0.16 0.08 0.04 0.02 0.01 0.005
RB-NetC 20 2.09 1.76
2 0.6 0.32 0.08 0.04 0.02 0.01 0.005
RB-NetC 40 2.12 1.83
3 0.8 0.16 0.04 0.02 0.01 0.005 0.0025
4 1.0 0 0 0 0 0 0
B. Multi-Scale Residual Learning
The loss weight scheduling technique which is initially
Instead of directly stacking DispNetC and DispNetS sub-
proposed in [5] is useful to learn the disparity in a coarse-
networks to conduct disparity reﬁnement procedure [12], we
to-ﬁne manner. Instead of just switching on/off the losses of
apply the multi-scale residual learning ﬁrstly proposed by
different scales, we apply different non-zero weight groups
[24]. The basic idea is that the second reﬁnement network
for tackling different scale of disparity. Let w denote the
learns the disparity residuals and accumulates them into s
the initial results generated by the ﬁrst network, instead of weight for the loss of th(cid:88)e scale of s. The ﬁnal loss function
is
directly predicting the whole disparity map. In this way,
6
the second network only needs to focus on learning the
L= w L (d ,dˆ). (6)
highlynonlinearresidual,whichiseffectivetoavoidgradient s s s s
s=0
vanishing.OurﬁnalFADNetisformedbystackingRB-NetC
and RB-NetS with multi-scale residual learning, which is The speciﬁc setting is listed in Table II. Totally there are
shown in Fig. 2. seven scales of predicted disparity maps. At the beginning,
As illustrated in Fig. 2, the upper RB-NetC takes the we assign low-value weights for those large scale disparity
left and right images as input and produces disparity maps maps to learn the coarse features. Then we increase the loss
at a total of 7 scales, denoted by c , where s is from 0 weights of large scales to let the network gradually learn
s
to 6. The bottom RB-NetS exploits the inputs of the left theﬁnerfeatures.Finally,wedeactivateallthelossesexcept
image,rightimage,andthewarpedleftimagestopredictthe theﬁnalpredictoneoftheoriginalinputsize.Withdifferent
residuals. The generated residuals (denoted by r ) from RB- roundsofweightscheduling,theevaluationEPEisgradually
s
NetS are then accumulated to the prediction results by RB- increased to the ﬁnal accurate performance which is shown
NetCtogeneratetheﬁnaldisparitymapswithmultiplescales in Table III on the Scene Flow dataset.
(s = 0,1,...,6). Thus, the ﬁnal disparity maps predicted by
FADNet, denoted by dˆ, can be calculated by TABLE III: Model accuracy with different rounds of weight
s
scheduling.
≤ ≤
dˆ =c +r ,0 s 6. (3)
s s s Round #Epochs TrainingEPE TestEPE Improvement(%)
C. Loss Function Design 1 20 1.85 1.57 -
2 20 1.33 1.32 18.9
Given a pair of stereo RGB images, our FADNet takes
3 20 1.04 0.93 41.9
themasinputandproducessevendisparitymapsatdifferent 4 30 0.92 0.83 12.0
×
scales. Assume that the input image size is H W. The Note: “Improvement” indicates the improvement of the current round of
dimension of the seven scales of the output disparity maps weightscheduleoveritsprevious.
× × × × ×
areH W, 1H 1W, 1H 1W, 1H 1W, 1 H 1 W,
× 2 2 ×4 4 8 8 16 16
1 H 1 W,and 1 H 1 W respectively.TotrainFADNet Table III lists the model accuracy improvements (around
32 32 64 64
inanend-to-endmanner,weadoptthepixel-wisesmoothL1 12%-41%) brought by the multiple round training of four
lossbetweenthepredicteddisparitymapandthegroundtruth loss weight groups. It is observed that both the training
(cid:88)
using and testing EPEs are decreased smoothly and close, which
indicates good generalization and advantages of our training
L (d ,dˆ)= 1 N smooth (di −dˆi), (4) strategy.
s s s N L1 s s
i=1
IV. PERFORMANCEEVALUATION
where N is the number of(cid:40)pixels of the disparity map, di is
∈R s
the ith element of d N and In this section, we present the experimental results of our
s
| | proposedFADNetcomparedtoexistingwork(i.e.,DispNetC
0.5x2, if x <1
smooth (x)= | |− (5) [5], PSMNet [6], GANet [7] and DenseMapNet [23]) in
L1 x 0.5, otherwise.
terms of accuracy and time efﬁciency.
104
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 10:19:04 UTC from IEEE Xplore.  Restrictions apply. TABLE IV: Disparity EPE on the scene ﬂow dataset
A. Experimental Setup
We implement our FADNet using PyTorch1, which is one Model EPE Memory Runtime(ms)
(GB) TitanX(Pascal) TeslaV100
ofpopulardeeplearningframeworks,andwemakethecodes
FADNet(ours) 0.83 3.87 65.5 48.1
and experimental setups be publicly available2. DispNetC 1.68 1.62 28.7 18.7
In terms of accuracy, the model is trained with Adam DenseMapNet 5.36 - <30 -
PSMNet 1.09 13.99 OOM 399.3
(β = 0.9,β = 0.999). We perform color normalization
1 2 GANet 0.84 29.1 OOM 2251.1
with the mean ([0.485, 0.456, 0.406]) and variation ([0.229,
Note:“OOM”indicatesthatitrunsoutofmemory.Runtimeistheinference
0.224, 0.225]) of the ImageNet [25] dataset for data pre- timeperpairofstereoimages,anditismeasuredby100runswithaverage.
processing. During training, images are randomly cropped Theunderlinenumbersarefromtheoriginalpaper.
to size H =384 and W =768. The batch size is set to 16
for the training on four Nvidia Titan X (Pascal) GPUs (each
than GANet and PSMNet respectively on an Nvidia Tesla
of 4). We apply a four-round training scheme illustrated in
V100 GPU. Even PSMNet and GANet are not runnable on
Section III-C, where each round adopts one different loss
the Titan X (Pascal) GPU, which implies high cost of them
weight group. At the beginning of each round, the learning
− in practice. Compared to DispNetC and DenseMapNet, even
rate is initialized as 10 4 and is decayed by half every 10
FADNetisrelativelyslow,itpredictsthedisparitymorethan
epochs. We train 20 epochs for the ﬁrst three rounds and 30 ×
2 accurate than DispNetC and DenseMapNet, which is a
for the last round.
hugeaccuracyimprovement.Thevisualizedcomparisonwith
Intermsoftimeefﬁciency,weevaluatetheinferencetime
predicted disparity maps are shown in Fig. 3.
of existing state-of-the-art DNNs including both 2D and
3D based networks using a pair of stereo images (H = TABLE V: Results on the KITTI 2015 dataset
576,W = 960) from the Scene Flow dataset [5] on a
desktop-level Nvidia Titan X (Pascal) GPU (with 12GB Model Noc(%) All(%)
D1-bg D1-fg D1-all D1-bg D1-fg D1-all
memory) and a server-level Nvidia Tesla V100 GPU (with FADNet(ours) 2.49% 3.07% 2.59% 2.68% 3.50% 2.82%
32GB memory). DispNetC 4.11% 3.72% 4.05% 4.32% 4.41% 4.34%
GC-Net 2.02% 5.58% 2.61% 2.21% 6.16% 2.87%
PSMNet 1.71% 4.31% 2.14% 1.86% 4.62% 2.32%
B. Dataset
GANet 1.34% 3.11% 1.63% 1.48% 3.46% 1.81%
We used two publicly popular available datasets to train Note: “Noc” and “All” indicates percentage of outliers averaged over ground truth
pixelsofnon-occludedandallregionsrespectively.“D1-bg”,“D1-fg”and“D1-all”
and evaluate the performance of our FADNet. The ﬁrst one
indicatespercentageofoutliersaveragedoverbackground,foregroundandallground
is Scene Flow which is produced by synthetic rendering truthpixelsrespectively.
techniques.ThesecondoneisKITTI2015whichiscaptured
From the visualized disparity maps shown in Fig. 3, we
by real world cameras and laser sensors.
canseethatthedetailsoftexturesaresuccessfullyestimated
1) Scene Flow [5]: a large synthetic dataset which pro-
byourFADNetwhilePSMNetisalittleworseandDispNetC
vides totally 39,824 samples of stereo RGB images (35,454
almost misses all the details. The visualization results are
for training and 4,370 for testing). The full resolution of
× dramatically different although the EPE gap between Disp-
the images is 960 540. The dataset covers a wide range of
NetCandFADNetisonly0.85.Inthequalitativeevaluation,
object shapes and texture and provides high-quality dense
FADNet is more robust and accurate than DispNetC with a
disparity ground truth. We use the endpoint error (EPE) as
2D based network and PSMNet with a 3D based network.
error measurement. We remove those pixels whose disparity
From Table IV, it is noticed that the CVM-Conv3D
values are larger than 192 in the loss computation, which is
architecturescannotbeusedonthedesktop-levelGPUwhich
typically done by the previous studies [6][7].
isequippedwith12GBmemory,whiletheproposedFADNet
2) KITTI 2015 [26]: an open benchmark dataset which
requires only 3.87 GB to perform the disparity estimation.
contains 200 stereo images which are grayscale and have
× The low memory requirement of FADNet makes it much
a resolution of 1241 376. The ground truth of disparity is
easier for deployment in real-world applications. DispNetC
generated by the LIDAR equipment, so the disparity map is
× is also an efﬁcient architecture in terms of both memory
very sparse. During training, we randomly crop 1024 256
consumption and computing efﬁciency, but its estimation
resolution of images and disparity maps. We use its full
performance is bad such that it cannot be used in real-world
resolution during test.
applications. In summary, FADNet not only achieves high
C. Experimental Results disparityestimationaccuracy,butitisalsoveryefﬁcientand
practical for deployment.
The experimental results on the Scene Flow dataset are
The experimental results on the KITTI 2015 dataset are
shown in Table IV. Regarding the model accuracy mea-
showninTableV.GANetachievesthebestestimationresults
sured with EPE, our proposed FADNet achieves comparable
among the evaluated models, and our proposed FADNet
performance compared to the state-of-the-art CVM-Conv3D
× × performscomparableerrorratesonthemetricofD1-fg.The
(PSMNet and GANet), while FADNet is 46 and 8 faster
qualitative evaluation of the KITTI 2015 dataset is shown in
Fig. 4, it is seen that the error maps of FADNet are close to
1https://pytorch.org
2https://github.com/HKBU-HPML/FADNet PSMNet, while they are much better than that of DispNetC.
105
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 10:19:04 UTC from IEEE Xplore.  Restrictions apply. (a)DispNetC (b)PSMNet (c)FADNet
Fig. 3: Results of disparity prediction for Scene Flow testing data. The leftest column shows the left images of the stereo
pairs. The rest three columns respectively show the disparity maps estimated by (a) DispNetC [5], (b) PSMNet [6], (c)
FADNet.
(a)DispNetC (b)PSMNet (c)FADNet
Fig. 4: Results of disparity prediction for KITTI 2015 testing data. The leftest column shows the left images of the stereo
pairs. The rest three columns respectively show the disparity maps estimated by (a) DispNetC [5], (b) PSMNet [6], (c)
FADNet, as well as their error maps.
V. CONCLUSIONANDFUTUREWORK thispaper.First,wewouldliketodevelopfastdisparityinfer-
ence of FADNet on edge devices. Since the computational
In this paper, we proposed an efﬁcient yet accurate neural
capability of them is much lower than that of the server
network, FADNet, for end-to-end disparity estimation to
GPUs used in our experiments, it is necessary to explore
embrace the time efﬁciency and estimation accuracy on the
the techniques of model compression, including pruning,
stereo matching problem. The proposed FADNet exploits
quantization, and so on. Second, we would also like to
point-wisecorrelationlayers,residualblocks,andmulti-scale
apply AutoML [9] for searching a well-performing network
residual learning strategy to make the model be accurate
structure for disparity estimation.
in many scenarios while preserving fast inference time. We
compared FADNet with existing state-of-the-art 2D and 3D
ACKNOWLEDGEMENTS
based methods on two popular datasets in terms of accu-
racy and speed. Experimental results showed that FADNet This research was supported by Hong Kong RGC GRF
achievescomparableaccuracywhileitrunsmuchfasterthan grant HKBU 12200418. We thank the anonymous reviewers
the 3D based models. Compared to the 2D based models, for their constructive comments and suggestions. We would
FADNet is more than two times accurate. alsoliketothankNVIDIAAITechnologyCentre(NVAITC)
We have two future directions following our discovery in for providing the GPU clusters for some experiments.
106
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 10:19:04 UTC from IEEE Xplore.  Restrictions apply. REFERENCES [22] E.Ilg,N.Mayer,T.Saikia,M.Keuper,A.Dosovitskiy,andT.Brox,
“Flownet 2.0: Evolution of optical ﬂow estimation with deep net-
[1] H. Hirschmuller, “Stereo processing by semiglobal matching and works,” in Proceedings of the IEEE conference on computer vision
mutual information,” IEEE Transactions on pattern analysis and andpatternrecognition,2017,pp.2462–2470.
machineintelligence,vol.30,no.2,pp.328–341,2007. [23] R.Atienza,“Fastdisparityestimationusingdensenetworks,”in2018
[2] S.ZagoruykoandN.Komodakis,“Learningtocompareimagepatches IEEEInternationalConferenceonRoboticsandAutomation(ICRA).
via convolutional neural networks,” in Proceedings of the IEEE
IEEE,2018,pp.3207–3212.
conference on computer vision and pattern recognition, 2015, pp.
[24] J. Pang, W. Sun, J. S. Ren, C. Yang, and Q. Yan, “Cascade resid-
4353–4361.
ual learning: A two-stage convolutional neural network for stereo
[3] J.Zbontar,Y.LeCunetal.,“Stereomatchingbytrainingaconvolu- matching,”inTheIEEEInternationalConferenceonComputerVision
tionalneuralnetworktocompareimagepatches.”JournalofMachine (ICCV)Workshops,Oct2017.
LearningResearch,vol.17,no.1-32,p.2,2016.
[25] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei,
[4] A.Dosovitskiy,P.Fischer,E.Ilg,P.Hausser,C.Hazirbas,V.Golkov, “ImageNet: A large-scale hierarchical image database,” in Computer
P. van der Smagt, D. Cremers, and T. Brox, “Flownet: Learning VisionandPatternRecognition,2009.CVPR2009.IEEEConference
opticalﬂowwithconvolutionalnetworks,”inTheIEEEInternational on. IEEE,2009,pp.248–255.
ConferenceonComputerVision(ICCV),December2015.
[26] M.Menze,C.Heipke,andA.Geiger,“Joint3destimationofvehicles
[5] N.Mayer,E.Ilg,P.Hausser,P.Fischer,D.Cremers,A.Dosovitskiy, and scene ﬂow,” in ISPRS Workshop on Image Sequence Analysis
and T. Brox, “A large dataset to train convolutional networks for (ISA),2015.
disparity,opticalﬂow,andsceneﬂowestimation,”inProceedingsof
the IEEE Conference on Computer Vision and Pattern Recognition,
2016,pp.4040–4048.
[6] J.-R. Chang and Y.-S. Chen, “Pyramid stereo matching network,” in
The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR),June2018.
[7] F. Zhang, V. Prisacariu, R. Yang, and P. H. Torr, “Ga-net: Guided
aggregation net for end-to-end stereo matching,” in The IEEE Con-
ference on Computer Vision and Pattern Recognition (CVPR), June
2019.
[8] T.Saikia,Y.Marrakchi,A.Zela,F.Hutter,andT.Brox,“Autodispnet:
Improving disparity estimation with automl,” in The IEEE Interna-
tionalConferenceonComputerVision(ICCV),October2019.
[9] X.He,K.Zhao,andX.Chu,“Automl:Asurveyofthestate-of-the-
art,”arXivpreprintarXiv:1908.00709,2019.
[10] A.Dosovitskiy,P.Fischer,E.Ilg,P.Hausser,C.Hazirbas,V.Golkov,
P. Van Der Smagt, D. Cremers, and T. Brox, “Flownet: Learning
opticalﬂowwithconvolutionalnetworks,”inProceedingsoftheIEEE
internationalconferenceoncomputervision,2015,pp.2758–2766.
[11] E.Ilg,N.Mayer,T.Saikia,M.Keuper,A.Dosovitskiy,andT.Brox,
“Flownet 2.0: Evolution of optical ﬂow estimation with deep net-
works,” in The IEEE Conference on Computer Vision and Pattern
Recognition(CVPR),July2017.
[12] E. Ilg, T. Saikia, M. Keuper, and T. Brox, “Occlusions, motion and
depth boundaries with a generic network for disparity, optical ﬂow
orsceneﬂowestimation,”inTheEuropeanConferenceonComputer
Vision(ECCV),September2018.
[13] Z. Liang, Y. Feng, Y. Guo, H. Liu, W. Chen, L. Qiao, L. Zhou,
and J. Zhang, “Learning for disparity estimation through feature
constancy,” in Proceedings of the IEEE Conference on Computer
VisionandPatternRecognition,2018,pp.2811–2820.
[14] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for
imagerecognition,”inTheIEEEConferenceonComputerVisionand
PatternRecognition(CVPR),June2016.
[15] A.E.OrhanandX.Pitkow,“Skipconnectionseliminatesingularities,”
arXivpreprintarXiv:1701.09175,2017.
[16] W. Zhan, X. Ou, Y. Yang, and L. Chen, “Dsnet: Joint learning for
scene segmentation and disparity estimation,” in 2019 International
Conference on Robotics and Automation (ICRA). IEEE, 2019, pp.
2946–2952.
[17] X. Du, M. El-Khamy, and J. Lee, “Amnet: Deep atrous
multiscale stereo disparity estimation networks,” arXiv preprint
arXiv:1904.09099,2019.
[18] A. Kendall, H. Martirosyan, S. Dasgupta, P. Henry, R. Kennedy,
A. Bachrach, and A. Bry, “End-to-end learning of geometry and
context for deep stereo regression,” in Proceedings of the IEEE
InternationalConferenceonComputerVision,2017,pp.66–75.
[19] G.-Y. Nie, M.-M. Cheng, Y. Liu, Z. Liang, D.-P. Fan, Y. Liu, and
Y.Wang,“Multi-levelcontextultra-aggregationforstereomatching,”
in Proceedings of the IEEE Conference on Computer Vision and
PatternRecognition,2019,pp.3283–3291.
[20] Y.Luo,J.Ren,M.Lin,J.Pang,W.Sun,H.Li,andL.Lin,“Single
viewstereomatching,”inTheIEEEConferenceonComputerVision
andPatternRecognition(CVPR),June2018.
[21] J. Pang, W. Sun, J. S. Ren, C. Yang, and Q. Yan, “Cascade resid-
ual learning: A two-stage convolutional neural network for stereo
matching,” in Proceedings of the IEEE International Conference on
ComputerVision,2017,pp.887–895.
107
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 10:19:04 UTC from IEEE Xplore.  Restrictions apply. 