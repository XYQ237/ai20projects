2020 IEEE International Conference on Robotics and Automation (ICRA)
31 May - 31 August, 2020. Paris, France
Grasp State Assessment of Deformable Objects Using Visual-Tactile
Fusion Perception
∗
Shaowei Cui1,2, Rui Wang1, Junhang Wei1,2, Fanrong Li 1,2, and Shuo Wang1,3,
Abstract—Humanscanquicklydeterminetheforcerequired
to grasp a deformable object to prevent its sliding or excessive
deformationthroughvisionandtouch,whichisstillachalleng-
ing task for robots. To address this issue, we propose a novel
3Dconvolution-basedvisual-tactilefusiondeepneuralnetwork
(C3D-VTFN) to evaluate the grasp state of various deformable
objects in this paper. Speciﬁcally, we divide the grasp states of
deformableobjectsintothreecategoriesofsliding,appropriate
and excessive. Also, a dataset for training and testing the
proposed network is built by extensive grasping and lifting
experiments with different widths and forces on 16 various
Fig.1:(a):Theslidinggraspstate.(b)Theappropriategrasp
deformable objects with a robotic arm equipped with a wrist
cameraandatactilesensor.Asaresult,aclassiﬁcationaccuracy state. (c) The excessive grasp state. The top row images are
ashighas99.97%isachieved.Furthermore,somedelicategrasp capturedbyaside-mountedcameraandthebottombyawrist
experiments based on the proposed network are implemented camera.
in this paper. The experimental results demonstrate that the
C3D-VTFN is accurate and efﬁcient enough for grasp state
assessment, which can be widely applied to automatic force
control, adaptive grasping, and other visual-tactile spatiotem- the grasp state of various deformable objects, as shown in
poral sequence learning problems. Fig. 1.
Vision and tactile sensing are two of the primary sensing
I. INTRODUCTION
modalities to perceive the ambient world for humans [7].
Robotic grasp capability is receiving increasing attention
Visionprovidestheappearance,shape,andothervisiblefea-
due to increased demand for various dexterity grasping and
tures of objects, while touch provides more accurate texture,
manipulation of service and industrial robots [1], [2]. To
roughness, contact strength, and other invisible details [8].
improve the general grasp ability of the robots, accurate and
For such a grasp state assessment task, humans are capable
efﬁcient grasp state assessment is a relatively critical part.
of intuitively performing the evaluation process. Someone
Traditional grasp quality assessment focuses on whether a
picking up a random object can automatically determine if
grasp process is stable and whether slippage has occurred.
the grasp is appropriate [9]. This information beneﬁts from
Many scholars have already researched in the grasp stability
bothtactileandvisualfeedback.Thesameistrueforrobots,
assessment [3], [4] and slip detection/prediction [5], [6].
andthispaperfocusesonhowtoendowrobotstheabilityto
Nevertheless, for a deformable or fragile object, it is
evaluate the grasp state of deformable objects using visual-
not enough to only detect whether it slides during a grasp
tactile fusion perception.
process. For example, for such a task of grasping paper
The difﬁcult primary issue involved in such a bimodal
cups, if the gripping force is set too large, although the
fusion perception task is how to learn effective fusion
paper cup can be prevented from slipping, the excessive
spatiotemporal features from two heterogeneous modal spa-
gripping force may cause the paper cup to undergo a large
tiotemporal sequences [5]. In this paper, we propose a
deformation, thereby causing irreversible damage. To this
novel3Dconvolution-basedvisual-tactilefusiondeepneural
end,amorecomprehensiveapproachtoassessthegraspstate
network(C3D-VTFN)toevaluatethegraspstate,mimicking
of deformable objects needs to be studied. In this paper, we
the strategy adopted by humans. Furthermore, we perform
deﬁnethegraspstateassessmenttaskfordeformableobjects
extensive grasping and lifting experiments with different
as a tri-classiﬁcation problem with sliding, appropriate, and
grasp settings to train and test the neural work on our hu-
excessivelabels.Thesethreegraspstatesareusedtodescribe
manoid robot platform. The visual and tactile sequences are
takenfromawristcameraﬁxedaboveagripperandaXELA
ThisworkwassupportedinpartbytheNationalNaturalScienceFoun-
dation of China under Grant U1713222, Grant 61773378. (corresponding [10] tactile sensor, respectively. The experimental setup is
author:ShuoWangshuo.wang@ia.ac.cn) shown in Fig. 2. Finally, some comparative experiments of
1 S. Cui, R. Wang, J. Wei, F. Li, and S. Wang are with Institute of
C3D-VTFN model with different inputs and two real-time
Automation,ChineseAcademyofSciences,Beijing100190,China.
2 S. Cui, J. Wei, and F. Li are with School of Future Technology, grasp state correction experiments based on proposed model
UniversityofChineseAcademyofSciences,Beijing100049,China. are implemented to verify the effectiveness of the proposed
3 S. Wang is also with Center for Excellence in Brain Science and
network further.
Intelligence Technology Chinese Academy of Sciences, Shanghai 200031,
China. This paper is organized as follows: in Section II, the
978-1-7281-7395-5/20/$31.00 ©2020 IEEE 538
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:55:56 UTC from IEEE Xplore.  Restrictions apply. Most of the above studies have focused on stability as-
sessment during the grasp process while ignoring dexterity.
However, excessive-force gripping can achieve stable grasp
of deformable objects but may also cause irreversible dam-
age.Therefore,weconductamorecomprehensivegraspstate
assessment of daily life deformable objects. By adding the
grasp state of excessive detection, a more comprehensive
grasp state assessment framework is proposed for more
sophisticatedandcomplexroboticgraspingandmanipulation
tasks.
B. Visual-tactile fusion perception
Vision and tactile sensing are two primary important
modalities in robotics perception. In the past decades, it is
still challenging to combine vision and touch modalities to
Fig.2:Left:Theexperimentsetup:aUR3robotarmwithan facilitate robot manipulations due to their different sensing
OnRobotRG2gripper.Oneﬁngerofthegripperisequipped principles and data structures. However, these limitations
with a XELA Tactile sensor [10]. A 1080P USB camera is have recently improved due to the increasing measurement
mounted on the top of the gripper. Upper right: The photo accuracyoftactilesensorsandadvancesinfusionalgorithms
taken by the wrist camera. Bottom right: Three-axis force with deep neural networks. The combination of visual and
distribution map from the tactile sensor. tactile perception plays an increasingly important role in
the robotic community [17]. Visual-tactile fusion perception
has long been used for a variety of tasks, such as surface
related work of grasp state assessment and visual-tactile classiﬁcation [18], object recognition [19], object 3D shape
fusion perception are explained. In Section III, the problem perception [20], etc.
statement, detailed architecture of C3D-VTFN, and training In the ﬁeld of grasping and manipulation, R Calandra et
speciﬁcations are described. In Section IV, the experimental al. investigated the question of whether touch sensing aids
results and discussions are provided. Finally, the contribu- in predicting grasp outcomes within a multimodal sensing
tions of this paper and future work are discussed in Section framework that combines vision and touch [17]. The exper-
V. imental results indicated that incorporating tactile readings
substantially improve grasp performance. Furthermore, an
II. RELATEDWORK end-to-end action-conditional model that learns re-grasping
policies from rowed visual-tactile data was proposed in
A. Grasp state assessment [21]. The re-grasping strategy using combined visual and
tactilesensinghadgreatlyimprovedthesuccessofgrasping.
Grasp state assessment is critical for robots to achieve
Michelle A. Lee et al. used self-supervision to learn a
high-quality grasping and manipulation tasks. In the past
compact and multimodal representation of RGBs, depth,
decades, most studies have focused on the stability in the
force-torque, and proprioception for different contact-rich
grasp process. Yasemin Bekiroglu et al. [11] studied the
manipulation [22].
problemoflearninggraspstabilityinroboticobjectgrasping
Nevertheless, these studies only use tactile and visual
based on tactile measurement and Hidden Markov Models
images at a speciﬁc moment as input and do not use time-
(HMMs). [12] proposed an integrating grasp planning with
domain information of the two modalities. Spatiotemporal
an online stability assessment based on tactile sensing. They
features of visual and tactile are extracted by Convolutional
also presented a probabilistic framework for grasp modeling
Neural Network (CNN)+Recurrent Neural Network (RNN)
and stability assessment [13]. Yevgeb Chebotar et al. intro-
architecture for slip detection [5]. However, they only detect
duced a framework for learning re-grasping behaviors based
slip, and the premise of this study is that the reading
ontactiledata.Theypresentedagraspstabilitypredictorthat
frequencyofthetactileandvisualdataisconsistent,butmost
used spatio-temporal tactile features [14].
tactile sensors read more quickly than the cameras. Hence,
Moreover, a novel method to incorporate exteroception
we present a novel C3D grounded framework to tackle the
and proprioception into grasp stability assessment was pro-
visual-tactile fusion perception problem.
posed by [3]. A convolutional neural network (CNN) was
used to extract features and fusion of different modality III. PROPOSEDMETHOD
information. More recently, A new method to predict grasp A. Problem statement
stability using a non-matrix tactile sensor was proposed by
Our goal is to obtain the current grasp state by visual-
[15]. Filipe Veiga et al. proposed a grip stabilization ap-
tactile fusion perception. Given the visual (X , X ,...,X )
proachfornovelobjectsbasedonslipprediction[6].Besides, v1 v2 vm
andtactile(X ,X ,...,X )1 sequences,weﬁrstextractvisual
Graph convolutional network method was also studied to t1 t2 tn
predict grasp stability with tactile sensors[16]. 1mandnarethelengthsofthetwosequences,respectively.
539
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:55:56 UTC from IEEE Xplore.  Restrictions apply. Visual
3D
Conv FC
4096
FC FC Grasping
Tactile 128 State
3D
FC
Conv
32
Fig. 3: The diagram of C3D-VTFN model. Blue and orange blocks denote visual and tactile 3D convolutional layers,
respectively. The cuboids with different colors represent FC layers.
TABLE I: Detailed network parameters of C3D-VTFN.
(F ) and tactile (F) features by visual (E ) and tactile
v t v
(E) encoder functions and then construct a fusion feature
t
Visuallayers Outputsize
(fFunv,ct)tiobnasFedtoonptrheedmic.tFthineaclulyr,reFnvt,tgirsasfpedstiantteoya.Tchlaissspiﬁrocbatlieomn 3d-conv1 3×3×3×64,padding(1,1,1),relu 112××112××5××64
c pool1 × ×Ma×x(1,2,2),stride(1,2,2) 56×56×5×64
is formulated as 3d-conv 3 3 3 128,padding(1,1,1),relu 56 56 5 128
2 × × ×
pool Max(2,2,2),stride(2,2,2) 28 28 2 128
⊕ 2 × × × × × ×
Fv,t =Ev(Xv1,Xv2,..,Xvm) Et(Xt1,Xt2,..,Xtn) (1) 3d-conv3a 3×3×3×256,padding(1,1,1),relu 28×28×2×256
3d-conv 3 3 3 256,padding(1,1,1),relu 28 28 2 256
y=Fc(Fv,t) y∈ 0,12 (2) 3dp-cooonl3v3b 3×3×M3a×x(521,22,,2p),adstdriindge(1(2,1,2,1,2),)relu 1144××1144××11××225566
4a × × × × × ×
Which 0, 1, and 2 refer to the sliding, appropriate, and 3d-conv4b 3 3 3 512,padding(1,1,1),relu 14×14× 1× 256
pool Max(1,2,2),stride(1,2,2) 7 7 1 512
excessive grasp states, respectively. Hence, the grasp state 4 × × × × × ×
3d-conv 3 3 3 512,padding(1,1,1),relu 7 7 1 512
assessment task is deﬁned as a tri-classiﬁcation problem. 3d-conv5a 3×3×3×512,padding(1,1,1),relu 7×7×1×512
5b × × ×
To address the above problem, we propose a novel pool5 Max(1,2,2),str(1,2,2),pad(1,0,0) 4×4×1 512
fc (8,192,4,096) 1 1 4,096
3D Convolution-based visual-tactile fusion network (C3D- 1 × ×
fc (4,096,4,096) 1 1 4,096
2
VTFN) in this paper, where E and E are implemented by
v t Tactilelayers Outputsize
3anDdcFocnvioslcuotinosntarulcnteeudrablynFetuwlloyr-kCsownnitehctpiaornam(FeCte)rslaθyverasndwθitth, 3dp-ocooln6v6 3××3M××a3x××(28,2,,p2a),ddstirnigd(e1,(12,,12),,2)relu 42×××42×××150×××88
parameters θc. 3dp-ocoolnv7 3 3Ma3x(21,62,,2p)a,dsdtrinidge(1(,21,,21,)2,)relu 21×21×52×1166
7 × × × × × ×
3d-conv 3 1 1 32,padding(1,0,0),relu 1 1 3 32
B. Model description 8 × × ×
pool Max(2,1,1),stride(2,1,1) 1 1 1 32
8
The overall architecture of C3D-VTFN model is shown Classiﬁcationlayers Outputsize
× ×
in Fig. 3. The proposed model consists of three components fc3 (4096+32,128) 1 ×1 ×128
fc (128,3)max 1 1 1
including visual feature extraction module (E ), tactile fea- 4
v
F
ture extraction module (E), and classiﬁcation module ( ).
t c
Given the current visual and tactile sequence, the output of
C3D-VTFN is the current grasp state category. Firstly, the visual C3D layers are fed to two FC layers and transformed
visual and tactile features of each spatiotemporal sequence into a 4096-dimensional feature vector. Similarly, the tactile
are extracted by C3D networks. Note that tactile modal featuresextractionmodulearecomposedofthreeC3Dlayers
input is also treated as a small image because of its matrix and followed by two FC layers. The difference is that the
distribution. Finally, the visual and tactile features are then output tactile feature vector is 32-dimensional. Finally, the
combined using FC layers to generate a classiﬁcation result. feature vectors from the two modalities are concatenated
together to output the ﬁnal grasp state category through two
× ×
In practice, we use ﬁve 2 112 112 3 3 visual images classiﬁcation FC layers.
× ×
and ten 4 4 3 tactile images as input and the detailed
Speciﬁcally, we use Xavier initialization [23] to initiate
networkparametersareshowninTableI.Thevisualfeatures
network weights and cross-entropy [24] as the loss function.
extractionmoduleincludesﬁveC3DandtwoFClayers.The −
Adam optimizer [25] with 1e 07 learning rate is adopted
convolution kernel size and stride size of each convolutional
in the training process. The model is implemented on the
layer are not exactly th×e s×ame×. The output size of the ﬁnal PyTorch platform 4 and trained on an NVIDIA DGX server.
visual C3D layer is 4 4 1 512. The features from the
The batch size is set as 8 in this paper.
2Thesequencelengthwasselectedbycomparisonexperiments.
3Weselectedthissizeasthedefaultvisualmodalinputsizebycomparing 4Sourcecodeforstudyreplicationisavailableat:https://github.
theperformanceofthemodelwithdifferentimagesizesasinput. com/swchui/Grasping-state-assessment
540
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:55:56 UTC from IEEE Xplore.  Restrictions apply. inter-frame interval, and input of single-mode or dual-mode
perception.
1) Different input lengths: Longer sequences not only
meanmoretemporalinformation,butalsoresultinredundant
calculations and bring more noise information. The visual
sequence lengths of 3, 4, 5, 6, 7, and 8 are selected as
the inputs of the model for comparative evaluation, and the
Fig. 4: Some deformable objects of the GSA dataset.
tactile sequence length is set to twice the visual according
to the reading frequency. The experiments results are shown
in Table II.
IV. EXPERIMENTS
In this section, we ﬁrst introduce our grasp state assess- TABLEII:Experimentalresultsofthemodelswithdifferent
mentdataset(GSAdataset)andtheexperimentalsetup.Then input length.
the performance comparison of C3D-VTFN with different
structures and parameters on the GSA Dataset is provided. Sequencelength
3 4 5 6 7 8
Finally, we perform two delicate grasp experiments of a
Precision 75.78 95.21 99.97 99.80 90.27 90.13
deformable object based on the C3D-VTFN model. Recall 67.27 95.49 99.98 99.74 88.81 79.91
F1score 67.42 95.08 99.98 99.77 88.79 79.83
A. The GSA dataset introduction Size(M) 78.53 78.53 78.53 78.54 78.54 78.54
All of the experiments are conducted with a 6-DOFs
UR3 robot arm equipped with an OnRobot RG2 gripper.
The results show that it is not the more extended the
Speciﬁcally, one ﬁnger of the gripper is covered by a XELA
input sequence is, the better the classiﬁcation performance
tactilesensoranda1080PUSBcameraismountedonthetop
is.Themodelwithasequencelengthof8hasaclassiﬁcation
of the gripper as a wrist-camera. The robot setup is shown
accuracyof10%lowerthanthatofthesequencelengthof5.
in Fig. 2.
Asaresult,theoptimalclassiﬁcationperformanceisobtained
TheGSAdatasetisbuiltbyextensivegraspingandlifting
when the sequence length is 5.
experiments on 16 deformable objects of different sizes,
2) Different inter-frame interval: Since sequences with
shapes, textures, materials, and weights, some of them are
different time intervals have different characteristics, the
shown in Fig. 4. Inspired by [5], different grasp widths and
large inter-frame interval can result in a reduction in sample
forces are selected to balance the number of routines with
rate, and whether this affects the performance of the model
differentlabels.Inthisway,thegraspstatesareautomatically
is still a question worth exploring. Due to the reading speed
labeled in each grasping and lifting trial. In each grasp
of sensors is ﬁxed, we set a basic input sample in which the
experiment, an object is grasped with the preset width and
visual and tactile images are consecutive recorded reading.
force and lifted slowly for 20.0 mm (The lifting speed is
We also build step input samples in which the data reading
set to 10.0 mm/s). During the grasping and lifting process,
is selected with step 2 and 3. We set the other parameters
the data are collected by the visual sensor with a 30 Hz
as default, and only change the inter-frame interval for a
and tactile sensor with 60 Hz, respectively. We perform
comparison test. The results are shown in Table III.
50 to 60 grasps per object, collecting approximately 30 to
40 frames of visual images and 60 to 80 frames of tactile
TABLEIII:Experimentalresultsofthemodelswithdifferent
images per grasp trial. As a result, the GSA dataset consists
input inter-frame intervals.
of approximately 20,000 5-frame visual image sequences
and corresponding tactile image sequence samples. Among
Frameinterval
them,thegraspingdataofrandomlyselectedthirteenobjects
1 2 3
is used to train the model, and the grasping data of the Precision 99.97 98.03 85.62
remainingthreeobjectsisusedfortesting.ThedetailedGSA Recall 99.98 98.50 85.50
F1score 99.98 98.23 83.80
dataset is available at https://github.com/swchui/
Size(M) 78.53 78.53 78.53
Grasping-state-assessment/graspingdata.
B. Performance comparison results
Table III suggests that the using step sampling method
To evaluate the performance of the proposed model more wouldbeworse,especiallyfortheappropriategraspingstate.
comprehensivelyandaccurately,wecomparedthePrecision, The confusion matrix shows that the grasp state of the
Recall, F1 score, and model size of the model with different appropriate is higher in the case where the step setting is
inputs and structures. The Precision, Recall, and F1 score smaller, as shown in Fig. 5. The intuitive explanation is that
are used to evaluate the classiﬁcation performance, and the the reduction of the sample rate will reduce the conﬁdence
model size is adopted to compute the real-time performance oftheproposedmodelindeterminingthepropergraspstate,
of different models. The performance of the model may be whichmakesitmorebiasedtowardslidingorexcessivestate.
affected by different length of input sequence, image size,
541
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:55:56 UTC from IEEE Xplore.  Restrictions apply. beneﬁts of visual-tactile fusion perception. For the visual-
only mode, we select the visual features extraction module
and classiﬁcation module from the C3D-VTFN model (as
shown in Fig. 3). Similarly, the tactile-only mode is tested
by combining the tactile features extraction module and a
classiﬁcation module. The comparison results are shown in
(a) Basic (b) Step2 (c) Step3
Table V.
Fig.5:Theconfusionmatricesofmodelswithdifferentinter-
TABLE V: Experimental results of the models with single-
frame interval.
or-dual modal perception.
Tactile-only Visual-only Visual-Tactilefusion
3) Different image size: The image size directly deter- Precision 70.30 79.74 99.97
minestheamountofinformationinputbythevisualmodality Recall 72.36 79.77 99.98
and the amount of model parameters. Therefore, we set the F1score 67.11 79.27 99.98
Size(M) 0.01 78.52 78.53
inter-frameintervalto1,settheinputimagesequencelength
to 5 (the tactile sequence length corresponds to 10), set the
× × × ×
input image size to 32 32, 64 64, 112 112, 224 224, Accordingtotheexperimentalresults,visual-tactilefusion
×
and 512 512 respectively, and modify the corresponding perception achieves much better precision, recall, and F1
model parameters, the results are shown in Table IV. scorethanthatofanysinglemodalperception.Theoretically,
thevisualimageprovidesthegeometricalinformationofthe
TABLEIV:Experimentalresultsofthemodelswithdifferent contact situation, which would have a better ability to dis-
image size. tinguish the excessive grasp state from others. This analysis
has been veriﬁed by the confusion matrices shown in Fig.
Imagesize 7(a) and Fig. 7(b). Meanwhile, the confusion matrices also
32 64 112 224 512
show that the tactile-only model achieves better detection
Precision 76.78 90.01 99.97 89.36 73.44
Recall 66.68 80.05 99.98 89.12 69.86 performance of the sliding grasp state than the visual-only
F1score 66.48 80.44 99.98 87.21 62.23 model.
Size(M) 55.53 64.37 78.53 92.69 106.84
The experimental results indicate that the model perfor-
mance is not directly proportional to image size. We ﬁnd
that as the size of the image increases, the model detects the
sliding state more accurately, but the appropriate grasp state
detection performance becomes worse, as shown in Fig. 6. (a) Visual-only (b) Tactile-only (c) VTF
Hence,weselectthevisualsequencewithimagesize112as
Fig. 7: The confusion matrices of models with different
the input to the model.
inputs.
C. Delicate grasp experiments based on C3D-VTFN model.
Two delicate grasp experiments in this section are per-
formed to further verify the effectiveness of the proposed
model. We have developed a roughly grasp adjustment
(a) imagesize(32) (b) imagesize(64) (c) imagesize(112) strategythatadjustsgraspwidthandforceinreal-timebased
on the grasp state detector (C3D-VTFN). The detailed grasp

regulation strategies are as follows,
 wt−1,ft+1 ct =sliding,
wt+1,ft+1= wt,ft ct =appropriate,
wt+1,ft+1 ct =excessive.
(d) imagesize(224) (e) imagesize(512) Where wt and ft represent the grasp width and force at
the current moment, and w and f represent the next
Fig. 6: The confusion matrices of models with different t+1 t+1
moment. Also, the adjustment of the grasp settings depends
image size.
on the evaluation of the grasp state of the current moment
c.
t
4) Single-or-dual modal perception: Furthermore, differ- On the one hand, a grasp adjustment experiment that
ent modal combinations are tested to verify the performance begins with a slip is ﬁrst performed, as described in Section
542
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:55:56 UTC from IEEE Xplore.  Restrictions apply. IV-C.1. On the other hand, a grasp adjustment experiment
withaninitialexcessive-forcegripisperformed,asdescribed
in Section IV-C.2.
1) The sliding grasp experiments: First, we set the grasp
force to 5N and the grasp width to 66 mm for the grasping
and lifting experiments of a deformable bottle (not included
in the GSA dataset) with (WA) and without (WoA) ad-
justment strategy. In these two experiments, the real-time
changing curve of the adjustment grasp force, grasp width,
and the detection grasp state is shown in Fig. 8.
Fig. 9: The real-time changing curve of different values in
the two excessive grasp experiments.
model.Pleasenotethattheretardationofthepredictedgrasp
state at the beginning of the grasp adjustment experiment is
due to the experimental settings and the initial grasp force
setting being too large.
The above two experiments have well veriﬁed the evalua-
tionperformanceoftheproposedmodelonthecurrentgrasp
state. However, this adjustment strategy is not enough for
Fig. 8: The real-time changing curve of different values delicate grasp, and it is necessary to set a more ﬁne-grained
in the two sliding grasp experiments. Solid line: the grasp adjustment strategy (See supplementary video materials for
process with adjustment. Dashed line: the grasp process more experimental details).
without adjustment. The second axis labels: (0) sliding, (1) Additionally, due to the ﬁxed angle of view during data
appropriate, (2) excessive. collection, the trained model has higher accuracy at ﬁxed
angles and less than ideal performance at other views. A
feasible way is adding multiple different views of grasp
Fig.8showsthattheproposedmodelcanaccuratelydetect
settings in each experiment, which signiﬁcantly increases
the current state of the grasp state regardless of there is
the scale of the GSA dataset, and makes the model more
a grasp strategy adjustment. In the experiment without the
generalized. Fortunately, this limitation does not prevent us
adjustment strategy, the detector always detected the grasp
for verifying the feasibility of the C3D-based visual-tactile
state as sliding (0). However, in the experiment with the
fusion perception approach.
adjustment strategy, the detection state changes with the
change of the grasp settings. In the grasp process with V. CONCLUSIONANDFUTUREWORK
adjustment, the model still detects it as a sliding state even
A network named C3D-VTFN is proposed to assess the
if has been adjusted at the beginning. The reason is that the
grasp state of various deformable objects by using visual-
preset grip width (68mm) is larger than the actual diameter
tactilefusionperceptioninthispaper.Weextractthefeatures
of the bottle and takes a few steps to adjust. After a few
of the visual and tactile modalities by 3D convolution
steps, the entire lifting process is adjusted between the three
layers, which provides a new feature extraction scheme
graspstatesaccordingtotheactualgraspsituationtoachieve
for the visual-tactile fusion perception tasks. Besides, the
a delicate grasp.
GSA dataset used to train and test the proposed model is
2) The excessive grasp experiments: Similar to the previ-
established by extensive grasping and lifting experiments in
ousexperiments,wepresetthegraspforceandwidthto32N
this paper, and the experimental results show the effective-
and 48mm, respectively, in the excessive grasp experiments.
ness and high accuracy of the proposed model. Finally, we
The changing curves of different grasp values are shown in
perform two delicate grasp experiments with a rough adjust-
Fig. 9.
ment strategy based on the proposed model and achieved
The grasp process in the experiment with adjustments can
convincing results.
ﬁnally stabilize the grasp force and width, similar to those
In the future, we will explore perceptual models that are
in the sliding experiment. Since the grasp force and width
more in line with human vision-tactile fusion properties and
adjustment settings are very rough in this paper, the ﬁnal
their applications in robotic grasping and manipulation.
steady-state of the two experiments is not wholly consistent.
But it is sufﬁcient to verify the performance of the proposed
543
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:55:56 UTC from IEEE Xplore.  Restrictions apply. REFERENCES International Conference on Robotics and Automation, Shanghai,
China,May.2011,pp.4750–4755.
[1] S.Levine,P.Pastor,A.Krizhevsky,J.Ibarz,andD.Quillen,“Learning
[13] Y. Bekiroglu, D. Song, L. Wang, and D. Kragic, “A probabilistic
hand-eye coordination for robotic grasping with deep learning and
frameworkfortask-orientedgraspstabilityassessment,”in2013IEEE
large-scale data collection,” The International Journal of Robotics
International Conference on Robotics and Automation, Karlsruhe,
Research,vol.37,no.4–5,pp.421–436,Apr.2018.
Germany,May.2013,pp.3040–3047.
[2] J. Sanchez, J. Corrales, B. Bouzgarrou, and Y. Mezouar, “Robotic
[14] Y. Chebotar, K. Hausman, Z. Su, G. S. Sukhatme, and S. Schaal,
manipulation and sensing of deformable objects in domestic and in-
“Self-supervisedregraspingusingspatio-temporaltactilefeaturesand
dustrialapplications:asurvey,”TheInternationalJournalofRobotics
reinforcement learning,” in 2016 IEEE/RSJ International Conference
Research,vol.37,no.7,pp.688–716,Jun.2018.
onIntelligentRobotsandSystems,Deajeon,SouthKorea,Oct.2016,
[3] J. Kwiatkowski, D. Cockburn, and V. Duchaine, “Grasp stability
pp.1960–1966.
assessment through the fusion of proprioception and tactile signals
[15] B.S.Zapata-Impata,P.Gil,andF.Torres,“Non-matrixtactilesensors:
usingconvolutionalneuralnetworks,”in2017IEEE/RSJInternational
How can be exploited their local connectivity for predicting grasp
Conference on Intelligent Robots and Systems, Vancouver, Canada,
stability?,”arXivpreprintarXiv:1809.05551,2018.
Sep.2017,pp.286–292.
[16] A. Garcia-Garcia, B. S. Zapata-Impata, S. Orts-Escolano, P. Gil,
[4] D.Cockbum,J.P.Roberge,A.Maslyczyk,V.Duchaine,etal.,“Grasp
andJ.Garcia-Rodriguez,“Tactilegcn:Agraphconvolutionalnetwork
stability assessment through unsupervised feature learning of tactile
for predicting grasp stability with tactile sensors,” arXiv preprint
images,” in 2017 IEEE International Conference on Robotics and
arXiv:1901.06181,2019.
Automation,Singapore,Singapore,May.2017,pp.2238–2244.
[17] R.Calandra,A.Owens,M.Upadhyaya,W.Yuan,J.Lin,E.H.Adel-
[5] J.Li,S.Dong,andE.Adelson,“Slipdetectionwithcombinedtactile
son,andS.Levine,“Thefeelingofsuccess:Doestouchsensinghelp
and visual information,” in 2018 IEEE International Conference on
predictgraspoutcomes?,”arXivpreprintarXiv:1710.05512,2017.
RoboticsandAutomation,Brisbane,Australia,May.2018,pp.7772–
[18] Y.Gao,L.A.Hendricks,K.J.Kuchenbecker,andT.Darrell,“Deep
7777.
learning for tactile understanding from visual and haptic data,” in
[6] F.Veiga,J.Peters,andT.Hermans,“Gripstabilizationofnovelobjects
2016 IEEE International Conference on Robotics and Automation,
using slip prediction,” IEEE transactions on haptics, vol. 11, no. 4,
Stockholm,Sweden,May.2016,pp.536–543.
pp.531–542,May.2018.
[19] H.P.Liu,Y.L.Yu,F.C.Sun,andJ.Gu,“Visual-TactileFusionfor
[7] R.S.JohanssonandJ.R.Flanagan,“Codinganduseoftactilesignals
Object Recognition,” IEEE Transactions on Automation Science and
from the ﬁngertips in object manipulation tasks,” Nature Reviews
Engineering,vol.14,no.2,pp.996-1008,Apr.2017.
Neuroscience,vol.10,no.5,pp.345,May.2009. [20] S. X. Wang et al., “3D Shape Perception from Monocular Vision,
[8] S. Cui, Y. Wang, S. Wang, R. Wang, W. Wang, and M. Tan, Touch, and Shape Priors,” 2018 IEEE/RSJ International Conference
“Real-time perception and positioning for creature picking of an onIntelligentRobotsandSystems(IROS),Madrid,Spain,Oct.2018,
underwatervehicle,”IEEETransactionsonVehicularTechology,DOI: pp.1606-1613.
10.1109/TVT.2020.2973656,2020. [21] R.Calandra,A.Owens,D.Jayaraman,J.Lin,W.Yuan,J.Malik,E.
[9] M. Stachowsky, T. Hummel, M. Moussa, and H. A. Abdullah, “A H. Adelson, and S. Levine, “More than a feeling: Learning to grasp
slip detection and correction strategy for precision robot grasping,” andregraspusingvisionandtouch,”IEEERoboticsandAutomation
IEEE/ASMETransactionsonMechatronics,vol.21,no.5,pp.2214– Letters,vol.3,no.4,pp.3300-3307,Jul.2018.
2226,Apr.2016. [22] M.A.Lee,Y.Zhu,K.Srinivasan,P.Shahetal.,“Makingsenseofvi-
[10] T. P. Tomo, S. Somlor, A. Schmitz, S. Hashimoto, S. Sugano, and sionandtouch:Self-supervisedlearningofmultimodalrepresentations
L.Jamone,“Developmentofahall-effectbasedskinsensor,”in2015 forcontact-richtasks,”arXivpreprintarXiv:1810.10191,2018.
IEEESENSORS,Busan,SouthKorea,Nov.2015,pp.1–4. [23] S.K.Kumar,“Onweightinitializationindeepneuralnetworks,”arXiv
[11] Y.Bekiroglu,D.Kragic,andV.Kyrki,“Learninggraspstabilitybased preprintarXiv:1704.08863,2017.
ontactiledataandhmms,”in19thInternationalSymposiuminRobot [24] P.DeBoer,D.P.Kroese,Owens,S.Mannor,andR.Y.Rubinstein,“A
and Human Interactive Communication, Viareggio, Italy, Sep. 2010, tutorialonthecross-entropymethod,”Annalsofoperationsresearch,
pp.132–137. vol.134,no.1,pp.19–67,Feb.2005.
[12] Y.Bekiroglu,K.Huebner,andD.Kragic,“Integratinggraspplanning [25] D.Kingma,andJ.Ba,“Adam:Amethodforstochasticoptimization,”
withonlinestabilityassessmentusingtactilesensing,”in2011IEEE arXivpreprintarXiv:1412.6980,2014.
544
Authorized licensed use limited to: Carleton University. Downloaded on September 20,2020 at 14:55:56 UTC from IEEE Xplore.  Restrictions apply. 