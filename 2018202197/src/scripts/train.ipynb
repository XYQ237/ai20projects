{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import torch\n",
    "from torch.jit import script, trace\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import codecs\n",
    "from io import open\n",
    "import itertools\n",
    "import math\n",
    "import jieba\n",
    "\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "100000\n['538405926243 买 二份 有没有 少点 呀', '亲亲 真的 不好意思 我们 已经 是 优惠价 了 呢 小本生意 请亲 谅解']\n['那 就 等 你们 处理 喽', '好 的 亲退 了']\n['那 我 不 喜欢', '颜色 的话 一般 茶刀 茶针 和 二合一 的话 都 是 红木 檀 和 黑木 檀 哦']\n['不是 免 运费', '本店 茶具 订单 满 99 包邮除 宁夏 青海 内蒙古 海南 新疆 西藏 满 39 包邮']\n['好吃 吗', '好吃 的']\n['为什么 迟迟 不 给 我 发货', '实在 抱歉 呢 由于 订单 量 大 您 的 订单 本来 安排 今天 发货 的 呢']\n['对 谢谢', '小店 尽快 给 您 发出 哦']\n['3 组送 什么', '拍 2 组送 2 包 湿巾 3 组 也 是 2 包']\n['那 我 马上 拍', '记得 勾选 优惠券 哦']\n['每 一样 都 要点 要 二百个', '单个 的话 价格 都 是 最低 了 哦 都 是 亏本 促销 的 只是 为了 前期 冲 销量 的 548881602868547107612966547393486364547259785739 这款 单买 可以 再 便宜 鲜肉 蜜枣 豆沙 最低 53 蛋黄 肉 粽 最低 63 元 哦']\n['百世', '好 的']\n['⊙ o ⊙ 哦 好 的', '优惠券 有效期 至 8 月 31 日 谢谢 客官 支持 小店 哦']\n['目前 我 只有 这些', '亲 只有 这些 齐全 的 才能 开']\n['可是 这个 没有 到 啊', '应该 这 两天 会到 的 这个 会 联系 下 快递']\n['单 怎么 下', '您 提交 以后 哪里 可以 修改 数量 的']\n['吃 起来 和 新鲜 的 有 很大 差别 嘛 保鲜', '吃 起来 也 很 松脆']\n['杭州 桐庐 几天 到', '一般 1 - 2 天']\n['别 给 我 像 发货 一样 的 慢', '不会 的 呢']\n['我 买 的 东西 发货 了 没 怎么 看不见 物流', '亲亲 实在 抱歉 仓库 那边 说 配货 的 时候 椒盐 味纸 皮 核桃 缺货 了 呢 新 的 一批 明天 才 到货 哦 到货 后 第一 时间 给 您 打包 哦 亲亲 不要 着急 哦']\n['嗯 呢 要 不到 不了 啊', '嗯 嗯 已经 给 您 修改 好 了 哦']\n"
     ]
    }
   ],
   "source": [
    "#数据预处理，拆成pair<question,answer>\n",
    "with open('chinese_data/question_answer.txt',\"r\",encoding='utf-8') as f:    #设置文件对象\n",
    "    data=f.readlines()\n",
    "    print(len(data))\n",
    "    after_deal=[]\n",
    "    for line in data:\n",
    "        line=line.strip(\"\\n\")\n",
    "        s = line.split(\"\\t\")\n",
    "        #after_deal.append([normalizeString(s[0]),normalizeString(s[1])])\n",
    "        after_deal.append(s)\n",
    "        \n",
    "for line in after_deal[:20]:\n",
    "    print(line)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default word tokens\n",
    "PAD_token = 0  # Used for padding short sentences\n",
    "SOS_token = 1  # Start-of-sentence token\n",
    "EOS_token = 2  # End-of-sentence token\n",
    "\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "class Voc:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3  # Count SOS, EOS, PAD\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    # Remove words below a certain count threshold\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "\n",
    "        keep_words = []\n",
    "\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "\n",
    "        print('keep_words {} / {} = {:.4f}'.format(\n",
    "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
    "        ))\n",
    "\n",
    "        # Reinitialize dictionaries\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3 # Count default tokens\n",
    "\n",
    "        for word in keep_words:\n",
    "            self.addWord(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "counted words 21393\n"
     ]
    }
   ],
   "source": [
    "#构造词典\n",
    "voc = Voc('communicate')\n",
    "pairs = after_deal\n",
    "for pair in pairs:\n",
    "    voc.addSentence(pair[0])\n",
    "    voc.addSentence(pair[1])\n",
    "print(\"counted words\", voc.num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "keep_words 9399 / 21390 = 0.4394\n",
      "Trimmed from 88237 pairs to 88237, 1.0000 of total\n"
     ]
    }
   ],
   "source": [
    "#去除低频词\n",
    "MIN_COUNT = 3 \n",
    "voc.trim(MIN_COUNT)\n",
    "keep_pairs = []\n",
    "for pair in pairs:\n",
    "    input_sentence = pair[0]\n",
    "    output_sentence = pair[1]\n",
    "    keep_input = True\n",
    "    keep_output = True\n",
    "    for word in input_sentence.split(' '):\n",
    "        if word not in voc.word2index:\n",
    "            keep_input = False\n",
    "            break\n",
    "    for word in output_sentence.split(' '):\n",
    "        if word not in voc.word2index:\n",
    "            keep_output = False\n",
    "            break\n",
    "    if keep_input and keep_output:\n",
    "        keep_pairs.append(pair)\n",
    "    pairs = keep_pairs\n",
    "print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input_variable: tensor([[  21,   79,   75,  227,   27],\n        [ 941,  144,   40,  481,   28],\n        [ 574,  706, 5644,    4,    2],\n        [  14,   14,   16,   62,    0],\n        [ 497,  160,  311,    2,    0],\n        [  28,   28,  180,    0,    0],\n        [   8, 2832, 8471,    0,    0],\n        [  30,  280,   30,    0,    0],\n        [   4,  230,  360,    0,    0],\n        [  28, 4540,   14,    0,    0],\n        [  14, 1065,    2,    0,    0],\n        [  62,   27,    0,    0,    0],\n        [  30,   28,    0,    0,    0],\n        [  31,    2,    0,    0,    0],\n        [2293,    0,    0,    0,    0],\n        [   2,    0,    0,    0,    0]])\nlengths: tensor([16, 14, 11,  5,  3])\ntarget_variable: tensor([[ 136,   27,  383,   14,   27],\n        [  87,   28,   30,   28,   28],\n        [  14,   17,  438,    9,   17],\n        [ 142,    2,  151,  227,    2],\n        [ 497,    0, 7433,   84,    0],\n        [  28,    0,   16,  206,    0],\n        [  17,    0,  136,   17,    0],\n        [   2,    0,    2,    2,    0]])\nmask: tensor([[ True,  True,  True,  True,  True],\n        [ True,  True,  True,  True,  True],\n        [ True,  True,  True,  True,  True],\n        [ True,  True,  True,  True,  True],\n        [ True, False,  True,  True, False],\n        [ True, False,  True,  True, False],\n        [ True, False,  True,  True, False],\n        [ True, False,  True,  True, False]])\nmax_target_len: 8\n"
     ]
    }
   ],
   "source": [
    "def indexesFromSentence(voc, sentence):\n",
    "    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
    "\n",
    "\n",
    "def zeroPadding(l, fillvalue=PAD_token):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "\n",
    "def binaryMatrix(l, value=PAD_token):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == PAD_token:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m\n",
    "\n",
    "# Returns padded input sequence tensor and lengths\n",
    "def inputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, lengths\n",
    "\n",
    "# Returns padded target sequence tensor, padding mask, and max target length\n",
    "def outputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    mask = binaryMatrix(padList)\n",
    "    mask = torch.BoolTensor(mask)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, mask, max_target_len\n",
    "\n",
    "# Returns all items for a given batch of pairs\n",
    "def batch2TrainData(voc, pair_batch):\n",
    "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "    input_batch, output_batch = [], []\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "    inp, lengths = inputVar(input_batch, voc)\n",
    "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
    "    return inp, lengths, output, mask, max_target_len\n",
    "\n",
    "\n",
    "# Example for validation\n",
    "small_batch_size = 5\n",
    "batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
    "\n",
    "print(\"input_variable:\", input_variable)\n",
    "print(\"lengths:\", lengths)\n",
    "print(\"target_variable:\", target_variable)\n",
    "print(\"mask:\", mask)\n",
    "print(\"max_target_len:\", max_target_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "\n",
    "        # 初始化GRU，这里输入和hidden大小都是hidden_size，这里假设embedding层的输出大小是hidden_size\n",
    "        # 如果只有一层，那么不进行Dropout，否则使用传入的参数dropout进行GRU的Dropout。\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        # 输入是(max_length, batch)，Embedding之后变成(max_length, batch, hidden_size)\n",
    "        embedded = self.embedding(input_seq)\n",
    "        # Pack padded batch of sequences for RNN module\n",
    "        # 因为RNN(GRU)要知道实际长度，所以PyTorch提供了函数pack_padded_sequence把输入向量和长度\n",
    "        # pack到一个对象PackedSequence里，这样便于使用。\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        # 通过GRU进行forward计算，需要传入输入和隐变量\n",
    "        # 如果传入的输入是一个Tensor (max_length, batch, hidden_size)\n",
    "        # 那么输出outputs是(max_length, batch, hidden_size*num_directions)。\n",
    "        # 第三维是hidden_size和num_directions的混合，它们实际排列顺序是num_directions在前面，\n",
    "        # 因此我们可以使用outputs.view(seq_len, batch, num_directions, hidden_size)得到4维的向量。\n",
    "        # 其中第三维是方向，第四位是隐状态。\n",
    "        \n",
    "        # 而如果输入是PackedSequence对象，那么输出outputs也是一个PackedSequence对象，我们需要用\n",
    "        # 函数pad_packed_sequence把它变成shape为(max_length, batch, hidden*num_directions)的向量以及\n",
    "        # 一个list，表示输出的长度，当然这个list和输入的input_lengths完全一样，因此通常我们不需要它。\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        # 参考前面的注释，我们得到outputs为(max_length, batch, hidden*num_directions)\n",
    "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        # 我们需要把输出的num_directions双向的向量加起来\n",
    "        # 因为outputs的第三维是先放前向的hidden_size个结果，然后再放后向的hidden_size个结果\n",
    "        # 所以outputs[:, :, :self.hidden_size]得到前向的结果\n",
    "        # outputs[:, :, self.hidden_size:]是后向的结果\n",
    "        # 注意，如果bidirectional是False，则outputs第三维的大小就是hidden_size，\n",
    "        # 这时outputs[:, : ,self.hidden_size:]是不存在的，因此也不会加上去。\n",
    "        # 对Python slicing不熟的读者可以看看下面的例子：\n",
    "        \n",
    "        # >>> a=[1,2,3]\n",
    "        # >>> a[:3]\n",
    "        # [1, 2, 3]\n",
    "        # >>> a[3:]\n",
    "        # []\n",
    "        # >>> a[:3]+a[3:]\n",
    "        # [1, 2, 3]\n",
    "        \n",
    "        # 这样就不用写下面的代码了：\n",
    "        # if bidirectional:\n",
    "        #     outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
    "        # 返回最终的输出和最后时刻的隐状态。 \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Luong 注意力layer\n",
    "class Attn(torch.nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
    "        self.hidden_size = hidden_size\n",
    "        if self.method == 'general':\n",
    "            self.attn = torch.nn.Linear(self.hidden_size, hidden_size)\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = torch.nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "\n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        # 输入hidden的shape是(1, batch=64, hidden_size=500)\n",
    "        # encoder_outputs的shape是(input_lengths=10, batch=64, hidden_size=500)\n",
    "        # hidden * encoder_output得到的shape是(10, 64, 500)，然后对第3维求和就可以计算出score。\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "    def general_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(encoder_output)\n",
    "        return torch.sum(hidden * energy, dim=2)\n",
    "\n",
    "    def concat_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), \n",
    "                  encoder_output), 2)).tanh()\n",
    "        return torch.sum(self.v * energy, dim=2)\n",
    "    \n",
    "    # 输入是上一个时刻的隐状态hidden和所有时刻的Encoder的输出encoder_outputs\n",
    "    # 输出是注意力的概率，也就是长度为input_lengths的向量，它的和加起来是1。\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # 计算注意力的score，输入hidden的shape是(1, batch=64, hidden_size=500)，\n",
    "        # 表示t时刻batch数据的隐状态\n",
    "        # encoder_outputs的shape是(input_lengths=10, batch=64, hidden_size=500) \n",
    "        if self.method == 'general':\n",
    "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'concat':\n",
    "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'dot':\n",
    "            # 计算内积，参考dot_score函数\n",
    "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "\n",
    "        # Transpose max_length and batch_size dimensions\n",
    "        # 把attn_energies从(max_length=10, batch=64)转置成(64, 10)\n",
    "        attn_energies = attn_energies.t()\n",
    "\n",
    "        # 使用softmax函数把score变成概率，shape仍然是(64, 10)，然后用unsqueeze(1)变成\n",
    "        # (64, 1, 10) \n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # 保存到self里，attn_model就是前面定义的Attn类的对象。\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # 定义Decoder的layers\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        # 注意：decoder每一步只能处理一个时刻的数据，因为t时刻计算完了才能计算t+1时刻。\n",
    "        # input_step的shape是(1, 64)，64是batch，1是当前输入的词ID(来自上一个时刻的输出)\n",
    "        # 通过embedding层变成(1, 64, 500)，然后进行dropout，shape不变。\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        # 把embedded传入GRU进行forward计算\n",
    "        # 得到rnn_output的shape是(1, 64, 500)\n",
    "        # hidden是(2, 64, 500)，因为是两层的GRU，所以第一维是2。\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "        # 计算注意力权重， 根据前面的分析，attn_weights的shape是(64, 1, 10)\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        \n",
    "        # encoder_outputs是(10, 64, 500) \n",
    "        # encoder_outputs.transpose(0, 1)后的shape是(64, 10, 500)\n",
    "        # attn_weights.bmm后是(64, 1, 500)\n",
    "        \n",
    "        # bmm是批量的矩阵乘法，第一维是batch，我们可以把attn_weights看成64个(1,10)的矩阵\n",
    "        # 把encoder_outputs.transpose(0, 1)看成64个(10, 500)的矩阵\n",
    "        # 那么bmm就是64个(1, 10)矩阵 x (10, 500)矩阵，最终得到(64, 1, 500)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        # 把context向量和GRU的输出拼接起来\n",
    "        # rnn_output从(1, 64, 500)变成(64, 500)\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        # context从(64, 1, 500)变成(64, 500)\n",
    "        context = context.squeeze(1)\n",
    "        # 拼接得到(64, 1000)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        # self.concat是一个矩阵(1000, 500)，\n",
    "        # self.concat(concat_input)的输出是(64, 500)\n",
    "        # 然后用tanh把输出返回变成(-1,1)，concat_output的shape是(64, 500)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "\n",
    "        # out是(500, 词典大小=7826)    \n",
    "        output = self.out(concat_output)\n",
    "        # 用softmax变成概率，表示当前时刻输出每个词的概率。\n",
    "        output = F.softmax(output, dim=1)\n",
    "        # 返回 output和新的隐状态 \n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # 保存到self里，attn_model就是前面定义的Attn类的对象。\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # 定义Decoder的layers\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskNLLLoss(inp, target, mask):\n",
    "    # 计算实际的词的个数，因为padding是0，非padding是1，因此sum就可以得到词的个数\n",
    "    nTotal = mask.sum()\n",
    "    \n",
    "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
    "    loss = crossEntropy.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n",
    "          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):\n",
    "\n",
    "    # 梯度清空\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # 设置device，从而支持GPU，当然如果没有GPU也能工作。\n",
    "    input_variable = input_variable.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    # 初始化变量\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "\n",
    "    # encoder的Forward计算\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "    # Decoder的初始输入是SOS，我们需要构造(1, batch)的输入，表示第一个时刻batch个输入。\n",
    "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "\n",
    "    # 注意：Encoder是双向的，而Decoder是单向的，因此从下往上取n_layers个\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "\n",
    "    # 确定是否teacher forcing\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    # 一次处理一个时刻 \n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # Teacher forcing: 下一个时刻的输入是当前正确答案\n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "            # 计算累计的loss\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # 不是teacher forcing: 下一个时刻的输入是当前模型预测概率最高的值\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            # 计算累计的loss\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "\n",
    "    # 反向计算 \n",
    "    loss.backward()\n",
    "\n",
    "    # 对encoder和decoder进行梯度裁剪\n",
    "    _ = torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    # 更新参数\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return sum(print_losses) / n_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, \n",
    "              embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, \n",
    "              print_every, save_every, clip, corpus_name, loadFilename):\n",
    "\n",
    "    # 随机选择n_iteration个batch的数据(pair)\n",
    "    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
    "                      for _ in range(n_iteration)]\n",
    "\n",
    "    # 初始化\n",
    "    print('Initializing ...')\n",
    "    start_iteration = 1\n",
    "    print_loss = 0\n",
    "    if loadFilename:\n",
    "        start_iteration = checkpoint['iteration'] + 1\n",
    "\n",
    "    # 训练\n",
    "    print(\"Training...\")\n",
    "    for iteration in range(start_iteration, n_iteration + 1):\n",
    "        training_batch = training_batches[iteration - 1]\n",
    "        \n",
    "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "\n",
    "        # 训练一个batch的数据\n",
    "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
    "                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
    "        print_loss += loss\n",
    "\n",
    "        # 进度\n",
    "        if iteration % print_every == 0:\n",
    "            print_loss_avg = print_loss / print_every\n",
    "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\"\n",
    "            .format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
    "            print_loss = 0\n",
    "\n",
    "        # 保存checkpoint\n",
    "        if (iteration % save_every == 0):\n",
    "            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'\n",
    "        .format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            torch.save({\n",
    "                'iteration': iteration,\n",
    "                'en': encoder.state_dict(),\n",
    "                'de': decoder.state_dict(),\n",
    "                'en_opt': encoder_optimizer.state_dict(),\n",
    "                'de_opt': decoder_optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'voc_dict': voc.__dict__,\n",
    "                'embedding': embedding.state_dict()\n",
    "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedySearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        # Encoder的Forward计算 \n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        # 把Encoder最后时刻的隐状态作为Decoder的初始值\n",
    "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "        # 因为我们的函数都是要求(time,batch)，因此即使只有一个数据，也要做出二维的。\n",
    "        # Decoder的初始输入是SOS\n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "        # 用于保存解码结果的tensor\n",
    "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=device)\n",
    "        # 循环，这里只使用长度限制，后面处理的时候把EOS去掉了。\n",
    "        for _ in range(max_length):\n",
    "            # Decoder forward一步\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, \n",
    "                            encoder_outputs)\n",
    "            # decoder_outputs是(batch=1, vob_size)\n",
    "            # 使用max返回概率最大的词和得分\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "            # 把解码结果保存到all_tokens和all_scores里\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "            # decoder_input是当前时刻输出的词的ID，这是个一维的向量，因为max会减少一维。\n",
    "            # 但是decoder要求有一个batch维度，因此用unsqueeze增加batch维度。\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "        # 返回所有的词和得分。\n",
    "        return all_tokens, all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
    "    ### 把输入的一个batch句子变成id\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
    "    # 创建lengths tensor\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    # 转置 \n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "    # 放到合适的设备上(比如GPU)\n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    # 用searcher解码\n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    # ID变成词。\n",
    "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
    "    return decoded_words\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = re.sub('\\W*', '', s)\n",
    "    s = re.sub(\"[0-9]\",\" \",s)\n",
    "    s = ' '.join(jieba.cut(s, cut_all = False))\n",
    "    return s\n",
    "\n",
    "def evaluateInput(encoder, decoder, searcher, voc):\n",
    "    input_sentence = ''\n",
    "    while(1):\n",
    "        try:\n",
    "            # 得到用户终端的输入\n",
    "            input_sentence = input('> ')\n",
    "            print(input_sentence)\n",
    "            # 是否退出\n",
    "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
    "            # 句子归一化\n",
    "            input_sentence = normalizeString(input_sentence)\n",
    "            # 生成响应Evaluate sentence\n",
    "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "            # 去掉EOS后面的内容\n",
    "            words = []\n",
    "            for word in output_words:\n",
    "                if word == 'EOS':\n",
    "                    break\n",
    "                elif word != 'PAD':\n",
    "                    words.append(word)\n",
    "            print('Bot:', ''.join(words))\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Building encoder and decoder ...\nModels built and ready to go!\n"
     ]
    }
   ],
   "source": [
    "# 配置模型\n",
    "model_name = 'cb_model'\n",
    "attn_model = 'dot'\n",
    "#attn_model = 'general'\n",
    "#attn_model = 'concat'\n",
    "hidden_size = 500\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 64\n",
    "\n",
    "# 从哪个checkpoint恢复，如果是None，那么从头开始训练。\n",
    "loadFilename = None\n",
    "checkpoint_iter = 4000\n",
    "  \n",
    "\n",
    "# 如果loadFilename不空，则从中加载模型 \n",
    "if loadFilename:\n",
    "    # 如果训练和加载是一条机器，那么直接加载 \n",
    "    checkpoint = torch.load(loadFilename)\n",
    "    # 否则比如checkpoint是在GPU上得到的，但是我们现在又用CPU来训练或者测试，那么注释掉下面的代码\n",
    "    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "    encoder_sd = checkpoint['en']\n",
    "    decoder_sd = checkpoint['de']\n",
    "    encoder_optimizer_sd = checkpoint['en_opt']\n",
    "    decoder_optimizer_sd = checkpoint['de_opt']\n",
    "    embedding_sd = checkpoint['embedding']\n",
    "    voc.__dict__ = checkpoint['voc_dict']\n",
    "\n",
    "\n",
    "print('Building encoder and decoder ...')\n",
    "# 初始化word embedding\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "if loadFilename:\n",
    "    embedding.load_state_dict(embedding_sd)\n",
    "# 初始化encoder和decoder模型\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, \n",
    "                decoder_n_layers, dropout)\n",
    "if loadFilename:\n",
    "    encoder.load_state_dict(encoder_sd)\n",
    "    decoder.load_state_dict(decoder_sd)\n",
    "# 使用合适的设备\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "print('Models built and ready to go!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4.2%; Average loss: 3.0858\n",
      "Iteration: 1684; Percent complete: 84.2%; Average loss: 2.9933\n",
      "Iteration: 1685; Percent complete: 84.2%; Average loss: 2.9105\n",
      "Iteration: 1686; Percent complete: 84.3%; Average loss: 3.0980\n",
      "Iteration: 1687; Percent complete: 84.4%; Average loss: 2.8948\n",
      "Iteration: 1688; Percent complete: 84.4%; Average loss: 3.0281\n",
      "Iteration: 1689; Percent complete: 84.5%; Average loss: 2.4605\n",
      "Iteration: 1690; Percent complete: 84.5%; Average loss: 2.5190\n",
      "Iteration: 1691; Percent complete: 84.5%; Average loss: 2.7284\n",
      "Iteration: 1692; Percent complete: 84.6%; Average loss: 2.5340\n",
      "Iteration: 1693; Percent complete: 84.7%; Average loss: 3.1901\n",
      "Iteration: 1694; Percent complete: 84.7%; Average loss: 2.7352\n",
      "Iteration: 1695; Percent complete: 84.8%; Average loss: 2.6584\n",
      "Iteration: 1696; Percent complete: 84.8%; Average loss: 2.3207\n",
      "Iteration: 1697; Percent complete: 84.9%; Average loss: 2.6560\n",
      "Iteration: 1698; Percent complete: 84.9%; Average loss: 2.7479\n",
      "Iteration: 1699; Percent complete: 85.0%; Average loss: 2.9545\n",
      "Iteration: 1700; Percent complete: 85.0%; Average loss: 3.0545\n",
      "Iteration: 1701; Percent complete: 85.0%; Average loss: 2.5287\n",
      "Iteration: 1702; Percent complete: 85.1%; Average loss: 2.5439\n",
      "Iteration: 1703; Percent complete: 85.2%; Average loss: 2.4515\n",
      "Iteration: 1704; Percent complete: 85.2%; Average loss: 2.8460\n",
      "Iteration: 1705; Percent complete: 85.2%; Average loss: 3.4748\n",
      "Iteration: 1706; Percent complete: 85.3%; Average loss: 2.6583\n",
      "Iteration: 1707; Percent complete: 85.4%; Average loss: 2.4453\n",
      "Iteration: 1708; Percent complete: 85.4%; Average loss: 2.8083\n",
      "Iteration: 1709; Percent complete: 85.5%; Average loss: 2.9788\n",
      "Iteration: 1710; Percent complete: 85.5%; Average loss: 3.3084\n",
      "Iteration: 1711; Percent complete: 85.5%; Average loss: 2.8199\n",
      "Iteration: 1712; Percent complete: 85.6%; Average loss: 2.6912\n",
      "Iteration: 1713; Percent complete: 85.7%; Average loss: 2.6210\n",
      "Iteration: 1714; Percent complete: 85.7%; Average loss: 2.7354\n",
      "Iteration: 1715; Percent complete: 85.8%; Average loss: 2.8402\n",
      "Iteration: 1716; Percent complete: 85.8%; Average loss: 3.0646\n",
      "Iteration: 1717; Percent complete: 85.9%; Average loss: 3.0023\n",
      "Iteration: 1718; Percent complete: 85.9%; Average loss: 2.9607\n",
      "Iteration: 1719; Percent complete: 86.0%; Average loss: 2.3849\n",
      "Iteration: 1720; Percent complete: 86.0%; Average loss: 2.7500\n",
      "Iteration: 1721; Percent complete: 86.1%; Average loss: 2.8456\n",
      "Iteration: 1722; Percent complete: 86.1%; Average loss: 2.1872\n",
      "Iteration: 1723; Percent complete: 86.2%; Average loss: 2.5739\n",
      "Iteration: 1724; Percent complete: 86.2%; Average loss: 2.9884\n",
      "Iteration: 1725; Percent complete: 86.2%; Average loss: 2.3820\n",
      "Iteration: 1726; Percent complete: 86.3%; Average loss: 3.2665\n",
      "Iteration: 1727; Percent complete: 86.4%; Average loss: 3.1419\n",
      "Iteration: 1728; Percent complete: 86.4%; Average loss: 1.9592\n",
      "Iteration: 1729; Percent complete: 86.5%; Average loss: 2.5049\n",
      "Iteration: 1730; Percent complete: 86.5%; Average loss: 2.6720\n",
      "Iteration: 1731; Percent complete: 86.6%; Average loss: 2.7372\n",
      "Iteration: 1732; Percent complete: 86.6%; Average loss: 3.0827\n",
      "Iteration: 1733; Percent complete: 86.7%; Average loss: 2.9658\n",
      "Iteration: 1734; Percent complete: 86.7%; Average loss: 3.4061\n",
      "Iteration: 1735; Percent complete: 86.8%; Average loss: 3.3803\n",
      "Iteration: 1736; Percent complete: 86.8%; Average loss: 2.6353\n",
      "Iteration: 1737; Percent complete: 86.9%; Average loss: 3.1221\n",
      "Iteration: 1738; Percent complete: 86.9%; Average loss: 3.4244\n",
      "Iteration: 1739; Percent complete: 87.0%; Average loss: 2.9756\n",
      "Iteration: 1740; Percent complete: 87.0%; Average loss: 2.7452\n",
      "Iteration: 1741; Percent complete: 87.1%; Average loss: 2.6737\n",
      "Iteration: 1742; Percent complete: 87.1%; Average loss: 2.6493\n",
      "Iteration: 1743; Percent complete: 87.2%; Average loss: 2.8854\n",
      "Iteration: 1744; Percent complete: 87.2%; Average loss: 2.5722\n",
      "Iteration: 1745; Percent complete: 87.2%; Average loss: 2.9488\n",
      "Iteration: 1746; Percent complete: 87.3%; Average loss: 3.2578\n",
      "Iteration: 1747; Percent complete: 87.4%; Average loss: 2.1209\n",
      "Iteration: 1748; Percent complete: 87.4%; Average loss: 2.5040\n",
      "Iteration: 1749; Percent complete: 87.5%; Average loss: 2.8275\n",
      "Iteration: 1750; Percent complete: 87.5%; Average loss: 2.7531\n",
      "Iteration: 1751; Percent complete: 87.5%; Average loss: 2.5824\n",
      "Iteration: 1752; Percent complete: 87.6%; Average loss: 3.4230\n",
      "Iteration: 1753; Percent complete: 87.6%; Average loss: 2.7274\n",
      "Iteration: 1754; Percent complete: 87.7%; Average loss: 2.3807\n",
      "Iteration: 1755; Percent complete: 87.8%; Average loss: 2.8763\n",
      "Iteration: 1756; Percent complete: 87.8%; Average loss: 3.2342\n",
      "Iteration: 1757; Percent complete: 87.8%; Average loss: 2.7772\n",
      "Iteration: 1758; Percent complete: 87.9%; Average loss: 3.5950\n",
      "Iteration: 1759; Percent complete: 87.9%; Average loss: 2.2665\n",
      "Iteration: 1760; Percent complete: 88.0%; Average loss: 2.1141\n",
      "Iteration: 1761; Percent complete: 88.0%; Average loss: 2.9052\n",
      "Iteration: 1762; Percent complete: 88.1%; Average loss: 3.0970\n",
      "Iteration: 1763; Percent complete: 88.1%; Average loss: 3.0907\n",
      "Iteration: 1764; Percent complete: 88.2%; Average loss: 2.3358\n",
      "Iteration: 1765; Percent complete: 88.2%; Average loss: 2.2875\n",
      "Iteration: 1766; Percent complete: 88.3%; Average loss: 2.4494\n",
      "Iteration: 1767; Percent complete: 88.3%; Average loss: 2.9421\n",
      "Iteration: 1768; Percent complete: 88.4%; Average loss: 3.1596\n",
      "Iteration: 1769; Percent complete: 88.4%; Average loss: 2.4481\n",
      "Iteration: 1770; Percent complete: 88.5%; Average loss: 2.2949\n",
      "Iteration: 1771; Percent complete: 88.5%; Average loss: 2.7331\n",
      "Iteration: 1772; Percent complete: 88.6%; Average loss: 2.3556\n",
      "Iteration: 1773; Percent complete: 88.6%; Average loss: 2.6680\n",
      "Iteration: 1774; Percent complete: 88.7%; Average loss: 3.4883\n",
      "Iteration: 1775; Percent complete: 88.8%; Average loss: 2.5049\n",
      "Iteration: 1776; Percent complete: 88.8%; Average loss: 2.8307\n",
      "Iteration: 1777; Percent complete: 88.8%; Average loss: 2.0402\n",
      "Iteration: 1778; Percent complete: 88.9%; Average loss: 3.2389\n",
      "Iteration: 1779; Percent complete: 88.9%; Average loss: 2.4118\n",
      "Iteration: 1780; Percent complete: 89.0%; Average loss: 2.9089\n",
      "Iteration: 1781; Percent complete: 89.0%; Average loss: 2.8937\n",
      "Iteration: 1782; Percent complete: 89.1%; Average loss: 2.9180\n",
      "Iteration: 1783; Percent complete: 89.1%; Average loss: 3.7734\n",
      "Iteration: 1784; Percent complete: 89.2%; Average loss: 1.9879\n",
      "Iteration: 1785; Percent complete: 89.2%; Average loss: 2.5986\n",
      "Iteration: 1786; Percent complete: 89.3%; Average loss: 3.1625\n",
      "Iteration: 1787; Percent complete: 89.3%; Average loss: 2.8127\n",
      "Iteration: 1788; Percent complete: 89.4%; Average loss: 2.7591\n",
      "Iteration: 1789; Percent complete: 89.5%; Average loss: 3.2826\n",
      "Iteration: 1790; Percent complete: 89.5%; Average loss: 3.0779\n",
      "Iteration: 1791; Percent complete: 89.5%; Average loss: 2.2437\n",
      "Iteration: 1792; Percent complete: 89.6%; Average loss: 2.6713\n",
      "Iteration: 1793; Percent complete: 89.6%; Average loss: 2.4827\n",
      "Iteration: 1794; Percent complete: 89.7%; Average loss: 3.4559\n",
      "Iteration: 1795; Percent complete: 89.8%; Average loss: 2.8897\n",
      "Iteration: 1796; Percent complete: 89.8%; Average loss: 1.9912\n",
      "Iteration: 1797; Percent complete: 89.8%; Average loss: 2.1997\n",
      "Iteration: 1798; Percent complete: 89.9%; Average loss: 2.2111\n",
      "Iteration: 1799; Percent complete: 90.0%; Average loss: 2.6976\n",
      "Iteration: 1800; Percent complete: 90.0%; Average loss: 2.6677\n",
      "Iteration: 1801; Percent complete: 90.0%; Average loss: 2.8233\n",
      "Iteration: 1802; Percent complete: 90.1%; Average loss: 2.2970\n",
      "Iteration: 1803; Percent complete: 90.1%; Average loss: 2.9201\n",
      "Iteration: 1804; Percent complete: 90.2%; Average loss: 2.8532\n",
      "Iteration: 1805; Percent complete: 90.2%; Average loss: 2.6773\n",
      "Iteration: 1806; Percent complete: 90.3%; Average loss: 2.5749\n",
      "Iteration: 1807; Percent complete: 90.3%; Average loss: 3.1857\n",
      "Iteration: 1808; Percent complete: 90.4%; Average loss: 2.8193\n",
      "Iteration: 1809; Percent complete: 90.5%; Average loss: 3.1456\n",
      "Iteration: 1810; Percent complete: 90.5%; Average loss: 2.8507\n",
      "Iteration: 1811; Percent complete: 90.5%; Average loss: 2.5802\n",
      "Iteration: 1812; Percent complete: 90.6%; Average loss: 2.5957\n",
      "Iteration: 1813; Percent complete: 90.6%; Average loss: 2.0845\n",
      "Iteration: 1814; Percent complete: 90.7%; Average loss: 2.5236\n",
      "Iteration: 1815; Percent complete: 90.8%; Average loss: 2.3189\n",
      "Iteration: 1816; Percent complete: 90.8%; Average loss: 2.8028\n",
      "Iteration: 1817; Percent complete: 90.8%; Average loss: 2.4367\n",
      "Iteration: 1818; Percent complete: 90.9%; Average loss: 2.6696\n",
      "Iteration: 1819; Percent complete: 91.0%; Average loss: 2.5017\n",
      "Iteration: 1820; Percent complete: 91.0%; Average loss: 2.7538\n",
      "Iteration: 1821; Percent complete: 91.0%; Average loss: 2.4042\n",
      "Iteration: 1822; Percent complete: 91.1%; Average loss: 3.1124\n",
      "Iteration: 1823; Percent complete: 91.1%; Average loss: 3.2898\n",
      "Iteration: 1824; Percent complete: 91.2%; Average loss: 2.8756\n",
      "Iteration: 1825; Percent complete: 91.2%; Average loss: 3.0218\n",
      "Iteration: 1826; Percent complete: 91.3%; Average loss: 2.7642\n",
      "Iteration: 1827; Percent complete: 91.3%; Average loss: 2.6818\n",
      "Iteration: 1828; Percent complete: 91.4%; Average loss: 2.5237\n",
      "Iteration: 1829; Percent complete: 91.5%; Average loss: 3.3133\n",
      "Iteration: 1830; Percent complete: 91.5%; Average loss: 2.9998\n",
      "Iteration: 1831; Percent complete: 91.5%; Average loss: 2.4269\n",
      "Iteration: 1832; Percent complete: 91.6%; Average loss: 2.0472\n",
      "Iteration: 1833; Percent complete: 91.6%; Average loss: 2.6304\n",
      "Iteration: 1834; Percent complete: 91.7%; Average loss: 2.5590\n",
      "Iteration: 1835; Percent complete: 91.8%; Average loss: 3.1612\n",
      "Iteration: 1836; Percent complete: 91.8%; Average loss: 2.4239\n",
      "Iteration: 1837; Percent complete: 91.8%; Average loss: 2.7256\n",
      "Iteration: 1838; Percent complete: 91.9%; Average loss: 2.5985\n",
      "Iteration: 1839; Percent complete: 92.0%; Average loss: 3.0544\n",
      "Iteration: 1840; Percent complete: 92.0%; Average loss: 2.9066\n",
      "Iteration: 1841; Percent complete: 92.0%; Average loss: 2.6178\n",
      "Iteration: 1842; Percent complete: 92.1%; Average loss: 2.9562\n",
      "Iteration: 1843; Percent complete: 92.2%; Average loss: 3.0337\n",
      "Iteration: 1844; Percent complete: 92.2%; Average loss: 2.8133\n",
      "Iteration: 1845; Percent complete: 92.2%; Average loss: 2.9463\n",
      "Iteration: 1846; Percent complete: 92.3%; Average loss: 2.4446\n",
      "Iteration: 1847; Percent complete: 92.3%; Average loss: 2.5736\n",
      "Iteration: 1848; Percent complete: 92.4%; Average loss: 2.1551\n",
      "Iteration: 1849; Percent complete: 92.5%; Average loss: 3.2140\n",
      "Iteration: 1850; Percent complete: 92.5%; Average loss: 2.8657\n",
      "Iteration: 1851; Percent complete: 92.5%; Average loss: 2.5803\n",
      "Iteration: 1852; Percent complete: 92.6%; Average loss: 2.3123\n",
      "Iteration: 1853; Percent complete: 92.7%; Average loss: 2.2744\n",
      "Iteration: 1854; Percent complete: 92.7%; Average loss: 1.8212\n",
      "Iteration: 1855; Percent complete: 92.8%; Average loss: 2.1234\n",
      "Iteration: 1856; Percent complete: 92.8%; Average loss: 2.4847\n",
      "Iteration: 1857; Percent complete: 92.8%; Average loss: 2.8169\n",
      "Iteration: 1858; Percent complete: 92.9%; Average loss: 2.4839\n",
      "Iteration: 1859; Percent complete: 93.0%; Average loss: 2.9240\n",
      "Iteration: 1860; Percent complete: 93.0%; Average loss: 2.1321\n",
      "Iteration: 1861; Percent complete: 93.0%; Average loss: 2.8416\n",
      "Iteration: 1862; Percent complete: 93.1%; Average loss: 2.4451\n",
      "Iteration: 1863; Percent complete: 93.2%; Average loss: 2.9215\n",
      "Iteration: 1864; Percent complete: 93.2%; Average loss: 2.5073\n",
      "Iteration: 1865; Percent complete: 93.2%; Average loss: 1.8549\n",
      "Iteration: 1866; Percent complete: 93.3%; Average loss: 2.5685\n",
      "Iteration: 1867; Percent complete: 93.3%; Average loss: 2.9915\n",
      "Iteration: 1868; Percent complete: 93.4%; Average loss: 2.9911\n",
      "Iteration: 1869; Percent complete: 93.5%; Average loss: 2.6264\n",
      "Iteration: 1870; Percent complete: 93.5%; Average loss: 2.4884\n",
      "Iteration: 1871; Percent complete: 93.5%; Average loss: 2.4519\n",
      "Iteration: 1872; Percent complete: 93.6%; Average loss: 2.4244\n",
      "Iteration: 1873; Percent complete: 93.7%; Average loss: 2.5226\n",
      "Iteration: 1874; Percent complete: 93.7%; Average loss: 2.8410\n",
      "Iteration: 1875; Percent complete: 93.8%; Average loss: 3.0977\n",
      "Iteration: 1876; Percent complete: 93.8%; Average loss: 2.5491\n",
      "Iteration: 1877; Percent complete: 93.8%; Average loss: 3.0721\n",
      "Iteration: 1878; Percent complete: 93.9%; Average loss: 3.1883\n",
      "Iteration: 1879; Percent complete: 94.0%; Average loss: 3.0514\n",
      "Iteration: 1880; Percent complete: 94.0%; Average loss: 2.9513\n",
      "Iteration: 1881; Percent complete: 94.0%; Average loss: 2.9374\n",
      "Iteration: 1882; Percent complete: 94.1%; Average loss: 2.8986\n",
      "Iteration: 1883; Percent complete: 94.2%; Average loss: 3.0937\n",
      "Iteration: 1884; Percent complete: 94.2%; Average loss: 2.3513\n",
      "Iteration: 1885; Percent complete: 94.2%; Average loss: 2.5031\n",
      "Iteration: 1886; Percent complete: 94.3%; Average loss: 2.5924\n",
      "Iteration: 1887; Percent complete: 94.3%; Average loss: 2.6158\n",
      "Iteration: 1888; Percent complete: 94.4%; Average loss: 3.1837\n",
      "Iteration: 1889; Percent complete: 94.5%; Average loss: 2.6871\n",
      "Iteration: 1890; Percent complete: 94.5%; Average loss: 2.2830\n",
      "Iteration: 1891; Percent complete: 94.5%; Average loss: 2.9832\n",
      "Iteration: 1892; Percent complete: 94.6%; Average loss: 2.9398\n",
      "Iteration: 1893; Percent complete: 94.7%; Average loss: 3.1315\n",
      "Iteration: 1894; Percent complete: 94.7%; Average loss: 2.4603\n",
      "Iteration: 1895; Percent complete: 94.8%; Average loss: 2.7357\n",
      "Iteration: 1896; Percent complete: 94.8%; Average loss: 3.2023\n",
      "Iteration: 1897; Percent complete: 94.8%; Average loss: 2.9691\n",
      "Iteration: 1898; Percent complete: 94.9%; Average loss: 3.4248\n",
      "Iteration: 1899; Percent complete: 95.0%; Average loss: 3.4793\n",
      "Iteration: 1900; Percent complete: 95.0%; Average loss: 2.4975\n",
      "Iteration: 1901; Percent complete: 95.0%; Average loss: 2.8540\n",
      "Iteration: 1902; Percent complete: 95.1%; Average loss: 3.0223\n",
      "Iteration: 1903; Percent complete: 95.2%; Average loss: 2.4079\n",
      "Iteration: 1904; Percent complete: 95.2%; Average loss: 2.3220\n",
      "Iteration: 1905; Percent complete: 95.2%; Average loss: 3.0445\n",
      "Iteration: 1906; Percent complete: 95.3%; Average loss: 2.4241\n",
      "Iteration: 1907; Percent complete: 95.3%; Average loss: 2.7750\n",
      "Iteration: 1908; Percent complete: 95.4%; Average loss: 2.8339\n",
      "Iteration: 1909; Percent complete: 95.5%; Average loss: 2.9435\n",
      "Iteration: 1910; Percent complete: 95.5%; Average loss: 2.5999\n",
      "Iteration: 1911; Percent complete: 95.5%; Average loss: 1.9467\n",
      "Iteration: 1912; Percent complete: 95.6%; Average loss: 2.4239\n",
      "Iteration: 1913; Percent complete: 95.7%; Average loss: 2.6217\n",
      "Iteration: 1914; Percent complete: 95.7%; Average loss: 2.8147\n",
      "Iteration: 1915; Percent complete: 95.8%; Average loss: 2.6695\n",
      "Iteration: 1916; Percent complete: 95.8%; Average loss: 2.6032\n",
      "Iteration: 1917; Percent complete: 95.9%; Average loss: 2.3226\n",
      "Iteration: 1918; Percent complete: 95.9%; Average loss: 2.8335\n",
      "Iteration: 1919; Percent complete: 96.0%; Average loss: 2.2915\n",
      "Iteration: 1920; Percent complete: 96.0%; Average loss: 2.5613\n",
      "Iteration: 1921; Percent complete: 96.0%; Average loss: 2.5836\n",
      "Iteration: 1922; Percent complete: 96.1%; Average loss: 2.6472\n",
      "Iteration: 1923; Percent complete: 96.2%; Average loss: 2.9739\n",
      "Iteration: 1924; Percent complete: 96.2%; Average loss: 2.5557\n",
      "Iteration: 1925; Percent complete: 96.2%; Average loss: 2.7534\n",
      "Iteration: 1926; Percent complete: 96.3%; Average loss: 2.8635\n",
      "Iteration: 1927; Percent complete: 96.4%; Average loss: 2.7728\n",
      "Iteration: 1928; Percent complete: 96.4%; Average loss: 3.1873\n",
      "Iteration: 1929; Percent complete: 96.5%; Average loss: 3.1572\n",
      "Iteration: 1930; Percent complete: 96.5%; Average loss: 2.7740\n",
      "Iteration: 1931; Percent complete: 96.5%; Average loss: 2.7778\n",
      "Iteration: 1932; Percent complete: 96.6%; Average loss: 2.3950\n",
      "Iteration: 1933; Percent complete: 96.7%; Average loss: 3.3258\n",
      "Iteration: 1934; Percent complete: 96.7%; Average loss: 2.8425\n",
      "Iteration: 1935; Percent complete: 96.8%; Average loss: 3.2711\n",
      "Iteration: 1936; Percent complete: 96.8%; Average loss: 3.0584\n",
      "Iteration: 1937; Percent complete: 96.9%; Average loss: 2.9156\n",
      "Iteration: 1938; Percent complete: 96.9%; Average loss: 1.9687\n",
      "Iteration: 1939; Percent complete: 97.0%; Average loss: 2.5582\n",
      "Iteration: 1940; Percent complete: 97.0%; Average loss: 2.4505\n",
      "Iteration: 1941; Percent complete: 97.0%; Average loss: 2.6188\n",
      "Iteration: 1942; Percent complete: 97.1%; Average loss: 2.6815\n",
      "Iteration: 1943; Percent complete: 97.2%; Average loss: 2.6677\n",
      "Iteration: 1944; Percent complete: 97.2%; Average loss: 2.6533\n",
      "Iteration: 1945; Percent complete: 97.2%; Average loss: 2.3338\n",
      "Iteration: 1946; Percent complete: 97.3%; Average loss: 2.2867\n",
      "Iteration: 1947; Percent complete: 97.4%; Average loss: 2.1611\n",
      "Iteration: 1948; Percent complete: 97.4%; Average loss: 2.3084\n",
      "Iteration: 1949; Percent complete: 97.5%; Average loss: 2.5548\n",
      "Iteration: 1950; Percent complete: 97.5%; Average loss: 2.7045\n",
      "Iteration: 1951; Percent complete: 97.5%; Average loss: 2.7850\n",
      "Iteration: 1952; Percent complete: 97.6%; Average loss: 2.8771\n",
      "Iteration: 1953; Percent complete: 97.7%; Average loss: 2.7913\n",
      "Iteration: 1954; Percent complete: 97.7%; Average loss: 3.2557\n",
      "Iteration: 1955; Percent complete: 97.8%; Average loss: 2.9186\n",
      "Iteration: 1956; Percent complete: 97.8%; Average loss: 2.8150\n",
      "Iteration: 1957; Percent complete: 97.9%; Average loss: 2.6681\n",
      "Iteration: 1958; Percent complete: 97.9%; Average loss: 3.1851\n",
      "Iteration: 1959; Percent complete: 98.0%; Average loss: 2.7097\n",
      "Iteration: 1960; Percent complete: 98.0%; Average loss: 3.1716\n",
      "Iteration: 1961; Percent complete: 98.0%; Average loss: 2.6662\n",
      "Iteration: 1962; Percent complete: 98.1%; Average loss: 2.7380\n",
      "Iteration: 1963; Percent complete: 98.2%; Average loss: 2.8290\n",
      "Iteration: 1964; Percent complete: 98.2%; Average loss: 2.6850\n",
      "Iteration: 1965; Percent complete: 98.2%; Average loss: 2.5966\n",
      "Iteration: 1966; Percent complete: 98.3%; Average loss: 2.4602\n",
      "Iteration: 1967; Percent complete: 98.4%; Average loss: 2.8835\n",
      "Iteration: 1968; Percent complete: 98.4%; Average loss: 3.0852\n",
      "Iteration: 1969; Percent complete: 98.5%; Average loss: 2.1616\n",
      "Iteration: 1970; Percent complete: 98.5%; Average loss: 2.5874\n",
      "Iteration: 1971; Percent complete: 98.6%; Average loss: 2.3926\n",
      "Iteration: 1972; Percent complete: 98.6%; Average loss: 2.9178\n",
      "Iteration: 1973; Percent complete: 98.7%; Average loss: 2.7644\n",
      "Iteration: 1974; Percent complete: 98.7%; Average loss: 2.9455\n",
      "Iteration: 1975; Percent complete: 98.8%; Average loss: 2.5123\n",
      "Iteration: 1976; Percent complete: 98.8%; Average loss: 2.8788\n",
      "Iteration: 1977; Percent complete: 98.9%; Average loss: 3.2964\n",
      "Iteration: 1978; Percent complete: 98.9%; Average loss: 2.6615\n",
      "Iteration: 1979; Percent complete: 99.0%; Average loss: 2.7364\n",
      "Iteration: 1980; Percent complete: 99.0%; Average loss: 2.8206\n",
      "Iteration: 1981; Percent complete: 99.1%; Average loss: 1.9526\n",
      "Iteration: 1982; Percent complete: 99.1%; Average loss: 2.3283\n",
      "Iteration: 1983; Percent complete: 99.2%; Average loss: 2.1399\n",
      "Iteration: 1984; Percent complete: 99.2%; Average loss: 2.8691\n",
      "Iteration: 1985; Percent complete: 99.2%; Average loss: 2.9532\n",
      "Iteration: 1986; Percent complete: 99.3%; Average loss: 2.5714\n",
      "Iteration: 1987; Percent complete: 99.4%; Average loss: 2.6060\n",
      "Iteration: 1988; Percent complete: 99.4%; Average loss: 2.8469\n",
      "Iteration: 1989; Percent complete: 99.5%; Average loss: 2.7711\n",
      "Iteration: 1990; Percent complete: 99.5%; Average loss: 2.8354\n",
      "Iteration: 1991; Percent complete: 99.6%; Average loss: 2.7469\n",
      "Iteration: 1992; Percent complete: 99.6%; Average loss: 2.2657\n",
      "Iteration: 1993; Percent complete: 99.7%; Average loss: 2.5963\n",
      "Iteration: 1994; Percent complete: 99.7%; Average loss: 3.0122\n",
      "Iteration: 1995; Percent complete: 99.8%; Average loss: 2.8726\n",
      "Iteration: 1996; Percent complete: 99.8%; Average loss: 2.8895\n",
      "Iteration: 1997; Percent complete: 99.9%; Average loss: 2.8987\n",
      "Iteration: 1998; Percent complete: 99.9%; Average loss: 2.2537\n",
      "Iteration: 1999; Percent complete: 100.0%; Average loss: 3.3252\n",
      "Iteration: 2000; Percent complete: 100.0%; Average loss: 2.7097\n"
     ]
    }
   ],
   "source": [
    "# 配置训练的超参数和优化器 \n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "#n_iteration = 4000\n",
    "n_iteration = 2000\n",
    "print_every = 1\n",
    "save_every = 500\n",
    "\n",
    "# save_dir = './save'\n",
    "# if not os.path.exists(save_dir):\n",
    "#     os.mkdir(save_dir)\n",
    "# corpus_name = \"chinese_data\"\n",
    "\n",
    "# 设置进入训练模式，从而开启dropout \n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "# 初始化优化器 \n",
    "print('Building optimizers ...')\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "# if loadFilename:\n",
    "#     encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
    "#     decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
    "\n",
    "# 开始训练\n",
    "print(\"Starting Training!\")\n",
    "trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
    "           print_every, save_every, clip, corpus_name, loadFilename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存模型\n",
    "torch.save(encoder.state_dict(), 'module/encoder_10')\n",
    "torch.save(decoder.state_dict(), 'module/decoder_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "#调用模型\n",
    "enstate = torch.load('module/encoder_10')\n",
    "destate = torch.load('module/decoder_10')\n",
    "#实例化\n",
    "test_encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "test_decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, \n",
    "                decoder_n_layers, dropout)\n",
    "#加载参数\n",
    "test_encoder.load_state_dict(enstate)\n",
    "test_decoder.load_state_dict(destate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "早上好\n",
      "Dumping model to file cache /var/folders/mp/bq47krjd0cl48zy5k6fzm3yc0000gn/T/jieba.cache\n",
      "Loading model cost 0.917 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Bot: 嗯嗯亲\n",
      "有什么推荐吗\n",
      "Bot: 有的哦\n",
      "你说一下\n",
      "Bot: 好的亲\n",
      "q\n"
     ]
    }
   ],
   "source": [
    "test_encoder.eval()\n",
    "test_decoder.eval()\n",
    "# 构造searcher对象 \n",
    "test_searcher = GreedySearchDecoder(test_encoder, test_decoder)\n",
    "\n",
    "# 测试\n",
    "evaluateInput(test_encoder, test_decoder, test_searcher, voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "e534e48711db4d1e1c48977d0d14ff85b1f16d41bcc4fdfd88268a329b3c9d66"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}