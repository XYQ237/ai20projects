# Task11 (NewsRec) Project I

GroupID:18   StudentID:2018202155

本项目主要是尝试用不同算法建立推荐系统模型，并建立评估模型，选出最优化算法模型，并作出进一步的改进。

我们目前已经实现了12/27个算法实例，其中我着重分析了以下6个算法：

1. Term Frequency - Inverse Document Frequency (TF-IDF)
2. LightFM/Hybrid Matrix Factorization
3. GeoIMC
4. xLearn/Factorization Machine (FM) & Field-Aware FM (FFM)
5. GRU4Rec
6. Short-term and Long-term preference Integrated Recommender (SLi-Rec)

 

## 搭建环境

1. 本地环境：安装带有3.6以上版本Python的Anaconda，用git将Recommerders仓库克隆到本地。然后创建并激活conda环境。

  

## 算法思路

**1. Term Frequency - Inverse Document Frequency (TF-IDF)**

用TF-IDF基于内容作推荐。该算法用到的是COVID-19 Open Research Dataset中license为cc0的数据，是文本类型的数据，数据量为258。

首先是数据预处理，将数据集中的原始数据转换为dataframe格式，选取cc0中的数据，并提取各文件里的完整文本内容。

然后文本分词，去除停用词，创建一个对象调用TfidfRecommende，训练模型，对每一个类型的参考文本返回与它相似度较高的前k个文本。其中，相似度的计算是基于TF-IDF算法，计算词频和逆文档频率，进而计算两篇文章之间的余弦相似值。

 

**2. LightFM/Hybrid Matrix Factorization**

推荐系统中会遇到冷启动问题（cold-start issue）也就是没有足够的历史数据，大致有3个类型：

1. 新平台，比如一个新网上购物平台，只有商品信息，没有用户、购买记录。

2. 新条目，比如新商品，缺乏访问次数，会导致推荐不准确，且缺少推荐，造成负反馈，导致流行偏见问题(popularity bias)

3. 新用户，缺乏访问或购买记录

为了解决这个问题，提出了组合协同过滤和内容推荐这两种推荐方法的混合推荐系统，其中之一是hybrid matrix factorisation模型，它组合了矩阵分解和LightFM包。

 

LightFM model:

U : 用户集

I : 物品集

每个用户有一系列用户特征
$$
f_u\subset F_U
$$
每个物品有一系列物品特征
$$
f_i\subset F_I
$$
该模型返回二值，评级会归一化为两组。对于显式评级，将用户物品对
$$
(u,i)\in U \times I
$$
分为正集合S+和负集合S-；对于隐性的反馈分为被观察的和不被观察的。

用户u和物品i的embedding为相应特征向量的和：
$$
q_u=\sum _{j\in f_u}e_j^U\\
p_i=\sum _{j\in f_i}e_j^I
$$
用户u和物品i的偏差为相应偏差向量的和：
$$
b_u=\sum _{j\in f_u}b_j^U\\
b_i=\sum _{j\in f_i}b_j^I
$$
 每个用户和物品表示为它特征向量的加权线性和。

预测用户u和物品i：
$$
r_{ui}=\sigma (q_u\cdot p_i+b_u+b_i)
$$
sigoid函数是进行归一化，以返回二值。

用随机梯度下降表示可能性，极大似然进行训模型拟合。可能性的表达式为：
$$
L=\prod _{(u,i)\in S^+}r_{ui}\times \prod_{(u,i)\in S^-}1-r_{ui}
$$



**3.  GeoIMC(Geometry Aware Inductive Matrix Completion)**

矩阵补全问题：有一个巨大的矩阵，人们只能观测到其中到部分元素，解决如何补全整个矩阵的问题。在推荐系统中，比如电影评分网站上，有很多用户（矩阵的行）给很多电影（矩阵的列）进行打分，但每个用户只会对很少一部分电影评分，那么矩阵中只有一部分值可被观测到，此时我们想预测用户对没有评分的电影的打分，也就是解决一个矩阵补全问题。

使用数据集MovieLens-100K，设
$$
X\in R^{m\times d_1} , Z\in R^{n\times d_2}
$$
分别为用户和电影的特征，
$$
U\in R^{m\times n}
$$
是部分观测到的评分矩阵。

GeoIMC将该矩阵建模为如下形式：
$$
M=XUBV^TZ^T
$$
其中
$$
U\in R^{d_1\times k},V\in R^{d_2\times k},B\in R^{k\times k}
$$
分别为正交矩阵、正交矩阵、对称正定矩阵。最优化问题通过Pymanopt解决。

 

 

**4. xLearn/Factorization Machine (FM) & Field-Aware FM (FFM)**

因子分解机，可处理高度稀疏的数据集，它不仅捕捉到输入的特征，而且**关注了特征与特征之间的相互关系** ，所以很强大。和其他传统算法比如SVM相比呈现出较好的泛化能力和表现。

最新的研究用深度学习方法延伸扩展了基本的FM算法，在几个应用实例中达到了显著的提升。

用户、物品、特征向量可表示为独热表达，此时传统的算法比如线性回归、SVM可能有以下问题：

1. 特征向量高度稀疏，因此，很难高效地优化参数拟合模型。

2. 特征的叉乘也会很稀疏，这样如果用它来刻画特征间的高阶交互，会降低模型的表现。

FM算法通过分解隐向量解决上述问题，它的大致思想是：
$$
y(x)=\omega_0+\sum _{i=1}^n \omega_ix_i+\sum_{i=1}^n\sum_{j=i+1}^n<v_i,v_j>x_ix_j
$$
其中x是输入的特征向量，y是待预测值，w_i是模型参数中的一阶元素，<vi,vj>是二阶相互关系，它是两个隐向量的点积，定义为：
$$
<v_i,v_j> =\sum_{f=1}^kv_{i,f}\cdot v_{j,f}
$$
计算复杂度为O(kn)，这里高阶交互元素中使用分解的向量与使用固定参数相比，可以提高模型的泛化能力和表现。

 

FFM是FM的一个延伸。直觉上FM中特征共用隐向量来表示不同类别的信息可能并不能很好地泛化相关度，FFM解决了这一问题，它在不同组的特征中使用了不同的分解隐向量，“组”在FFM中表述为'field'。FFM中二阶交互定义为：
$$
\theta_{FFM}(wx)=\sum_{j_1=1}^n\sum_{j_2=j_1+1}^n<v_{j_1,f_2},v_{j_2,f_1}>x_{j_1}x_{j_2}
$$
时间复杂度提高到了O(k*n^2)，但是FFM中的隐向量只需要在field内计算，所以FFM的k值通常会比FM中的小很多。

在本实验中，使用xlearn进行实现，它是用C++实现的，带Python接口，在能较高效地计算且不损失模型的有效性。



**序列推荐(sequential recommendation)**

以下五个模型都采用了序列推荐(sequential recommendation)，通过对用户（user）行为序列，比如购买商品（item）的序列来建模，学到user 兴趣的变化，从而能够对用户在短期内的行为或是下一次的行为进行预测。

- Attentive Asynchronous Singular Value Decomposition (A2SVD)

- Convolutional Sequence Embedding Recommendation (Caser)

- GRU4Rec

- Next Item Recommendation (NextItNet)

- Short-term and Long-term preference Integrated Recommender (SLi-Rec)

 

**5. GRU4Rec**

使用循环神经网络(recurrent neural network)捕捉用户的长期和短期偏好，在该模型中，使用完整的session行为序列信息，以session为粒度并行mini-batch，极大地加快了RNN-based模型训练。



**6. Short-term and Long-term preference Integrated Recommender (SLi-Rec)**

本实验中用到的是基于深度学习的SLi_Rec模型，它捕捉了长期的和短期的用户偏好以更精准地进行推荐，它有几个关键性质：

1. 长期兴趣的建模采用了注意力机制的不对称SVD；

2. 通过修改LSTM中的门控机制，同时考虑到了时间和语义的不规则性

3. 使用注意力机制动态地将长期偏好和短期偏好融合在一起。

   

## 后期规划

算法的代码运行中还有一些问题需要调试，之后我们将继续把给出的算法逐一理解原理并实现，再提出改进方法。



# Task11 (NewsRec) Project II

这个阶段我们分析了剩余的几个算法，其中我着重分析了以下7个算法：

1. Deep Knowledge-Aware Network (DKN)

2. Extreme Deep Factorization Machine (xDeepFM)
3. FastAI Embedding Dot Bias (FAST)
4. LightGCN
5. Attentive Asynchronous Singular Value Decomposition (A2SVD)
6. Convolutional Sequence Embedding Recommendation (Caser)
7. Neural Recommendation with Multi-Head Self-Attention (NRMS)



## 算法思路

**1. Deep Knowledge-Aware Network (DKN)**

**整体思路：**用TransX知识表示学习，用一个CNN框架，对于一个新闻文章，将实体嵌入和单词嵌入结合起来产生一个最终的嵌入向量，作出点击率CTR(click-through-rate)的预测。用到了注意力机制来动态计算用户的历史数据。
**具体说明：**考虑到新闻的两个特点，首先，新闻语言高度浓缩，且包含很多知识实体和常识，之前的模型大部分仅仅从语义层面(semantic level)进行表示学习，较少考虑新闻包含的外部知识，没有充分挖掘新闻文本在知识层面(knowledge level)的联系。此外，新闻有很强的时效性，一个好的新闻推荐算法应该能随用户的兴趣的改变做出相应的变化。DKN模型主将知识图谱实体嵌入表示与神经网络结合起来，首先使用一种融合了知识的卷积神经网络KCNN(knowledge-aware convolutional neural network)，将新闻的语义表示与知识表示融合起来形成新的embedding表示，再建立从用户的新闻点击历史到候选新闻的attention机制，选出得分较高的新闻推荐给用户。
知识图谱网络嵌入：
知识图谱由大量节点及节点之间的边组成，节点表示实体，边表示节点之间的关系，可以看作是许多三元组(head,relation,tail)构成的集合。知识图谱的网络嵌入是要用一个低维稠密的向量来表示节点，保证该向量包含了节点间的相似性关系以及网络的结构信息。目前已有的translation-based的嵌入表示方法有：
(1)TransE：设h,r,t 分别是head, relation, tail对应的向量，优化目标是使
$$
h+r\approx t
$$
评分函数是
$$
f_r(h,t)=||h+r-t||_2^2
$$
数值越小说明对应向量之间的关系越符合网络中实体节点间的关系。

(2)TransH:评分函数为
$$
f_r(h,t)=||h_\perp+r-t_\perp||_2^2\\其中h_\perp=h-w_r^Thw_r,t_\perp=t-w_r^Ttw_r,||w_r||_2=1
$$
(3)TransR: 使用一个映射矩阵，把不同的实体向量映射到相同的向量空间中,
$$
f_r(h,t)=||h_r+r-t_r||_2^2\\其中h_r=hM_r,t_r=tM_r
$$
(4)TransD: 新的映射方式，
$$
f_r(h,t)=||h_\perp+r-t_\perp||_2^2\\其中h_\perp=(r_ph_p^T+I)h,t_\perp=(r_pt_p^T+I)t
$$
h_p, r_p, t_p是另一组实体及关系的向量表示，I是单位矩阵。

网络嵌入表示方法的损失函数均为：
$$
L=\sum_{(h,r,t)\in\Delta}\sum_{(h^{'},r,t^{'})\in\Delta^{'}}max(0,f_r(h,t)+\gamma-f_r(h^{'},t^{'}))
$$



**2. Extreme Deep Factorization Machine (xDeepFM)**

基于Field的vector-wise思想引入Cross，并且保留了Cross的优势。

CIN是在vector-wise level，DNN是在bit-wise level



整体思路：

(1)对原始特征的Field形式包装，把特征one-hot形式包装进同一个field来克服稀疏性；
(2)在embeding层对样本做embeding转换，按照 deepFM相似的形式来获取每个样本长度为 field_size的 embedding表示，这样embedding后的样本矩阵为(field_size, embedding_size)；
(3)简单的一阶计算部分，这部分没有用到特征embedding的结果，而是用w\*x那种类似LR的一阶计算;
(4)CIN模型(Compressed Interaction Network)进行可控的自动学习显式的高阶特征交互，运用deep & cross思想，CIN层的输入来自Embedding层，设有m 个field，每个field的embedding vector维度为D, 则输入可表示为矩阵
$$
X^0\in R^{m*D}
$$
(5)最后是 DNN部分的结构，对经过embedding转换后的样本特征进行隐式的高阶特征交互。



其中比较重要的部分是CIN层，每层的计算方法为：

令
$$
X^k\in R^{H_k*D}
$$
表示第k层的vector个数，其中H_k表示第k层的vector个数，vecor维度始终为D，保持和输入层一致。第k层每个vector的计算方式为：
$$
X_{h,*}^k=\sum_{i=1}^{H_{k-1}}\sum_{j=1}^m W_{ij}^{k,h}(X_{i,*}^{k-1}\omicron X_{j,*}^0)\in R^{1*D}, where\ 1\le h\le H_k
$$
其中W^{k,h}表示第k层的第h个vector的权重矩阵，空心圈表示Hadamard乘积，即逐元素乘。这个公式取前一层X^{k-1}中的H_{k-1}个vector，与输入层X^0中的m个vector，进行两两Hadamard乘积运算，得到H\_{k-1}\*m个 vector，然后加权求和。第k层的不同vector区别在于，对这H\_{k-1}\*m个 vector 求和的权重矩阵不同。H_k即对应有多少个不同的权重矩阵W^k, 是一个可以调整的超参。



**3. FastAI Embedding Dot Bias (FAST)**

提供一个封装的框架，用到嵌入(embedding)、对用户和物品的偏置(bias)。



**4. LightGCN**

深度学习算法，是GCN预测隐式反馈的简化方法，只包含了GCN中最基本的邻域聚集部分，更适合推荐系统。

整体思路：先将用户和物品节点的领域聚合，再使用三层卷积层分别生成每层的嵌入，然后将节点的原始输入与生成的每层的嵌入做加权和，最后将用户和物品最终的生成节点表示做内积，生成预测的分数。



计算过程：

使用节点的邻域加权运算下一层的节点表示：
$$
e_u^{(k+1)}=\sum_{i\in N_u}\frac 1 {\sqrt{|N_u||N_i|}}e_i^{(k)}\\
e_i^{(k+1)}=\sum_{u\in N_i}\frac 1 {\sqrt{|N_i||N_u|}}e_u^{(k)}
$$
将每层生成的节点表示累加，生成最终的节点表示：
$$
e_u=\sum _{k=0}^K\alpha_ke_u^{(k)}\\
e_i=\sum _{k=0}^K\alpha_ke_i^{(k)}\\
其中设\alpha_k=\frac 1 {(K+1)}
$$
最后根据生成的节点表示做内积得到最终的预测分数：
$$
y_{ui}=e_u^Te_i
$$





**5. Attentive Asynchronous Singular Value Decomposition (A2SVD)**

动态地将用户的长期和短期偏好结合起来，将传统的RNN模型做改进来更好地用于个性化推荐系统。主要提出了两个方法：

1. 直觉上，在短时间间隔内的两个动作比长时间间隔内的两个动作关系更紧密，所以设置一个time-aware controller来动态刻画时间上的信息。

2. 用户行为背后的用户意图总是在动态变化，不相关的动作对于预测将来的行为没有用处，所以需要动态描述潜在意图，设置一个content-aware controller来动态刻画上下文信息。

并且进一步基于attention机制将用户的短期和长期偏好结合起来。



Short-Term Modeling：

对于时间的不规律性：在传统的LSTM中引入两个时间特征，时间间隔delta_t_k和时间跨度s_t_k，然后计算相应的时间门控T_delta和T_s。

对于语义的不规律性：采用注意力机制压制那些偏离目标方向的信息。

将用户用所有算得的隐状态的加权和来表示。

Long-Term Modeling：

将用户用与他们有过交互的物品来表示。

短期和长期偏好的结合方法：采用动态构建的方法，它们各自占的比重取决于特定的环境，比如什么时候（如果下一个动作紧接着上一个动作，那么短期偏好的特征更有用），或者什么内容（比如关于手机的偏好可能用长期偏好刻画更有效，手机配件可能用短期偏好刻画更有效）



**6. Convolutional Sequence Embedding Recommendation (Caser)**

基于卷积捕捉用户的一般偏好和序列模式。将每个用户有过交互的物品看作一个序列，用来预测用户将来可能有交互的前n个物品的排序，Caser模型的主要思想是将与当前物品在时间和空间上相近的商品形成一个“图像”，用卷积滤波器学习序列模式作为图像的局部特征。



具体方法：

用卷积神经网络(CNN)学习序列特征，用隐因子模型(LFM)学习用户特征，caser由三部分组成：e mbedding layer, convolutional layer, fully-connected layer，对于每个用户u，从用户序列S_u中提取每L个连续项作为输入，将他们的下T项作为目标，将长为L的序列映射为embedding，堆积在一起，而用户特征是用LFM生成。水平卷积层有n个水平滤波器，在该例中，有8个滤波器，每个h={1,2,3,4}各自对应两个滤波器，Fk从上到下滑动，与物品的横向维度发生相互作用， 再对卷积的结果做最大池化操作。垂直卷积网络中每个滤波器Fk与列交互从左到右滑动。在全连接层将两个卷积层的结果连接起来，进行嵌入得到z，z包含了之前项的所有序列特征，为了捕捉用户的一般偏好，嵌入用户P_u，将z和P_u连接，再将他们投射到有|I|个节点的输出层，
$$
y^{(u,t)}=W'\left[\begin{matrix}z\\P_u\end{matrix}\right]+b'
$$



**7. Neural Recommendation with Multi-Head Self-Attention (NRMS)**

使用multi-head self-attention方法，NRMS的核心是一个新闻编码器和一个用户编码器，在新闻编码器中，通过构建单词之间的互动来学习新闻的表示，在用户编码器中，通过捕捉用户浏览过的新闻之间的关系来学习用户的表示。并用另外的注意力机制通过挑选重要的单词和新闻来学习那些更能提供信息的新闻和用户的表示。



## 后期规划

目前已分析完给出的推荐系统的算法，但是在代码运行中还存在一些问题，下一阶段先把代码中的问题进行修复，然后分析不同方法的性能，比较各自的优劣势，提出改进算法。