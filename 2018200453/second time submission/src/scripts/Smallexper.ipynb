{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取每张图片描述语句信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 8092 \n",
      "Vocabulary Size: 4484\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def load_descriptions(doc):\n",
    "    mapping = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        # split line by white space\n",
    "        tokens = line.split()\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        #take the first token as the image id, the rest as the description\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        # remove filename from image id(.jpg)\n",
    "        image_id = image_id.split('.')[0]\n",
    "        # convert description tokens back to string\n",
    "        image_desc = ' '.join(image_desc)\n",
    "        # store the first description for each image\n",
    "        if image_id not in mapping:\n",
    "            mapping[image_id] = image_desc\n",
    "    return mapping\n",
    "\n",
    "def clean_descriptions(descriptions):\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for key, desc in descriptions.items():\n",
    "        # tokenize\n",
    "        desc = desc.split()\n",
    "        # convert to lower case\n",
    "        desc = [word.lower() for word in desc]\n",
    "        # remove punctuation from each token\n",
    "        desc = [w.translate(table) for w in desc]\n",
    "        # remove hanging 's' and 'a'\n",
    "        desc = [word for word in desc if len(word)>1]\n",
    "        # store as string\n",
    "        descriptions[key] =  ' '.join(desc)\n",
    "\n",
    "# save descriptions to file, one per line\n",
    "def save_doc(descriptions, filename):\n",
    "    lines = list()\n",
    "    for key, desc in descriptions.items():\n",
    "        lines.append(key + ' ' + desc)\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "filename = 'Flickr8k_text/Flickr8k.token.txt'\n",
    "doc = load_doc(filename)\n",
    "descriptions = load_descriptions(doc)\n",
    "print('Loaded: %d ' % len(descriptions))\n",
    "clean_descriptions(descriptions)\n",
    "\n",
    "# summarize vocabulary\n",
    "all_tokens = ' '.join(descriptions.values()).split()\n",
    "vocabulary = set(all_tokens)\n",
    "print('Vocabulary Size: %d' % len(vocabulary))\n",
    "\n",
    "# save descriptions\n",
    "save_doc(descriptions, 'descriptions.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 利用训练好的卷积神经网络VGG提取图片特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from pickle import dump\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.layers import Input\n",
    " \n",
    "# extract features from each photo in the directory\n",
    "def extract_features(directory):\n",
    "    # load the model\n",
    "    in_layer = Input(shape=(224, 224, 3))\n",
    "    model = VGG16(include_top=False, input_tensor=in_layer)\n",
    "    print(model.summary())\n",
    "    # extract features from each photo\n",
    "    features = dict()\n",
    "    for name in listdir(directory):\n",
    "        # load an image from file\n",
    "        filename = directory + '/' + name\n",
    "        image = load_img(filename, target_size=(224, 224))\n",
    "        # convert the image pixels to a numpy array\n",
    "        image = img_to_array(image)\n",
    "        # reshape data for the model\n",
    "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "        # prepare the image for the VGG model\n",
    "        image = preprocess_input(image)\n",
    "        # get features\n",
    "        feature = model.predict(image, verbose=0)\n",
    "        # get image id\n",
    "        image_id = name.split('.')[0]\n",
    "        # store feature\n",
    "        features[image_id] = feature\n",
    "        print('>%s' % name)\n",
    "    return features\n",
    " \n",
    "extract features from all images\n",
    "directory = 'Flicker8k_Dataset'\n",
    "features = extract_features(directory)\n",
    "print('Extracted Features: %d' % len(features))\n",
    "save to file\n",
    "dump(features, open('features.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备好训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 1000\n",
      "Train=800, Test=200\n",
      "Descriptions: train=800, test=200\n",
      "Photos: train=800, test=200\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "\n",
    "# load a pre-defined list of photo identifiers\n",
    "def load_set(filename):\n",
    "    doc = load_doc(filename)\n",
    "    dataset = list()\n",
    "    # process line by line\n",
    "    for line in doc.split('\\n'):\n",
    "        # skip empty lines\n",
    "        if len(line) < 1:\n",
    "            continue\n",
    "        # get the image identifier\n",
    "        identifier = line.split('.')[0]\n",
    "        dataset.append(identifier)\n",
    "    return set(dataset)\n",
    " \n",
    "# split a dataset into train/test elements\n",
    "def train_test_split(dataset):\n",
    "    # order keys so the split is consistent\n",
    "    ordered = sorted(dataset)\n",
    "    # return split dataset as two new sets\n",
    "    return set(ordered[:800]), set(ordered[800:1000])\n",
    " \n",
    "# load clean descriptions into memory\n",
    "def load_clean_descriptions(filename, dataset):\n",
    "    # load document\n",
    "    doc = load_doc(filename)\n",
    "    descriptions = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        # split line by white space\n",
    "        tokens = line.split()\n",
    "        # split id from description\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        # skip images not in the set\n",
    "        if image_id in dataset:\n",
    "            # store\n",
    "            descriptions[image_id] = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "    return descriptions\n",
    " \n",
    "# load photo features\n",
    "def load_photo_features(filename, dataset):\n",
    "    # load all features\n",
    "    all_features = load(open(filename, 'rb'))\n",
    "    # filter features\n",
    "    features = {k: all_features[k] for k in dataset}\n",
    "    return features\n",
    " \n",
    "# load dev set\n",
    "filename = 'Flickr8k_text/Flickr_8k.devImages.txt'\n",
    "dataset = load_set(filename)\n",
    "print('Dataset: %d' % len(dataset))\n",
    "# train-test split\n",
    "train, test = train_test_split(dataset)\n",
    "print('Train=%d, Test=%d' % (len(train), len(test)))\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
    "test_descriptions = load_clean_descriptions('descriptions.txt', test)\n",
    "print('Descriptions: train=%d, test=%d' % (len(train_descriptions), len(test_descriptions)))\n",
    "# photo features\n",
    "train_features = load_photo_features('features.pkl', train)\n",
    "test_features = load_photo_features('features.pkl', test)\n",
    "print('Photos: train=%d, test=%d' % (len(train_features), len(test_features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分词器把描述中的单词转换为数字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 4485\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# fit a tokenizer given caption descriptions\n",
    "def create_tokenizer(descriptions):\n",
    "    lines = list(descriptions.values())\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description Length: 25\n",
      "Model: \"model_28\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_57 (InputLayer)           [(None, 7, 7, 512)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_58 (InputLayer)           [(None, 25)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 25088)        0           input_57[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_28 (Embedding)        (None, 25, 50)       18300       input_58[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_112 (Dense)               (None, 128)          3211392     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_59 (LSTM)                  (None, 25, 256)      314368      embedding_28[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_28 (RepeatVector) (None, 25, 128)      0           dense_112[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_28 (TimeDistri (None, 25, 128)      32896       lstm_59[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_28 (Concatenate)    (None, 25, 256)      0           repeat_vector_28[0][0]           \n",
      "                                                                 time_distributed_28[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_60 (LSTM)                  (None, 500)          1514000     concatenate_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_114 (Dense)               (None, 500)          250500      lstm_60[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_115 (Dense)               (None, 366)          183366      dense_114[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 5,524,822\n",
      "Trainable params: 5,524,822\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "50/50 - 4s - loss: 5.4462 - accuracy: 0.0889\n",
      "Epoch 2/50\n",
      "50/50 - 4s - loss: 5.1770 - accuracy: 0.0956\n",
      "Epoch 3/50\n",
      "50/50 - 4s - loss: 5.1133 - accuracy: 0.0985\n",
      "Epoch 4/50\n",
      "50/50 - 4s - loss: 5.0502 - accuracy: 0.0975\n",
      "Epoch 5/50\n",
      "50/50 - 4s - loss: 4.9589 - accuracy: 0.1013\n",
      "Epoch 6/50\n",
      "50/50 - 4s - loss: 4.8836 - accuracy: 0.1013\n",
      "Epoch 7/50\n",
      "50/50 - 4s - loss: 4.8148 - accuracy: 0.0975\n",
      "Epoch 8/50\n",
      "50/50 - 4s - loss: 4.7344 - accuracy: 0.1004\n",
      "Epoch 9/50\n",
      "50/50 - 4s - loss: 4.6628 - accuracy: 0.0975\n",
      "Epoch 10/50\n",
      "50/50 - 4s - loss: 4.4897 - accuracy: 0.1004\n",
      "Epoch 11/50\n",
      "50/50 - 4s - loss: 4.3815 - accuracy: 0.1033\n",
      "Epoch 12/50\n",
      "50/50 - 4s - loss: 4.1968 - accuracy: 0.1061\n",
      "Epoch 13/50\n",
      "50/50 - 4s - loss: 4.0020 - accuracy: 0.1099\n",
      "Epoch 14/50\n",
      "50/50 - 4s - loss: 3.8280 - accuracy: 0.1071\n",
      "Epoch 15/50\n",
      "50/50 - 4s - loss: 3.6700 - accuracy: 0.1071\n",
      "Epoch 16/50\n",
      "50/50 - 4s - loss: 3.5503 - accuracy: 0.1109\n",
      "Epoch 17/50\n",
      "50/50 - 4s - loss: 3.4180 - accuracy: 0.1071\n",
      "Epoch 18/50\n",
      "50/50 - 4s - loss: 3.2632 - accuracy: 0.1109\n",
      "Epoch 19/50\n",
      "50/50 - 4s - loss: 3.2371 - accuracy: 0.1109\n",
      "Epoch 20/50\n",
      "50/50 - 4s - loss: 3.1826 - accuracy: 0.1119\n",
      "Epoch 21/50\n",
      "50/50 - 4s - loss: 3.1555 - accuracy: 0.1128\n",
      "Epoch 22/50\n",
      "50/50 - 4s - loss: 3.1086 - accuracy: 0.1138\n",
      "Epoch 23/50\n",
      "50/50 - 4s - loss: 3.0420 - accuracy: 0.1128\n",
      "Epoch 24/50\n",
      "50/50 - 4s - loss: 3.0113 - accuracy: 0.1071\n",
      "Epoch 25/50\n",
      "50/50 - 4s - loss: 2.9814 - accuracy: 0.1128\n",
      "Epoch 26/50\n",
      "50/50 - 4s - loss: 2.9500 - accuracy: 0.1128\n",
      "Epoch 27/50\n",
      "50/50 - 4s - loss: 2.8809 - accuracy: 0.1099\n",
      "Epoch 28/50\n",
      "50/50 - 4s - loss: 2.9119 - accuracy: 0.1099\n",
      "Epoch 29/50\n",
      "50/50 - 4s - loss: 2.8841 - accuracy: 0.1080\n",
      "Epoch 30/50\n",
      "50/50 - 4s - loss: 2.7879 - accuracy: 0.1195\n",
      "Epoch 31/50\n",
      "50/50 - 4s - loss: 2.7826 - accuracy: 0.1205\n",
      "Epoch 32/50\n",
      "50/50 - 4s - loss: 2.8033 - accuracy: 0.1205\n",
      "Epoch 33/50\n",
      "50/50 - 4s - loss: 2.8395 - accuracy: 0.1252\n",
      "Epoch 34/50\n",
      "50/50 - 4s - loss: 2.8210 - accuracy: 0.1166\n",
      "Epoch 35/50\n",
      "50/50 - 4s - loss: 2.8055 - accuracy: 0.1185\n",
      "Epoch 36/50\n",
      "50/50 - 4s - loss: 2.7661 - accuracy: 0.1195\n",
      "Epoch 37/50\n",
      "50/50 - 4s - loss: 2.7434 - accuracy: 0.1243\n",
      "Epoch 38/50\n",
      "50/50 - 4s - loss: 2.7114 - accuracy: 0.1310\n",
      "Epoch 39/50\n",
      "50/50 - 4s - loss: 2.7382 - accuracy: 0.1185\n",
      "Epoch 40/50\n",
      "50/50 - 4s - loss: 2.7360 - accuracy: 0.1272\n",
      "Epoch 41/50\n",
      "50/50 - 4s - loss: 2.7650 - accuracy: 0.1214\n",
      "Epoch 42/50\n",
      "50/50 - 4s - loss: 2.6911 - accuracy: 0.1300\n",
      "Epoch 43/50\n",
      "50/50 - 4s - loss: 2.6965 - accuracy: 0.1214\n",
      "Epoch 44/50\n",
      "50/50 - 4s - loss: 2.6852 - accuracy: 0.1338\n",
      "Epoch 45/50\n",
      "50/50 - 4s - loss: 2.6611 - accuracy: 0.1338\n",
      "Epoch 46/50\n",
      "50/50 - 4s - loss: 2.6203 - accuracy: 0.1463\n",
      "Epoch 47/50\n",
      "50/50 - 4s - loss: 2.6476 - accuracy: 0.1434\n",
      "Epoch 48/50\n",
      "50/50 - 4s - loss: 2.6498 - accuracy: 0.1300\n",
      "Epoch 49/50\n",
      "50/50 - 4s - loss: 2.6693 - accuracy: 0.1319\n",
      "Epoch 50/50\n",
      "50/50 - 4s - loss: 2.6255 - accuracy: 0.1310\n",
      ">1: train=0.004302 test=0.002610\n",
      "Model: \"model_29\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_59 (InputLayer)           [(None, 7, 7, 512)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_60 (InputLayer)           [(None, 25)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 25088)        0           input_59[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_29 (Embedding)        (None, 25, 50)       18300       input_60[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_116 (Dense)               (None, 128)          3211392     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_61 (LSTM)                  (None, 25, 256)      314368      embedding_29[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_29 (RepeatVector) (None, 25, 128)      0           dense_116[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_29 (TimeDistri (None, 25, 128)      32896       lstm_61[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_29 (Concatenate)    (None, 25, 256)      0           repeat_vector_29[0][0]           \n",
      "                                                                 time_distributed_29[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_62 (LSTM)                  (None, 500)          1514000     concatenate_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_118 (Dense)               (None, 500)          250500      lstm_62[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_119 (Dense)               (None, 366)          183366      dense_118[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 5,524,822\n",
      "Trainable params: 5,524,822\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "50/50 - 4s - loss: 5.4370 - accuracy: 0.0870\n",
      "Epoch 2/50\n",
      "50/50 - 4s - loss: 5.1516 - accuracy: 0.0956\n",
      "Epoch 3/50\n",
      "50/50 - 4s - loss: 5.0527 - accuracy: 0.0975\n",
      "Epoch 4/50\n",
      "50/50 - 4s - loss: 4.9543 - accuracy: 0.0975\n",
      "Epoch 5/50\n",
      "50/50 - 4s - loss: 4.8870 - accuracy: 0.0985\n",
      "Epoch 6/50\n",
      "50/50 - 4s - loss: 4.7902 - accuracy: 0.0994\n",
      "Epoch 7/50\n",
      "50/50 - 4s - loss: 4.7034 - accuracy: 0.1023\n",
      "Epoch 8/50\n",
      "50/50 - 4s - loss: 4.5651 - accuracy: 0.1004\n",
      "Epoch 9/50\n",
      "50/50 - 4s - loss: 4.4313 - accuracy: 0.0994\n",
      "Epoch 10/50\n",
      "50/50 - 4s - loss: 4.2899 - accuracy: 0.0994\n",
      "Epoch 11/50\n",
      "50/50 - 4s - loss: 4.1696 - accuracy: 0.1071\n",
      "Epoch 12/50\n",
      "50/50 - 4s - loss: 3.9509 - accuracy: 0.1071\n",
      "Epoch 13/50\n",
      "50/50 - 4s - loss: 3.7314 - accuracy: 0.1109\n",
      "Epoch 14/50\n",
      "50/50 - 4s - loss: 3.5652 - accuracy: 0.1080\n",
      "Epoch 15/50\n",
      "50/50 - 4s - loss: 3.4851 - accuracy: 0.1052\n",
      "Epoch 16/50\n",
      "50/50 - 4s - loss: 3.3401 - accuracy: 0.1013\n",
      "Epoch 17/50\n",
      "50/50 - 4s - loss: 3.2749 - accuracy: 0.1033\n",
      "Epoch 18/50\n",
      "50/50 - 4s - loss: 3.1910 - accuracy: 0.1061\n",
      "Epoch 19/50\n",
      "50/50 - 4s - loss: 3.0909 - accuracy: 0.1052\n",
      "Epoch 20/50\n",
      "50/50 - 4s - loss: 3.1219 - accuracy: 0.1071\n",
      "Epoch 21/50\n",
      "50/50 - 4s - loss: 3.0847 - accuracy: 0.1119\n",
      "Epoch 22/50\n",
      "50/50 - 4s - loss: 3.0249 - accuracy: 0.1099\n",
      "Epoch 23/50\n",
      "50/50 - 4s - loss: 2.9564 - accuracy: 0.1071\n",
      "Epoch 24/50\n",
      "50/50 - 4s - loss: 2.9543 - accuracy: 0.1147\n",
      "Epoch 25/50\n",
      "50/50 - 4s - loss: 2.9690 - accuracy: 0.1205\n",
      "Epoch 26/50\n",
      "50/50 - 4s - loss: 2.8994 - accuracy: 0.1176\n",
      "Epoch 27/50\n",
      "50/50 - 4s - loss: 2.8204 - accuracy: 0.1166\n",
      "Epoch 28/50\n",
      "50/50 - 4s - loss: 2.8451 - accuracy: 0.1138\n",
      "Epoch 29/50\n",
      "50/50 - 4s - loss: 2.8207 - accuracy: 0.1138\n",
      "Epoch 30/50\n",
      "50/50 - 4s - loss: 2.7825 - accuracy: 0.1166\n",
      "Epoch 31/50\n",
      "50/50 - 4s - loss: 2.7452 - accuracy: 0.1185\n",
      "Epoch 32/50\n",
      "50/50 - 4s - loss: 2.7065 - accuracy: 0.1233\n",
      "Epoch 33/50\n",
      "50/50 - 4s - loss: 2.6777 - accuracy: 0.1319\n",
      "Epoch 34/50\n",
      "50/50 - 4s - loss: 2.7524 - accuracy: 0.1205\n",
      "Epoch 35/50\n",
      "50/50 - 4s - loss: 2.6845 - accuracy: 0.1300\n",
      "Epoch 36/50\n",
      "50/50 - 4s - loss: 2.7560 - accuracy: 0.1233\n",
      "Epoch 37/50\n",
      "50/50 - 4s - loss: 2.7448 - accuracy: 0.1272\n",
      "Epoch 38/50\n",
      "50/50 - 4s - loss: 2.7197 - accuracy: 0.1338\n",
      "Epoch 39/50\n",
      "50/50 - 4s - loss: 2.6806 - accuracy: 0.1367\n",
      "Epoch 40/50\n",
      "50/50 - 4s - loss: 2.6884 - accuracy: 0.1405\n",
      "Epoch 41/50\n",
      "50/50 - 4s - loss: 2.6571 - accuracy: 0.1367\n",
      "Epoch 42/50\n",
      "50/50 - 4s - loss: 2.6673 - accuracy: 0.1281\n",
      "Epoch 43/50\n",
      "50/50 - 4s - loss: 2.7242 - accuracy: 0.1415\n",
      "Epoch 44/50\n",
      "50/50 - 4s - loss: 2.6606 - accuracy: 0.1434\n",
      "Epoch 45/50\n",
      "50/50 - 4s - loss: 2.6226 - accuracy: 0.1472\n",
      "Epoch 46/50\n",
      "50/50 - 4s - loss: 2.6002 - accuracy: 0.1491\n",
      "Epoch 47/50\n",
      "50/50 - 4s - loss: 2.5875 - accuracy: 0.1587\n",
      "Epoch 48/50\n",
      "50/50 - 4s - loss: 2.5573 - accuracy: 0.1750\n",
      "Epoch 49/50\n",
      "50/50 - 4s - loss: 2.5471 - accuracy: 0.1635\n",
      "Epoch 50/50\n",
      "50/50 - 4s - loss: 2.5060 - accuracy: 0.1778\n",
      ">2: train=0.006495 test=0.004101\n",
      "Model: \"model_30\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_61 (InputLayer)           [(None, 7, 7, 512)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_62 (InputLayer)           [(None, 25)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 25088)        0           input_61[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_30 (Embedding)        (None, 25, 50)       18300       input_62[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_120 (Dense)               (None, 128)          3211392     flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_63 (LSTM)                  (None, 25, 256)      314368      embedding_30[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_30 (RepeatVector) (None, 25, 128)      0           dense_120[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_30 (TimeDistri (None, 25, 128)      32896       lstm_63[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_30 (Concatenate)    (None, 25, 256)      0           repeat_vector_30[0][0]           \n",
      "                                                                 time_distributed_30[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_64 (LSTM)                  (None, 500)          1514000     concatenate_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_122 (Dense)               (None, 500)          250500      lstm_64[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_123 (Dense)               (None, 366)          183366      dense_122[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 5,524,822\n",
      "Trainable params: 5,524,822\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "50/50 - 5s - loss: 5.4475 - accuracy: 0.0870\n",
      "Epoch 2/50\n",
      "50/50 - 4s - loss: 5.1333 - accuracy: 0.0975\n",
      "Epoch 3/50\n",
      "50/50 - 4s - loss: 5.0467 - accuracy: 0.0975\n",
      "Epoch 4/50\n",
      "50/50 - 4s - loss: 4.9596 - accuracy: 0.1004\n",
      "Epoch 5/50\n",
      "50/50 - 4s - loss: 4.8779 - accuracy: 0.0975\n",
      "Epoch 6/50\n",
      "50/50 - 4s - loss: 4.7634 - accuracy: 0.0975\n",
      "Epoch 7/50\n",
      "50/50 - 4s - loss: 4.6113 - accuracy: 0.1013\n",
      "Epoch 8/50\n",
      "50/50 - 4s - loss: 4.3955 - accuracy: 0.0966\n",
      "Epoch 9/50\n",
      "50/50 - 4s - loss: 4.1803 - accuracy: 0.1004\n",
      "Epoch 10/50\n",
      "50/50 - 4s - loss: 3.9853 - accuracy: 0.1013\n",
      "Epoch 11/50\n",
      "50/50 - 4s - loss: 3.8005 - accuracy: 0.1042\n",
      "Epoch 12/50\n",
      "50/50 - 4s - loss: 3.6182 - accuracy: 0.1023\n",
      "Epoch 13/50\n",
      "50/50 - 4s - loss: 3.4667 - accuracy: 0.1033\n",
      "Epoch 14/50\n",
      "50/50 - 4s - loss: 3.2899 - accuracy: 0.1023\n",
      "Epoch 15/50\n",
      "50/50 - 4s - loss: 3.1934 - accuracy: 0.1013\n",
      "Epoch 16/50\n",
      "50/50 - 4s - loss: 3.1498 - accuracy: 0.1128\n",
      "Epoch 17/50\n",
      "50/50 - 4s - loss: 3.0720 - accuracy: 0.1080\n",
      "Epoch 18/50\n",
      "50/50 - 4s - loss: 3.0616 - accuracy: 0.1099\n",
      "Epoch 19/50\n",
      "50/50 - 4s - loss: 2.9437 - accuracy: 0.1099\n",
      "Epoch 20/50\n",
      "50/50 - 4s - loss: 2.9115 - accuracy: 0.1099\n",
      "Epoch 21/50\n",
      "50/50 - 4s - loss: 3.0105 - accuracy: 0.1166\n",
      "Epoch 22/50\n",
      "50/50 - 4s - loss: 2.9525 - accuracy: 0.1147\n",
      "Epoch 23/50\n",
      "50/50 - 4s - loss: 2.9384 - accuracy: 0.1109\n",
      "Epoch 24/50\n",
      "50/50 - 4s - loss: 2.9705 - accuracy: 0.1128\n",
      "Epoch 25/50\n",
      "50/50 - 4s - loss: 2.9570 - accuracy: 0.1119\n",
      "Epoch 26/50\n",
      "50/50 - 4s - loss: 2.9522 - accuracy: 0.1166\n",
      "Epoch 27/50\n",
      "50/50 - 4s - loss: 2.9135 - accuracy: 0.1109\n",
      "Epoch 28/50\n",
      "50/50 - 4s - loss: 2.8989 - accuracy: 0.1272\n",
      "Epoch 29/50\n",
      "50/50 - 4s - loss: 2.8414 - accuracy: 0.1233\n",
      "Epoch 30/50\n",
      "50/50 - 4s - loss: 2.8391 - accuracy: 0.1099\n",
      "Epoch 31/50\n",
      "50/50 - 4s - loss: 2.8083 - accuracy: 0.1281\n",
      "Epoch 32/50\n",
      "50/50 - 4s - loss: 2.7970 - accuracy: 0.1205\n",
      "Epoch 33/50\n",
      "50/50 - 4s - loss: 2.7296 - accuracy: 0.1243\n",
      "Epoch 34/50\n",
      "50/50 - 4s - loss: 2.7224 - accuracy: 0.1224\n",
      "Epoch 35/50\n",
      "50/50 - 4s - loss: 2.7849 - accuracy: 0.1281\n",
      "Epoch 36/50\n",
      "50/50 - 4s - loss: 2.7705 - accuracy: 0.1262\n",
      "Epoch 37/50\n",
      "50/50 - 4s - loss: 2.7791 - accuracy: 0.1281\n",
      "Epoch 38/50\n",
      "50/50 - 4s - loss: 2.7404 - accuracy: 0.1367\n",
      "Epoch 39/50\n",
      "50/50 - 4s - loss: 2.7343 - accuracy: 0.1243\n",
      "Epoch 40/50\n",
      "50/50 - 4s - loss: 2.6961 - accuracy: 0.1405\n",
      "Epoch 41/50\n",
      "50/50 - 4s - loss: 2.6311 - accuracy: 0.1367\n",
      "Epoch 42/50\n",
      "50/50 - 4s - loss: 2.6121 - accuracy: 0.1386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50\n",
      "50/50 - 4s - loss: 2.6451 - accuracy: 0.1472\n",
      "Epoch 44/50\n",
      "50/50 - 4s - loss: 2.6654 - accuracy: 0.1453\n",
      "Epoch 45/50\n",
      "50/50 - 4s - loss: 2.6311 - accuracy: 0.1472\n",
      "Epoch 46/50\n",
      "50/50 - 4s - loss: 2.6433 - accuracy: 0.1453\n",
      "Epoch 47/50\n",
      "50/50 - 4s - loss: 2.6332 - accuracy: 0.1358\n",
      "Epoch 48/50\n",
      "50/50 - 4s - loss: 2.6239 - accuracy: 0.1453\n",
      "Epoch 49/50\n",
      "50/50 - 4s - loss: 2.6179 - accuracy: 0.1520\n",
      "Epoch 50/50\n",
      "50/50 - 4s - loss: 2.6254 - accuracy: 0.1597\n",
      ">3: train=0.004642 test=0.004212\n",
      "          train      test\n",
      "count  3.000000  3.000000\n",
      "mean   0.005146  0.003641\n",
      "std    0.001180  0.000895\n",
      "min    0.004302  0.002610\n",
      "25%    0.004472  0.003356\n",
      "50%    0.004642  0.004101\n",
      "75%    0.005569  0.004157\n",
      "max    0.006495  0.004212\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from pandas import DataFrame\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from pickle import load\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.pooling import GlobalMaxPooling2D,GlobalAveragePooling2D\n",
    "\n",
    "# create sequences of images, input sequences and output words for an image\n",
    "def create_sequences(tokenizer, desc, image, max_length):\n",
    "    Ximages, XSeq, y = list(), list(),list()\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    # integer encode the description\n",
    "    seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "    # split one sequence into multiple X,y pairs\n",
    "    for i in range(1, len(seq)):\n",
    "        # select\n",
    "        in_seq, out_seq = seq[:i], seq[i]\n",
    "        # pad input sequence\n",
    "        in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "        # encode output sequence\n",
    "        out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "        # store\n",
    "        Ximages.append(image)\n",
    "        XSeq.append(in_seq)\n",
    "        y.append(out_seq)\n",
    "    # Ximages, XSeq, y = array(Ximages), array(XSeq), array(y)\n",
    "    return [Ximages, XSeq, y]\n",
    "\n",
    "# define the captioning model\n",
    "def define_model(vocab_size, max_length):\n",
    "    # feature extractor (encoder)\n",
    "    inputs1 = Input(shape=(7, 7, 512))\n",
    "    fe1 = GlobalMaxPooling2D()(inputs1)\n",
    "    fe2 = Dense(128, activation='relu')(fe1)\n",
    "    fe3 = RepeatVector(max_length)(fe2)\n",
    "    # embedding\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    emb2 = Embedding(vocab_size, 50, mask_zero=True)(inputs2)\n",
    "    emb3 = LSTM(256, return_sequences=True)(emb2)\n",
    "    emb3 = LSTM(256, return_sequences=True)(emb3)\n",
    "    emb4 = TimeDistributed(Dense(128, activation='relu'))(emb3)\n",
    "    # merge inputs\n",
    "    merged = concatenate([fe3, emb4])\n",
    "    # language model (decoder)\n",
    "    lm2 = LSTM(256)(merged)\n",
    "    lm3 = Dense(256, activation='relu')(lm2)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(lm3)\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    plot_model(model, show_shapes=True, to_file='plot.png')\n",
    "    return model\n",
    "\n",
    "# data generator, intended to be used in a call to model.fit_generator()\n",
    "def data_generator(descriptions, features, tokenizer, max_length, n_step):\n",
    "    # loop until we finish training\n",
    "    while 1:\n",
    "        # loop over photo identifiers in the dataset\n",
    "        keys = list(descriptions.keys())\n",
    "        for i in range(0, len(keys), n_step):\n",
    "            Ximages, XSeq, y = list(), list(),list()\n",
    "            for j in range(i, min(len(keys), i+n_step)):\n",
    "                image_id = keys[j]\n",
    "                # retrieve photo feature input\n",
    "                image = features[image_id][0]\n",
    "                # retrieve text input\n",
    "                desc = descriptions[image_id]\n",
    "                # generate input-output pairs\n",
    "                in_img, in_seq, out_word = create_sequences(tokenizer, desc, image, max_length)\n",
    "                for k in range(len(in_img)):\n",
    "                    Ximages.append(in_img[k])\n",
    "                    XSeq.append(in_seq[k])\n",
    "                    y.append(out_word[k])\n",
    "            # yield this batch of samples to the model\n",
    "            yield ([array(Ximages), array(XSeq)], array(y))\n",
    "\n",
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "# generate a description for an image\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "    # seed the generation process\n",
    "    in_text = 'startseq'\n",
    "    # iterate over the whole length of the sequence\n",
    "    for i in range(max_length):\n",
    "        # integer encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # pad input\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        # predict next word\n",
    "        yhat = model.predict([photo,sequence], verbose=0)\n",
    "        # convert probability to integer\n",
    "        yhat = argmax(yhat)\n",
    "        # map integer to word\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        # stop if we cannot map the word\n",
    "        if word is None:\n",
    "            break\n",
    "        # append as input for generating the next word\n",
    "        in_text += ' ' + word\n",
    "        # stop if we predict the end of the sequence\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text\n",
    "\n",
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
    "    actual, predicted = list(), list()\n",
    "    # step over the whole set\n",
    "    for key, desc in descriptions.items():\n",
    "        # generate description\n",
    "        yhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
    "        # store actual and predicted\n",
    "        actual.append([desc.split()])\n",
    "        predicted.append(yhat.split())\n",
    "    \n",
    "    # calculate BLEU score\n",
    "    bleu = corpus_bleu(actual, predicted, smoothing_function=SmoothingFunction().method1)\n",
    "    return bleu\n",
    "\n",
    "# determine the maximum sequence length\n",
    "max_length = max(len(s.split()) for s in list(train_descriptions.values()))\n",
    "print('Description Length: %d' % max_length)\n",
    "\n",
    "# define experiment\n",
    "model_name = 'baseline1'\n",
    "verbose = 2\n",
    "n_epochs = 50\n",
    "n_photos_per_update = 2\n",
    "n_batches_per_epoch = int(len(train) / n_photos_per_update)\n",
    "n_repeats = 3\n",
    "\n",
    "# run experiment\n",
    "train_results, test_results = list(), list()\n",
    "for i in range(n_repeats):\n",
    "    # define the model\n",
    "    model = define_model(vocab_size, max_length)\n",
    "    # fit model\n",
    "    model.fit_generator(data_generator(train_descriptions, train_features, tokenizer, max_length, n_photos_per_update), steps_per_epoch=n_batches_per_epoch, epochs=n_epochs, verbose=verbose)\n",
    "    \n",
    "    \n",
    "    # evaluate model on training data\n",
    "    train_score = evaluate_model(model, train_descriptions, train_features, tokenizer, max_length)\n",
    "    test_score = evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)\n",
    "    # store\n",
    "    train_results.append(train_score)\n",
    "    test_results.append(test_score)\n",
    "    print('>%d: train=%f test=%f' % ((i+1), train_score, test_score))\n",
    "# save results to file\n",
    "df = DataFrame()\n",
    "df['train'] = train_results\n",
    "df['test'] = test_results\n",
    "print(df.describe())\n",
    "df.to_csv(model_name+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual:    startseq child and woman are at waters edge in big city endseq\n",
      "Predicted: startseq child and woman are at waters edge in city endseq\n",
      "Actual:    startseq boy with stick kneeling in front of goalie net endseq\n",
      "Predicted: startseq boy with stick kneeling in of goalie goalie net endseq\n",
      "Actual:    startseq woman crouches near three dogs in field endseq\n",
      "Predicted: startseq woman crouches near three dogs field endseq\n",
      "Actual:    startseq boy bites hard into treat while he sits outside endseq\n",
      "Predicted: startseq boy bites hard into treat while sits outside endseq\n",
      "Actual:    startseq person eats takeout while watching small television endseq\n",
      "Predicted: startseq person eats takeout while watching small television endseq\n",
      "Actual:    startseq couple with young child wrapped in blanket sitting on concrete step endseq\n",
      "Predicted: startseq man of people while while in in traffic endseq\n",
      "Actual:    startseq adults and children stand and play in front of steps near wooded area endseq\n",
      "Predicted: startseq boy and toddler at stand by walk park endseq\n",
      "Actual:    startseq boy in grey pajamas is jumping on the couch endseq\n",
      "Predicted: startseq boy dog with formally with with and and and and endseq\n",
      "Actual:    startseq boy holding kitchen utensils and making threatening face endseq\n",
      "Predicted: startseq baby children dressed formally formally for with with and and and and and and standing in smiling endseq\n",
      "Actual:    startseq man in green hat is someplace up high endseq\n",
      "Predicted: startseq girl in people walking up the the water endseq\n"
     ]
    }
   ],
   "source": [
    "model_name = 'baseline_generate'\n",
    "\n",
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
    "    actual, predicted = list(), list()\n",
    "    # step over the whole set\n",
    "    for key, desc in descriptions.items():\n",
    "        # generate description\n",
    "        yhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
    "        # store actual and predicted\n",
    "        actual.append([desc.split()])\n",
    "        predicted.append(yhat.split())\n",
    "        print('Actual:    %s' % desc)\n",
    "        print('Predicted: %s' % yhat)\n",
    "        if len(actual) >= 5:\n",
    "            break\n",
    "    # calculate BLEU score\n",
    "    bleu = corpus_bleu(actual, predicted)\n",
    "    return bleu\n",
    "\n",
    "# evaluate model on training data\n",
    "train_score = evaluate_model(model, train_descriptions, train_features, tokenizer, max_length)\n",
    "test_score = evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
