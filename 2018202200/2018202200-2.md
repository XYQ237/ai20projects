11.20第二次汇报

刘睿衡 2018202200

1.总概

这三周我们主要围绕openCV对小车做进一步的研究。在本段时间中，半期考试以及复习占据了主要任务，在复习之余我们接住前一次对openCV初步的探索寻迹，利用其框架识别更为复杂灵活的图像，以实现更为智能化的动作。

2.二维码识别

在上一次报告中，我们初步实现了黑色胶带路径识别。更进一步的，我们基于块色区识别，实现同样是黑白表意的二维码的识别。

不同的是，我们寻迹采用的是直接的色块划分，寻找中心点以矫正路径偏差实现导航。而对于二维码，其本身色块分布并不具有实际意义，二维码识别主要是识别二维码表达的意思，并根据其表意实现相应动作的操控。从这个意义上说，二维码或许比黑色胶带路径更为简单。

    def t_up(speed,t_time):
        content='机器人前进'
        robot_speech(content)
        L_Motor.ChangeDutyCycle(speed)
        GPIO.output(AIN2,False)#AIN2
        GPIO.output(AIN1,True) #AIN1
        R_Motor.ChangeDutyCycle(speed)
        GPIO.output(BIN2,False)#BIN2
        GPIO.output(BIN1,True) #BIN1
        time.sleep(t_time)
车辆运动函数还是没有变化，这里识别文档content，content=“前进”则小车做出前进动作的响应。可以看到，由于只涉及前进后退转向等为数不多的命令，这里直接用人工一一对应的方式写死了。当然，如果命令更多样化，必然会用到自然语言识别以理解二维码背后的content的意思————如果有机会有时间的话之后会加入NPL以实现更为灵活的二维码语言识别。这一点在我们实现语音识别之后可能会迎刃而解。

    if scanner_Flag == 1:
        for symbol in zarimage:
            if not symbol.count:
                print 'decoded', symbol.type, 'symbol', '"%s"' % symbol.data
                symbolPos = symbol.location
                draw_rect(img, symbolPos, (0,255,0), 3)
                qr_data = str(symbol.data)
            else:
                qr_data = 'no qrcode'
当然，怎么识别二维码我们就不得而知，二维码的生成也很简单，随便上网上找一个网站就能够制作。更为具体的可以参考我们的参考代码。

3.人脸识别以及人脸追踪

我们采用openCV的最主要目的便是为了实现人脸识别以及通过人脸实现导航。人脸识别原理与黑胶寻迹比较类似，摄像头会捕捉图像，小车识别图像中人脸的位置并圈出，找出中心点，计算中心点与摄像图像中心的偏差，以确定转向或直行。同时，根据标准屏幕大小与人脸识别出的大小比较，判断相对距离，调整转向角度。

我们的人脸识别训练数据来自opencv的官网。

    <opencv_storage>
    <cascade type_id="opencv-cascade-classifier">
    <stageType>BOOST</stageType>
    <featureType>LBP</featureType>
    <height>24</height>
    <width>24</width>
    <stageParams>
    <boostType>GAB</boostType>
    <minHitRate>0.9950000047683716</minHitRate>
    <maxFalseAlarm>0.5000000000000000</maxFalseAlarm>
    <weightTrimRate>0.9500000000000000</weightTrimRate>
    <maxDepth>1</maxDepth>
    <maxWeakCount>100</maxWeakCount>
    </stageParams>
    <featureParams>
    <maxCatCount>256</maxCatCount>
    </featureParams>
    <stageNum>20</stageNum>
在简易数据集中，有3000个样例作为学习用。此外我们准备了完整数据集，其xml标签文档就有1M大小，图像包含更多，识别精度更好。

我们选取openCV中特征脸识别的方法，将PCA运用到人脸图像中，将训练集中的人脸提取出矩阵，平均化得到标准人脸矩阵；然后根据摄像头传回的图像找图像中符合与标准人脸相近的区块加以识别。

更为精确的，我们引入人眼识别来增加准确度：

    face_cascade = cv2.CascadeClassifier('face.xml')
    eye_cascade = cv2.CascadeClassifier('eye.xml')
我们的计划是完备的，但是在实施阶段还是遇到了不少的问题。最大的问题在上一次报告中也提及，我们发现视频流传回有延迟，而且延迟还不小；同时，由于摄像头很便宜，像素也不高，质量不咋地，树莓派处理器也没法与骁龙或者A系列处理器相比较，传回图像很卡，这也给图像处理带来了不小的麻烦。通常我们在控制台看到小车在某一位置识别到人脸，并控制前进后图像会卡一小会，此时视频流图像很模糊，但OpenCV程序并不理解，还是当成正常图像进行识别，这会导致很多错误。

在实验中，通常最开始小车能够正常识别人脸；但在前进一段时间后，受到传输影响会失去目标一段时间，通常它会停下来等图像稳定，这样小车运动就是断断续续的；更为糟糕的情况是如果在图像不稳定期间错误识别出人脸的话，它就会朝着错误的方向前进。针对这样的情况我们也没有什么比较好的办法，毕竟是设备问题。

    if x_p > x_Lower and x_p < x_Upper:
            t_up(50,20)
        elif x_p < x_Lower:
            t_left(50,20)
        elif x_p > x_Upper:
            t_right(50,20)
我们在运动函数中人为增加一个等待时间20，确保图像稳定，至少不要出现错误。

4.球识别

单纯的识别静态的球并不难。但是我们要求能够实现球的动态追踪。遗憾的是，我们经过实验，摄像头捕捉到的运动图像实在过于模糊，根本无法进行识别。这也证明我们在项目选择时的英明之处：摆在地上的小车尚且在运动识别上表现模糊，更不要说本身就在空中摇来晃去的无人机了。

5.接下来……

我们下一步将进行语音控制的研究，借助百度语音实现联网在线语音识别，并导入百度开发者智能库实现智能操控。希望语音识别能够克服摄像头图像不稳定的难题，实现实时而流畅的操控。